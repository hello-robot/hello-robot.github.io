{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains documentation exclusively for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license (the \"License\"); you may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>https://creativecommons.org/licenses/by-nd/4.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Patents pending and trademark rights cover the Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\"</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"developing/basics/","title":"The Basics","text":"<p>This tutorial introduces the basics of developing with Stretch. It'll cover the concepts and development tools you'll come across, including:</p> <ol> <li>the robot's operating system</li> <li>where important files live</li> <li>using the terminal</li> </ol>"},{"location":"developing/basics/#operating-system","title":"Operating System","text":"<p>Stretch's onboard computer is running a desktop Linux operating system called Ubuntu. When you power up Stretch, you'll see the desktop. You'll find Ubuntu's desktop has the same elements as Windows or Mac OS desktops.</p> <p></p>"},{"location":"developing/basics/#connecting-to-wifi","title":"Connecting to Wifi","text":"<p>The system tray shows whether Stretch is connected to the internet.</p> <ul> <li>Connected via Wifi: </li> <li>Connected via Ethernet: </li> </ul> <p>To connect via Wifi to a new network, first open Wifi settings.</p> <p></p> <p>Click on your Wifi network and fill out the connection form.</p> <p></p>"},{"location":"developing/basics/#desktop-applications","title":"Desktop Applications","text":"<p>Since Ubuntu is a desktop operating system, you can download any applications you need (e.g. Slack, Discord). Ubuntu has an app store called \"Ubuntu Software\" that makes the install easy. You can also find software online.</p> <p></p> <p>Some of the preinstalled applications:</p> <ul> <li>Web Browser: Firefox</li> <li>Code Editors: Visual Studio Code, PyCharm, </li> <li>Media Player: VLC</li> <li>File Explorer: Files</li> </ul>"},{"location":"developing/basics/#user-accounts","title":"User Accounts","text":"<p>The default user account is called Hello Robot with the username <code>hello-robot</code>. This is also the admin account.</p> <p></p>"},{"location":"developing/basics/#changing-your-credentials","title":"Changing your Credentials","text":"<p>The password for the Hello Robot account is printed on the welcome sheet included in the box with Stretch. If you're having trouble finding it, reach out to support. We recommend changing the password for this account.</p> <p></p>"},{"location":"developing/basics/#adding-new-users","title":"Adding New Users","text":"<p>If you're sharing Stretch with other developers, it can be helpful to create separate accounts for yourself and your team members. Check out the Adding a New User guide.</p>"},{"location":"developing/basics/#where-files-live","title":"Where Files Live","text":"<p>In this section, we'll cover where to find the files that are important for developing code.</p> <p></p>"},{"location":"developing/basics/#the-home-folder","title":"The Home Folder","text":"<p>Your account's files all live under the home folder. This directory contains standard organizational folders like <code>Documents</code> or <code>Downloads</code>, as well as folders that are unique to Stretch's SDK, such as:</p> <ul> <li><code>stretch_user</code>: The Stretch User folder is required to be present in the home directory for Stretch's SDK to work correctly. It contains calibration data unique to your robot and assets (e.g. neural net checkpoints, maps for navigation, etc.) used by various programs. If you're missing this directory, reach out to support.</li> <li><code>ament_ws</code>: The Ament Workspace folder contains Stretch's ROS2 SDK. This directory will be covered in more detail in the Using ROS 2 with Stretch tutorial. If this folder is missing, you may be using an older robot distribution and should consider upgrading.</li> <li><code>catkin_ws</code>: The Catkin Workspace folder contains Stretch's ROS1 SDK. If this folder exists, you may be using an older robot distribution and should consider upgrading.</li> </ul> <p></p>"},{"location":"developing/basics/#the-stretch-user-folder","title":"The Stretch User Folder","text":"<p>The Stretch User folder contains calibration data unique to your robot and assets (e.g. neural net checkpoints, maps for navigation, etc.) used by various programs in Stretch's SDK. The directories in this folder are:</p> <ul> <li><code>stretch-yyy-xxxx</code>: This is the robot calibration directory. Its naming scheme is \"stretch-\" following by \"re1\" for a Stretch RE1, \"re2\" for a Stretch 2, or \"se3\" for a Stretch 3, finally followed by your robot's four digit serial number.</li> <li><code>stretch_deep_perception_models</code>: This directory contains open deep learning models from third parties, including object detection, head detection, facial landmarks detection, and human pose detection.</li> <li><code>maps</code>: This directory contains occupancy maps that are to be used with the Stretch Nav2 ROS2 package.</li> <li><code>debug</code>: This directory contains debug output from the Stretch FUNMAP ROS2 package.</li> <li><code>log</code>: This directory contains logfiles from various Stretch programs.</li> </ul>"},{"location":"developing/basics/#the-robot-calibration-folder","title":"The Robot Calibration Folder","text":"<p>The naming scheme for the robot calibration directory is <code>stretch-yyy-xxxx</code>, where \"stretch-\" is followed by \"re1\" for a Stretch RE1, \"re2\" for a Stretch 2, or \"se3\" for a Stretch 3, and finally followed by your robot's four digit serial number. For example, a Stretch 3 with serial number 3001 would be called <code>stretch-se3-3001</code>. The important directories in this folder are:</p> <ul> <li><code>calibration_ros</code>: This folder contains calibrated URDFs. The concepts of URDFs and calibrations will be covered in more detail in the URDF Calibration section below.</li> <li><code>exported_urdf</code>: This folder contains a single calibrated standalone URDF of your Stretch. It will be covered in more detail in the URDF Calibration section below.</li> <li><code>stretch_user_params.yaml</code>: This YAML file is for you to edit when you want to change a robot parameter. The Advanced Features tutorial later in this series explains what robot parameters are and why you might want to change them.</li> <li><code>stretch_configuration_params.yaml</code>: This YAML file contains robot parameters unique to your robot. You don't want to edit this file.</li> <li><code>udev</code>: This folder contains a backup of the udev rules that enable Stretch's USB system to function correctly.</li> </ul>"},{"location":"developing/basics/#the-etchello-robot-folder","title":"The /etc/hello-robot Folder","text":"<p>This is a system level folder, available to all users on the robot. It contains:</p> <ul> <li><code>stretch-yyy-xxxx</code>: A system-wide backup of the robot calibration directory. Often, this is the oldest version since the Stretch User copy in each user account is updated by the SDK.</li> </ul>"},{"location":"developing/basics/#terminal","title":"Terminal","text":"<p>The terminal is an essential tool to developing on Stretch and on Linux systems in general. We'll become familiar with it in this section.</p> <p></p>"},{"location":"developing/basics/#terminology","title":"Terminology","text":"<p>What exactly is a \"terminal\"? How does it relate to a \"command prompt\" or a \"shell\"? Let's clarify some terminology:</p> <ul> <li>Terminal Emulator or \"terminal\", as it's often shortened to, is an application with a simple user interface: you type commands into the terminal, press enter to execute, and it prints out the results. On Windows, this application is called Command Prompt, and on MacOS and Ubuntu, it's called the Terminal.<ul> <li></li> </ul> </li> <li>Shell is the program that actually interprets your commands, performs some action with them, and returns the output to the terminal. The shell on your Stretch is called Bash. You may see the term \"shell script\", which refers to \".sh\" or \".bash\" files containing a series of commands written for Bash. Each shell has unique commands it accepts, so for example, commands for Bash won't work on a Windows shell. Fortunately, Bash is ubiquitous in computing, and learning its commands will be useful.</li> <li>Command Line refers to the line on which you type the commands. It looks like this: . You'll notice the line contains some information before the blinking cursor. The format is usually <code>user@hostname:current_path$ type_command_here</code>. Here, the user is hello-robot, the hostname is stretch-re2-2002 (hostnames for Stretch robots follows the <code>stretch-yyy-xxxx</code> format described above), the current path/folder is <code>~</code> (the tilde represents the home directory), then the dollar sign, and finally, the cursor where you can type your commands. On this docs site, instructions will often provide commands for you to run in the terminal and you will see that they often have <code>$</code> before them. The dollar sign is signifying that what comes after is meant to be run on the command line (<code>$</code> is not actually part of the command).<ul> <li>For example, the command below is proceeded by a dollar sign, signifying <code>pwd</code> is a command line command.   <pre><code>pwd\n</code></pre></li> </ul> </li> <li>CLI stands for \"command line interface\" and refers to the commands that a program makes available for you to use from the command line. The Stretch SDK comes with a CLI and the Command Line Tools tutorial explores this CLI in more detail.</li> <li>Git is a program for keeping track of code changes and the git CLI is a popular method of collaboratively developing code.</li> <li>SSH is a program that allows you to access the shell of another machine. You can \"secure shell\" or \"ssh into\" another machine using the SSH CLI. SSH is a popular method of running commands on Stretch while having no wires connected to the robot.</li> </ul>"},{"location":"developing/basics/#copyingpasting-to-the-terminal","title":"Copying/Pasting to the Terminal","text":"<p>TODO - Ctrl C vs Ctrl Shift C</p>"},{"location":"developing/basics/#the-change-directory-command","title":"The Change Directory Command","text":"<p>You can use the terminal to navigate the file system. When you open a terminal, you'll be in the home directory (often denoted by <code>~</code>). <code>cd</code> allows you to change directory to another directory. For example, you can go into the Stretch User directory using:</p> <pre><code>cd stretch_user/\n</code></pre> <p>After typing or copying this command into the terminal, press Enter to execute the command.</p> <p></p> <p>Your location among the directories is called a \"file path\" (e.g. the command above places you in the <code>~/stretch_user</code> filepath). You can return to the home directory at anytime by typing <code>cd</code> without any following arguments.</p>"},{"location":"developing/basics/#tab-completion","title":"Tab Completion","text":"<p>TODO</p> <pre><code>cd stre\n</code></pre>"},{"location":"developing/basics/#commands","title":"Commands","text":"Command Arguments Description Examples pwd none Print Working Directory - Prints out your current filepath <code>pwd</code> ls any number of optional filepaths List - Used to display a list of all files and folders in the current directory <code>ls</code> mkdir a single filepath Make New Directory - Used to create folders <code>mkdir temporary_folder</code> rmdir a single filepath Remove Directory - Used to delete folders <code>rmdir temporary_folder</code> touch a single filepath File Creation - Used to create empty files <code>touch temporary_file</code> gedit a single filepath File Editor - Brings up a text editor <code>gedit temporary_file</code> cat a single filepath Concatenate - Prints out the contents of a file <code>cat temporary_file</code> mv two filepaths Move - Used to move files between directories and/or rename files <code>mv temporary_file ~/stretch_user</code> rm a single filepath Remove - Used to delete files <code>rm ~/stretch_user/temporary_file</code> <p>TODO - Link to file system video here</p>"},{"location":"developing/basics/#flags","title":"Flags","text":"<p>TODO - CLI flags</p>"},{"location":"developing/basics/#apt-package-manager","title":"APT Package Manager","text":"<p>APT is a system package manager used to install applications. It can install packages that have no GUI. For example, all ROS2 software is installed through APT. The APT CLI enables you to introspect and install new software onto Stretch. Since APT makes changes at the system level, it requires \"super user access\", which means each of the commands below is preceded by <code>sudo</code> and will ask for your password.</p> Command Arguments Description Examples apt list none List - Displays all of the packages installed <code>sudo apt list</code> apt update none Update - Refreshes APT's indices of packages available online <code>sudo apt update</code> apt upgrade none Upgrade - Updates all APT packages to their latest versions <code>sudo apt upgrade</code> apt install any number of package names Install - Used to install new packages <code>sudo apt install sl</code> apt remove a single package name Uninstall - Used to remove packages <code>sudo apt remove sl</code> <p>Note</p> <p>You may not be able to run APT immediately after a reboot as the OS may be running automatic updates in the background. You might see <code>Could not get lock /var/lib/dpkg/lock-frontend</code> when trying to use the APT CLI. Typically, waiting 10-20 minutes will allow you to use APT again.</p>"},{"location":"developing/basics/#git-github","title":"Git &amp; GitHub","text":"<p>Git is a open source tool used for version control of source code. The Git CLI is a common way of interacting with Git repositories of code. Independently, GitHub is a popular website for publishing your Git repositories, and GitHub's features facilitate collaborative development online. At Hello Robot, we use both of these tools in our development flow (see the Contributing guide for info on our Github repos). Github has published a great quick start tutorial covering Git's CLI and GitHub's features, so it's recommended that you follow that guide to learn more about these tools.</p>"},{"location":"developing/basics/#environment-variables","title":"Environment Variables","text":"<p>Bash environment variables are configurable variables that programs can read in to configure their behavior. Some variables are standard across Linux. For example, the <code>HOME</code> variable will point to your account's home folder. You can print out the value of the <code>HOME</code> variable using the <code>echo</code> command. When evaluating an environment variable in a command, prepend a dollar sign to the variable.</p> <pre><code>echo $HOME\n</code></pre> <p>There are also custom environment variables. For example, Stretch robots are preconfigured with two custom variables that enable the Stretch SDK to function. They are <code>HELLO_FLEET_PATH</code> and <code>HELLO_FLEET_ID</code>. <code>HELLO_FLEET_PATH</code> points to the robot's Stretch User directory, and <code>HELLO_FLEET_ID</code> is the robot's <code>stretch-yyy-xxxx</code> formatted name.</p> <p></p> <p>Bash environment variables can be configured to persist between terminal sessions by defining them in a hidden configuration file located at <code>~/.bashrc</code>. Try opening this file with <code>gedit ~/.bashrc</code> and scrolling to the bottom to see Stretch's BashRC setup.</p>"},{"location":"developing/basics/#file-descriptors","title":"File Descriptors","text":"<p>TODO - Link to file descriptors video here</p>"},{"location":"developing/basics/#piping","title":"Piping","text":"<p>TODO - Link to piping video here</p>"},{"location":"developing/cli/","title":"Command Line Tools","text":"<p>This tutorial covers the command line tools included in the Stretch CLI. For a primer on using CLIs and the terminal, see The Basics tutorial. The Stretch CLI is split among Stretch's Python packages. They are:</p> <ul> <li>Stretch Body - Tools that perform common tasks that are useful when developing with Stretch.</li> <li>Stretch PyFUNMAP - Tools that explore the capabilities of PyFUNMAP.</li> <li>Stretch Diagnostics - Tools for diagnosing issues with your robot.</li> <li>Stretch URDF - Tools for working with Stretch's URDFs.</li> <li>Stretch Factory - Tools used at Hello Robot during the robot's bring-up.</li> </ul>"},{"location":"developing/cli/#stretch-body-tools","title":"Stretch Body Tools","text":"<p>These tools perform common tasks that are useful when working with Stretch (e.g. homing routine, joint jogging, etc.).</p>"},{"location":"developing/cli/#stretch_robot_system_checkpy","title":"<code>stretch_robot_system_check.py</code>","text":"<p>Link to source code</p> <p>The robot system check tool runs a series of tests, such as if each subsystem is online, and reports pass or fail. If everything passes in the report, the robot is ready to be used. You would use this tool everytime you use the robot. A passing output should look like:</p> <pre><code>stretch_robot_system_check.py \nFor use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\n---- Checking Devices ----\n[Pass] : hello-wacc\n[Pass] : hello-motor-left-wheel\n[Pass] : hello-pimu\n[Pass] : hello-lrf\n[Pass] : hello-dynamixel-head\n[Pass] : hello-dynamixel-wrist\n[Pass] : hello-motor-arm\n[Pass] : hello-motor-right-wheel\n[Pass] : hello-motor-lift\n[Pass] : hello-respeaker\n\n---- Checking Pimu ----\n[Pass] Voltage = 12.863744497299194\n[Pass] Current = 2.67002009121435\n[Pass] Temperature = 23.45359845039172\n[Pass] Cliff-0 = 16.7066650390625\n[Pass] Cliff-1 = 0.47015380859375\n[Pass] Cliff-2 = -15.3138427734375\n[Pass] Cliff-3 = -5.50537109375\n[Pass] IMU AZ = -9.81534139672\n[Pass] IMU Pitch = 0.0\n[Pass] IMU Roll = 0.0\n\n---- Checking EndOfArm ----\n[Pass] Ping of: wrist_pitch\n[Pass] Ping of: wrist_roll\n[Pass] Ping of: wrist_yaw\n[Pass] Homed: wrist_yaw\n[Pass] Ping of: stretch_gripper\n[Pass] Homed: stretch_gripper\n\n\n---- Checking Head ----\n[Pass] Ping of: head_pan\n[Pass] Ping of: head_tilt\n\n---- Checking Wacc ----\n[Pass] AX = 9.628660202026367\n\n---- Checking hello-motor-left-wheel ----\n[Pass] Position = -3.035825729370117\n\n---- Checking hello-motor-right-wheel ----\n[Pass] Position = 2.784149408340454\n\n---- Checking hello-motor-arm ----\n[Pass] Position = 0.013962420634925365\n[Pass] Position Homed = True\n\n---- Checking hello-motor-lift ----\n[Pass] Position = 31.580163955688477\n[Pass] Position Homed = True\n\n---- Checking for Intel D435i ----\nBus 002 Device 002: ID 8086:0b3a Intel Corp. Intel(R) RealSense(TM) Depth Camera 435i\n[Pass] : Device found\n\n---- Checking Software ----\n[Pass] Ubuntu 22.04 is ready\n[Pass] All APT pkgs are setup correctly\n[Pass] Firmware is up-to-date\n         hello-pimu = v0.6.2p4\n         hello-wacc = v0.5.1p3\n         hello-motor-arm = v0.6.2p4\n         hello-motor-lift = v0.6.3p4\n         hello-motor-left-wheel = v0.6.2p4\n         hello-motor-right-wheel = v0.6.2p4\n[Pass] Python pkgs are up-to-date\n         hello-robot-stretch-body = 0.6.8\n         hello-robot-stretch-body-tools = .6.3\n         hello-robot-stretch-tool-share = 0.2.8\n         hello-robot-stretch-factory = 0.4.13\n         hello-robot-stretch-diagnostics = 0.0.14\n         hello-robot-stretch-urdf = 0.0.18\n[Pass] ROS2 Humble is ready\n         Workspace at ~/ament_ws/src/stretch_ros2\n</code></pre> <p>In addition to checking the robot's hardware, this tool also prints out a software report. This includes which version of Stretch's Python, ROS2, etc. software your system has installed. Check out the Keeping your Software Up-to-date guide for a how-to on using this report to keep your robot's software up-to-date.</p>"},{"location":"developing/cli/#stretch_robot_homepy","title":"<code>stretch_robot_home.py</code>","text":"<p>Link to source code</p> <p>This tool will start Stretch's homing procedure, where every joint's zero is found. Robots with relative encoders (vs absolute encoders) need a homing procedure when they power on. For Stretch, it's a 30-second procedure that must occur everytime the robot is powered on before you may send motion commands to or read correct joint positions from Stretch's joints. Normal output from this tool looks like:</p> <pre><code>stretch_robot_home.py\nFor use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\n--------- Homing Head ----\n--------- Homing Lift ----\nHoming Lift...\nHardstop detected at motor position (rad) 105.44721221923828\nMarking Lift position to 1.096683 (m)\nMarking Lift position to 0.000000 (m)\n[INFO] [robot_monitor]: Guarded contact lift\nLift homing successful\n--------- Homing Arm ----\nHoming Arm...\nHardstop detected at motor position (rad) -1.5079643726348877\nMarking Arm position to 0.000000 (m)\n[INFO] [robot_monitor]: Guarded contact arm\n[INFO] [robot_monitor]: Wrist single tap: 9\nArm homing successful\nMoving to first hardstop...\nFirst hardstop contact at position (ticks): 4097\n-----\nHoming offset was 3671\nMarking current position to zero ticks\nHoming offset is now  -427 (ticks)\n-----\nCurrent position (ticks): 24\nMoving to calibrated zero: (rad)\n[INFO] [robot_monitor]: Wrist single tap: 11\n[INFO] [robot_monitor]: Wrist single tap: 15\n[INFO] [robot_monitor]: Wrist single tap: 21\n[INFO] [robot_monitor]: Wrist single tap: 27\n[INFO] [robot_monitor]: Wrist single tap: 28\n[INFO] [robot_monitor]: Wrist single tap: 33\n[INFO] [robot_monitor]: Wrist single tap: 38\n[INFO] [robot_monitor]: Wrist single tap: 40\nMoving to first hardstop...\nFirst hardstop contact at position (ticks): -9\n-----\nHoming offset was 2234\nMarking current position to zero ticks\nHoming offset is now  2237 (ticks)\n-----\nCurrent position (ticks): 35\nMoving to calibrated zero: (rad)\n</code></pre>"},{"location":"developing/cli/#stretch_robot_stowpy","title":"<code>stretch_robot_stow.py</code>","text":"<p>Useful to return the robot arm to a safe position within the base footprint. Normal output from this tool looks like:</p> <pre><code>stretch_robot_stow.py\nFor use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\n--------- Stowing Arm ----\n--------- Stowing EOA_Wrist_DW3_Tool_SG3 ----\n--------- Stowing Lift ----\n</code></pre>"},{"location":"developing/cli/#stretch_robot_battery_checkpy","title":"<code>stretch_robot_battery_check.py</code>","text":"<p>A quick way to check the robot's battery voltage / current consumption. Normal output from this tool looks like:</p> <pre><code>stretch_robot_battery_check.py\nFor use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\n[Pass] Voltage with 12.791140079498291\n[Pass] Current with 6.154119995023523\n[Pass] CPU Temp with 45\n</code></pre>"},{"location":"developing/cli/#stretch_robot_keyboard_teleoppy","title":"<code>stretch_robot_keyboard_teleop.py</code>","text":"<p>This tool enables jogging of the robot's joints from the keyboard.</p> <p>5</p> <p>What's the difference between jog and <code>stretch_robot_keyboard_teleop.py</code></p>"},{"location":"developing/cli/#stretch_xbox_controller_teleoppy","title":"<code>stretch_xbox_controller_teleop.py</code>","text":"<p>6</p> <p>Useful to quickly test if a robot can achieve a task by manually teleoperating the robot</p>"},{"location":"developing/cli/#stretch_gamepad_teleoppy","title":"<code>stretch_gamepad_teleop.py</code>","text":"<p>7</p>"},{"location":"developing/cli/#stretch_free_robot_processpy","title":"<code>stretch_free_robot_process.py</code>","text":"<p>8</p>"},{"location":"developing/cli/#stretch_paramspy","title":"<code>stretch_params.py</code>","text":"<p>9</p> <p>This tool prints the Stretch parameters to the console.</p>"},{"location":"developing/cli/#stretch_realsense_visualizerpy","title":"<code>stretch_realsense_visualizer.py</code>","text":"<p>10</p> <p>This is a tool to test the Realsense D435i Camera. Pass the '-h' flag along with the command to see optional arguments.</p>"},{"location":"developing/cli/#stretch_rp_lidar_jogpy","title":"<code>stretch_rp_lidar_jog.py</code>","text":"<p>11</p>"},{"location":"developing/cli/#stretch_audio_testpy","title":"<code>stretch_audio_test.py</code>","text":"<p>12</p> <p>This tool allows you to test the audio system.</p>"},{"location":"developing/cli/#stretch_respeaker_testpy","title":"<code>stretch_respeaker_test.py</code>","text":"<p>13</p> <p>This tool allows you to record and playback audio via Respeaker.</p>"},{"location":"developing/cli/#stretch_device_homepy","title":"<code>stretch_&lt;device&gt;_home.py</code>","text":"<p>14</p> <ul> <li>stretch_arm_home.py</li> <li>stretch_gripper_home.py</li> <li>stretch_lift_home.py</li> <li>stretch_wrist_yaw_home.py</li> </ul>"},{"location":"developing/cli/#stretch_device_jogpy","title":"<code>stretch_&lt;device&gt;_jog.py</code>","text":"<p>15</p> <ul> <li>stretch_arm_jog.py</li> <li>stretch_base_jog.py</li> <li>stretch_gripper_jog.py</li> <li>stretch_head_jog.py</li> <li>stretch_lift_jog.py</li> <li>stretch_pimu_jog.py</li> <li>stretch_wacc_jog.py</li> <li>stretch_wrist_yaw_jog.py</li> </ul>"},{"location":"developing/cli/#stretch_device_scopepy","title":"<code>stretch_&lt;device&gt;_scope.py</code>","text":"<p>16</p> <ul> <li>stretch_pimu_scope.py</li> <li>This tool allows you to visualize Pimu (Power+IMU) board data with an oscilloscope. Pass the '-h' flag along with the command to see optional arguments.</li> <li>stretch_wacc_scope.py</li> <li>This is a tool to visualize Wacc (Wrist+Accel) board data with an oscilloscope. Pass the '-h' flag along with the command to see optional arguments.</li> </ul>"},{"location":"developing/cli/#stretch_aboutpy","title":"<code>stretch_about.py</code>","text":"<p>17</p> <p>This tool displays the model and serial number information as an image.</p>"},{"location":"developing/cli/#stretch_about_textpy","title":"<code>stretch_about_text.py</code>","text":"<p>18</p> <p>This tool displays the model and serial number information as text.</p>"},{"location":"developing/cli/#stretch_hardware_echopy","title":"<code>stretch_hardware_echo.py</code>","text":"<p>Can we get rid of this?</p> <p>This tool echoes the robot and computer hardware details to the console.</p>"},{"location":"developing/cli/#stretch_trajectory_jogpy","title":"<code>stretch_trajectory_jog.py</code>","text":"<p>19</p>"},{"location":"developing/cli/#stretch_robot_dynamixel_rebootpy","title":"<code>stretch_robot_dynamixel_reboot.py</code>","text":"<p>20</p> <p>Resets all Dynamixels in the robot, which might be necessary if a servo overheats during use and enters an error state.</p> <p>This tool reboots all Dynamixel servos on the robot.</p>"},{"location":"developing/cli/#stretch_robot_monitorpy","title":"<code>stretch_robot_monitor.py</code>","text":"<p>21</p> <p>What does this do?</p> <p>This tool runs the Robot Monitor and prints to the console.</p>"},{"location":"developing/cli/#stretch_robot_urdf_visualizerpy","title":"<code>stretch_robot_urdf_visualizer.py</code>","text":"<p>Can we replace this with the new Stretch URDF tools?</p> <p>This tool allows you to visualize robot URDF.</p>"},{"location":"developing/cli/#stretch_versionsh","title":"<code>stretch_version.sh</code>","text":"<p>Can we get rid of this?</p> <p>This script prints the version information for various software packages on the robot.</p>"},{"location":"developing/cli/#stretch-pyfunmap-tools","title":"Stretch PyFUNMAP Tools","text":""},{"location":"developing/cli/#pyfunmap_head_scan_visualizerpy","title":"pyfunmap_head_scan_visualizer.py","text":""},{"location":"developing/cli/#stretch-diagnostics-tools","title":"Stretch Diagnostics Tools","text":""},{"location":"developing/cli/#stretch_diagnostic_checkpy","title":"<code>stretch_diagnostic_check.py</code>","text":""},{"location":"developing/cli/#stretch-urdf-tools","title":"Stretch URDF Tools","text":""},{"location":"developing/cli/#stretch_urdf_examplepy","title":"stretch_urdf_example.py","text":""},{"location":"developing/cli/#stretch_urdf_vizpy","title":"stretch_urdf_viz.py","text":""},{"location":"developing/cli/#stretch-factory-tools","title":"Stretch Factory Tools","text":"<p>These tools are used at Hello Robot during the robot's system bring-up. They generally interact with the lowest level interface of the hardware, making measurements and writing calibration data to the robot's calibration folder.</p> <p>Warning</p> <p>It is possible to cause bodily harm and/or break your robot with these tools. Used improperly, these tools might not respect joint torque and position limits. They may overwrite existing calibration data as well.</p>"},{"location":"developing/cli/#re1_migrate_contactspy","title":"RE1_migrate_contacts.py","text":""},{"location":"developing/cli/#re1_migrate_paramspy","title":"RE1_migrate_params.py","text":""},{"location":"developing/cli/#rex_d435i_checkpy","title":"REx_D435i_check.py","text":""},{"location":"developing/cli/#rex_base_calibrate_imu_collectpy","title":"REx_base_calibrate_imu_collect.py","text":""},{"location":"developing/cli/#rex_base_calibrate_imu_processpy","title":"REx_base_calibrate_imu_process.py","text":""},{"location":"developing/cli/#rex_base_calibrate_wheel_separationpy","title":"REx_base_calibrate_wheel_separation.py","text":""},{"location":"developing/cli/#rex_calibrate_gravity_comppy","title":"REx_calibrate_gravity_comp.py","text":""},{"location":"developing/cli/#rex_calibrate_guarded_contactpy","title":"REx_calibrate_guarded_contact.py","text":""},{"location":"developing/cli/#rex_calibrate_rangepy","title":"REx_calibrate_range.py","text":""},{"location":"developing/cli/#rex_cliff_sensor_calibratepy","title":"REx_cliff_sensor_calibrate.py","text":""},{"location":"developing/cli/#rex_comm_ratespy","title":"REx_comm_rates.py","text":""},{"location":"developing/cli/#rex_discover_hello_devicespy","title":"REx_discover_hello_devices.py","text":""},{"location":"developing/cli/#rex_dmesg_monitorpy","title":"REx_dmesg_monitor.py","text":""},{"location":"developing/cli/#rex_dynamixel_id_changepy","title":"REx_dynamixel_id_change.py","text":""},{"location":"developing/cli/#rex_dynamixel_id_scanpy","title":"REx_dynamixel_id_scan.py","text":""},{"location":"developing/cli/#rex_dynamixel_jogpy","title":"REx_dynamixel_jog.py","text":""},{"location":"developing/cli/#rex_dynamixel_rebootpy","title":"REx_dynamixel_reboot.py","text":""},{"location":"developing/cli/#rex_dynamixel_set_baudpy","title":"REx_dynamixel_set_baud.py","text":""},{"location":"developing/cli/#rex_firmware_flashpy","title":"REx_firmware_flash.py","text":""},{"location":"developing/cli/#rex_firmware_updaterpy","title":"REx_firmware_updater.py","text":""},{"location":"developing/cli/#rex_gamepad_configurepy","title":"REx_gamepad_configure.py","text":""},{"location":"developing/cli/#rex_gripper_calibratepy","title":"REx_gripper_calibrate.py","text":""},{"location":"developing/cli/#rex_hello_dynamixel_jogpy","title":"REx_hello_dynamixel_jog.py","text":""},{"location":"developing/cli/#rex_stepper_calibration_yaml_to_flashpy","title":"REx_stepper_calibration_YAML_to_flash.py","text":""},{"location":"developing/cli/#rex_stepper_calibration_flash_to_yamlpy","title":"REx_stepper_calibration_flash_to_YAML.py","text":""},{"location":"developing/cli/#rex_stepper_calibration_runpy","title":"REx_stepper_calibration_run.py","text":""},{"location":"developing/cli/#rex_stepper_ctrl_tuningpy","title":"REx_stepper_ctrl_tuning.py","text":""},{"location":"developing/cli/#rex_stepper_gainspy","title":"REx_stepper_gains.py","text":""},{"location":"developing/cli/#rex_stepper_jogpy","title":"REx_stepper_jog.py","text":""},{"location":"developing/cli/#rex_stepper_mechaduino_menupy","title":"REx_stepper_mechaduino_menu.py","text":""},{"location":"developing/cli/#rex_trace_firmwarepy","title":"REx_trace_firmware.py","text":""},{"location":"developing/cli/#rex_trace_robotpy","title":"REx_trace_robot.py","text":""},{"location":"developing/cli/#rex_usb_resetpy","title":"REx_usb_reset.py","text":""},{"location":"developing/cli/#rex_wacc_calibratepy","title":"REx_wacc_calibrate.py","text":""},{"location":"developing/networking/","title":"Networking Guide","text":"<p>TODO</p>"},{"location":"developing/onboot/","title":"Launching Apps on Boot","text":"<p>When you deploy Stretch with your application running on it, you might want your application to start automatically when the robot boots up. Linux provides a couple ways of doing this, but the documentation online can be difficult to comb through. This tutorial provides one way to get apps to launch reliably on boot. In particular, this tutorial looks at Systemd unit files.</p> <p>This tutorial assumes a basic understanding of the Linux terminal.</p>"},{"location":"developing/onboot/#systemd-unit-files","title":"Systemd Unit Files","text":"<p>Systemd manages the OS's services and boot initialization sequence. It provides a CLI called <code>systemctl</code> for start/stopping services, configuring whether the service is \"enabled\" (will launch on next boot) or \"disabled\", and checking the status of services. Many Linux apps rely on systemd to keep things running in the background (e.g. did you know that SSH-ing into a computer works because that computer is running a SSH daemon via systemd. Try running <code>systemctl status sshd</code>). You can describe a service by writing a \"systemd unit file\". For example, here's the unit file for Stretch Web Teleop:</p> <p>Filename: web-teleop.service <pre><code>[Unit]\nDescription=Web interface for Stretch robots.\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nExecStartPre=/bin/sh -c 'until ping -c1 web.hello-robot.com; do sleep 1; done;'\nExecStart=/home/hello-robot/ament_ws/src/stretch_web_teleop/launch_interface.sh -f\nRemainAfterExit=yes\n\n[Install]\nWantedBy=graphical-session.target\n</code></pre></p> <p>Notice the 3 sections: Unit, Service, and Install.</p> <p>\"Unit\" describes the service and declares what the service needs. For web teleop, we need the robot to be connected to the network before launching our application, so we say <code>Wants=network-online.target</code> and <code>After=network-online.target</code>. There are many targets available. For example, you could target <code>graphical-session.target</code> if your application needs a screen to render to. Also notice <code>ExecStartPre=/bin/sh -c 'until ping -c1 web.hello-robot.com; do sleep 1; done;'</code> in the next section, which is a reliable way of ensuring the robot is connected to the network and able to connect to web.hello-robot.com before launching the service.</p> <p>\"Service\" describes what the service should run. <code>ExecStart=/home/hello-robot/ament_ws/src/stretch_web_teleop/launch_interface.sh -f</code> is telling systemd that I want it to run the <code>launch_interface.sh</code> bash script in my ROS2 workspace. I use the absolute path including <code>/home/hello-robot</code> because there is only one user on my robot called \"hello-robot\" and my unit file is specific to the \"hello-robot\" user. Here's what launch_interface.sh looks like:</p> <pre><code>#!/bin/bash\nset -e\n\nREDIRECT_LOGDIR=\"$HOME/stretch_user/log/web_teleop\"\nmkdir -p $REDIRECT_LOGDIR\nREDIRECT_LOGFILE=\"$REDIRECT_LOGDIR/start_ros2.`date '+%Y%m%d%H%M'`_redirected.txt\"\n\necho \"Setup environment...\"\n. /etc/hello-robot/hello-robot.conf\nexport HELLO_FLEET_ID HELLO_FLEET_ID\nexport HELLO_FLEET_PATH=$HOME/stretch_user\nsource /opt/ros/humble/setup.bash &amp;&gt;&gt; $REDIRECT_LOGFILE\nsource ~/ament_ws/install/setup.bash &amp;&gt;&gt; $REDIRECT_LOGFILE\nsource /usr/share/colcon_cd/function/colcon_cd.sh &amp;&gt;&gt; $REDIRECT_LOGFILE\n\necho \"Freeing robot process...\"\n/usr/bin/python3 $HOME/.local/bin/stretch_free_robot_process.py &amp;&gt;&gt; $REDIRECT_LOGFILE\n\necho \"Stopping previous instances...\"\n./stop_interface.sh &amp;&gt;&gt; $REDIRECT_LOGFILE\n\necho \"Reload USB bus...\"\nsudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger &amp;&gt;&gt; $REDIRECT_LOGFILE\n\necho \"Start ROS2...\"\nsleep 2;\nscreen -dm -S \"web_teleop_ros\" ros2 launch stretch_web_teleop web_interface.launch.py $MAP_ARG $TTS_ARG &amp;&gt;&gt; $REDIRECT_LOGFILE\nsleep 3;\n</code></pre> <p>Notice that almost every command has <code>&amp;&gt;&gt; $REDIRECT_LOGFILE</code> after it. The reason behind this is to make it easier for ourselves to debug issues later. Since services start on boot, there's no way for us to see any warning/error messages they emit (technically, there's a tool called journalctl, but in practice, it's difficult to explain to your end-user that they need to learn journalctl to collect logs for you. It's easier to request they zip up a few files and send it your way). Since the output is logged to a timestamped file in <code>~/stretch_user/log</code>, it's possible to check it for errors later if issues come up.</p> <p>This bash script performs four steps before starting the application:</p> <ul> <li>Setting up the environment. It's important that env vars like HELLO_FLEET_ID are set and that ROS2 is enabled if your application is based on ROS2</li> <li>Freeing the robot process. Systemd can automatically restart services if they experience an error, so it's nice for this script to ensure the robot is freed up before continuing.</li> <li>Stopping previous instances of web teleop.</li> <li>Reloading the USB bus. <code>network-online.target</code> comes after the USB bus is configured in the boot sequence, but it doesn't hurt to ensure USB is ready.</li> </ul> <p>Lastly, the script launches our Web Teleop ROS2 application inside of GNU <code>screen</code>. We use <code>screen</code> because it collects ROS2 logs in a nicer way than other tools. However, because <code>screen</code> launches another process to run the app, we need to inform systemd that the application is still running in the background. Hence, we include <code>RemainAfterExit=yes</code> in the unit file</p> <p>In the last section, \"Install\", we set <code>WantedBy=graphical-session.target</code>. It's important that our service is wanted by some other service, otherwise our service would never be executed by systemd. By telling systemd that our service is wanted by <code>graphical-session.target</code>, which is part of the boot sequence, our service also becomes part of the boot sequence.</p>"},{"location":"developing/onboot/#sudo","title":"Sudo","text":"<p>Your application might require sudo. The Web Teleop application requires it to reload the USB bus (<code>sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</code>). This can be a problem because using sudo requires users to enter their passwords in an interactive prompt, which is impossible when the service is running in the background while the robot boots.</p> <p></p> <p>The solution is to enable the user to run some sudo commands without needing a password. There is a directory called <code>/etc/sudoers.d/</code> where you can put rules for commands that are granted this privledge. E.g. when installing Web Teleop, we put the following file in the directory:</p> <p>Filename: hello_sudoers <pre><code>Cmnd_Alias ONBOOT_WEBTELEOP_SERVICES = /sbin/shutdown, /usr/bin/udevadm, /usr/bin/pkill, /usr/local/sbin/wifi-connect\n\nALL ALL=(ALL) NOPASSWD:ONBOOT_WEBTELEOP_SERVICES\n</code></pre></p> <p>This allows ALL users to run <code>shutdown</code>, <code>udevadm</code>, <code>pkill</code>, and <code>wifi-connect</code> with sudo without the password prompt appearing. In effect, this enables your systemd service to succeed in starting your service.</p>"},{"location":"developing/onboot/#where-to-put-unit-files","title":"Where to put unit files","text":"<p>The unit file described above is a \"user unit file\", as opposed to a \"system unit file\". User unit files run when the user logins. By default, the \"hello-robot\" user is configured to login automatically on boot. Furthermore, since the Web Teleop unit file needs a ROS2 workspace in the \"hello-robot\" user, it makes sense to use a user unit file.</p> <p>User unit files live in the <code>~/.config/systemd/user</code> directory. So when installing the Web Teleop application to launch on boot, we run:</p> <pre><code>cp ./web-teleop.service ~/.config/systemd/user/web-teleop.service\n</code></pre> <p>Next, we \"enable\" the service using:</p> <pre><code>systemctl enable --user web-teleop\n</code></pre> <p>Now, we can reboot the robot and the application should start on boot! \ud83c\udf89</p>"},{"location":"developing/onboot/#examples","title":"Examples","text":"<p>The following pull requests show examples of Stretch applications being configured by Stretch's install scripts to launch on boot.</p> <ul> <li>https://github.com/hello-robot/stretch_install/pull/83</li> <li>https://github.com/hello-robot/stretch_install/pull/61/files</li> </ul>"},{"location":"developing/python/","title":"Using Python with Stretch","text":"<p>This tutorial covers the TODO.</p> <p>TODO - Link to video here</p>"},{"location":"developing/python/#creating-python-scripts","title":"Creating Python Scripts","text":""},{"location":"developing/python/#python-in-the-terminal","title":"Python in the Terminal","text":""},{"location":"developing/python/#python-ides","title":"Python IDEs","text":""},{"location":"developing/python/#pip-pythons-package-manager","title":"Pip - Python's Package Manager","text":""},{"location":"developing/python/#stretch-packages","title":"Stretch Packages","text":""},{"location":"developing/python/#api-refs","title":"API Refs","text":""},{"location":"developing/python/#useful-packages-for-robotics","title":"Useful Packages for Robotics","text":"<ul> <li>numpy</li> <li>scipy</li> <li>cv2</li> <li>pytorch</li> </ul>"},{"location":"developing/python/#editable-install","title":"Editable Install","text":""},{"location":"developing/python/#debugging","title":"Debugging","text":""},{"location":"developing/python/#interpreting-stack-traces","title":"Interpreting Stack Traces","text":""},{"location":"developing/python/#pdb","title":"pdb","text":""},{"location":"developing/python/#ipython","title":"iPython","text":""},{"location":"developing/python/#pickle","title":"Pickle","text":""},{"location":"developing/python/#profiling","title":"Profiling","text":""},{"location":"developing/python/#cprofile","title":"cProfile","text":""},{"location":"developing/python/#virtual-environments","title":"Virtual Environments","text":""},{"location":"developing/python/#conda","title":"Conda","text":""},{"location":"extending_stretch/changing_tools/","title":"Changing Tools","text":"<p>Stretch was designed to support more than one type of gripper, so you can change the tool attached to the end of the robot's arm. This tutorial will go through the different end-of-arm tools available and how configure them in software after a hardware tool change.</p>"},{"location":"extending_stretch/changing_tools/#stretch-supported-tools","title":"Stretch Supported Tools","text":"<p>The supported end-of-arm tool configurations are based on your robot model and can be found in the table below.</p> Tool Stretch 3 Stretch RE2 Stretch RE1 eoa_wrist_dw3_tool_sg3 eoa_wrist_dw3_tool_nil tool_stretch_dex_wrist tool_stretch_gripper tool_none <p>The Stretch software has one parameter, called <code>robot.tool</code>, that decides what tool the software is currently configured for. You can run the following command in a Terminal to print out the <code>robot.tool</code> parameter: <pre><code>stretch_params.py | grep robot.tool\n</code></pre> The output will look like: <pre><code>stretch_configuration_params.yaml     param.robot.tool     eoa_wrist_dw3_tool_sg3\n</code></pre></p>"},{"location":"extending_stretch/changing_tools/#configuring-a-new-tool","title":"Configuring a new Tool","text":"<p>Once you have completed the hardware installation of a new tool, you can use a CLI called <code>stretch_configure_tool.py</code> to configure your software for the new tool. This CLI automatically assesses the hardware found on the robot and configures the appropriate software configuration. The configuration changes would involve updating the parameter shown above and generating updated URDFs.</p> <pre><code>~$ stretch_configure_tool.py -h\n\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nusage: stretch_configure_tool.py [-h] [-v]\n\nConfigure Stretch's software for the attached tool\n\noptions:\n  -h, --help     show this help message and exit\n  -v, --verbose  Prints more information\n</code></pre> <p>You can use this CLI any time to verify the integrity of your software configuration for the officially supported tools.</p> <pre><code>$ stretch_configure_tool.py\n\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nLoading...\nYour software is currently configured for the eoa_wrist_dw3_tool_sg3 tool\nWhich seems correct based on the hardware connected to your robot\nDone!\n</code></pre> <p>If the CLI is unable to determine the right tool for your robot, it will let you know. In this case, reach out to Hello Robot Support for help.</p> <pre><code>$ stretch_configure_tool.py\n\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nLoading...\nYour software is currently configured for the eoa_wrist_dw3_tool_sg3 tool\n...\nUnable to find any tool that matches the hardware connected to your robot. Contact Hello Robot support for help.\nNot changing anything. Exiting.\n</code></pre>"},{"location":"extending_stretch/changing_tools/#add-support-for-a-new-tool","title":"Add support for a new tool","text":"<p>If you've created a new tool for Stretch, you may want it to be supported by the <code>stretch_configure_tools.py</code> CLI. The source code for the CLI can be found here. Feel free to open a Github pull request to merge support for your tool into the CLI.</p>"},{"location":"extending_stretch/custom_dof/","title":"Custom DOF","text":"<p>TODO</p>"},{"location":"extending_stretch/extending_stretch_additional_hardware/","title":"Extending Stretch with Additional Hardware","text":"<p>This tutorial will walk you through different ways you can add additional hardware and integrate them with Stretch.</p>"},{"location":"extending_stretch/extending_stretch_additional_hardware/#mounting-your-hardware","title":"Mounting your Hardware","text":"<p>Stretch provides various threaded mounting points (M4 threaded) around it's body where you could mount your hardware securely.</p> Base Head Shoulder Wrist Tool Plate"},{"location":"extending_stretch/extending_stretch_additional_hardware/#powering-your-hardware","title":"Powering your Hardware","text":"<p>Stretch providess additional 12V auxilary power ports on the base, head and shoulder  which you could use to power your additional hardware.</p> Base Head Shoulder <p>Checkout Adding Jetson to Stretch tutorial which utilizes Stretch's mounting holes and auxilary power ports.</p>"},{"location":"extending_stretch/extending_stretch_additional_hardware/#integrating-actuators-and-sensors","title":"Integrating Actuators and sensors","text":"<p>The robot's Wrist has different types of interfaces provided through which you could integrate additional actuators and sensors.</p>"},{"location":"extending_stretch/extending_stretch_additional_hardware/#wrist-dynamixel-interface","title":"Wrist Dynamixel Interface","text":"<p>The wrist yaw degree-of-freedom uses a Dynamixel XL430 servo. Additional Dynamixel servos can be daisy chained off of this servo, allowing for more additional motors. Stretch comes with a XL430 compatible control cable preinstalled into this servo which can be used to daisy chain more XL430 compatible motors. If a different cable needs to be installed the servo cap can be removed as shown. </p> <p></p> <p>Checkout Custom DOF tutorial to learn how to integrate custom or additional Dynamixel joints and tool chains with Stretch Body.</p>"},{"location":"extending_stretch/extending_stretch_additional_hardware/#wrist-expansion-usb","title":"Wrist Expansion USB","text":"<p>The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V.</p>"},{"location":"extending_stretch/extending_stretch_additional_hardware/#wrist-expansion-header","title":"Wrist Expansion Header","text":"<p>The wrist includes an expansion header that provides access to pins of the wrist Arduino board.  The header connector can be accessed by removing the cap at the end of the arm.</p> <p></p> <p>The header is wired to a Atmel SAMD21G18A-AUT (datasheet) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow:</p> <ul> <li>General purpose digital I/O</li> <li>Analog input</li> </ul> <p>In addition, the firmware can be configured for other pin functions, including:</p> <ul> <li>Serial SPI</li> <li>Serial I2C</li> <li>Serial UART</li> </ul> <p>Checkout Wrist Expansion Header tutorial to learn how to write custom Wacc firmware to utilize the above pin functions to read sensor data and integrate it with Stretch Body.</p> <p>**The header pins utilize 3V3 TTL logic. They have limited interface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded **</p> <p>The pin mapping is:</p> Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS  Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input  Analog in (A0) <p>The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN).  It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305).</p> <p>Pin 1 &amp; 10 are indicated below.</p> <p></p> <p>The expansion DIO schematic shown below.</p> <p></p>"},{"location":"extending_stretch/urdf_management/","title":"URDF Management","text":"<p>This tutorial will show you how to access URDFs of Stretch. The URDF (Unified Robot Description Format) is the most popular file format to represent a robot's physical descpriction and geometry. A robot's URDF is used in different applications such as collision checking, navigation planners and perform forward/inverse kinematics.</p>"},{"location":"extending_stretch/urdf_management/#stretch-urdf","title":"Stretch URDF","text":"<p>All the Stretch robot model's URDF and meshes are managed by the standalone python package Stretch URDF. </p> <p>This package can be installed by: <pre><code>python3 -m pip install  -U hello-robot-stretch-urdf\n</code></pre></p> <p>The URDF data is uncalibrated and is managed seperately from the Stretch ROS/ROS2 URDF data but could be ported to over using the provided CLI tools.</p>"},{"location":"extending_stretch/urdf_management/#accessing-urdf-files","title":"Accessing URDF Files","text":"<p>The URDF and mesh data is installed as a Python package. It's location can be found as: <pre><code>import importlib.resources as importlib_resources\npkg_path = str(importlib_resources.files(\"stretch_urdf\"))\n</code></pre></p> <p>The URDF naming convention is stretch_description_[model_name]_[tool_name].urdf. Example to retrieve a URDF file and mesh files directory: <pre><code>model_name = 'SE3' # RE1V0, RE2V0, SE3\ntool_name = 'eoa_wrist_dw3_tool_sg3' # eoa_wrist_dw3_tool_sg3, tool_stretch_gripper, etc\nurdf_file_path = pkg_path + f\"/{model_name}/stretch_description_{model_name}_{tool_name}.urdf\"\nmesh_files_directory_path = pkg_path + f\"/{model_name}/meshes\"\n</code></pre></p>"},{"location":"extending_stretch/urdf_management/#stretch-urdf-cli-tools","title":"Stretch URDF CLI Tools","text":"<p>The package comes with some useful stretch_urdf_* CLI tools for you to take advantage of.</p>"},{"location":"extending_stretch/urdf_management/#visualize-urdfs","title":"Visualize URDFs","text":"<p>You can use the stretch_urdf_viz.py tool to visualize any robot model with a tool configuration of your choice. You can also pass different arguments while on robot to visualize URDF with stretch body drivers running and visualize realtime robot motions. <pre><code>$ stretch_urdf_viz.py -h\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nusage: stretch_urdf_viz.py [-h] [-t] [-c] [-g]\n\nPython based URDF visualization\n\noptions:\n  -h, --help        show this help message and exit\n  -t, --trajectory  Visualize predefined trajectory\n  -c, --collision   Use collision meshes\n  -g, --gamepad     Use gamepad to control pose\n</code></pre> </p>"},{"location":"extending_stretch/urdf_management/#copy-urdf-and-meshes-to-ros","title":"Copy URDF and Meshes to ROS","text":"<p>You can use the stretch_urdf_ros_update.py tool to port the right URDF and meshes in to your ROS/ROS2 stretch_description package. <pre><code>$ stretch_urdf_ros_update.py -h\n\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nusage: stretch_urdf_ros_update.py [-h] [-v] [--model {RE1V0,RE2V0,SE3}] [--tool TOOL]\n                                  [--ros2_rebuild] [-y]\n\nTool to update the ROS Stretch Description package Xacros and Mesh files based on the configured\nEnd-of-Arm tool.\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --verbose         Prints more info\n  --model {RE1V0,RE2V0,SE3}\n                        Choose a Robot model name.\n  --tool TOOL           Choose a supported Robot tool name.\n  --ros2_rebuild        Rebuild ROS2's Stretch Description package\n  -y, --yes             Override Confirmation prompt.\n</code></pre></p>"},{"location":"extending_stretch/wrist_expansion_header/","title":"Wrist Expansion Header","text":"<p>TODO</p>"},{"location":"getting_started/community_resources/","title":"Community Resources","text":"<p>While this documentation is the ideal way to get started with your robot, there are a number of other resources worth noting that provide additional information and ways to interact with Stretch and the larger Hello Robot community.</p>"},{"location":"getting_started/community_resources/#forum","title":"Forum","text":"<p>Stretch has a diverse and vibrant user community, and one of our goals at Hello Robot is to help our customers connect, collaborate, and share with one another. To this end, we have created a public Hello Robot Forum where you can ask questions, post about your work, request support, or search an archive of resolved issues. Our engineers regularly read and respond to threads here, as well as posting helpful tips and tricks in the Knowledge Base. </p>"},{"location":"getting_started/community_resources/#github","title":"Github","text":"<p>Nearly all the code we write for Stretch is open-source and freely available on our Hello Robot Github. Feel free to browse through the repos or dig into the code. For information on how to contribute to Stretch software, see the [Contribution Guide].</p>"},{"location":"getting_started/community_resources/#stretch-tool-share","title":"Stretch Tool Share","text":"<p>One particular Github repo of note is our [Tool Share], which contains open-source tools and accessories that you can use with your Stretch.</p>"},{"location":"getting_started/community_resources/#community-repositories","title":"Community Repositories","text":"<p>Another Github repo worth exploring our [Community Repos] page, where we collect some of the open-source code developed by our community. If you've developed something you'd like to see shared here, please [let us know!] </p>"},{"location":"getting_started/community_resources/#community-updates","title":"Community Updates","text":"<p>We love to help spread the work about the great work our community is constantly publishing! Every month, we publish a [Stretch Community Update] with some highlights of the recent research done by Stretch customers on our website, mailing list and social media. When you have work that is ready to make public, [let us know] and we'll make sure it's seen by our community! </p>"},{"location":"getting_started/community_resources/#next-steps","title":"Next Steps","text":"<p>The next guide in our Quickstart tutorial, Getting Help, will explain some resources to explore if you have questions or run into trouble while working with Stretch. We recommend all new users complete the full Quickstart tutorial series before beginning to develop with the robot.</p>"},{"location":"getting_started/connecting_to_stretch/","title":"Connecting to Stretch","text":"<p>In this guide, we will look at two ways of connecting to Stretch, tethered and untethered. Once connected, we'll run the Stretch Visual Servoing demo.</p>"},{"location":"getting_started/connecting_to_stretch/#tethered-setup","title":"Tethered Setup","text":"<p>There is a computer inside Stretch called the \"Intel NUC\". You can develop and test your code directly on this computer.</p> <p>In the robot's trunk, where the On/Off button is located, you will find several USB ports, a HDMI port, and an ethernet port. All of these are connected directly to the NUC inside the robot. There are additional USB ports located in the robot's head, wrist, and shoulder, which also connect to the NUC. This makes setting up a wired connection to the robot very easy.</p> <p></p> <p>The simplest setup for developing with Stretch is a monitor with an HDMI cable, and a USB keyboard/mouse. At Hello Robot, we prefer to use a wireless keyboard and mouse combo.</p> <p></p> <p>Simply plug the monitor into your robot using the included HDMI cable, and plug your keyboard and mouse into USB ports on the robot. Now turn on the robot again. That's it! After a few seconds, you'll see the NUC boot up sequence on the monitor.</p> <p></p> <p>Next, you'll see the Ubuntu desktop appear on the monitor. You can change your desktop background to a Stretch-themed wallpaper if you'd like.</p> <p>The default user login credentials came in the box with the robot. By default, the robot is not configured to ask for your password on boot, but may ask for it later if the NUC goes to sleep. Stretch is running the Ubuntu 22.04 operating system.</p> <p>Note</p> <p>If you're using a Stretch 2 or Stretch RE1, your robot may be running an older operating system. See the troubleshooting advice below to identify which OS your robot is running.</p>"},{"location":"getting_started/connecting_to_stretch/#untethered-setup","title":"Untethered Setup","text":"<p>As a mobile manipulator, the robot can only travel so far when tethered by the cables of a monitor, keyboard, and mouse. There are several ways to set up a wireless connection to Stretch, e.g. SSH, VNC, TeamViewer, AnyDesk, and in later tutorials we will explore some of these options and other programmatic methods of networking (e.g. ROS 2 networking, PyZMQ), but in this series we will be using a fast, open-source solution called \"Moonlight\".</p> <p>Moonlight is software for streaming video games, but its ability to stream high-resolution, low-latency video makes it ideal for robotic applications as well. To set it up, follow these instructions:</p> <ol> <li>You will need a \"dummy HDMI dongle\". If you're using a Stretch 3, one should be included in the accessories box. Otherwise, they can be found online for low cost.<ul> <li></li> </ul> </li> <li>You will need access to the robot's desktop. Go through the Wired Connection section if you haven't already.</li> <li>On the robot, open Wifi settings and connect to a network. If you're using the Stretch WiFi Access Point, power up the router and Stretch will automatically connect to it.</li> <li>On the robot, open a terminal and run:     <pre><code>systemctl start --user sunshine\n</code></pre></li> <li>Then, in the same terminal, run:     <pre><code>systemctl enable --user sunshine\n</code></pre></li> <li>Since this is your first time, we'll setup the Moonlight server (called \"Sunshine\") on Stretch. On the robot, open a browser and visit https://localhost:47990. You may see a warning and need to click \"Advanced...\" -&gt; \"Continue/Proceed\". Next, it will ask you to choose credentials. Make sure to choose secure credentials and share them with anyone else using the robot. Then, log in and navigate to the \"PIN\" tab.</li> <li>On your laptop or PC, go to moonlight-stream.org and download Moonlight.</li> <li>Ensure your laptop or PC is connected to the same network as Stretch. Moonlight can stream over the internet, but it is configured to work on a local network by default.</li> <li> <p>Open Moonlight. It should automatically detect your Stretch. Click on it and you'll be given a 4 digit PIN. Enter this PIN in the Sunshine PIN page to pair Moonlight with your Stretch. Moonlight will remember this in the future.</p> <p>Note</p> <p>Sometimes Moonlight will not automatically detect the robot, even though both devices are on the same network. If this happens to you, see the troubleshooting advice below.</p> </li> <li> <p>Moonlight will now show you a page with a button labeled \"Desktop\". This will start the stream of Stretch's desktop to your laptop or PC. Don't click on it yet. If you already did, exit out using Ctrl+Alt+Shift+Q.</p> </li> <li>Locate the dummy HDMI dongle included with Stretch. Disconnect all cables from Stretch, swapping the HDMI cable with the dummy HDMI dongle.<ul> <li></li> </ul> </li> <li>You can now click on the \"Desktop\" app in Moonlight to begin streaming the robot's desktop to your laptop/PC. You now have a wireless connection to Stretch!</li> </ol> <p>To exit out of the stream, press Ctrl+Alt+Shift+Q. In the future, you can skip the pairing steps since the devices will already be paired.</p>"},{"location":"getting_started/connecting_to_stretch/#tips-to-optimize-moonlight","title":"Tips to optimize Moonlight","text":"<ul> <li>If the stream feels laggy, you can reduce the bitrate in Moonlight's settings.<ul> <li></li> </ul> </li> <li>If the mouse feels too fast, you can adjust the mouse behavior for remote desktop in Moonlight's settings.<ul> <li></li> </ul> </li> <li>If part of the Ubuntu desktop is cut off or the resolution is a poor match for your screen, you can adjust the display settings in Moonlight's settings.<ul> <li></li> </ul> </li> </ul>"},{"location":"getting_started/connecting_to_stretch/#stretch-visual-servoing","title":"Stretch Visual Servoing","text":"<p>If you've reached this point, you're connected to Stretch and ready to roll. Congrats! Let's try the Visual Servoing demo.</p> <p></p> <p>Visual servoing is a powerful way to achieve fast, precise, and robust motions. For this demo, your Stretch robot needs a Dex Wrist 3, which comes with an Intel Realsense D405 depth camera and a gripper with ArUco markers on the fingers.</p> <p>Note</p> <p>If you're using a Stretch 2 or Stretch RE1, your robot may not have a Dex Wrist 3. See the troubleshooting advice below to identify which tool your robot has.</p> <p>To start the demo, follow these instructions:</p> <ol> <li> <p>Clone this repository on your robot</p> <pre><code>git clone https://github.com/hello-robot/stretch_visual_servoing/\n</code></pre> </li> <li> <p>Next, find the transparent acrylic cube that came with your robot. It has a 30 mm by 30 mm ArUco marker on it that encodes the ArUco ID# 202. The robot will try to reach for and grab this cube when it sees the ArUco marker with the D405 camera on its gripper.</p> </li> <li> <p>Home the robot</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>You may be asked to run <code>stretch_free_robot_process.py</code> before homing.</p> </li> <li> <p>Enter into the home directory of the cloned repository</p> <pre><code>cd ./stretch_visual_servoing\n</code></pre> </li> <li> <p>Run the visual servoing demo</p> <pre><code>python3 visual_servoing_demo.py\n</code></pre> </li> </ol> <p>The Visual Servoing demo also works with a tennis ball. Check out the instructions in the repo to try it out.</p> <p></p> <p></p>"},{"location":"getting_started/connecting_to_stretch/#next-steps","title":"Next Steps","text":"<p>In the next guide, Stretch Hardware Overview, we will start to get familiar with the robot hardware and some common command line tools.</p>"},{"location":"getting_started/connecting_to_stretch/#troubleshooting","title":"Troubleshooting","text":"<p>If you're having trouble with the steps in the guide, please check the following tips:</p>"},{"location":"getting_started/connecting_to_stretch/#which-operating-system-is-my-robot-running","title":"Which operating system is my robot running?","text":"<ol> <li>Open system setting by clicking \"Settings\" in the control panel<ul> <li></li> </ul> </li> <li>Scroll down the \"About\" page<ul> <li></li> </ul> </li> <li>Locate the \"OS Name\". Stretch has had 3 OS releases: 18.04, 20.04, and 22.04<ul> <li></li> </ul> </li> </ol> <p>Ubuntu 22.04 is the latest at the time of this writing. The tutorials on this website have been written with 22.04 in mind. If your Stretch is running an older version, consider upgrading using the Upgrading your Operating System guide.</p>"},{"location":"getting_started/connecting_to_stretch/#stretch-isnt-automatically-detected-by-moonlight","title":"Stretch isn't automatically detected by Moonlight","text":"<ul> <li>Some routers block local networking. You can try adding the robot by entering it's IP address into Moonlight. On the robot, open a terminal and enter <code>ifconfig</code>. The output typically looks like this. You can find the robot's IP address by searching for the address near terms like \"wlp2s0\" and \"inet\" (e.g. 192.168.1.29 in the image below).</li> <li></li> <li>Some routers block all local networking, so it isn't possible to use Moonlight. If possible, use a different router.</li> </ul>"},{"location":"getting_started/connecting_to_stretch/#does-my-robot-have-a-dex-wrist-3","title":"Does my robot have a Dex Wrist 3?","text":"<ul> <li>Here's what the Dex Wrist 3 looks like.<ul> <li> </li> </ul> </li> <li>If your robot doesn't have a Dex Wrist 3, you can skip the Visual Servoing section.</li> </ul>"},{"location":"getting_started/connecting_to_stretch/#ive-forgotten-my-login-credentials-for-sunshines-web-ui","title":"I've forgotten my login credentials for Sunshine's Web UI","text":"<p>Connect to your robot using the tethered setup. Open a terminal and run the following command:</p> <pre><code># Choose a new username and password\nsunshine --creds &lt;new-username&gt; &lt;new-password&gt;\n</code></pre>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"getting_started/demos_dexterous_teleop/","title":"Demo #3: Dexterous Teleop","text":"<p>TODO</p>"},{"location":"getting_started/demos_mapping_and_navigation/","title":"Demo #1: Mapping and Navigation","text":"<p>Stretch can perceive and navigate through its environment in two different ways: using the onboard 2D lidar with the ROS 2 Navigation Stack, or using the Intel RealSense depth camera with Stretch FUNMAP (Fast Unified Navigation, Mapping, and Planning). The following tutorial will walk you through the basics of launching and using each of these options. </p>"},{"location":"getting_started/demos_mapping_and_navigation/#nav2-overview","title":"Nav2 Overview","text":"<p>The ROS 2 Navigation Stack, or Nav2, is a motion planning and control framework for the mobile base that works right out of the box with Stretch. To use the Nav2 stack, we will first create a map of the local area, using the gamepad teleoperation controller to move the robot around your environment. Once complete, Stretch will be able to localize itself on the map and find its way around the environment autonomously.</p>"},{"location":"getting_started/demos_mapping_and_navigation/#mapping","title":"Mapping","text":"<p>To start the mapping demo, open a terminal window and run the following command:</p> <pre><code>ros2 launch stretch_nav2 offline_mapping.launch.py\n</code></pre> <p>An ROS Visualizer (Rviz) window will open and show the robot and map that is being constructed.</p> <p></p> <p>To fill in the map, we need to move the robot through the environment so that the lidar can see the entire area you'd like to map. The gamepad controller will allow you to do this. In this mode, the front left bumper acts as a dead man's switch - keeping this button pressed, drive the robot around the environment using a joystick.</p> <p></p> <p>Warning</p> <p>This mapping method creates a 2D map at the height of the lidar, approximately 6.5\" inches above the ground. Later ROS 2 tutorials will cover how to create a 3D map by combining data from the lidar and the head camera.</p> <p>Keep in mind that some obstacles in the environment will not be apparent in this 2D map. For example, the robot may only be able to see the legs of a table, which could result in attempting to navigate through the obstacle. The robot also will not be able to see areas that it should not attempt to navigate if no obstacle protrudes above 6.5\" - for example, an HVAC vent in the floor, or a downwards staircase. For safety, you should consider constructing temporary barriers to block off these regions while the map is being created.</p>"},{"location":"getting_started/demos_mapping_and_navigation/#tips-for-mapping","title":"Tips for Mapping","text":"<ul> <li> <p>Keep an eye on the map displayed in Rviz. The dark teal color indicates areas the robot has not seen. Make sure there are no teal 'holes' in the middle of areas you'd like the robot to be able to traverse. </p> </li> <li> <p>The robot lidar is able to see nearly 360 degree, but has a blind spot created by the robot mast. Make sure to command the robot to rotate every now and then to help fill in the gaps as you navigate around.</p> </li> <li> <p>Floor surfaces that cause Stretch's wheels to slip can result in errors in the map. During this step, try to avoid surfaces like loose rugs, thick-pile carpet, or large thresholds.</p> </li> </ul>"},{"location":"getting_started/demos_mapping_and_navigation/#saving-the-map","title":"Saving the Map","text":"<p>Once you map captures the space you want Stretch to be able to navigate in, open a new terminal window and run the following command to save the map:</p> <pre><code>ros2 run nav2_map_server map_saver_cli -f ${HELLO_FLEET_PATH}/maps/nav2_demo_map\n</code></pre> <p>This will save two files to the \"~/stretch_user/maps\" directory - <code>nav2_demo_map.pgm</code> and <code>nav2_demo_map.yaml</code>.</p> <p>Tip</p> <p><code>.pgm</code> files are just images. You can open previously saved map in an image viewer to inspect them. For example, try:</p> <pre><code>eog ${HELLO_FLEET_PATH}/maps/nav2_demo_map.pgm\n</code></pre>"},{"location":"getting_started/demos_mapping_and_navigation/#navigation","title":"Navigation","text":"<p>Now that we have a saved map of the environment, we can command the robot to move around the mapped space. Run the following command:</p> <pre><code>ros2 launch stretch_nav2 navigation.launch.py map:=${HELLO_FLEET_PATH}/maps/nav2_demo_map.yaml\n</code></pre> <p>A RViz window should open. At the bottom left of the window, there should be a button labeled \"Startup\". Press this button, which will launch all navigation related lifecycle nodes. </p> <p>Rviz will now attempt to show the robot location on your map; however, it is likely that initially this won't actually match the robot location in real space. To correct for this, click \"2D Pose Estimate\" from the top bar of the Rviz window, then click on your map in roughly the current location of your robot, and drag in the direction the robot is facing. This gives an initial estimate of the robot's location to the localization package (ACML).</p> <p>Now let's command the robot to move to another location. Click \"2D Nav Goal\" in the top bar, then click on the map in another location. In the terminal window, you'll see Nav2 go through the planning phase, and then navigate the robot to the goal. If the planning fails, the robot will being a recovery behavior - rotating in place or backing up.</p> <p></p>"},{"location":"getting_started/demos_mapping_and_navigation/#funmap-overview","title":"FUNMAP Overview","text":"<p>The other option for mapping and navigation with Stretch is to use the FUNMAP package. FUNMAP, which stands for \"Fast Unified Navigation, Mapping, and Planning\", is a software package built to take advantages of some of the unique elements of Stretch, including the eye-height D435if depth camera in Stretch's head.</p> <p>To build our map in FUNMAP, first make sure the robot is in a clear area where it can move and rotate the mobile base freely. First run the following command in the terminal:</p> <pre><code>ros2 launch stretch_funmap mapping.launch.py\n</code></pre> <p>Second, open a new terminal window or tab, and launch the keyboard operation node:</p> <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p mapping_on:=True\n</code></pre> <p>Once Rviz has launched, press the spacebar while in the same terminal window to initiate a head scan - commanding the robot to create a map by panning the depth camera around the room, then rotating the mobile base and panning the camera again to overcome the blindspot due to the mast.</p> <p>At this point, you should see a 3D map resulting from the head scan in RViz. You can click and drag in the window to rotate it around and look at it. It has been created by merging many 3D scans from the head camera.</p> <p>You can now specify a navigation goal for the robot. If the robot finds a navigation plan to the goal, it will attempt to navigate to it. While navigating, it will use the depth camera to avoid obstacles dynamic obstacles that walk in front of the robot. In RViz, press the \"2D Nav Goal\" button on the top bar with a magenta arrow icon, then specify a nearby navigation goal pose on the floor of the map by clicking and drawing a magenta arrow. For this to work, the navigation goal must be in a place that the robot can find a path to reach and that the robot has scanned well. For example, the robot will only navigate across floor regions that it has in its map.</p> <p>If the robot succesfully plans a path, it will display in RViz as green lines connecting white spheres, and the robot will begin moving along this path until it reaches the goal you provided.</p> <p>Now lets continue building our map. With the robot in this new location, go back to the terminal window and press spacebar again, to initiate a new head scan. The robot will follow the same procedure as before, and data from the new location will be added to the map. In this manner, you can map around obstacles, larger areas, etc. You can continue this process until the entire area of interest is mapped and well-represented in the map.</p>"},{"location":"getting_started/demos_mapping_and_navigation/#saving-a-map","title":"Saving a Map","text":"<p>FUNMAP maps are top-down images, where the pixel value represents the maximum observed 3D point at that location. Maps are automatically saved when generated - the default location is: </p> <pre><code>~/stretch_user/debug/merged_maps/\n</code></pre> <p>You can see the image representations by looking at files with the following naming pattern:</p> <pre><code>~/stretch_user/debug/merged_maps/merged_map_&lt;DATETIME&gt;_mhi_visualization.png\n</code></pre>"},{"location":"getting_started/demos_mapping_and_navigation/#additional-information","title":"Additional Information","text":"<p>To read more about Nav2, FUNMAP, and mapping and navigation with Stretch, check out the Choosing your Navigation &amp; Motion Planner guide.</p>"},{"location":"getting_started/demos_mapping_and_navigation/#next-steps","title":"Next Steps","text":"<p>In the next guide, Demo #2 - Web Teleop, we will teleoperate the robot from a web browser or mobile phone using Stretch's Web Teleoperation interface.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"getting_started/demos_web_teleop/","title":"Demo #2: Web Teleop","text":"<p>In this demo, we'll introduce the Stretch Web Teleop codebase and try out the interface.</p>"},{"location":"getting_started/demos_web_teleop/#background","title":"Background","text":"<p>Stretch has a website for teleoperating the robot from a web browser. This website can be set up to teleoperate the robot remotely from anywhere in the world with an internet connection, or simply eyes-off teleop from the next room on a local network. The codebase is built on ROS 2, WebRTC, Nav2, and TypeScript.</p>"},{"location":"getting_started/demos_web_teleop/#launching-the-interface","title":"Launching the interface","text":"<p>First, navigate to the folder containing the codebase using:</p> <pre><code>colcon_cd stretch_web_teleop\n</code></pre> <p>Next, launch the interface (if you captured a map in the previous tutorial, use <code>./launch_interface -m ${HELLO_FLEET_PATH}/maps/nav2_demo_map.yaml</code> instead):</p> <pre><code>./launch_interface.sh\n</code></pre> <p>In the terminal, you will see output similar to:</p> <pre><code>Visit the URL(s) below to see the web interface:\nhttps://localhost/operator\nhttps://192.168.1.14/operator\n</code></pre> <p>Look for a URL like <code>https://&lt;ip_address&gt;/operator</code>. Visit this URL in a web browser on your personal laptop or desktop to see the web interface. Ensure your personal computer is connected to the same network as Stretch. You might see a warning that says \"Your connection is not private\". If you do, click <code>Advanced</code> and <code>Proceed</code>.</p> <p>Once you're done with the interface, close the browser and run:</p> <pre><code>./stop_interface.sh\n</code></pre> <p>Note: Only one browser can be connected to the interface at a time.</p>"},{"location":"getting_started/demos_web_teleop/#usage-guide","title":"Usage Guide","text":"<p>The web interface currently has a variety of control modes, displays and customization options. This tutorial will explain how to use the standard version of the interface that appears when you load it for the first time.</p>"},{"location":"getting_started/demos_web_teleop/#overview-of-layout","title":"Overview of Layout","text":"<p>There are three panels. The <code>Camera Views</code> panel contains the wide angle and gripper camera views. The second panel has three tabs: (1) <code>Base</code>, (2) <code>Wrist &amp; Gripper</code>, and (3) <code>Arm &amp; Lift</code>. Each of these tabs contains a button pad for controlling the respective joints. The <code>Safety</code> panel contains the run stop and battery gauge. The header contains a drop down for three action modes, the speed controls (<code>Slowest</code>, <code>Slow</code>, <code>Medium</code>, <code>Fast</code>, and <code>Fastest</code>) and a button to enable the customization mode.</p>"},{"location":"getting_started/demos_web_teleop/#wide-angle-camera-view","title":"Wide-Angle Camera View","text":"<p>The wide angle camera is attached to the robot's head which can pan and tilt. There are four buttons bordering the camera feed the will pan and tilt the camera.</p> <p> </p>"},{"location":"getting_started/demos_web_teleop/#quick-look","title":"Quick Look","text":"<p>There are three built-in quick look options: <code>Look Ahead</code>, <code>Look at Base</code> and <code>Look at Gripper</code>. </p> <p> </p>"},{"location":"getting_started/demos_web_teleop/#follow-gripper","title":"Follow Gripper","text":"<p>The <code>follow gripper</code> button will automatically pan/tilt the head to focus on the gripper as the arm is moved. This is can be useful when trying to pick something up.</p> <p> </p>"},{"location":"getting_started/demos_web_teleop/#predictive-display","title":"Predictive Display","text":"<p>The 'predictive display' mode will overlay a trajectory over the video stream that Stretch will follow. Stretch's speed and heading will depend on the length and curve of the trajectory. Stretch will move faster the longer the trajectory is and slower the shorter the trajectory is. The trajectory will turn red when you click and the robot is moving. The robot will rotate in place when you click on the base and will move backwards when you click behind the base. In the <code>press-release</code> and <code>click-click</code> action modes you can move the cursor to update the trajectory while the robot is moving. Additionally, you can scale the speed by selecting one of the speed controls. </p> <p> </p>"},{"location":"getting_started/demos_web_teleop/#gripper-camera","title":"Gripper Camera","text":"<p>There are two quick actions for the gripper camera view: (1) <code>center wrist</code> and (2) <code>stow wrist</code>. Center wrist will turn the wrist out and align it with the arm. Stow wrist will rotate the wrist to the stow position.</p> <p> </p>"},{"location":"getting_started/demos_web_teleop/#button-pads","title":"Button Pads","text":"<p>Each button pad controls a different set of joints on the robot. When you click a button the robot will move and the button will highlight blue while the robot is moving. The button will turn red when the respective joint is at its limit. </p> Drive Dex Wrist Arm &amp; Lift"},{"location":"getting_started/demos_web_teleop/#action-modes","title":"Action Modes","text":"<p>The action modes can be selected in the dropdown in the top-left corner of the interface. The action modes provides varying degrees of discrete and continuous control.</p> <ul> <li>Step Actions: When you click, Stretch will move a fixed distance based on the selected speed.</li> <li>Press-Release: Stretch will move while you are pressing and holding the button and will stop when you release.</li> <li>Click-Click: Stretch will start moving when you click and will stop when you click again. You can also stop Stretch by moving the cursor outside the button you clicked.</li> </ul>"},{"location":"getting_started/demos_web_teleop/#troubleshooting","title":"Troubleshooting","text":"<p>If you're having trouble with the steps in the guide, please check the following tips:</p>"},{"location":"getting_started/demos_web_teleop/#stretch-fails-launch-the-interface","title":"Stretch fails launch the interface","text":"<p>The interface will have created a .zip file with its logs. Please contact Hello Robot support and include the file. The log files will typically include information on what went wrong.</p>"},{"location":"getting_started/getting_help/","title":"Getting Help","text":"<p>If you run into any issues or find yourself in need of additional resources while developing with Stretch, we want to hear from you! </p>"},{"location":"getting_started/getting_help/#forum","title":"Forum","text":"<p>One of the best parts of working with Stretch is the active and helpful user community. We encourage all our users to make use of the Hello Robot Forum, where you can post your own questions or search an archive of resolved issues. Our engineers regularly read and respond to threads here, as well as posting helpful tips and tricks in the Knowledge Base. </p>"},{"location":"getting_started/getting_help/#support","title":"Support","text":"<p>If you've run into a hardware problem, need urgent assistance, or just can't find the answer to your questions and want to chat directly with a Hello Robot engineer, please feel free to contact us directly at support@hello-robot.com. We love talking to Stretch users, and your feedback helps us improve the product for all our users.</p>"},{"location":"getting_started/getting_help/#next-steps","title":"Next Steps","text":"<p>This concludes the guides in our Quickstart tutorial. We recommend continuing to learn about Stretch in one of the following sections:</p> <ul> <li>Developing with Stretch gives additional information and recommendations about how to approach writing software for Stretch.</li> <li>ROS 2 Tutorials and Python Tutorials explain how to use the respective APIs to start coding and accessing some of Stretch's more advanced features.</li> </ul>"},{"location":"getting_started/hello_robot/","title":"Hello Robot!","text":"<p>Congratulations on your new robot, and welcome to the community of Stretch developers! This guide is intended to be the starting point for a new user to explore the features and capabilities of the Stretch 3 robot.</p>"},{"location":"getting_started/hello_robot/#introducing-stretch","title":"Introducing Stretch","text":"<p>Meet your new robot! Stretch is a mobile manipulator - a robot consisting of a mobile base as well as an arm and tool, capable of both navigating and interacting with its environment. The design of Stretch is intended to facilitate interactions with people and in human environments. Its compact footprint and slender manipulator have unique advantages for working in cluttered, real-world spaces. The lightweight body and linear joints makes Stretch a friendlier and less complicated robot.</p> <p>Over the course of these Getting Started tutorials, we will explore the basics of working with Stretch, get familiar with all of its joints and sensors, and run some exciting demonstrations of some of Stretch's more advanced features. In this first tutorial, we'll learn how to turn on the robot and drive it around using the gamepad controller.</p>"},{"location":"getting_started/hello_robot/#before-you-begin","title":"Before You Begin","text":""},{"location":"getting_started/hello_robot/#safety","title":"Safety","text":"<p>Stretch has the potential to cause harm if not properly used. We recommend that all users review the Stretch Safety Guide before operating the robot.</p>"},{"location":"getting_started/hello_robot/#unboxing","title":"Unboxing","text":"<p>If you need assistance unboxing the robot, please refer to the Stretch unboxing video.</p>"},{"location":"getting_started/hello_robot/#powering-up","title":"Powering Up","text":"<p>Before we begin, confirm that all of the packing material has been removed from the robot, and the area around the robot is free and clear from any furniture or obstacles.</p> <p>To power up the robot, press the On/Off switch found in the base. The LED on the switch should illuminate. The robot may take 20-30 seconds to fully boot, at which point you should hear the Ubuntu startup sound<sup>1</sup> play over the robot speakers.</p> <p></p>"},{"location":"getting_started/hello_robot/#safety-features","title":"Safety Features","text":"<p>Before we start operating Stretch, there are two features that are important to understand for the safe use of the robot - the Runstop button and the battery indicator lightbar.</p>"},{"location":"getting_started/hello_robot/#runstop-button","title":"Runstop Button","text":"<p>The Runstop button is located on the side of the robot's head. Normally, this button will be steadily illuminated white. Pressing this button puts the robot into \"runstop\"; it interrupts the motion of the robot's primary joints during operation, causing it to stop moving and making all of these joints backdrivable. Just tap it, you'll hear a beep and the button will start flashing. You can now freely move the arm, lift, and wheels of the robot.</p> <p>This can be useful if the robot makes an unsafe motion, or if you just want to roll the robot around or reposition its arm. To disable runstop, hold the button down for two to three seconds. After the beep, the button will illuminate steadily again and motion can resume.</p>"},{"location":"getting_started/hello_robot/#battery-indicator-lightbar","title":"Battery Indicator Lightbar","text":"<p>The battery indicator lightbar in the base provides a simple way to quickly ascertain the robot's battery level. Its color always indicates the battery voltage. Green indicates a fully charged robot, yellow indicates a robot whose charge is starting to decline, and orange-red indicates a robot that should be plugged into its battery charger immediately.</p> <p>It is important not to run the robot fully out of battery, which will cause the battery capacity to degrade over time, or occasionally lead to more serious electrical damage to the robot. If you notice that your battery indicator starts to look orange or red during this session, please skip ahead to the Battery Charging Basics section of this guide.</p> <p>The lightbar will also flash as follows to indicate the state of the robot:</p> Mode Flashing Gif Normal Operation Solid Runstop is active Rapid blink at 1 Hz Charger plugged in Slow strobe at 0.5 Hz"},{"location":"getting_started/hello_robot/#gamepad-teleoperation","title":"Gamepad Teleoperation","text":"<p>Stretch comes ready to drive out of the box. Use the included gamepad controller to drive the robot and quickly test out some of its capabilities.</p> <p>To start the demo:</p> <ol> <li>Find the gamepad controller that shipped with Stretch.</li> <li> <p>If you have not already done so, turn the robot on and wait for 20-30 seconds. You will hear the Ubuntu startup sound<sup>1</sup>, followed by two beeps (indicating that the demo is running).</p> <p>Note</p> <p>If you don't hear the two beeps, or you are following this guide on a robot that has already been used by someone else, gamepad teleop may not be configured to launch when the robot is booting up. See the troubleshooting advice below.</p> </li> <li> <p>Press the center \"Connect\" button on the controller. It will look like either:</p> <ul> <li><ul> <li>The upper two LEDs of the ring will illuminate, indicating that a connection has been made to the USB dongle in the robot's trunk.</li> </ul> </li> <li><ul> <li>The button will illuminate solid, indicating that a connection has been made to the USB dongle in the robot's trunk.</li> </ul> </li> <li>If the button or LED ring instead flashes, it needs to be paired with the dongle. Hold the center button for 6 seconds until the controller vibrates, and the upper two LEDs illuminates steadily. If the dongle has a button on it, you may need to press it beforehand to put in pairing mode as well.</li> </ul> </li> <li> <p>Make sure the space around the robot is clear. Hit the Home Robot  button (\"Start\" on the gamepad). Stretch will begin its homing routine, finding the zero position of all of its joints. Be careful not to interfere with these movements, as it may cause the zero position to be set incorrectly. When this procedure is completed (after ~30 seconds), the robot should beep once.</p> <p>Note</p> <p>If nothing happens when you hit the Start button, see the troubleshooting advice below.</p> </li> <li> <p>Stretch is now ready to drive! This image shows the mapping between the controller's buttons and the robot's joints:</p> </li> </ol> <p></p> <p>Here's a few things to try:</p> <ul> <li>Hold the Stow Robot  button for 2 seconds. The robot will assume a stowed pose, with the arm low and the gripper tucked inside the robot's footprint. This position keeps the robot compact and its center of gravity low, so we recommend it when driving the robot base.</li> <li>Practice driving the robot around using the Mobile Base  joystick.</li> <li>While driving the mobile base, hold down on the Fast Base  trigger. When the shoulder is low and the arm fully retracted, Stretch is able to drive at faster speeds safely.</li> <li>Practice positioning the arm using the Lift and Arm  joystick.</li> <li>Practice positioning the wrist using the Wrist Yaw  &amp;  shoulder buttons, and Wrist Pitch &amp; Roll  D-Pad.</li> <li>Press the  button to switch the D-pad from Wrist Control to Head Control - now use the  D-pad to control the Head Pan &amp; Tilt joints of the head camera. Press the  button again to switch back to wrist control.</li> <li>Try grasping an object from the ground or a countertop. Use the  button to close the gripper. And use the  button to open the gripper.</li> <li>When close to the object, hold down on the Precision Mode  trigger - this will slow the motion of all of the robot joints, so you can easily line up even a difficult grasp.</li> <li>Try delivering an object to a person.</li> <li>Try a task that requires you to use two or more of the robot's joints at once, like opening a door or cabinet.</li> <li>Try pressing the Runstop button while moving the robot, then reset it by holding the button down for 2 seconds.</li> </ul>"},{"location":"getting_started/hello_robot/#shutting-down-stretch","title":"Shutting Down Stretch","text":"<p>Once you're finished with the gamepad teleoperation demo, let's learn how to properly shut down the robot. It is recommended to always wait for the onboard PC to fully shutdown prior to hitting the On/Off switch in the trunk (which will cut off power). From the gamepad teleoperation demo, you can simply hold down the Shutdown PC  button (\"Back\" on the gamepad) for 2 seconds. The robot will move to its Stow position, and then the PC will turn off. When the lidar stop spinning, the PC has shutdown.</p> <p>If you're working with a Stretch 3, turn off the robot with the On/Off switch and the lift joint will slowly descend. If you're working with a Stretch 2 or Stretch RE1, place the included clamp below the shoulder before turning off the On/Off switch because the lift joint will descend rapidly. </p> <p></p>"},{"location":"getting_started/hello_robot/#battery-charging-basics","title":"Battery Charging Basics","text":"<p>Stretch contains two 12V sealed lead-acid (SLA) batteries. For the health of your robot, is is important to keep Stretch charged and not to let the battery voltage run down too low. It is good practice to plug in Stretch whenever you are not using the robot - the charger will automatically maintain the batteries and keep them topped off and healthy.</p> <p>Stretch comes with a NOCO Genius10 battery charger with a custom cable. Unbox this charger and set it next to the robot. Plug the charger into the wall, then attach the other end into the robot's charging port.</p> <p></p> <p>Because the robot is now powered off, press the Mode button until the 12V AGM mode is illuminated. This is the recommended mode whenever the robot is turned off.</p> <p>When the robot is on, switch the charger into Supply mode as indicated, by holding down and then pressing the Mode button.</p>"},{"location":"getting_started/hello_robot/#moving-stretch","title":"Moving Stretch","text":"<p>Stretch can be transported between different rooms, buildings, or research sites. Here are some suggestions for moving Stretch around more easily:</p> <ul> <li>Over short distances and on smooth floors, backdriving Stretch is the easiest approach. With the robot powered off, or with the Runstop enabled, the wheels can be backdriven freely. Grab the robot by its mast, tilt it over, and push or pull it to where it needs to go.</li> <li>Over longer distances or more difficult terrain, we would recommend the use of a hand truck. For extra protection, place the robot base into the foam block that it was shipped in, and use the truck to move it around securely.</li> <li>Stretch can be transported by car. We recommend using the foam blocks from the box Stretch was shipped in when laying the robot down, so the mast is propped up and no weight is on the robot's arm and gripper.<ul> <li></li> </ul> </li> <li>Flying with Stretch is not typically recommended - the packaged robot is outside of normal weight and dimensional restrictions for checked luggage, and most passenger airlines have specific rules around shipping lead-acid batteries in equipment. It is usually much easier to repackage Stretch in its original packing material and ship it to your destination. Hello Robot can provide a repackaging guide on request.</li> <li>Stretch is not waterproof! If transporting it outside, make sure to keep it safe from the elements.</li> </ul>"},{"location":"getting_started/hello_robot/#next-steps","title":"Next Steps","text":"<p>The next guide in our Getting Started tutorial series, Connecting to Stretch, will explain how to connect to the Stretch PC to explore its functions in more detail. We recommend all new users complete the Getting Started series before beginning to develop with the robot.</p>"},{"location":"getting_started/hello_robot/#troubleshooting","title":"Troubleshooting","text":"<p>If you're having trouble with the steps in the guide, please check the following tips:</p>"},{"location":"getting_started/hello_robot/#stretch-never-played-the-ubuntu-startup-sound","title":"Stretch never played the Ubuntu startup sound","text":"<ul> <li>The robot may still be booting up, but is delayed due to a software update or some sort of error - wait up to 2 minutes to be sure</li> <li>If the audio cue never played, it may just have been disabled in software; or the volume knob in the head may have been turned down all the way. Try continuing with the guide.<ul> <li></li> </ul> </li> </ul>"},{"location":"getting_started/hello_robot/#i-dont-hear-two-beeps-when-the-robot-powers-on","title":"I don't hear two beeps when the robot powers on","text":"<ul> <li>The gamepad controller script might be disabled - this is common on a robot that others have used. You'll need to run <code>stretch_gamepad_teleop.py</code> in a Terminal window on the robot. See the next tutorial, Connecting to Stretch, for details on how to accomplish this.</li> </ul>"},{"location":"getting_started/hello_robot/#nothing-happens-when-i-try-to-home-the-robot","title":"Nothing happens when I try to home the robot","text":"<ul> <li>If the robot has already been homed, pressing the Home Robot  button won't do anything. Try moving the other controller buttons - if the robot moves then you are already good to go.</li> <li>The gamepad controller script might be disabled - this is common on a robot that others have used. You'll need to run <code>stretch_gamepad_teleop.py</code> in a Terminal window on the robot. See the next tutorial, Connecting to Stretch, for details on how to accomplish this.</li> </ul>"},{"location":"getting_started/hello_robot/#the-gamepad-connection-is-intermittent-or-stops-working","title":"The gamepad connection is intermittent or stops working","text":"<ul> <li>The gamepad connection works best at distances under 30ft, and can be obstructed by walls or large objects - try to have a clear line of sight to the robot.</li> <li>If the center button starts flashing, or if no lights illuminate at all, this indicates low controller battery. Plug the controller into a USB port using the included USB-micro cable and leave it to charge for at least 15 minutes, then try again.</li> <li>If the controller stops responding, make sure the USB dongle in the robot trunk is still securely attached, and make sure the Runstop function is not enabled (Runstop button and the battery indicator lightbar will flash if runstop is enabled).</li> </ul>"},{"location":"getting_started/hello_robot/#the-gamepad-button-mapping-feels-incorrect","title":"The gamepad button mapping feels incorrect","text":"<ul> <li>Check to see which of the LEDs around the center button are illuminated. In the correct gamepad mode, the top two lights should be on and the bottom two should be off. If you see a different pattern (top-left and bottom-right for instance) then the controller is in the incorrect mode. To resolve this, hold down the center button for at least five seconds, until the LEDs begin flashing. Release the botton and check the LED pattern again. You may need to do this two or three times before the controller returns to the correct mode.<ul> <li>The correct mode will have the following pattern:</li> <li></li> </ul> </li> </ul>"},{"location":"getting_started/hello_robot/#my-stretch-doesnt-have-a-battery-indicator-lightbar","title":"My Stretch doesn't have a Battery Indicator Lightbar","text":"<ul> <li>Stretch RE1, the first model of Stretch, does not have an Battery Indicator Lightbar. Instead, you'll need to run <code>stretch_robot_battery_check.py</code> in a Terminal window on the robot. See the next tutorial, Connecting to Stretch, for details on how to accomplish this.</li> </ul>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. <ol> <li> <p>The audio asset 'desktop-login.ogg' is provided by the Ubuntu Sounds package under the CCPL license. The original license is available at this link: https://git.launchpad.net/ubuntu/+source/ubuntu-sounds/tree/COPYING \u21a9\u21a9</p> </li> </ol>"},{"location":"getting_started/stretch_hardware_overview/","title":"Stretch Hardware Overview","text":"<p>This guide will walk you through Stretch's hardware from top to bottom, including some simple command line tools you can use to interact with the robot.</p> <p>To complete this tutorial, you will need a monitor and keyboard/mouse, or a wireless connection to the robot, as discussed in the previous guide.</p>"},{"location":"getting_started/stretch_hardware_overview/#powering-up","title":"Powering Up","text":"<p>Turn on Stretch if the robot isn't already powered on. Because the robot is now on, switch the battery charger into \"Supply\" mode.</p> <p></p> <p>If the NUC goes to sleep, it may ask for a password upon waking up. The default user login credentials came in the box with the robot.</p>"},{"location":"getting_started/stretch_hardware_overview/#turning-off-gamepad-teleoperation","title":"Turning off Gamepad Teleoperation","text":"<p>Out of the box, Stretch is configured to launch the gamepad teleoperation demo in the background at startup. While this is running, other code cannot use the robot. You will need to free the \"robot process\" so that your code can use it. Open a Terminal window and input the following:</p> <pre><code>stretch_free_robot_process.py\n</code></pre> <p>You can also disable the autoboot feature that starts gamepad teleop everytime the robot is booted. Search for 'Startup Applications' from the Apps menu. Uncheck the box for 'hello_robot_gamepad_teleop'.</p> <p></p>"},{"location":"getting_started/stretch_hardware_overview/#system-check","title":"System Check","text":"<p>First, let's check the status of our robot. This is a powerful tool that runs a series of hardware and software checks to confirm that the system is ready for use.</p> <p>In a Terminal window, run the command:</p> <pre><code>stretch_system_check.py\n</code></pre> <p>If all checks pass, you know your robot is ready to go.</p>"},{"location":"getting_started/stretch_hardware_overview/#homing","title":"Homing","text":"<p>If your robot needs to be homed, the system check will tell you. The homing procedure is a ~30 second procedure run once every time the robot is powered on, and it will find the zero position of all of Stretch's joints. If you ran the gamepad teleop demo earlier, then you have already seen this homing behavior when starting the demo.</p> <p>To home the robot, first make sure the robot is in a clear position where the lift, arm, and wrist can move freely without collisions. In a Terminal window, run the command:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>The robot will beep when finished. Run a system check one more time - the output should now indicate that all of the motors are homed.</p> <p>The motors will remember their homing state while the robot remains powered on - using the Runstop, backdriving the robot, or even restarting the PC will not require rehoming.</p>"},{"location":"getting_started/stretch_hardware_overview/#motors-and-joints","title":"Motors and Joints","text":"<p>Next, let's explore the robot hardware, starting with the actuators.</p>"},{"location":"getting_started/stretch_hardware_overview/#stepper-motors","title":"Stepper Motors","text":""},{"location":"getting_started/stretch_hardware_overview/#base","title":"Base","text":"<p>Stretch has a differential-drive base, consisting of two closed-loop stepper motors driving the left and right wheels. The third wheel, located at the back of the robot, is an passive omni wheel that allows motion in any direction.</p>"},{"location":"getting_started/stretch_hardware_overview/#lift-and-arm","title":"Lift and Arm","text":"<p>Stretch has a lift degree of freedom that provides vertical translation of the arm. It is driven by a closed-loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive.</p> <p>The arm degree of freedom comprises 5 telescoping links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion.</p> <p>In combination, the lift, arm, and mobile base translation provide three orthogonal axes of motion - in effect, a Cartesian system for end-effector placement.</p>"},{"location":"getting_started/stretch_hardware_overview/#contact-sensitivity","title":"Contact Sensitivity","text":"<p>Stretch's lift and arm joints have a contact detection system that we call Guarded Contact. This safety feature is designed to limit the unwanted forces Stretch could apply to a person or its environment, and can be tuned or configured for your application.</p> <p>The Guarded Contact behavior uses current sensing to determine if actuator effort exceeds a user-specified threshold during joint motion, and enables the safety controller for the joint until a subsequent movement command is received.</p> <p>To test this system out, launch the gamepad demo again, this time by running <code>stretch_gamepad_teleop.py</code> from the Terminal. Try driving the arm and lift joint, then impeding the motion. After a moderate amount of force, Stretch should halt the motion. When you're done, quit the program by pressing Ctrl+C in the Terminal window.</p>"},{"location":"getting_started/stretch_hardware_overview/#dynamixel-motors","title":"Dynamixel Motors","text":"<p>The actuators for the head, wrist, and gripper joints on Stretch are Dynamixel servo motors.</p>"},{"location":"getting_started/stretch_hardware_overview/#head-camera","title":"Head Camera","text":"<p>Stretch has a camera module on a pan-tilt mechanism controlled by Dynamixel actuators.</p> Axis Range Head Pan 346deg Head Tilt 115deg <p></p>"},{"location":"getting_started/stretch_hardware_overview/#dexterous-wrist","title":"Dexterous Wrist","text":"<p>Stretch has a three degree-of-freedom wrist with yaw, pitch, and roll actuation. All of these joints use Dynamixel actuators.</p> Axis Range Graphic Wrist Yaw 340deg Wrist Pitch 100deg Wrist Roll 340deg"},{"location":"getting_started/stretch_hardware_overview/#gripper","title":"Gripper","text":"<p>The compliant gripper is a robust and compliant single-degree-of-freedom tool. A Dynamixel actuator drives the center of the spring mechanism, which causes the outer fingers to flex and provide a grasping force. ArUco tags attached to each finger can be detected by the gripper camera to provide precise fingertip positions.</p>"},{"location":"getting_started/stretch_hardware_overview/#dynamixel-errors-and-reset-tool","title":"Dynamixel Errors and Reset Tool","text":"<p>While using Stretch, sometimes the Dynamixel motors may end up in an error state. The most common causes of this are over-force errors (usually from attempting to apply a very high force on one joint for more than a couple of seconds) or over-temperature errors (usually from attempting to maintain a high force on one joint for an extended period of time). When a motor encounters this error, it goes into a backdrivable state and stops responding to commands, and the error LED on the motor body blinks red.</p> <p>Powering down the robot and powering it back on will clear this error (notably, rebooting the robot PC will NOT as it will not disconnect the power). A more convenient way is to run the following command in the terminal:</p> <pre><code>stretch_robot_dynamixel_reboot.py\n</code></pre> <p>This will reboot all of Stretch's Dynamixel motors, resetting their error status. Doing this will also clear the homing position from the wrist_yaw and gripper joints, so you will want to re-run the <code>stretch_robot_home.py</code> script after rebooting the servos.</p>"},{"location":"getting_started/stretch_hardware_overview/#sensors","title":"Sensors","text":"<p>Stretch includes a number of sensors that allow it to perceive the environment:</p> <ul> <li>Intel Realsense D435if Active Depth Camera</li> <li>Intel Realsense D405 Passive Stereo Depth Camera</li> <li>Arducam OV9782 Wide-angle Global Shutter Color Camera</li> <li>Slamtec RPLIDAR A1 2D 360deg Lidar</li> <li>Respeaker v2.0 Microphone Array</li> <li>Bosch BNO085 9-DOF IMU</li> <li>Analog ADXL343 Accelerometer</li> <li>Sharp GP2Y0A51SK0F Infrared Cliff Sensors</li> </ul>"},{"location":"getting_started/stretch_hardware_overview/#d435if-depth-camera","title":"D435if Depth Camera","text":"<p>An Intel RealSense D435if Depth + RGB Camera is attached to the pan-tilt apparatus in the robot head. With a wide field of view, range up to 10m, active IR projector and global shutter depth sensor, the D435 is an ideal camera for indoor robotics applications. The IF model indicates that the camera has both an internal IMU, and an IR pass filter to improve depth performance.The camera is mounted vertically, so that the robot can see its own body and what is ahead of it simultaneously.</p> <p>To quickly visualize the RGB and depth streams from this camera, you can run the following command in the Terminal:</p> <pre><code>stretch_realsense_visualizer.py\n</code></pre>"},{"location":"getting_started/stretch_hardware_overview/#d405-depth-camera","title":"D405 Depth Camera","text":"<p>Stretch has a second depth camera attached to its gripper - an Intel RealSense D405 Depth Camera. This camera has an ideal range of 7cm to 50cm, perfect for in-hand manipulation tasks.</p> <pre><code>stretch_camera_streams_check.py --d405\n</code></pre>"},{"location":"getting_started/stretch_hardware_overview/#arducam-wide-angle-color-camera","title":"Arducam Wide-angle Color Camera","text":"<p>Stretch has a third camera, attached to the same pan-tilt apparatus as the D435if. This smaller camera is an Arducam 1MP RGB camera with a wide-angle lens and global shutter. It is primarily useful when operating the robot remotely, as we will see in an upcoming Web Interface Demo tutorial.</p> <pre><code>stretch_camera_streams_check.py --navigation\n</code></pre>"},{"location":"getting_started/stretch_hardware_overview/#rplidar-a1","title":"RPLIDAR A1","text":"<p>On Stretch's mobile base there is a 2D lidar - the Slamtec RPLIDAR A1 model. This lidar has a range of 0.15-12m and an angular resolution of 1deg. The lidar spins when in use, and can be turned on and off programatically.</p> <pre><code>stretch_rp_lidar_jog.py --motor_on\n</code></pre> <p>In an upcoming Mapping &amp; Navigation Demo tutorial, we will learn how to map an area and navigate through it autonomously using the lidar.</p>"},{"location":"getting_started/stretch_hardware_overview/#respeaker-microphone-array","title":"ReSpeaker Microphone Array","text":"<p>On the top of the robot head, there is a ReSpeaker v2.0 Microphone Array, a far-field array of four microphones capable of detecting voices up to 5m away. The 12 RGB LEDs in the light ring are programmable. By default, a simple audio localization algorithm illuminates the LEDs when noise is made near the robot and highlights the single LED closest to the direction of sound.</p> <pre><code>stretch_respeaker_test.py\n</code></pre>"},{"location":"getting_started/stretch_hardware_overview/#speakers","title":"Speakers","text":"<p>Also on the robot head are a pair of stereo speakers for audio output. On the top of the head, next to the USB port, is a rotary knob that you can turn to adjust the volume.</p> <p></p> <p>To quickly test the speaker output, run the following command:</p> <pre><code>stretch_audio_test.py\n</code></pre>"},{"location":"getting_started/stretch_hardware_overview/#base-imu","title":"Base IMU","text":"<p>The mobile base has a 9-DoF IMU using the Bosch BNO085 chipset. This is the same chipset used on the Adafruit IMU fusion board.</p> <p></p> <p>The IMU combines an accelerometer with a gyroscope and magnetometer, to measure the robot base's velocity, orientation, and rotation with respect to gravity.</p> <pre><code>stretch_pimu_scope.py --gx\n</code></pre>"},{"location":"getting_started/stretch_hardware_overview/#wrist-accelerometer","title":"Wrist Accelerometer","text":"<p>The wrist includes a 3 axis accelerometer using the Analog ADXL343 chipset which provides bump and tap detection capabilities. The sensor is mounted inside the distal link of the arm.</p> <pre><code>stretch_wacc_scope.py --ax\n</code></pre>"},{"location":"getting_started/stretch_hardware_overview/#cliff-sensors","title":"Cliff Sensors","text":"<p>Stretch has four Sharp GP2Y0A51SK0F IR cliff sensors pointed toward the floor. These report the distance to the floor, allowing for the detection of thresholds, stair edges, etc.</p> <p>While these sensors are calibrated at the Hello Robot factory, their readings are highly dependent on the type of floor surface where Stretch is being used. These sensors can be calibrated to the robot's new environment.</p> <p>Because using these sensors to detect hazardous terrain relies on several external factors (e.g. floor surface, velocity of the robot, type of obstacle) and can be risky for the robot and those around it, cliff detection is disabled by default.</p> <pre><code>stretch_pimu_scope.py --cliff\n</code></pre>"},{"location":"getting_started/stretch_hardware_overview/#developer-io","title":"Developer I/O","text":"<p>Stretch contains several additional ports connected to the onboard PC that can be used for additional hardware or accessories:</p> <ul> <li> <p>In the trunk, there are 4 USB-A 3.2 ports, an Ethernet port, and an HDMI port. If needed, battery (12V @ 5A) power can also be accessed from inside the robot base by removing the rubber plug in the side of the trunk.</p> <ul> <li></li> </ul> </li> <li> <p>In the shoulder, there is a USB-A 2.0 port and battery (12V @ 3A) power.</p> <ul> <li></li> </ul> </li> <li> <p>At the end-of-arm, there is a USB-A 2.0 port (typically used by the gripper camera), as well as an Arduino that can be accessed through a 10-pin JST header (12V @ 500mA, SPI, I2C, and digital IO).</p> <ul> <li></li> </ul> </li> <li> <p>On top of the head, there is a USB-A 2.0 port (typically used by the RGB camera), as well as battery (12V @ 3A) power.</p> <ul> <li></li> </ul> </li> <li> <p>At the wrist, there's a quick-connect mechanism, as well as a Dynamixel X-Series TTL chain. The Extending Stretch tutorials contain information on designing your own tool for the quick-connect mechanism.</p> <ul> <li> </li> </ul> </li> <li> <p>And there's threaded mounting points on the base, shoulder, head, and wrist.</p> </li> </ul>"},{"location":"getting_started/stretch_hardware_overview/#next-steps","title":"Next Steps","text":"<p>In the next tutorial, Writing Code, we will write a simple Python program to actuate all of the different joints of the robot.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"getting_started/wallpapers/","title":"Wallpapers","text":"<p>If you'd like to change your Desktop's background to a Stretch-themed wallpaper, check out the options below. You can download them in full resolution by Right Click -&gt; Save Image as...</p>"},{"location":"getting_started/wallpapers/#stretch-3-background-v3","title":"Stretch 3 Background V3","text":""},{"location":"getting_started/wallpapers/#stretch-re1-and-2-background-v3","title":"Stretch RE1 and 2 Background V3","text":""},{"location":"getting_started/writing_code/","title":"Writing Code","text":"<p>This tutorial will introduce the two most common ways of developing software with Stretch - Python and ROS 2. The Stretch Body Robot API will be introduced, and you will write a simple Python program using commands to move the various joints of the robot.</p>"},{"location":"getting_started/writing_code/#background","title":"Background","text":"<p>Stretch supports two different approaches to developing software - ROS 2 and Python.</p> <p>ROS 2 (Robot Operating System 2) is commonly used by robotics developers as a way to bootstrap their software development. It consists of a set of software libraries and tools for building robot applications. In technical terms, ROS is a middleware framework that is a collection of transport protocols, development and debugging tools, and open-source packages.</p> <p>Python is one of the most popular programming languages in the world. It is easy to learn, flexible, and commonly used in academic and research settings. In robotics, Python is particularly popular among the machine learning community, but it is suitable for other types of projects as well and can be a great way to get used to working with Stretch.</p> <p>You can learn more about the difference between these two approaches in Developing With Stretch. As Python for Stretch is quite straightforward and ROS 2 can have a bit of a learning curve, this guide will focus on writing a Python program for Stretch. Continuing with these tutorials, you will find examples of ROS programs in the Demos section.</p>"},{"location":"getting_started/writing_code/#stretch-body","title":"Stretch Body","text":"<p>The low-level Python interface for the robot is called Stretch Body, which is a Python library pre-installed on your robot. Let's write our first program exploring some of the features of this API.</p> <p>Open up a Terminal. First, let's verify that the system is ready to go:</p> <pre><code>stretch_system_check.py\n</code></pre> <p>You may need to home the robot.</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>Once the robot has homed, launch iPython. iPython is an interactive console where each line of code runs immediately.</p> <pre><code>ipython3\n</code></pre> <p>To begin, we need to initialize the Robot class which encapsulates all devices on the robot. Run the following in your iPython window, hitting enter after each line:</p> <pre><code>import stretch_body.robot\nrobot = stretch_body.robot.Robot()\nrobot.startup()\n</code></pre> <p>The <code>startup()</code> command starts communication with the robot's devices. Next, command the robot to move. Run the following:</p> <pre><code>robot.stow()\n</code></pre> <p>The robot will move to its stow position, with the shoulder lowered, arm retracted, and wrist and gripper tucked in.</p> <p>You can send commands to one joint at a time. Run the following:</p> <pre><code>robot.arm.move_to(0.25)\nrobot.push_command()\n</code></pre> <p>Move the lift joint half way up:</p> <pre><code>robot.lift.move_to(0.55)\nrobot.push_command()\n</code></pre> <p>The <code>move_to()</code> command tells one of the robot joints to go to a specific absolute position. You can also use <code>move_by()</code> to instead command the joint to move incrementally from its current position:</p> <pre><code>robot.lift.move_by(0.1)\nrobot.push_command()\n\nrobot.lift.move_by(-0.1)\nrobot.push_command()\n</code></pre> <p>You can queue up multiple movements on different joints before pushing the command to the robot. These will execute simultaneously.</p> <pre><code>robot.lift.move_to(0.7)\nrobot.arm.move_by(0.1)\nrobot.push_command()\n</code></pre> <p>If you queue up two movements on the same joint, they will overwrite the previous command and only the last will execute when the command is pushed.</p> <p>Next, you can  make the robot print out the status of a joint in a human-interpretable fashion. Try:</p> <pre><code>robot.pretty_print()\n</code></pre> <p>As you can see, this is a lot of information. You can also call pretty_print() on just one robot subsystem at a time:</p> <pre><code>robot.lift.pretty_print()\n</code></pre> <p>Moving the robot head is made easy by a number of pre-programmed positions in useful orientations. Try:</p> <pre><code>robot.head.pose('tool')\nrobot.head.pose('ahead')\nrobot.head.pose('wheels')\nrobot.head.pose('back')\n</code></pre> <p>The wrist and gripper joints are part of <code>end_of_arm</code>. Note that the following commands execute when sent - no need for <code>robot.push_command()</code>. Try:</p> <p><pre><code>robot.end_of_arm.move_to('wrist_yaw',0.5)\nrobot.end_of_arm.move_to('wrist_pitch', -0.5)\nrobot.end_of_arm.move_to('wrist_roll', 1)\nrobot.end_of_arm.move_by('wrist_roll', -1)\n\nrobot.end_of_arm.move_to('stretch_gripper',50)\nrobot.end_of_arm.move_to('stretch_gripper',-50)\n</code></pre> When you're ready to end the session, run the following command to shut down the Robot instance and stop communication with the robot's devices. This is important as leaving this running could impede another process from accessing these devices later.</p> <pre><code>robot.stop()\n</code></pre> <p>To close the iPython session, type <code>exit()</code> and press enter; or simply close the Terminal window.</p> <p>Additional information on the Stretch Body Python API is available in the Python Tutorial series.</p>"},{"location":"getting_started/writing_code/#learn-more","title":"Learn More","text":"<p>The Developing with Stretch guides are a great place to start for additional helpful information on how to approach writing software for Stretch. Tutorial tracks for working with Stretch in both Python and ROS 2 are also available on Stretch Docs.</p>"},{"location":"getting_started/writing_code/#next-steps","title":"Next Steps","text":"<p>In the next tutorial, Demo #1 - Mapping &amp; Navigation, we will look at two ways that Stretch can navigate within a map.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"hardware/battery_maintenance_guide_re1/","title":"Stretch RE1 - Battery Maintenance Guide","text":""},{"location":"hardware/battery_maintenance_guide_re1/#overview","title":"Overview","text":"<p>Stretch RE1 utilizes two 12V AGM SLA batteries that provide a combined 18AH of capacity. Maintaining an adequate level of charge on the battery system will enhance the battery lifetime.</p> <p>The run time for a fully charged system is dependent on the load use case. The majority of battery power is consumed by the NUC computer as Stretch uses relatively low power motors.</p> <p>A fully charged robot running a high CPU load can run approximately 2 hours before requiring a recharge.</p> <p>A fully charged robot powered on but with minimal load can run approximately 5 hours before requiring a recharge.</p>"},{"location":"hardware/battery_maintenance_guide_re1/#charger","title":"Charger","text":"<p>Stretch ships with a NOCO Genius 10 charger. Earlier versions of Stretch use the NOCO G7200. These two chargers are functionally very similar.</p> <p>Please review the battery charger user manuals prior to following the guidance in this document. </p> <ul> <li>Genius 10 Manual </li> <li>G7200 Manual</li> </ul> <p>Stretch utilizes four of the available modes on these chargers.</p> Mode Function STANDBY Charger not charging the robot 12V AGM Charging  robot powered down SUPPLY 1) Power the robot during tethered use2) Repair damaged batteries. REPAIR Repair damaged batteries."},{"location":"hardware/battery_maintenance_guide_re1/#noco-genius-10-interface","title":"NOCO Genius 10 - Interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached2) Press and hold MODE button for 3s3) Press MODE button until SUPPLY indicator is illuminated4) Attach charger REPAIR 1) From STANDBY, charger attached2) Press and hold MODE button for 3s3) Press MODE button until REPAIR indicator is illuminated"},{"location":"hardware/battery_maintenance_guide_re1/#noco-g7200-interface","title":"NOCO G7200 - Interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached2) Press and hold MODE button for 3s3) Press MODE button until SUPPLY indicator is illuminated4) Attach charger REPAIR 1) From STANDBY, charger attached2) Press and hold MODE button for 3s3) Press MODE button until REPAIR indicator is illuminated"},{"location":"hardware/battery_maintenance_guide_re1/#checking-the-battery-charge","title":"Checking the Battery Charge","text":"<p>The battery charger LEDs provide an approximate indicator of battery charge when it is in 12V AGM mode. Batteries are 100% fully charge when the green LED is not fading in and out.</p> <p></p> <p></p>"},{"location":"hardware/battery_maintenance_guide_re1/#charging-best-practices","title":"Charging Best Practices","text":"<p>It is possible to accidentally deeply discharge the batteries by leaving the robot on for long durations without the charger attached. This is similar to leaving the lights on your car where the battery will continue to drain until fully discharged.</p> <p>We recommend following the best practices below to avoid deep discharge of the batteries and to ensure they have a long lifespan.</p> Use Case Best Practice Reason Robot is in use - tethered Leave the charger attached in SUPPLY mode while developing on the robot whenever possible.Shutdown and power off the robot when development is done. Running the robot while attempting to charge in 12V AGM mode can cause issues and is generally bad for battery health. SUPPLY mode is preferred whenever the robot needs to be powered on. Robot is in use - untethered Regularly check the battery voltage using the command line tool.Shutdown the computer and power off the robot when voltage falls below 11.5V.Attach the charger in 12V AGM mode.Charge to 100% before resuming operation. The 12V AGM charge mode expects the battery voltage to be above 10.5-11V in order to operate. Robot is not in use Shutdown the computer and turn off the robot power. Leave the charger attached and place it in 12V AGM mode. Leaving the robot power on may cause the batteries to deep dischargeThe charger will maintain a \u2018trickle charge\u2019 on the battery, keeping the charge at 100%. Robot is coming out of storage Attach charger in 12V AGM mode and charge for 2-3 hours until charger reports 100% SLA batteries naturally lose charge over time due to \u2018self-discharge\u2019."},{"location":"hardware/battery_maintenance_guide_re1/#when-to-plug-in-the-charger","title":"When To Plug in the Charger","text":"<p>We recommend keeping the charger attached whenever the robot is not running untethered.</p> <p>When the battery voltage drops below \u2018low voltage\u2019 threshold the robot will produce an intermittent double beep sound. This is a reminder to the user to plug in the charger.</p> <p>If desired, the intermittent beep functionality can be disabled by setting the <code>stop_at_low_voltage</code> field in the User YAML to <code>0</code>.</p>"},{"location":"hardware/battery_maintenance_guide_re1/#troubleshooting","title":"Troubleshooting","text":"Issue How to Diagnose Cause Corrective Procedure Robot shows no power on activity Nothing happens when you toggle on the robot\u2019s power switch. There is no visible illumination of LEDs, motion of the laser range finder, or audible noise of the robot fans. The robot fuse may have blown. When the batteries drain the current required to maintain power goes up, which can ultimately blow the fuse. Proceed to \u201cChanging the Fuse\u201d steps below Robot powers on momentarily When you toggle on the robot\u2019s power switch some activity occurs  (illumination of LEDs, audible noise of robot fans, etc) but the computer fails to boot. The battery voltage is too low to maintain power. As the power draw increases during power-on, the voltage dips and causes the system to shut down. Connect the battery charger in 12V AGM mode and leave until fully charged. Battery won\u2019t charge in 12V AGM mode When the robot is powered down and the charger is connected in 12V AGM mode, the charger eventually switches to a different mode. The battery voltage is too low for the charger to function correctly in normal operation. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger reports 100% charge but the batteries are discharged When the robot is powered down and the charger is connected in 12V AGM mode, the charger status shows 100%.  However the robot fails to turn on properly. Damage to the batteries (usually caused by excessively low voltage) may artificially raise the open circuit voltage of the battery, causing the battery to appear fully charged, while providing low capacity. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger will not charge or stay in any mode. When placed in 12V AGM, SUPPLY, or REPAIR  mode, it continually reverts to STANDBY mode after ~ 20 minutes. Charger may be defective. Contact Hello Robot Support for a replacement."},{"location":"hardware/battery_maintenance_guide_re1/#recovering-from-low-battery-voltage","title":"Recovering from Low Battery Voltage","text":"<ol> <li>Turn off the robot power switch and detach the charger from the robot</li> <li>Place charger in SUPPLY Mode </li> <li>Allow robot to charge for 4-8 hours, or up to 24 hours for extreme discharge</li> <li>Switch the charger to 12V AGM mode</li> <li>Charge until at 100%</li> </ol>"},{"location":"hardware/battery_maintenance_guide_re1/#additional-information","title":"Additional Information","text":""},{"location":"hardware/battery_maintenance_guide_re1/#powering-down-the-robot","title":"Powering Down the Robot","text":"<p>The recommended power down procedure is</p> <ol> <li>Place a clamp on the mast below the shoulder to prevent dropping</li> <li>Shutdown the computer from the Desktop or via SSH</li> <li>When the laser range finder has stopped spinning, turn off the main power switch</li> </ol>"},{"location":"hardware/battery_maintenance_guide_re1/#replacing-the-fuse","title":"Replacing the Fuse","text":"<p>Stretch RE1 has an automotive fuse inside the base that may need to be replaced. The type of fuse depends on your build version of the RE1</p> Build Version Fuse Type Recommended Fuse Guthrie 8A 5x20mm Fast Blow Glass Bussman S505-8-R Hank and later 7.5A ATM Fast Blow Blade Bussman VP/ATM-7-1/2-RP <p>The fuse location is shown below.  For guidance on replacing the fuse, contact Hello Robot support: support@hello-robot.com.</p> <p></p>"},{"location":"hardware/battery_maintenance_guide_re1/#checking-the-battery-voltage","title":"Checking the Battery Voltage","text":"<p>Battery voltage is not always an accurate indicator of battery charge but it can be a useful proxy. </p> <p>A charged battery will typically report a voltage of 12-12.8V and will maintain that voltage across load conditions. Meanwhile, a partially charged battery may report anywhere from 10-12.8V but its voltage will drop rapidly when loaded.</p> <p>Measuring Battery Voltage from the Command Line</p> <p>The battery voltage and current draw can be checked from the command line:</p> <pre><code>$ stretch_robot_battery_check.py\n[Pass] Voltage with 12.9889035225\n[Pass] Current with 2.46239192784\n[Pass] CPU Temp with 56.0\n</code></pre> <p>Measuring Battery Voltage with a DMM</p> <p>When troubleshooting a deeply discharged battery it may be useful to directly measure the battery voltage with a digital multimeter (DMM). To do this we recommend detaching the charger cable at its inline connector and applying the DMM to the connector contacts as shown.</p> <p>NOTE: Caution should be taken as it is possible to short the battery when doing this. </p> <p></p>"},{"location":"hardware/battery_maintenance_guide_re1/#repairing-damaged-batteries","title":"Repairing Damaged Batteries","text":"<p>It is possible for Stretch's batteries to become damaged due to repeated deep discharge. If the robot has continued issues maintaining a charge we recommend attempting the following procedure:</p> <p>CAUTION: The repair cycle procedure requires you to do a repair  cycle on one battery at a time, which means you need to unplug each  battery, perform the repair, and then repeat the process on the other  battery. If you fail to repeat the procedure on the other battery, there is a potential risk that high amounts of current may flow from the  repaired battery to the other one, causing damage to both the battery  and the system.</p> <ol> <li>Turn off the robot power switch</li> <li>Attach the charger to the robot and set it to 12V AGM mode. Allow the robot to fully charge</li> <li>Detach the charger from the robot</li> <li>Unplug 1 battery from the robot. For guidance on how to access the battery connectors, please contact Hello Robot support: support@hello-robot.com</li> <li>Attach the charger and place it in REPAIR mode</li> <li>Place the charger in REPAIR mode</li> <li>Wait until the repair cycle has completed and the charger returns to standby - up to 4 hours</li> <li>Repeat steps 3 to 6 for the second battery</li> </ol>"},{"location":"hardware/battery_maintenance_guide_re1/#replacing-dead-batteries","title":"Replacing Dead Batteries","text":"<p>It is possible for a mechanically skilled person to replace the Stretch batteries should it be necessary . Please contact Hello Robot Support for more information (support@hello-robot.com)</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"hardware/battery_maintenance_guide_re2/","title":"Stretch RE2 - Battery Maintenance Guide","text":""},{"location":"hardware/battery_maintenance_guide_re2/#overview","title":"Overview","text":"<p>Stretch RE2 utilizes two 12V AGM SLA batteries that provide a combined 18AH of capacity. Maintaining an adequate level of charge on the battery system will enhance the battery lifetime.</p> <p>The run time for a fully charged system is dependent on the load use case. The majority of battery power is consumed by the NUC computer as Stretch uses relatively low power motors.</p> <p>A fully charged robot running a high CPU load can run approximately 2 hours before requiring a recharge.</p> <p>A fully charged robot powered on but with minimal load can run approximately 5 hours before requiring a recharge. </p>"},{"location":"hardware/battery_maintenance_guide_re2/#state-of-battery-charge","title":"State of Battery Charge","text":"<p>An accurate measure of the battery charge isn't available on Stretch (as this requires a coulomb counting system). A coarse approximation of battery charge is given by the battery voltage. </p> <p>For RE2 during normal operation with a moderate load the relationship between voltage and charge is roughly:</p> Voltage Charge LED Light Bar 11.5 - 13.4V 80%+ Bright green 11.0 - 11.5V 60%+ Yellow 10.5 - 11.0V 40%+ Orange Below 10.5V &lt;40% Red <p>As shown below, the LED lightbar color provides an indication of the battery voltage. </p> <p></p>"},{"location":"hardware/battery_maintenance_guide_re2/#when-to-plug-in-the-charger","title":"When To Plug in the Charger","text":"<p>We recommend plugging in the charger whenever:</p> <ul> <li>The battery voltage is below 11V (lightbar is orange). </li> <li>The robot is not running untethered</li> <li>The robot is producing a periodic beeping sound</li> </ul>"},{"location":"hardware/battery_maintenance_guide_re2/#accidental-full-discharge","title":"Accidental Full Discharge","text":"<p>Stretch includes a few feature to help prevent accidental full discharge (for example, if the robot is left on overnight not plugged to its charger)</p> <ul> <li>Audible warning: Stretch will beep every 2 seconds if the battery voltage drops below 10.5V</li> <li>Hard power off: Stretch will completely power off if the battery voltage drops below 9.75V</li> </ul>"},{"location":"hardware/battery_maintenance_guide_re2/#charger","title":"Charger","text":"<p>Stretch ships with a NOCO Genius 10 charger. Please review the battery charger user manual prior to following the guidance in this document. </p> <ul> <li>Genius 10 Manual </li> </ul> <p>Stretch utilizes four of the available modes on these chargers.</p> Mode Function STANDBY Charger not charging the robot 12V AGM Charging  robot powered down SUPPLY 1) Power the robot during tethered use2) Repair damaged batteries. REPAIR Repair damaged batteries."},{"location":"hardware/battery_maintenance_guide_re2/#noco-genius-10-interface","title":"NOCO Genius 10 - Interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached2) Press and hold MODE button for 3s3) Press MODE button until SUPPLY indicator is illuminated4) Attach charger REPAIR 1) From STANDBY, charger attached2) Press and hold MODE button for 3s3) Press MODE button until REPAIR indicator is illuminated"},{"location":"hardware/battery_maintenance_guide_re2/#checking-the-battery-charge","title":"Checking the Battery Charge","text":"<p>The battery charger LEDs provide an approximate indicator of battery charge when it is in 12V AGM mode. Batteries are 100% fully charge when the green LED is not fading in and out.</p> <p></p>"},{"location":"hardware/battery_maintenance_guide_re2/#charging-best-practices","title":"Charging Best Practices","text":"<p>It is possible to accidentally deeply discharge the batteries by leaving the robot on for long durations without the charger attached. This is similar to leaving the lights on your car where the battery will continue to drain until fully discharged.</p> <p>We recommend following the best practices below to avoid deep discharge of the batteries and to ensure they have a long lifespan.</p> Use Case Best Practice Reason Robot is in use - tethered Leave the charger attached in SUPPLY mode while developing on the robot whenever possible.Shutdown and power off the robot when development is done. Running the robot while attempting to charge in 12V AGM mode can cause issues and is generally bad for battery health. SUPPLY mode is preferred whenever the robot needs to be powered on. Robot is in use - untethered Regularly check the battery voltage using the command line tool.Shutdown the computer and power off the robot when voltage falls below 11.5V.Attach the charger in 12V AGM mode.Charge to 100% before resuming operation. The 12V AGM charge mode expects the battery voltage to be above 10.5-11V in order to operate. Robot is not in use Shutdown the computer and turn off the robot power. Leave the charger attached and place it in 12V AGM mode. Leaving the robot power on may cause the batteries to deep dischargeThe charger will maintain a \u2018trickle charge\u2019 on the battery, keeping the charge at 100%. Robot is coming out of storage Attach charger in 12V AGM mode and charge for 2-3 hours until charger reports 100% SLA batteries naturally lose charge over time due to \u2018self-discharge\u2019."},{"location":"hardware/battery_maintenance_guide_re2/#troubleshooting","title":"Troubleshooting","text":"Issue How to Diagnose Cause Corrective Procedure Robot shows no power on activity Nothing happens when you toggle on the robot\u2019s power switch. There is no visible illumination of LEDs, motion of the laser range finder, or audible noise of the robot fans. The robot fuse may have blown. When the batteries drain the current required to maintain power goes up, which can ultimately blow the fuse. Proceed to \u201cChanging the Fuse\u201d steps below Robot powers on momentarily When you toggle on the robot\u2019s power switch some activity occurs  (illumination of LEDs, audible noise of robot fans, etc) but the computer fails to boot. The battery voltage is too low to maintain power. As the power draw increases during power-on, the voltage dips and causes the system to shut down. Connect the battery charger in 12V AGM mode and leave until fully charged. Battery won\u2019t charge in 12V AGM mode When the robot is powered down and the charger is connected in 12V AGM mode, the charger eventually switches to a different mode. The battery voltage is too low for the charger to function correctly in normal operation. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger reports 100% charge but the batteries are discharged When the robot is powered down and the charger is connected in 12V AGM mode, the charger status shows 100%.  However the robot fails to turn on properly. Damage to the batteries (usually caused by excessively low voltage) may artificially raise the open circuit voltage of the battery, causing the battery to appear fully charged, while providing low capacity. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger will not charge or stay in any mode. When placed in 12V AGM, SUPPLY, or REPAIR  mode, it continually reverts to STANDBY mode after ~ 20 minutes. Charger may be defective. Contact Hello Robot Support for a replacement."},{"location":"hardware/battery_maintenance_guide_re2/#recovering-from-low-battery-voltage","title":"Recovering from Low Battery Voltage","text":"<ol> <li>Turn off the robot power switch and detach the charger from the robot</li> <li>Place charger in SUPPLY Mode </li> <li>Allow robot to charge for 4-8 hours, or up to 24 hours for extreme discharge</li> <li>Switch the charger to 12V AGM mode</li> <li>Charge until at 100%</li> </ol>"},{"location":"hardware/battery_maintenance_guide_re2/#additional-information","title":"Additional Information","text":""},{"location":"hardware/battery_maintenance_guide_re2/#powering-down-the-robot","title":"Powering Down the Robot","text":"<p>The recommended power down procedure is</p> <ol> <li>Place a clamp on the mast below the shoulder to prevent dropping</li> <li>Shutdown the computer from the Desktop or via SSH</li> <li>When the laser range finder has stopped spinning, turn off the main power switch</li> </ol>"},{"location":"hardware/battery_maintenance_guide_re2/#replacing-the-fuse","title":"Replacing the Fuse","text":"<p>Stretch RE2 has two automotive fuses inside the base that may need to be replaced. </p> Fuse Type Recommended Fuse 7.5A ATM Fast Blow Blade Bussman VP/ATM-7-1/2-RP <p>The fuse locations are shown below.  For guidance on replacing the fuse, contact Hello Robot support: support@hello-robot.com.</p> <p></p>"},{"location":"hardware/battery_maintenance_guide_re2/#checking-the-battery-voltage","title":"Checking the Battery Voltage","text":"<p>Battery voltage is not always an accurate indicator of battery charge but it can be a useful proxy. </p> <p>A charged battery will typically report a voltage of 12-12.8V and will maintain that voltage across load conditions. Meanwhile, a partially charged battery may report anywhere from 10-12.8V but its voltage will drop rapidly when loaded.</p> <p>Measuring Battery Voltage from the Command Line</p> <p>The battery voltage and current draw can be checked from the command line:</p> <pre><code>$ stretch_robot_battery_check.py\n[Pass] Voltage with 12.9889035225\n[Pass] Current with 2.46239192784\n[Pass] CPU Temp with 56.0\n</code></pre> <p>Measuring Battery Voltage with a DMM</p> <p>When troubleshooting a deeply discharged battery it may be useful to directly measure the battery voltage with a digital multimeter (DMM). To do this we recommend </p> <ul> <li>Detach the charger cable at its inline connector </li> <li>Apply the DMM to the connector contacts as shown</li> <li>Plug the charge cable into the charge port of the robot</li> </ul> <p>NOTE: Caution should be taken as it is possible to short the battery when doing this. </p> <p></p>"},{"location":"hardware/battery_maintenance_guide_re2/#repairing-damaged-batteries","title":"Repairing Damaged Batteries","text":"<p>It is possible for Stretch's batteries to become damaged due to repeated deep discharge. If the robot has continued issues maintaining a charge we recommend attempting the following procedure:</p> <p>CAUTION: The repair cycle procedure requires you to do a repair  cycle on one battery at a time, which means you need to unplug each  battery, perform the repair, and then repeat the process on the other  battery. If you fail to repeat the procedure on the other battery, there is a potential risk that high amounts of current may flow from the  repaired battery to the other one, causing damage to both the battery  and the system.</p> <ol> <li>Turn off the robot power switch</li> <li>Attach the charger to the robot and set it to 12V AGM mode. Allow the robot to fully charge</li> <li>Detach the charger from the robot</li> <li>Unplug 1 battery from the robot. For guidance on how to access the battery connectors, please contact Hello Robot support: support@hello-robot.com</li> <li>Attach the charger and place it in REPAIR mode</li> <li>Place the charger in REPAIR mode</li> <li>Wait until the repair cycle has completed and the charger returns to standby - up to 4 hours</li> <li>Repeat steps 3 to 6 for the second battery</li> </ol>"},{"location":"hardware/battery_maintenance_guide_re2/#replacing-damaged-batteries","title":"Replacing Damaged Batteries","text":"<p>It is possible for a mechanically skilled person to replace the Stretch batteries should it be necessary . Please contact Hello Robot Support for more information (support@hello-robot.com)</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"hardware/battery_maintenance_guide_se3/","title":"Stretch 3 - Battery Maintenance Guide","text":""},{"location":"hardware/battery_maintenance_guide_se3/#overview","title":"Overview","text":"<p>Stretch 3 utilizes two 12V AGM SLA batteries that provide a combined 18AH of capacity. Maintaining an adequate level of charge on the battery system will enhance the battery lifetime.</p> <p>The run time for a fully charged system is dependent on the load use case. The majority of battery power is consumed by the NUC computer as Stretch uses relatively low power motors.</p> <p>A fully charged robot running a high CPU load can run approximately 2 hours before requiring a recharge.</p> <p>A fully charged robot powered on but with minimal load can run approximately 5 hours before requiring a recharge. </p>"},{"location":"hardware/battery_maintenance_guide_se3/#state-of-battery-charge","title":"State of Battery Charge","text":"<p>An accurate measure of the battery charge isn't available on Stretch (as this requires a coulomb counting system). A coarse approximation of battery charge is given by the battery voltage. </p> <p>For Stretch 3, during normal operation with a moderate load the relationship between voltage and charge is roughly:</p> Voltage Charge LED Light Bar 11.5 - 13.4V 80%+ Bright green 11.0 - 11.5V 60%+ Yellow 10.5 - 11.0V 40%+ Orange Below 10.5V &lt;40% Red <p>As shown below, the LED lightbar color provides an indication of the battery voltage. </p> <p></p>"},{"location":"hardware/battery_maintenance_guide_se3/#when-to-plug-in-the-charger","title":"When To Plug in the Charger","text":"<p>We recommend plugging in the charger whenever:</p> <ul> <li>The battery voltage is below 11V (lightbar is orange). </li> <li>The robot is not running untethered</li> <li>The robot is producing a periodic beeping sound</li> </ul>"},{"location":"hardware/battery_maintenance_guide_se3/#accidental-full-discharge","title":"Accidental Full Discharge","text":"<p>Stretch includes a few feature to help prevent accidental full discharge (for example, if the robot is left on overnight not plugged to its charger)</p> <ul> <li>Audible warning: Stretch will beep every 2 seconds if the battery voltage drops below 10.5V</li> <li>Hard power off: Stretch will completely power off if the battery voltage drops below 9.75V</li> </ul>"},{"location":"hardware/battery_maintenance_guide_se3/#charger","title":"Charger","text":"<p>Stretch ships with a NOCO Genius 10 charger. Please review the battery charger user manual prior to following the guidance in this document. </p> <ul> <li>Genius 10 Manual </li> </ul> <p>Stretch utilizes four of the available modes on these chargers.</p> Mode Function STANDBY Charger not charging the robot 12V AGM Charging robot while powered down SUPPLY 1) Power the robot during tethered use2) Repair damaged batteries. REPAIR Repair damaged batteries."},{"location":"hardware/battery_maintenance_guide_se3/#noco-genius-10-interface","title":"NOCO Genius 10 - Interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached2) Press and hold MODE button for 3s3) Press MODE button until SUPPLY indicator is illuminated4) Attach charger REPAIR 1) From STANDBY, charger attached2) Press and hold MODE button for 3s3) Press MODE button until REPAIR indicator is illuminated"},{"location":"hardware/battery_maintenance_guide_se3/#checking-the-battery-charge","title":"Checking the Battery Charge","text":"<p>The battery charger LEDs provide an approximate indicator of battery charge when it is in 12V AGM mode. Batteries are 100% fully charge when the green LED is not fading in and out.</p> <p></p>"},{"location":"hardware/battery_maintenance_guide_se3/#charging-best-practices","title":"Charging Best Practices","text":"<p>It is possible to accidentally deeply discharge the batteries by leaving the robot on for long durations without the charger attached. This is similar to leaving the lights on your car where the battery will continue to drain until fully discharged.</p> <p>We recommend following the best practices below to avoid deep discharge of the batteries and to ensure they have a long lifespan.</p> Use Case Best Practice Reason Robot is in use - tethered Leave the charger attached in SUPPLY mode while developing on the robot whenever possible.Shutdown and power off the robot when development is done. Running the robot while attempting to charge in 12V AGM mode can cause issues and is generally bad for battery health. SUPPLY mode is preferred whenever the robot needs to be powered on. Robot is in use - untethered Regularly check the battery voltage using the command line tool.Shutdown the computer and power off the robot when voltage falls below 11.5V.Attach the charger in 12V AGM mode.Charge to 100% before resuming operation. The 12V AGM charge mode expects the battery voltage to be above 10.5-11V in order to operate. Robot is not in use Shutdown the computer and turn off the robot power. Leave the charger attached and place it in 12V AGM mode. Leaving the robot power on may cause the batteries to deep dischargeThe charger will maintain a \u2018trickle charge\u2019 on the battery, keeping the charge at 100%. Robot is coming out of storage Attach charger in 12V AGM mode and charge for 2-3 hours until charger reports 100% SLA batteries naturally lose charge over time due to \u2018self-discharge\u2019."},{"location":"hardware/battery_maintenance_guide_se3/#troubleshooting","title":"Troubleshooting","text":"Issue How to Diagnose Cause Corrective Procedure Robot shows no power on activity Nothing happens when you toggle on the robot\u2019s power switch. There is no visible illumination of LEDs, motion of the laser range finder, or audible noise of the robot fans. The robot fuse may have blown. When the batteries drain the current required to maintain power goes up, which can ultimately blow the fuse. Proceed to \u201cChanging the Fuse\u201d steps below Robot powers on momentarily When you toggle on the robot\u2019s power switch some activity occurs  (illumination of LEDs, audible noise of robot fans, etc) but the computer fails to boot. The battery voltage is too low to maintain power. As the power draw increases during power-on, the voltage dips and causes the system to shut down. Connect the battery charger in 12V AGM mode and leave until fully charged. Battery won\u2019t charge in 12V AGM mode When the robot is powered down and the charger is connected in 12V AGM mode, the charger eventually switches to a different mode. The battery voltage is too low for the charger to function correctly in normal operation. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger reports 100% charge but the batteries are discharged When the robot is powered down and the charger is connected in 12V AGM mode, the charger status shows 100%.  However the robot fails to turn on properly. Damage to the batteries (usually caused by excessively low voltage) may artificially raise the open circuit voltage of the battery, causing the battery to appear fully charged, while providing low capacity. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger will not charge or stay in any mode. When placed in 12V AGM, SUPPLY, or REPAIR  mode, it continually reverts to STANDBY mode after ~ 20 minutes. Charger may be defective. Contact Hello Robot Support for a replacement."},{"location":"hardware/battery_maintenance_guide_se3/#recovering-from-low-battery-voltage","title":"Recovering from Low Battery Voltage","text":"<ol> <li>Turn off the robot power switch and detach the charger from the robot</li> <li>Place charger in SUPPLY Mode </li> <li>Allow robot to charge for 4-8 hours, or up to 24 hours for extreme discharge</li> <li>Switch the charger to 12V AGM mode</li> <li>Charge until at 100%</li> </ol>"},{"location":"hardware/battery_maintenance_guide_se3/#additional-information","title":"Additional Information","text":""},{"location":"hardware/battery_maintenance_guide_se3/#powering-down-the-robot","title":"Powering Down the Robot","text":"<p>The recommended power down procedure is</p> <ol> <li>Place a clamp on the mast below the shoulder to prevent dropping</li> <li>Shutdown the computer from the Desktop or via SSH</li> <li>When the laser range finder has stopped spinning, turn off the main power switch</li> </ol>"},{"location":"hardware/battery_maintenance_guide_se3/#replacing-the-fuse","title":"Replacing the Fuse","text":"<p>Stretch 3 has two automotive fuses inside the base that may need to be replaced. </p> Fuse Type Recommended Fuse 7.5A ATM Fast Blow Blade Bussman VP/ATM-7-1/2-RP <p>The fuse locations are shown below.  For guidance on replacing the fuse, contact Hello Robot support: support@hello-robot.com.</p> <p></p>"},{"location":"hardware/battery_maintenance_guide_se3/#checking-the-battery-voltage","title":"Checking the Battery Voltage","text":"<p>Battery voltage is not always an accurate indicator of battery charge but it can be a useful proxy. </p> <p>A charged battery will typically report a voltage of 12-12.8V and will maintain that voltage across load conditions. Meanwhile, a partially charged battery may report anywhere from 10-12.8V but its voltage will drop rapidly when loaded.</p> <p>Measuring Battery Voltage from the Command Line</p> <p>The battery voltage and current draw can be checked from the command line:</p> <pre><code>$ stretch_robot_battery_check.py\n[Pass] Voltage with 12.9889035225\n[Pass] Current with 2.46239192784\n[Pass] CPU Temp with 56.0\n</code></pre> <p>Measuring Battery Voltage with a DMM</p> <p>When troubleshooting a deeply discharged battery it may be useful to directly measure the battery voltage with a digital multimeter (DMM). To do this we recommend </p> <ul> <li>Detach the charger cable at its inline connector </li> <li>Apply the DMM to the connector contacts as shown</li> <li>Plug the charge cable into the charge port of the robot</li> </ul> <p>NOTE: Caution should be taken as it is possible to short the battery when doing this. </p> <p></p>"},{"location":"hardware/battery_maintenance_guide_se3/#repairing-damaged-batteries","title":"Repairing Damaged Batteries","text":"<p>It is possible for Stretch's batteries to become damaged due to repeated deep discharge. If the robot has continued issues maintaining a charge we recommend attempting the following procedure:</p> <p>CAUTION: The repair cycle procedure requires you to do a repair  cycle on one battery at a time, which means you need to unplug each  battery, perform the repair, and then repeat the process on the other  battery. If you fail to repeat the procedure on the other battery, there is a potential risk that high amounts of current may flow from the  repaired battery to the other one, causing damage to both the battery  and the system.</p> <ol> <li>Turn off the robot power switch</li> <li>Attach the charger to the robot and set it to 12V AGM mode. Allow the robot to fully charge</li> <li>Detach the charger from the robot</li> <li>Unplug 1 battery from the robot. For guidance on how to access the battery connectors, please contact Hello Robot support: support@hello-robot.com</li> <li>Attach the charger and place it in REPAIR mode</li> <li>Place the charger in REPAIR mode</li> <li>Wait until the repair cycle has completed and the charger returns to standby - up to 4 hours</li> <li>Repeat steps 3 to 6 for the second battery</li> </ol>"},{"location":"hardware/battery_maintenance_guide_se3/#replacing-damaged-batteries","title":"Replacing Damaged Batteries","text":"<p>It is possible for a mechanically skilled person to replace the Stretch batteries should it be necessary . Please contact Hello Robot Support for more information (support@hello-robot.com)</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"hardware/dex_wrist_guide_re1/","title":"Stretch RE1 - Dex Wrist User Guide","text":"<p>In this guide, we will cover the installation, configuration, and use of the Stretch Dex Wrist.</p>"},{"location":"hardware/dex_wrist_guide_re1/#overview","title":"Overview","text":"<p>The Stretch Dex Wrist is an optional add-on to the RE1. It adds pitch and roll degrees of freedom to the standard wrist yaw joint. It also includes a slightly modified version of the standard Stretch Compliant Gripper. </p> <p>NOTE: If your robot did not ship with the Stretch Dex Wrist pre-installed you will want to first proceed to the Appendix: Installation and Configuration at the end of this guide. </p>"},{"location":"hardware/dex_wrist_guide_re1/#functional-specification","title":"Functional Specification","text":""},{"location":"hardware/dex_wrist_guide_re1/#working-with-the-dex-wrist","title":"Working with the Dex Wrist","text":""},{"location":"hardware/dex_wrist_guide_re1/#safe-use","title":"Safe Use","text":"<p>The Dex Wrist requires added attention to safety. Its additional dexterity introduces new pinch points around the wrist pitch and roll degrees of freedom.</p> <p>NOTE: Please review the Robot Safety Guide prior to working with the Dex Wrist.</p> <p>In addition to these precautions, the Dex Wrist requires attention to pinch points between:</p> <ul> <li>The wrist pitch and wrist yaw structures during yaw motion</li> <li>The gripper and wrist pitch structures during pitch motion</li> </ul> <p>The Dex Wrist includes a pinch point safety marking as a reminder to users:</p> <p></p>"},{"location":"hardware/dex_wrist_guide_re1/#avoiding-collisions","title":"Avoiding Collisions","text":"<p>The added dexterity of the Dex Wrist introduces new opportunities for self-collision between the robot tool and the robot. These include</p> <ul> <li>Running the tool into the base during lift downward motion</li> <li>Running the tool into the ground</li> <li>Running the tool into the wrist yaw structure</li> </ul> <p>We recommend becoming familiar with the potential collision points of the Dex Wrist by commanding careful motions through the <code>stretch_xbox_controller_teleop.py</code> tool. </p> <p>With Stretch Body v0.1.0 we introduce a simple collision avoidance controller. </p> <p>The collision avoidance behavior acts to dynamically set the robot joint limits according to simple models of its kinematic state.  The avoidance behavior is defined in <code>collision_model.py</code></p> <p>For performance reasons this collision avoidance behavior is coarse and does not prevent all self-collisions and considered 'experimental'. The collision avoidance is off by default for the standard Stretch RE1. For robots with the wrist we turn it on by default. It be turned on or off by modifying the following in your user YAML:</p> <pre><code>robot:\n  use_collision_manager: 1\n</code></pre>"},{"location":"hardware/dex_wrist_guide_re1/#xbox-teleoperation","title":"XBox Teleoperation","text":"<p>The Dex Wrist can be teleoperated using the XBox controller. When the Dex Wrist is installed the <code>stretch_xbox_controller_teleop.py</code> tool will automatically remap control of the pan-tilt head to control of the pitch-roll wrist.</p> <pre><code>$ stretch_xbox_controller_teleop.py\n</code></pre> <p>The new key mapping is shown below. A printable version is available here.</p> <p></p>"},{"location":"hardware/dex_wrist_guide_re1/#stretch-body-interface","title":"Stretch Body Interface","text":"<p>The new WristPitch and WristRoll joints are accessed from Stretch Body in the same manner as the WristYaw joint. </p> <p>Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints.  For example:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move arm to safe manipulation location\nrobot.stow()\nrobot.lift.move_to(0.4)\nrobot.push_command()\ntime.sleep(2.0)\n\n#Pose the Dex Wrist\nrobot.end_of_arm.move_to('wrist_yaw',0)\nrobot.end_of_arm.move_to('wrist_pitch',0)\nrobot.end_of_arm.move_to('wrist_roll',0)\nrobot.end_of_arm.move_to('stretch_gripper',50)\ntime.sleep(2.0)\n\n#Go back to stow and shutdown\nrobot.stow()\nrobot.stop()\n</code></pre> <p>You can jog the individual joints of the wrist with the Stretch Body interface using the <code>stretch_dex_wrist_jog.py</code> tool that installs with the Stretch Tool Share:</p> <pre><code>$ stretch_dex_wrist_jog.py --pitch\n$ stretch_dex_wrist_jog.py --yaw\n$ stretch_dex_wrist_jog.py --roll\n</code></pre> <p>For reference, the parameters for the Stretch Dex Wrist (which can be overridden in the user YAML) can be seen in  params.py.</p>"},{"location":"hardware/dex_wrist_guide_re1/#stretch-ros-interface","title":"Stretch ROS Interface","text":"<p>The Dex Wrist can be controlled via ROS as well, as shown in the keyboard teleoperation code. To test the interface:</p> <pre><code>$ roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre> <p>You can use Ctrl-C to exit when done. The menu interface is:</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------\n\n              i HEAD UP                    \n j HEAD LEFT            l HEAD RIGHT       \n              , HEAD DOWN                  \n\n\n 7 BASE ROTATE LEFT     9 BASE ROTATE RIGHT\n home                   page-up            \n\n\n              8 LIFT UP                    \n              up-arrow                     \n 4 BASE FORWARD         6 BASE BACK        \n left-arrow             right-arrow        \n              2 LIFT DOWN                  \n              down-arrow                   \n\n\n              w ARM OUT                    \n a WRIST FORWARD        d WRIST BACK       \n              x ARM IN                     \n\n\n c PITCH FORWARD        v PITCH BACK       \n o ROLL FORWARD         p ROLL BACK        \n\n\n              5 GRIPPER CLOSE              \n              0 GRIPPER OPEN               \n\n  step size:  b BIG, m MEDIUM, s SMALL     \n\n              q QUIT                       \n\n-------------------------------------------\n</code></pre> <p></p>"},{"location":"hardware/dex_wrist_guide_re1/#appendix-installation-and-configuration","title":"Appendix: Installation and Configuration","text":"<p>Robots that did not ship with the Dex Wrist installed will require additional hardware and software installation.</p>"},{"location":"hardware/dex_wrist_guide_re1/#production-batch-variation","title":"Production Batch Variation","text":"<p>Earlier production 'batches' of Stretch will require a hardware upgrade prior to use the Dex Wrist. To check your robot's batch, run:</p> <pre><code>$ stretch_about.py\n</code></pre> <p>Refer to this table to determine what changes are required for your robot.</p> Batch Name Upgrade Wacc Board Update Baud Rate Guthrie Y Y Hank Y Y Irma Y Y Joplin N Y Kendrick or later N N"},{"location":"hardware/dex_wrist_guide_re1/#upgrade-wacc-board","title":"Upgrade Wacc Board","text":"<p>If your robot requires a Wacc Board upgrade please follow the instructions here with the assistance of Hello Robot support. This must be done before attaching the Dex Wrist to our robot.</p>"},{"location":"hardware/dex_wrist_guide_re1/#update-baud-rate","title":"Update Baud Rate","text":"<p>The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600 (depending on your production batch).  Use the commands below.</p> <pre><code>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n</code></pre>"},{"location":"hardware/dex_wrist_guide_re1/#attaching-the-dex-wrist","title":"Attaching the Dex Wrist","text":"<p>Power down your Stretch before installing the Dex Wrist.</p> <p>The Dex Wrist mounts to the bottom of the Stretch Wrist Tool Plate. Installation requires</p> <ul> <li>8 M2x6mm Torx FHCS bolts (provided)</li> <li>4 M2.5x4mm Torx FHCS bolts (provided)</li> <li>2 M2.5x8mm SHCS bolts (provided)</li> <li>T6 Torx wrench (provided)</li> <li>T8 Torx wrench (provided)</li> <li>2mm Hex key (provided)</li> </ul> <p>First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide. </p>"},{"location":"hardware/dex_wrist_guide_re1/#mounting-bracket","title":"Mounting Bracket","text":"<p>Note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the  additional alignment hole that is just outside the bolt pattern (shown pointing down in the image)</p> <p></p> <p>Using the T6 Torx wrench, attach the wrist mount bracket (A) to the bottom of the tool plate using the provided  M2x6mm bolts (B). </p> <p>NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate.</p> <p></p> <p>Now route the Dynamixel cable coming from the Stretch Wrist Yaw through the hollow bore of the wrist yaw joint.</p> <p>NOTE: During this step ensure the Dynamixel cable from the wrist yaw exits out the back (towards the shoulder)</p> <p>Next, raise the wrist module up vertically into the mounting bracket</p> <p></p> <p>, then sliding it over horizontally so that the bearing mates onto its post. </p> <p></p> <p>Now rotate the wrist yaw joint so the wrist pitch servo body is accessible. Attach the pitch servo to the mounting bracket using the 4 M2.5x4mm screws (C) using the T8 Torx wrench.</p> <p></p> <p>Finally, route the Dynamixel cable into the wrist pitch servo (pink) and install the cable clip (D) using the M2.5x8mm bolts and the 2mm hex wrench.</p> <p></p> <p></p>"},{"location":"hardware/dex_wrist_guide_re1/#software-configuration","title":"Software Configuration","text":"<p>Robots that did not ship with the Dex Wrist pre-installed will require their software to be updated and configured.</p> <p>NOTE: Each user account on the robot will need to run the following steps to configure the Dex Wrist.</p>"},{"location":"hardware/dex_wrist_guide_re1/#upgrade-stretch-body","title":"Upgrade Stretch Body","text":"<p>Ensure the latest version of Stretch Body and Stretch Factory are installed</p> <pre><code>$ pip2 install hello-robot-stretch-body -U --no-cache-dir\n$ pip2 install hello-robot-stretch-body-tools -U --no-cache-dir\n$ pip2 install hello-robot-stretch-factory -U --no-cache-dir\n$ pip2 install hello-robot-stretch-tool-share -U --no-cache-dir\n</code></pre>"},{"location":"hardware/dex_wrist_guide_re1/#backup-user-yaml","title":"Backup User YAML","text":"<pre><code>$ cd $HELLO_FLEET_PATH/$HELLO_FLEET_ID\n$ cp stretch_re1_user_params.yaml stretch_re1_user_params.yaml.bak\n</code></pre>"},{"location":"hardware/dex_wrist_guide_re1/#run-installation-script","title":"Run Installation Script","text":"<pre><code>$ cd ~/repos\n$ git clone https://github.com/hello-robot/stretch_install\n$ cd ./stretch_install\n$ git pull\n$ ./stretch_new_dex_wrist_install.sh\n</code></pre> <p>NOTE: The factory gripper calibration may not provide the full range of motion in some cases. If necessary you can dial in the gripper calibration with the tool <code>RE1_gripper_calibrate.py</code></p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"hardware/dex_wrist_guide_re2/","title":"Stretch RE2 - Dex Wrist User Guide","text":"<p>In this guide, we will cover the installation, configuration, and use of the Stretch Dex Wrist.</p>"},{"location":"hardware/dex_wrist_guide_re2/#overview","title":"Overview","text":"<p>The Stretch Dex Wrist is an optional add-on to the RE2. It adds pitch and roll degrees of freedom to the standard wrist yaw joint. It also includes a slightly modified version of the standard Stretch Compliant Gripper. </p> <p>NOTE: If your robot did not ship with the Stretch Dex Wrist pre-installed you will want to first proceed to the Appendix: Installation and Configuration at the end of this guide. </p>"},{"location":"hardware/dex_wrist_guide_re2/#functional-specification","title":"Functional Specification","text":""},{"location":"hardware/dex_wrist_guide_re2/#working-with-the-dex-wrist","title":"Working with the Dex Wrist","text":""},{"location":"hardware/dex_wrist_guide_re2/#safe-use","title":"Safe Use","text":"<p>The Dex Wrist requires added attention to safety. Its additional dexterity introduces new pinch points around the wrist pitch and roll degrees of freedom.</p> <p>NOTE: Please review the Robot Safety Guide prior to working with the Dex Wrist.</p> <p>In addition to these precautions, the Dex Wrist requires attention to pinch points between:</p> <ul> <li>The wrist pitch and wrist yaw structures during yaw motion</li> <li>The gripper and wrist pitch structures during pitch motion</li> </ul> <p>The Dex Wrist includes a pinch point safety marking as a reminder to users:</p> <p></p>"},{"location":"hardware/dex_wrist_guide_re2/#avoiding-collisions","title":"Avoiding Collisions","text":"<p>The added dexterity of the Dex Wrist introduces new opportunities for self-collision between the robot tool and the robot. These include</p> <ul> <li>Running the tool into the base during lift downward motion</li> <li>Running the tool into the ground</li> <li>Running the tool into the wrist yaw structure</li> </ul> <p>We recommend becoming familiar with the potential collision points of the Dex Wrist by commanding careful motions through the <code>stretch_xbox_controller_teleop.py</code> tool. </p> <p>With Stretch Body v0.1.0 we introduce a simple collision avoidance controller. </p> <p>The collision avoidance behavior acts to dynamically set the robot joint limits according to simple models of its kinematic state.  The avoidance behavior is defined in <code>collision_model.py</code></p> <p>For performance reasons this collision avoidance behavior is coarse and does not prevent all self-collisions and considered 'experimental'. The collision avoidance is off by default for the standard Stretch RE1. For robots with the wrist we turn it on by default. It be turned on or off by modifying the following in your user YAML:</p> <pre><code>robot:\n  use_collision_manager: 1\n</code></pre>"},{"location":"hardware/dex_wrist_guide_re2/#xbox-teleoperation","title":"XBox Teleoperation","text":"<p>The Dex Wrist can be teleoperated using the XBox controller. When the Dex Wrist is installed the <code>stretch_xbox_controller_teleop.py</code> tool will automatically remap control of the pan-tilt head to control of the pitch-roll wrist.</p> <pre><code>$ stretch_xbox_controller_teleop.py\n</code></pre> <p>The new key mapping is shown below. A printable version is available here.</p> <p></p>"},{"location":"hardware/dex_wrist_guide_re2/#stretch-body-interface","title":"Stretch Body Interface","text":"<p>The new WristPitch and WristRoll joints are accessed from Stretch Body in the same manner as the WristYaw joint. </p> <p>Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints.  For example:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move arm to safe manipulation location\nrobot.stow()\nrobot.lift.move_to(0.4)\nrobot.push_command()\ntime.sleep(2.0)\n\n#Pose the Dex Wrist\nrobot.end_of_arm.move_to('wrist_yaw',0)\nrobot.end_of_arm.move_to('wrist_pitch',0)\nrobot.end_of_arm.move_to('wrist_roll',0)\nrobot.end_of_arm.move_to('stretch_gripper',50)\ntime.sleep(2.0)\n\n#Go back to stow and shutdown\nrobot.stow()\nrobot.stop()\n</code></pre> <p>You can jog the individual joints of the wrist with the Stretch Body interface using the <code>stretch_dex_wrist_jog.py</code> tool that installs with the Stretch Tool Share:</p> <pre><code>$ stretch_dex_wrist_jog.py --pitch\n$ stretch_dex_wrist_jog.py --yaw\n$ stretch_dex_wrist_jog.py --roll\n</code></pre> <p>For reference, the parameters for the Stretch Dex Wrist (which can be overridden in the user YAML) can be seen in  params.py.</p>"},{"location":"hardware/dex_wrist_guide_re2/#stretch-ros-interface","title":"Stretch ROS Interface","text":"<p>The Dex Wrist can be controlled via ROS as well, as shown in the keyboard teleoperation code. To test the interface:</p> <pre><code>$ roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre> <p>You can use Ctrl-C to exit when done. The menu interface is:</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------\n\n              i HEAD UP                    \n j HEAD LEFT            l HEAD RIGHT       \n              , HEAD DOWN                  \n\n\n 7 BASE ROTATE LEFT     9 BASE ROTATE RIGHT\n home                   page-up            \n\n\n              8 LIFT UP                    \n              up-arrow                     \n 4 BASE FORWARD         6 BASE BACK        \n left-arrow             right-arrow        \n              2 LIFT DOWN                  \n              down-arrow                   \n\n\n              w ARM OUT                    \n a WRIST FORWARD        d WRIST BACK       \n              x ARM IN                     \n\n\n c PITCH FORWARD        v PITCH BACK       \n o ROLL FORWARD         p ROLL BACK        \n\n\n              5 GRIPPER CLOSE              \n              0 GRIPPER OPEN               \n\n  step size:  b BIG, m MEDIUM, s SMALL     \n\n              q QUIT                       \n\n-------------------------------------------\n</code></pre> <p></p>"},{"location":"hardware/dex_wrist_guide_re2/#appendix-installation-and-configuration","title":"Appendix: Installation and Configuration","text":"<p>Robots that did not ship with the Dex Wrist installed will require additional hardware and software installation.</p>"},{"location":"hardware/dex_wrist_guide_re2/#attaching-the-dex-wrist","title":"Attaching the Dex Wrist","text":"<p>Power down your Stretch before installing the Dex Wrist.</p> <p>The Dex Wrist mounts to the bottom of the Stretch Wrist Tool Plate. Installation requires</p> <ul> <li>8 M2x6mm Torx FHCS bolts (provided)</li> <li>4 M2.5x4mm Torx FHCS bolts (provided)</li> <li>2 M2.5x8mm SHCS bolts (provided)</li> <li>T6 Torx wrench (provided)</li> <li>T8 Torx wrench (provided)</li> <li>2mm Hex key (provided)</li> </ul> <p>First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide. </p>"},{"location":"hardware/dex_wrist_guide_re2/#mounting-bracket","title":"Mounting Bracket","text":"<p>Note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the  additional alignment hole that is just outside the bolt pattern (shown pointing down in the image)</p> <p></p> <p>Using the T6 Torx wrench, attach the wrist mount bracket (A) to the bottom of the tool plate using the provided  M2x6mm bolts (B). </p> <p>NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate.</p> <p></p> <p>Now route the Dynamixel cable coming from the Stretch Wrist Yaw through the hollow bore of the wrist yaw joint.</p> <p>NOTE: During this step ensure the Dynamixel cable from the wrist yaw exits out the back (towards the shoulder)</p> <p>Next, raise the wrist module up vertically into the mounting bracket</p> <p></p> <p>, then sliding it over horizontally so that the bearing mates onto its post. </p> <p></p> <p>Now rotate the wrist yaw joint so the wrist pitch servo body is accessible. Attach the pitch servo to the mounting bracket using the 4 M2.5x4mm screws (C) using the T8 Torx wrench.</p> <p></p> <p>Finally, route the Dynamixel cable into the wrist pitch servo (pink) and install the cable clip (D) using the M2.5x8mm bolts and the 2mm hex wrench.</p> <p></p> <p></p>"},{"location":"hardware/dex_wrist_guide_re2/#software-configuration","title":"Software Configuration","text":"<p>Robots that did not ship with the Dex Wrist pre-installed will require their software to be updated and configured.</p> <p>NOTE: Each user account on the robot will need to run the following steps to configure the Dex Wrist.</p>"},{"location":"hardware/dex_wrist_guide_re2/#upgrade-stretch-body","title":"Upgrade Stretch Body","text":"<p>Ensure the latest version of Stretch Body and Stretch Factory are installed</p> <pre><code>$ pip3 install hello-robot-stretch-body -U \n$ pip3 install hello-robot-stretch-body-tools -U\n$ pip3 install hello-robot-stretch-factory -U \n$ pip3 install hello-robot-stretch-tool-share -U\n</code></pre>"},{"location":"hardware/dex_wrist_guide_re2/#backup-user-yaml","title":"Backup User YAML","text":"<pre><code>$ cd $HELLO_FLEET_PATH/$HELLO_FLEET_ID\n$ cp stretch_user_params.yaml stretch_user_params.yaml.bak\n</code></pre>"},{"location":"hardware/dex_wrist_guide_re2/#run-installation-script","title":"Run Installation Script","text":"<pre><code>$ cd ~/repos\n$ git clone https://github.com/hello-robot/stretch_install\n$ cd ./stretch_install\n$ git pull\n$ ./stretch_new_dex_wrist_install.sh\n</code></pre> <p>NOTE: The factory gripper calibration may not provide the full range of motion in some cases. If necessary you can dial in the gripper calibration with the tool <code>RE1_gripper_calibrate.py</code></p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"hardware/hardware_guide_re1/","title":"Stretch RE1 - Hardware Guide","text":"<p>This manual provides the engineering data and user guidance for working with the Hello Robot Stretch RE1 hardware.  </p>"},{"location":"hardware/hardware_guide_re1/#disclaimer","title":"Disclaimer","text":"<p>The Hello Robot Stretch is intended for use in the research of mobile manipulation applications by users experienced in the use and programming of research robots. This product is not intended for general use in the home by consumers, and lacks the required certifications for such use. Please see the section on   Regulatory Compliance for further details.</p>"},{"location":"hardware/hardware_guide_re1/#functional-specification","title":"Functional Specification","text":""},{"location":"hardware/hardware_guide_re1/#body-plan","title":"Body Plan","text":""},{"location":"hardware/hardware_guide_re1/#hardware-architecture","title":"Hardware Architecture","text":""},{"location":"hardware/hardware_guide_re1/#robot-subsystems","title":"Robot Subsystems","text":""},{"location":"hardware/hardware_guide_re1/#base","title":"Base","text":"<p>The base is a two wheel differential drive with a passive Mecanum wheel for a caster.  It includes four cliff sensors to allow detection of stairs, thresholds, etc.</p> <p></p> Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm <p>The base has 6 M4 threaded inserts available for mounting user accessories such as a tray. The mounting pattern is shown below.</p> <p>The inserts are recessed 1mm from the top of the base shell.</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_re1/#base-imu","title":"Base IMU","text":"<p>The base has a 9 DOF IMU using the 9 DOF FXOS8700 + FXAS21002 chipset. The IMU orientation is as shown below:</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_re1/#trunk","title":"Trunk","text":"<p>Development and charge ports are at the back of the base in the trunk. The trunk cover slides into place vertically and is non-latching.</p> <p>The trunk height has been designed to accommodate one or more USB based Intel Neural Compute Sticks.</p> <p>Two mounting holes are provided inside the trunk. These allow the user to strain relief tethered cables (eg, HDMI and keyboard) during development. It is recommended to strain relief such cables to prevent accidental damage during base motion.</p> <p></p> Item Notes A Vent Intake vent for computer fan B 6 Port USB Hub USB 3.0 , powered 5V/3A C Ethernet Connected to computer NIC D On/Off Robot power on / off. Switch is illuminated when on. E Charge Rated for upplied 12V/7A charger F HDMI Connected to computer HDMI G Mounting points M4 threaded holes <p></p>"},{"location":"hardware/hardware_guide_re1/#head","title":"Head","text":"<p>The head provides the audio interface to the robot, a pan tilt depth camera, a runstop, as well as a developer interface to allow the addition of additional user hardware.</p> <p></p> Item Notes A Pan tilt depth camera Intel RealSense D435i Two Dynamixel XL430-W250-T servos B Speakers C Mounting holes 2x M4 threaded, spacing 25mm D Developer Interface USB2.0-A with 5V@500mA fused  JST XHP-2,  12V@3A fused Pin 1: 12V Pin 2: GND E Microphone array With programmable 12 RGB LED ring  F Runstop G Audio volume control"},{"location":"hardware/hardware_guide_re1/#pan-tilt","title":"Pan Tilt","text":"<p>The head pan-tilt unit utilizes two Dynamixel XL430-W250-T servos. It incorporates a small fan in order to ensure proper cooling of the servo and camera during dynamic repeated motions of the tilt DOF.</p> <p>The nominal \u2018zero\u2019 position is of the head is shown below, along with the corresponding range of motion.</p> <p></p> DOF Range (deg) Min(deg) Max (deg) Pan 346 -234  112 Tilt 115 -25 90"},{"location":"hardware/hardware_guide_re1/#respeaker-microphone-array","title":"ReSpeaker Microphone Array","text":"<p>The ReSPeaker has 12 RGB LEDs that can be controlled programatically. By default they display sound intensity and direction of the microphone array. The ReSpeaker has 4 mems microphones mounted on a 64.61mm circle at 45 degree spacing. The drawing below shows the position and orientation of the microphone array relative to the head pan axis.</p> <p></p>"},{"location":"hardware/hardware_guide_re1/#runstop","title":"Runstop","text":"<p>The runstop allows the user to pause the motion of the four primary DOF (base, lift, and arm) by tapping the illuminated button on the head. When the runstop is enabled, these DOF are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume. </p>"},{"location":"hardware/hardware_guide_re1/#lift","title":"Lift","text":"<p>The lift degree of freedom provides vertical translation of the arm. It is driven by a closed loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive. The \u2018shoulder\u2019 includes two mounting holes and a small delivery tray.</p> <p>NOTE: When using the shoulder mounting screws do not use M4 bolts longer than 7mm in length. Otherwise damage may occur. </p> <p></p> <p></p> Item Notes A Delivery tray B Mounting holes Threaded M4. Spacing 34.5 mm. Length not to exceed 7mm C Aruco Tag Size 40x40 mm"},{"location":"hardware/hardware_guide_re1/#arm","title":"Arm","text":"<p>The arm comprises 5 telescoping carbon fiber links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion.</p> <p>The arm exhibits a small amount of play (lash) in the X, Y, Z, and theta directions which is a normal characteristic of its design. Despite this it can achieve good repeatability, in part because its gravity loading is fairly constant.</p> <p>The retracted arm and wrist combined are designed to fit within the footprint of the base. The arm is designed to have:</p> <ul> <li>Reach: 0.52m</li> </ul>"},{"location":"hardware/hardware_guide_re1/#wrist","title":"Wrist","text":"<p>The wrist includes:</p> <ul> <li>Yaw DOF to allow for stowing of the tool</li> <li>2 Aruco tags for calibration and visual localization of the tool</li> <li>Expansion port with</li> <li>Arduino expansion header</li> <li>USB-A connector</li> <li>Tool plate with dual sided mounting</li> <li>Dynamixel X-Series TTL bus</li> </ul>"},{"location":"hardware/hardware_guide_re1/#wrist-control-interface","title":"Wrist Control Interface","text":"<p>The wrist yaw degree-of-freedom uses a Dynamixel XL430 servo. Additional Dynamixel servos can be daisy chained off of this servo, allowing for one more additional degree-of-freedoms to be easily  integrated onto the robot (such as the provided Stretch Gripper). </p> <p>Stretch comes with a XL430 compatible control cable preinstalled into this servo. If a different cable needs to be installed the servo cap can be removed as shown.</p> <p></p>"},{"location":"hardware/hardware_guide_re1/#wrist-tool-plate","title":"Wrist Tool Plate","text":"<p>The tool plate allows for mounting on the top or the bottom using the M2 bolt pattern. The mounting pattern is compatible with Robotis Dynamixel frames as well:</p> <ul> <li>FR12-H101K</li> <li>FR12-S102K</li> <li>FR12-S101K</li> </ul> <p></p>"},{"location":"hardware/hardware_guide_re1/#wrist-yaw-range-of-motion","title":"Wrist Yaw Range of Motion","text":"<p>The wrist yaw DOF is calibrated so that the index hole faces forward at the 'zero' position. From this pose the wrist has a ROM of +256/-76 degrees as shown.</p> <p></p>"},{"location":"hardware/hardware_guide_re1/#wrist-accelerometer","title":"Wrist Accelerometer","text":"<p>The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The  sensor is mounted inside the distal link of the arm as shown below.</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_re1/#wrist-expansion-usb","title":"Wrist Expansion USB","text":"<p>The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V.</p>"},{"location":"hardware/hardware_guide_re1/#wrist-expansion-header","title":"Wrist Expansion Header","text":"<p>The wrist includes an expansion header that provides access to pins of the wrist Arduino board.  The header connector can be accessed by removing the cap at the end of the arm.</p> <p></p> <p>The header is wired to a Atmel SAMD21G18A-AUT (datasheet) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow:</p> <ul> <li>General purpose digital I/O</li> <li>Analog input</li> </ul> <p>In addition, the firmware can be configured for other pin functions, including:</p> <ul> <li>Serial SPI</li> <li>Serial I2C</li> <li>Serial UART</li> </ul> <p>The Stretch Firmware Manual covers this modification.</p> <p>**The header pins utilize 3V3 TTL logic. They do not have interface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded **</p> <p>The pin mapping is:</p> Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS  Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input  Analog in (A0) <p>The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN).  It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305).</p> <p>Pin 1 &amp; 10 are indicated below.</p> <p></p> <p>The expansion DIO schematic shown below.</p> <p></p>"},{"location":"hardware/hardware_guide_re1/#wrist-mounts","title":"Wrist Mounts","text":""},{"location":"hardware/hardware_guide_re1/#gripper","title":"Gripper","text":"<p>The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position.  As shown, it includes mounting features on  one side to allow for attachment of simple rigid tools such as hooks and pullers. </p> <p></p> Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 <p>The attachment features are spaced at 9mm.</p> <p>The weight of the Stretch Compliant Gripper is 240g.</p>"},{"location":"hardware/hardware_guide_re1/#gripper-removal","title":"Gripper Removal","text":"<p>Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse.</p> <ol> <li>Unplug the Dynamixel cable from the back of the gripper. </li> <li>Remove the 4 screws holding the gripper to the bracket.</li> <li>Remove the gripper from the mounting bracket</li> <li>Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate.</li> </ol> <p></p> <p></p>"},{"location":"hardware/hardware_guide_re1/#robot-care","title":"Robot Care","text":""},{"location":"hardware/hardware_guide_re1/#battery-maintenance","title":"Battery Maintenance","text":"<p>Please review the Battery Maintenance Guide for proper care and charging of the Stretch batteries.</p>"},{"location":"hardware/hardware_guide_re1/#belt-tension","title":"Belt Tension","text":"<p>A neoprene timing belt drives the arm up and down the lift. It may detension over long periods of time if it experiences sustained loading. In this case, slack will become visually apparent in the belt as the lift moves.</p> <p>The belt is very straightforward to re-tension. Please contact support@hello-robot.com for tensioning instructions.</p>"},{"location":"hardware/hardware_guide_re1/#keeping-the-robot-clean","title":"Keeping the Robot Clean","text":"<p>The robot surfaces can be wiped down with an alcohol wipe or a moist rag from time to time in order to remove and debris or oils that accumulate on the shells or mast. </p> <p>The drive wheels can accumulate dust over time and begin to lose traction. They should be periodically wiped down as well.</p> <p>When possible, the Trunk cover for the base should be kept on in order to keep dust and debris out of the Trunk connectors.</p> <p>If the D435i camera requires cleaning use appropriate lens cleaning fluid and a microfiber cloth.</p>"},{"location":"hardware/hardware_guide_re1/#keeping-the-robot-calibrated","title":"Keeping the Robot Calibrated","text":"<p>The robot comes pre-calibrated with a robot-specific URDF. This calibration allows the D435i depth sensor to accurately estimate where the robot wrist, and body, is in the depth image.</p> <p>The robot may become slightly uncalibrated over time for a variety of reasons:</p> <ul> <li>Normal wear and tear and loosening of joints of the robot</li> <li>The head structure is accidentally load and the structure becomes very slightly bent</li> <li>The wrist and should structure become accidentally highly loaded and become slightly bent</li> </ul> <p>The calibration accuracy can be checked using the provided ROS tools. If necessary, the user can recalibrate the robot. See the Stretch URDF Calibration Guide for more information.</p>"},{"location":"hardware/hardware_guide_re1/#transporting-the-robot","title":"Transporting the Robot","text":"<p>Stretch was designed to be easily transported in the back of a car, up a stair case, or around a building.</p> <p>For short trips, the robot can be simply rolled around by grabbing its mast. It may be picked up by its mast and carried up stairs as well. </p> <p>For safety, please use two people to lift the robot.</p> <p>For longer trips it is recommended to transport the robot in its original cardboard box with foam packaging. The metal protective cage that surrounds the head is only necessary if the robot might be shipped and the box will not remain upright.</p>"},{"location":"hardware/hardware_guide_re1/#system-check","title":"System Check","text":"<p>It is useful to periodically run stretch_robot_system_check.py. This  will check that the robot's hardware devices are  present and within normal operating conditions. </p> <pre><code>$ stretch_robot_system_check.py\n\n---- Checking Devices ----\n[Pass] : hello-wacc\n[Pass] : hello-motor-left-wheel\n[Pass] : hello-motor-arm\n[Pass] : hello-dynamixel-wrist\n[Pass] : hello-motor-right-wheel\n[Pass] : hello-motor-lift\n[Pass] : hello-pimu\n[Pass] : hello-respeaker\n[Pass] : hello-lrf\n[Pass] : hello-dynamixel-head\n\n---- Checking Pimu ----\n[Pass] Voltage = 12.8763639927\n[Pass] Current = 3.25908634593\n[Pass] Temperature = 36.3404559783\n[Pass] Cliff-0 = -4.72064208984\n[Pass] Cliff-1 = -8.56213378906\n[Pass] Cliff-2 = 1.08505249023\n[Pass] Cliff-3 = 5.68453979492\n[Pass] IMU AZ = -9.80407142639\n\n\n---- Checking EndOfArm ----\n[Dynamixel ID:013] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: wrist_yaw\n[Pass] Calibrated: wrist_yaw\n\n[Dynamixel ID:014] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: stretch_gripper\n[Pass] Calibrated: stretch_gripper\n\n\n---- Checking Head ----\n[Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: head_tilt\n\n[Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: head_pan\n\n\n---- Checking Wacc ----\n[Pass] AX = 9.4840593338\n\n\n---- Checking hello-motor-left-wheel ----\n[Pass] Position = 43.9992256165\n\n\n---- Checking hello-motor-right-wheel ----\n[Pass] Position = 15.1164712906\n\n\n---- Checking hello-motor-arm ----\n[Pass] Position = 59.7719421387\n[Pass] Position Calibrated = True\n\n\n---- Checking hello-motor-lift ----\n[Pass] Position = 83.7744064331\n[Pass] Position Calibrated = True\n\n\n---- Checking for Intel D435i ----\nBus 002 Device 016: ID 8086:0b3a Intel Corp. \n[Pass] : Device found \n</code></pre>"},{"location":"hardware/hardware_guide_re1/#regulatory-compliance","title":"Regulatory Compliance","text":"<p>The Stretch Research Edition 1 (Stretch RE1) is not certified for use as a consumer device in the U.S.</p> <p>Unless stated otherwise, the Stretch RE1 is not subjected to compliance testing nor certified to meet any requirements, such as requirements for EMI, EMC, or ESD.</p> <p>Per FCC 47 CFR, Part 15, Subpart B, section 15.103(c), we claim the Stretch Research Edition 1 as an exempted device, since it is a digital device used exclusively as industrial, commercial, or medical test equipment, where test equipment is equipment intended primarily for purposes of performing scientific investigations.</p> <p>OET BULLETIN NO. 62, titled \"UNDERSTANDING THE FCC REGULATIONS FOR COMPUTERS AND OTHER DIGITAL DEVICES\" from December 1993 provides further clarification of the Section 15.103(c) exemption: \u201cTest equipment includes devices used for maintenance, research, evaluation, simulation and other analytical or scientific applications in areas such as industrial plants, public utilities, hospitals, universities, laboratories, automotive service centers and electronic repair shops.\u201d</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"hardware/hardware_guide_re2/","title":"Stretch RE2 - Hardware Guide","text":"<p>This manual provides the engineering data and user guidance for working with the Hello Robot Stretch RE2 hardware.  </p>"},{"location":"hardware/hardware_guide_re2/#disclaimer","title":"Disclaimer","text":"<p>The Hello Robot Stretch is intended for use in the research of mobile manipulation applications by users experienced in the use and programming of research robots. This product is not intended for general use in the home by consumers, and lacks the required certifications for such use. Please see the section on Regulatory Compliance for further details.</p>"},{"location":"hardware/hardware_guide_re2/#functional-specification","title":"Functional Specification","text":""},{"location":"hardware/hardware_guide_re2/#body-plan","title":"Body Plan","text":""},{"location":"hardware/hardware_guide_re2/#hardware-architecture","title":"Hardware Architecture","text":""},{"location":"hardware/hardware_guide_re2/#robot-subsystems","title":"Robot Subsystems","text":""},{"location":"hardware/hardware_guide_re2/#base","title":"Base","text":"<p>The base is a two wheel differential drive with a passive Mecanum wheel for a caster.  It includes four cliff sensors to allow detection of stairs, thresholds, etc.</p> <p></p> Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm <p>The base has 6 M4 threaded inserts available for mounting user accessories such as a tray. The mounting pattern is shown below.</p> <p>The inserts are recessed 1mm from the top of the base shell.</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_re2/#base-imu","title":"Base IMU","text":"<p>The base has a 9 DOF IMU using the 9 DOF FXOS8700 + FXAS21002 chipset. The IMU orientation is as shown below:</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_re2/#trunk","title":"Trunk","text":"<p>Development and charge ports are at the back of the base in the trunk. </p> <p></p> Item Notes A Vent Intake vent for computer fan B 6 Port USB Hub USB 3.0 , powered 5V/3A C Ethernet Connected to computer NIC D On/Off Robot power on / off. Switch is illuminated when on. E Charge Rated for upplied 12V/7A charger F HDMI Connected to computer HDMI G LED Light Bar Indicates battery voltage H 12V access plug Allows customer cable access to 12V Aux on Pimu PCBA"},{"location":"hardware/hardware_guide_re2/#12v-access-plug","title":"12V Access Plug","text":"<p>The 12V access plug allows users to supply 12V power to an external device. This 12V power is directly connected to the battery and is not a regulated voltage supply. As the battery voltage decreases, the 12V access plug voltage also decreases. It is recommended that the user purchase a regulated voltage adapter to connect to this access plug and then connect the external device to the regulated voltage adapter, instead of connecting it directly to the 12V access plug. The maximum continuous current of the 12V access plug is 6A max.</p> <p>[!WARNING] If the cable on the 12V access plug is wired incorrectly, the main power board of the robot will get damaged</p>"},{"location":"hardware/hardware_guide_re2/#head","title":"Head","text":"<p>The head provides the audio interface to the robot, a pan tilt depth camera, a runstop, as well as a developer interface to allow the addition of additional user hardware.</p> <p></p> Item Notes A Pan tilt depth camera Intel RealSense D435i Two Dynamixel XL430-W250-T servos B Speakers D Developer Interface Volume control, USB2.0-A with 5V@500mA fused  JST XHP-2,  12V@3A fused Pin 1: 12V Pin 2: GND E Microphone array With programmable 12 RGB LED ring  F Runstop"},{"location":"hardware/hardware_guide_re2/#pan-tilt","title":"Pan Tilt","text":"<p>The head pan-tilt unit utilizes two Dynamixel XL430-W250-T servos. It incorporates a small fan in order to ensure proper cooling of the servo and camera during dynamic repeated motions of the tilt DOF.</p> <p>The nominal \u2018zero\u2019 position is of the head is shown below, along with the corresponding range of motion.</p> <p></p> DOF Range (deg) Min(deg) Max (deg) Pan 346 -234  112 Tilt 115 -25 90"},{"location":"hardware/hardware_guide_re2/#respeaker-microphone-array","title":"ReSpeaker Microphone Array","text":"<p>The ReSPeaker has 12 RGB LEDs that can be controlled programatically. By default they display sound intensity and direction of the microphone array. The ReSpeaker has 4 mems microphones mounted on a 64.61mm circle at 45 degree spacing. The drawing below shows the position and orientation of the microphone array relative to the head pan axis.</p>"},{"location":"hardware/hardware_guide_re2/#mounting-points","title":"Mounting Points","text":"<p>The top of the head includes 3x M4 threaded mounting points as shown below.</p> <p></p>"},{"location":"hardware/hardware_guide_re2/#runstop","title":"Runstop","text":"<p>The runstop allows the user to pause the motion of the four primary DOF (base, lift, and arm) by tapping the illuminated button on the head. When the runstop is enabled, these DOF are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume. </p>"},{"location":"hardware/hardware_guide_re2/#lift","title":"Lift","text":"<p>The lift degree of freedom provides vertical translation of the arm. It is driven by a closed loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive. The \u2018shoulder\u2019 includes four mounting holes and a small delivery tray.</p> <p>NOTE: When using the shoulder mounting screws do not use M4 bolts longer than 7mm in length. Otherwise damage may occur. </p> <p></p> <p></p> Item Notes A Delivery tray B Mounting holes Threaded M4. Length not to exceed 7mm. C Aruco Tag Size 40x40 mm D Developer ports USB2.0-A with 5V@500mA fused ;  JST XHP-2,  12V@3A fused Pin 1: 12V Pin 2: GND"},{"location":"hardware/hardware_guide_re2/#arm","title":"Arm","text":"<p>The arm comprises 5 telescoping carbon fiber links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion.</p> <p>The arm exhibits a small amount of play (lash) in the X, Y, Z, and theta directions which is a normal characteristic of its design. Despite this it can achieve good repeatability, in part because its gravity loading is fairly constant.</p> <p>The retracted arm and wrist combined are designed to fit within the footprint of the base. The arm is designed to have:</p> <ul> <li>Reach: 0.52m</li> </ul>"},{"location":"hardware/hardware_guide_re2/#wrist","title":"Wrist","text":"<p>The wrist includes:</p> <ul> <li>Yaw DOF to allow for stowing of the tool</li> <li>2 Aruco tags for calibration and visual localization of the tool</li> <li>Expansion port with</li> <li>Arduino expansion header</li> <li>USB-A connector</li> <li>Tool plate with dual sided mounting</li> <li>Dynamixel X-Series TTL bus</li> </ul>"},{"location":"hardware/hardware_guide_re2/#wrist-control-interface","title":"Wrist Control Interface","text":"<p>The wrist yaw degree-of-freedom uses a Dynamixel XL430 servo. Additional Dynamixel servos can be daisy chained off of this servo, allowing for one more additional degree-of-freedoms to be easily  integrated onto the robot (such as the provided Stretch Gripper). </p> <p>Stretch comes with a XL430 compatible control cable preinstalled into this servo. If a different cable needs to be installed the servo cap can be removed as shown.</p> <p></p>"},{"location":"hardware/hardware_guide_re2/#wrist-tool-plate","title":"Wrist Tool Plate","text":"<p>The tool plate allows for mounting on the top or the bottom using the M2 bolt pattern. The mounting pattern is compatible with Robotis Dynamixel frames as well:</p> <ul> <li>FR12-H101K</li> <li>FR12-S102K</li> <li>FR12-S101K</li> </ul> <p>The tool plate includes a 'Zero indicator'. This mark indicates the forward position of the tool. It will point in the direction of the arm extension when the wrist yaw joint is at its zero position.</p> <p>In addition, the tool plate includes two index holes. These can be used to index a tool (e.g., Stretch Gripper) during installation. Compatible pins on the tool ensure that it is installed at the correct orientation.</p> <p></p>"},{"location":"hardware/hardware_guide_re2/#wrist-yaw-range-of-motion","title":"Wrist Yaw Range of Motion","text":"<p>The wrist yaw DOF is calibrated so that the index hole faces forward at the 'zero' position. From this pose the wrist has a ROM of +256/-76 degrees as shown.</p> <p></p>"},{"location":"hardware/hardware_guide_re2/#wrist-accelerometer","title":"Wrist Accelerometer","text":"<p>The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The  sensor is mounted inside the distal link of the arm as shown below.</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_re2/#wrist-expansion-usb","title":"Wrist Expansion USB","text":"<p>The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V.</p>"},{"location":"hardware/hardware_guide_re2/#wrist-expansion-header","title":"Wrist Expansion Header","text":"<p>The wrist includes an expansion header that provides access to pins of the wrist Arduino board.  The header connector can be accessed by removing the cap at the end of the arm.</p> <p></p> <p>The header is wired to a Atmel SAMD21G18A-AUT (datasheet) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow:</p> <ul> <li>General purpose digital I/O</li> <li>Analog input</li> </ul> <p>In addition, the firmware can be configured for other pin functions, including:</p> <ul> <li>Serial SPI</li> <li>Serial I2C</li> <li>Serial UART</li> </ul> <p>The Stretch Firmware Manual covers this modification.</p> <p>**The header pins utilize 3V3 TTL logic. They have limited nterface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded **</p> <p>The pin mapping is:</p> Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS  Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input  Analog in (A0) <p>The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN).  It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305).</p> <p>Pin 1 &amp; 10 are indicated below.</p> <p></p> <p>The expansion DIO schematic shown below.</p> <p></p>"},{"location":"hardware/hardware_guide_re2/#wrist-mounts","title":"Wrist Mounts","text":""},{"location":"hardware/hardware_guide_re2/#gripper","title":"Gripper","text":"<p>The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position.  As shown, it includes mounting features on  one side to allow for attachment of simple rigid tools such as hooks and pullers. </p> <p></p> Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 <p>The attachment features are spaced at 9mm.</p> <p>The weight of the Stretch Compliant Gripper is 240g.</p>"},{"location":"hardware/hardware_guide_re2/#gripper-removal","title":"Gripper Removal","text":"<p>Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse.</p> <ol> <li>Unplug the Dynamixel cable from the back of the gripper. </li> <li>Remove the 4 screws holding the gripper to the bracket.</li> <li>Remove the gripper from the mounting bracket</li> <li>Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate.</li> </ol> <p></p> <p></p>"},{"location":"hardware/hardware_guide_re2/#robot-care","title":"Robot Care","text":""},{"location":"hardware/hardware_guide_re2/#battery-maintenance","title":"Battery Maintenance","text":"<p>Please review the Battery Maintenance Guide for proper care and charging of the Stretch batteries.</p>"},{"location":"hardware/hardware_guide_re2/#belt-tension","title":"Belt Tension","text":"<p>A neoprene timing belt drives the arm up and down the lift. It may detension over long periods of time if it experiences sustained loading. In this case, slack will become visually apparent in the belt as the lift moves.</p> <p>The belt is very straightforward to re-tension. Please contact support@hello-robot.com for tensioning instructions.</p>"},{"location":"hardware/hardware_guide_re2/#keeping-the-robot-clean","title":"Keeping the Robot Clean","text":"<p>The robot surfaces can be wiped down with an alcohol wipe or a moist rag from time to time in order to remove and debris or oils that accumulate on the shells or mast. </p> <p>The drive wheels can accumulate dust over time and begin to lose traction. They should be periodically wiped down as well.</p> <p>When possible, the Trunk cover for the base should be kept on in order to keep dust and debris out of the Trunk connectors.</p> <p>If the D435i camera requires cleaning use appropriate lens cleaning fluid and a microfiber cloth.</p>"},{"location":"hardware/hardware_guide_re2/#keeping-the-robot-calibrated","title":"Keeping the Robot Calibrated","text":"<p>The robot comes pre-calibrated with a robot-specific URDF. This calibration allows the D435i depth sensor to accurately estimate where the robot wrist, and body, is in the depth image.</p> <p>The robot may become slightly uncalibrated over time for a variety of reasons:</p> <ul> <li>Normal wear and tear and loosening of joints of the robot</li> <li>The head structure is accidentally load and the structure becomes very slightly bent</li> <li>The wrist and should structure become accidentally highly loaded and become slightly bent</li> </ul> <p>The calibration accuracy can be checked using the provided ROS tools. If necessary, the user can recalibrate the robot. See the Stretch URDF Calibration Guide for more information.</p>"},{"location":"hardware/hardware_guide_re2/#transporting-the-robot","title":"Transporting the Robot","text":"<p>Stretch was designed to be easily transported in the back of a car, up a stair case, or around a building.</p> <p>For short trips, the robot can be simply rolled around by grabbing its mast. It may be picked up by its mast and carried up stairs as well. </p> <p>For safety, please use two people to lift the robot.</p> <p>For longer trips it is recommended to transport the robot in its original cardboard box with foam packaging. The metal protective cage that surrounds the head is only necessary if the robot might be shipped and the box will not remain upright.</p>"},{"location":"hardware/hardware_guide_re2/#system-check","title":"System Check","text":"<p>It is useful to periodically run stretch_robot_system_check.py. This  will check that the robot's hardware devices are  present and within normal operating conditions. </p> <pre><code>$ stretch_robot_system_check.py\n\n---- Checking Devices ----\n[Pass] : hello-wacc\n[Pass] : hello-motor-left-wheel\n[Pass] : hello-motor-arm\n[Pass] : hello-dynamixel-wrist\n[Pass] : hello-motor-right-wheel\n[Pass] : hello-motor-lift\n[Pass] : hello-pimu\n[Pass] : hello-respeaker\n[Pass] : hello-lrf\n[Pass] : hello-dynamixel-head\n\n---- Checking Pimu ----\n[Pass] Voltage = 12.8763639927\n[Pass] Current = 3.25908634593\n[Pass] Temperature = 36.3404559783\n[Pass] Cliff-0 = -4.72064208984\n[Pass] Cliff-1 = -8.56213378906\n[Pass] Cliff-2 = 1.08505249023\n[Pass] Cliff-3 = 5.68453979492\n[Pass] IMU AZ = -9.80407142639\n\n\n---- Checking EndOfArm ----\n[Dynamixel ID:013] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: wrist_yaw\n[Pass] Calibrated: wrist_yaw\n\n[Dynamixel ID:014] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: stretch_gripper\n[Pass] Calibrated: stretch_gripper\n\n\n---- Checking Head ----\n[Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: head_tilt\n\n[Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: head_pan\n\n\n---- Checking Wacc ----\n[Pass] AX = 9.4840593338\n\n\n---- Checking hello-motor-left-wheel ----\n[Pass] Position = 43.9992256165\n\n\n---- Checking hello-motor-right-wheel ----\n[Pass] Position = 15.1164712906\n\n\n---- Checking hello-motor-arm ----\n[Pass] Position = 59.7719421387\n[Pass] Position Calibrated = True\n\n\n---- Checking hello-motor-lift ----\n[Pass] Position = 83.7744064331\n[Pass] Position Calibrated = True\n\n\n---- Checking for Intel D435i ----\nBus 002 Device 016: ID 8086:0b3a Intel Corp. \n[Pass] : Device found \n</code></pre>"},{"location":"hardware/hardware_guide_re2/#regulatory-compliance","title":"Regulatory Compliance","text":"<p>The Stretch Research Edition 1 (Stretch re2) is not certified for use as a consumer device in the U.S.</p> <p>Unless stated otherwise, the Stretch re2 is not subjected to compliance testing nor certified to meet any requirements, such as requirements for EMI, EMC, or ESD.</p> <p>Per FCC 47 CFR, Part 15, Subpart B, section 15.103(c), we claim the Stretch Research Edition 1 as an exempted device, since it is a digital device used exclusively as industrial, commercial, or medical test equipment, where test equipment is equipment intended primarily for purposes of performing scientific investigations.</p> <p>OET BULLETIN NO. 62, titled \"UNDERSTANDING THE FCC REGULATIONS FOR COMPUTERS AND OTHER DIGITAL DEVICES\" from December 1993 provides further clarification of the Section 15.103(c) exemption: \u201cTest equipment includes devices used for maintenance, research, evaluation, simulation and other analytical or scientific applications in areas such as industrial plants, public utilities, hospitals, universities, laboratories, automotive service centers and electronic repair shops.\u201d</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"hardware/hardware_guide_stretch_3/","title":"Stretch 3 Hardware Guide","text":"<p>This manual provides the engineering data and user guidance for working with the Hello Robot Stretch 3 hardware.  </p>"},{"location":"hardware/hardware_guide_stretch_3/#disclaimer","title":"Disclaimer","text":"<p>The Hello Robot Stretch is intended for use in the research of mobile manipulation applications by users experienced in the use and programming of research robots. This product is not intended for general use in the home by consumers, and lacks the required certifications for such use. Please see the section on Regulatory Compliance for further details.</p>"},{"location":"hardware/hardware_guide_stretch_3/#functional-specification","title":"Functional Specification","text":"<p>Subsystem data sheets available here.</p>"},{"location":"hardware/hardware_guide_stretch_3/#body-plan","title":"Body Plan","text":""},{"location":"hardware/hardware_guide_stretch_3/#hardware-architecture","title":"Hardware Architecture","text":""},{"location":"hardware/hardware_guide_stretch_3/#robot-subsystems","title":"Robot Subsystems","text":""},{"location":"hardware/hardware_guide_stretch_3/#base","title":"Base","text":"<p>The base is a two wheel differential drive with a passive omniwheel for a caster.  It includes four cliff sensors to allow detection of stairs, thresholds, etc.</p> <p></p> Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Omni wheel Diameter 50mm <p>The base has 6 M4 threaded inserts available for mounting user accessories such as a tray. The mounting pattern is shown below.</p> <p>The inserts are recessed 1mm from the top of the base shell. Maximum safe fastener depth is 7mm below top of shell.</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_stretch_3/#base-imu","title":"Base IMU","text":"<p>The base has an IMU using the 9 DOF BNO085 chipset. The IMU orientation is as shown below:</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_stretch_3/#trunk","title":"Trunk","text":"<p>Development and charge ports are at the back of the base in the trunk. </p> <p></p> Item Notes A Vent Intake vent for computer fan B 4 Port USB Hub USB 3.0 , powered 5V C Ethernet Connected to computer NIC D On/Off Robot power on / off. Switch is illuminated when on. E Charge Rated for applied 12V/7A charger F HDMI Connected to computer HDMI G LED Light Bar Indicates battery voltage H 12V access plug Allows customer cable access to 12V Aux on Pimu PCBA"},{"location":"hardware/hardware_guide_stretch_3/#12v-access-plug","title":"12V Access Plug","text":"<p>The 12V access plug allows users to supply 12V power to an external device. This 12V power is directly connected to the battery and is not a regulated voltage supply. As the battery voltage decreases, the 12V access plug voltage also decreases. It is recommended that the user purchase a regulated voltage adapter to connect to this access plug and then connect the external device to the regulated voltage adapter, instead of connecting directly to the 12V access plug. The maximum continuous current of the 12V access plug is 6A max.</p> <p>Warning</p> <p>If the cable on the 12V access plug is wired incorrectly, the main power board of the robot will get damaged</p>"},{"location":"hardware/hardware_guide_stretch_3/#head","title":"Head","text":"<p>The head provides the audio interface to the robot, a pan tilt depth camera and RGB wide-angle camera, a runstop, and a developer interface to allow for additional user hardware.</p> <p></p> Item Notes A Pan tilt camera module Intel RealSense D435if, Arducam B0385+LK001, Dynamixel XL430-W250-T, Dynamixel XC430-W240-T B Speakers C Runstop D Developer Interface Volume control, USB2.0-A with 5V@500mA fused  JST XHP-2,  12V@3A fused Pin 1: 12V Pin 2: GND E Microphone array ReSpeaker Mic Array v2.0, with programmable 12 RGB LED ring"},{"location":"hardware/hardware_guide_stretch_3/#pan-tilt","title":"Pan Tilt","text":"<p>The head pan-tilt unit utilizes two Dynamixel servos, 1x XL430-W250-T and 1x XC430-W240-T. It incorporates a small fan in order to ensure proper cooling of the servo and camera during dynamic repeated motions of the tilt DOF.</p> <p>The nominal \u2018zero\u2019 position is of the head is shown below, along with the corresponding range of motion.</p> <p></p> DOF Range (deg) Min(deg) Max (deg) Pan 346 -234  112 Tilt 115 -25 90"},{"location":"hardware/hardware_guide_stretch_3/#respeaker-microphone-array","title":"ReSpeaker Microphone Array","text":"<p>The ReSpeaker has 12 RGB LEDs that can be controlled programatically. By default they display sound intensity and direction of the microphone array. The ReSpeaker has 4 mems microphones mounted on a 64.61mm circle at 45 degree spacing. The drawing below shows the position and orientation of the microphone array relative to the head pan axis.</p>"},{"location":"hardware/hardware_guide_stretch_3/#mounting-points","title":"Mounting Points","text":"<p>The top of the head includes 3x M4 threaded mounting points as shown below.</p> <p></p>"},{"location":"hardware/hardware_guide_stretch_3/#runstop","title":"Runstop","text":"<p>The runstop allows the user to pause the motion of the four primary DOF (base, lift, and arm) by tapping the illuminated button on the head. When the runstop is enabled, these DOF are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume.</p>"},{"location":"hardware/hardware_guide_stretch_3/#lift","title":"Lift","text":"<p>The lift degree of freedom provides vertical translation of the arm. It is driven by a closed loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive. The \u2018shoulder\u2019 includes four mounting holes and a small delivery tray.</p> <p>[!WARNING] Maximum fastener length for shoulder mounting holes is 7mm - exceeding this depth may cause damage</p> <p></p> <p></p> Item Notes A Delivery tray B Mounting holes Threaded M4. Length not to exceed 7mm. C Aruco Tag Size 40x40 mm D Developer ports USB2.0-A with 5V@500mA fused ;  JST XHP-2,  12V@3A fused Pin 1: 12V Pin 2: GND <p>Stretch 3 introduces a new lift brake feature. Upon power being disconnected to the lift motor (eg when the robot power switch is turned off), a brake will engage that prevents the arm and shoulder from falling rapidly. Instead, the robot arm will very slowly descend from its current position to the base of the robot. </p> <p>It takes approximately 2 minutes for the arm to fully descend from the top of the lift. After about 3 minutes, the brake function will disengage, and the lift will be backdrivable as normal. If it is necessary to backdrive the lift during the period where the brake is engaged, it can be overcome with a sufficient amount of external force. Hold the arm securely when backdriving it in this way.</p>"},{"location":"hardware/hardware_guide_stretch_3/#arm","title":"Arm","text":"<p>The arm comprises 5 telescoping aluminum links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion.</p> <p>The arm exhibits a small amount of play (lash) in the X, Y, Z, and theta directions which is a normal characteristic of its design. Despite this it can achieve good repeatability, in part because its gravity loading is fairly constant.</p> <p>The retracted arm and wrist combined are designed to fit within the footprint of the base. The arm is designed to have:</p> <ul> <li>Reach: 0.52m</li> <li>Recommended Max payload - 2kg</li> </ul>"},{"location":"hardware/hardware_guide_stretch_3/#dexterous-wrist","title":"Dexterous Wrist","text":"<p>The Stetch 3 dexterous wrist includes:</p> <ul> <li>Yaw, Pitch, and Roll DOFs</li> <li>2 Aruco tags for calibration and visual localization of the tool</li> <li>Expansion port with</li> <li>Arduino expansion header</li> <li>USB-A connector</li> <li>Dynamixel X-Series TTL bus</li> <li>Integrated cabling for USB and 12V Power cables</li> <li>Quick-disconnect tool interface</li> </ul> <p></p> Item Notes A Wrist Yaw B Wrist Pitch C Wrist Roll D Quick Disconnect E Wrist USB and Expansion Header"},{"location":"hardware/hardware_guide_stretch_3/#wrist-yaw","title":"Wrist Yaw","text":"<p>The wrist yaw is driven by a Dynamixel XC430-W240-T servo.</p> <p>The wrist yaw shaft includes a 'Zero indicator'. This mark indicates the forward position of the tool. It will point in the direction of the arm extension when the wrist yaw joint is at its zero position.</p> <p></p> <p>The wrist yaw DOF is calibrated so that the index hole faces forward at the 'zero' position. From this pose the wrist has a Range of Motion of +256/-76 degrees as shown.</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_stretch_3/#wrist-pitch","title":"Wrist Pitch","text":"<p>The wrist pitch is driven by a Dynamixel XM540-W270-T servo.</p> <p>The wrist pitch DOF is calibrated so that the tool is parallel to the ground at the 'zero' position. From this pose the wrist has a Range of Motion of +20/-90 degrees.</p> <p></p>"},{"location":"hardware/hardware_guide_stretch_3/#wrist-roll","title":"Wrist Roll","text":"<p>The wrist roll is driven by a Dynamixel XM430-W350-T servo.</p> <p>The wrist roll DOF is calibrated such that the zero position is directly in the center of its range of motion. From this pose the wrist has a Range of Motion of +172.5/-172.5 degrees.</p> <p></p>"},{"location":"hardware/hardware_guide_stretch_3/#wrist-accelerometer","title":"Wrist Accelerometer","text":"<p>The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The sensor is mounted inside the distal link of the arm as shown below.</p> <p></p> <p></p>"},{"location":"hardware/hardware_guide_stretch_3/#wrist-usb","title":"Wrist USB","text":"<p>The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V. By default, this port is utilized by the Intel d405 gripper camera.</p>"},{"location":"hardware/hardware_guide_stretch_3/#wrist-expansion-header","title":"Wrist Expansion Header","text":"<p>The wrist includes an expansion header that provides access to pins of the wrist Arduino board.  The header connector can be accessed by removing the cap at the end of the arm.</p> <p></p> <p>The header is wired to a Atmel SAMD21G18A-AUT (datasheet) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow:</p> <ul> <li>General purpose digital I/O</li> <li>Analog input</li> </ul> <p>In addition, the firmware can be configured for other pin functions, including:</p> <ul> <li>Serial SPI</li> <li>Serial I2C</li> <li>Serial UART</li> </ul> <p>The Stretch Firmware Manual covers this modification.</p> <p>**The header pins utilize 3V3 TTL logic. They have limited nterface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded **</p> <p>The pin mapping is:</p> Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS  Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input  Analog in (A0) <p>The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN).  It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305).</p> <p>Pin 1 &amp; 10 are indicated below.</p> <p></p> <p>The expansion DIO schematic shown below.</p> <p></p>"},{"location":"hardware/hardware_guide_stretch_3/#wrist-mounts","title":"Wrist Mounts","text":""},{"location":"hardware/hardware_guide_stretch_3/#wrist-quick-disconnect","title":"Wrist Quick Disconnect","text":""},{"location":"hardware/hardware_guide_stretch_3/#gripper","title":"Gripper","text":"<p>The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position.  </p> <p>The gripper includes an Intel RealSense d405 depth camera pointed at the fingertips. It also includes ArUco fiducial markers attached to each fingertip, which can be used to visually track the position and orientation of the fingertips.</p> <p></p> Item Notes A Gripper Camera Intel RealSense d405 B Fingertip ArUco tags <p>The weight of the Stretch Compliant Gripper is 240g.</p>"},{"location":"hardware/hardware_guide_stretch_3/#gripper-removal","title":"Gripper Removal","text":"<p>Here we describe removing the Stretch gripper. Installation is simply these steps in reverse.</p> <ol> <li> <p>Unplug the micro-USB cable from the d405 gripper camera. </p> </li> <li> <p>Unplug the Dynamixel cable at the back of the pitch joint.</p> </li> <li> <p>If necessary, clip any zip-ties connecting these cables to the robot.</p> </li> <li> <p>Depress the quick-disconnect button, and slid the gripper upwards to disconnect.</p> </li> </ol>"},{"location":"hardware/hardware_guide_stretch_3/#robot-care","title":"Robot Care","text":""},{"location":"hardware/hardware_guide_stretch_3/#battery-maintenance","title":"Battery Maintenance","text":"<p>Please review the Battery Maintenance Guide for proper care and charging of the Stretch batteries.</p>"},{"location":"hardware/hardware_guide_stretch_3/#belt-tension","title":"Belt Tension","text":"<p>A neoprene timing belt drives the arm up and down the lift. It may detension over long periods of time if it experiences sustained loading. In this case, slack will become visually apparent in the belt as the lift moves.</p> <p>The belt is very straightforward to re-tension. Please contact support@hello-robot.com for tensioning instructions.</p>"},{"location":"hardware/hardware_guide_stretch_3/#keeping-the-robot-clean","title":"Keeping the Robot Clean","text":"<p>The robot surfaces can be wiped down with an alcohol wipe or a moist rag from time to time in order to remove and debris or oils that accumulate on the shells or mast. </p> <p>The drive wheels can accumulate dust over time and begin to lose traction. They should be periodically wiped down as well.</p> <p>If the robot cameras requires cleaning, use appropriate lens cleaning fluid and a microfiber cloth.</p>"},{"location":"hardware/hardware_guide_stretch_3/#keeping-the-robot-calibrated","title":"Keeping the Robot Calibrated","text":"<p>The robot comes pre-calibrated with a robot-specific URDF. This calibration allows the D435i depth sensor to accurately estimate where the robot wrist, and body, is in the depth image.</p> <p>The robot may become slightly uncalibrated over time for a variety of reasons, including normal wear and tear and loosening of joints of the robot, or accidental collisions or falls leading to high loads on the robot joints.</p> <p>The calibration accuracy can be checked using the provided ROS tools. If necessary, the user can recalibrate the robot. See the Stretch URDF Calibration Guide for more information.</p>"},{"location":"hardware/hardware_guide_stretch_3/#transporting-the-robot","title":"Transporting the Robot","text":"<p>Stretch was designed to be easily transported in the back of a car, up a stair case, or around a building.</p> <p>For short trips, the robot can be simply rolled around by grabbing its mast. It may be picked up by its mast and carried up stairs as well. </p> <p>For safety, please use two people to lift the robot.</p> <p>For longer trips it is recommended to transport the robot in its original foam base packaging, or in the original cardboard box. The metal protective cage that surrounds the head is only necessary if the robot might be shipped and the box will not remain upright. If laying the robot down, take care not to put weight on the arm, wrist, tool, or camera assemblies. </p>"},{"location":"hardware/hardware_guide_stretch_3/#system-check","title":"System Check","text":"<p>It is useful to periodically run stretch_system_check.py. This will check that the robot's hardware devices are present and within normal operating conditions. </p>"},{"location":"hardware/hardware_guide_stretch_3/#regulatory-compliance","title":"Regulatory Compliance","text":"<p>Stretch is not certified for use as a consumer device in the U.S.</p> <p>Unless stated otherwise, Stretch is not subjected to compliance testing nor certified to meet any requirements, such as requirements for EMI, EMC, or ESD.</p> <p>Per FCC 47 CFR, Part 15, Subpart B, section 15.103(c), we claim Stretch as an exempted device, since it is a digital device used exclusively as industrial, commercial, or medical test equipment, where test equipment is equipment intended primarily for purposes of performing scientific investigations.</p> <p>OET BULLETIN NO. 62, titled \"UNDERSTANDING THE FCC REGULATIONS FOR COMPUTERS AND OTHER DIGITAL DEVICES\" from December 1993 provides further clarification of the Section 15.103(c) exemption: \u201cTest equipment includes devices used for maintenance, research, evaluation, simulation and other analytical or scientific applications in areas such as industrial plants, public utilities, hospitals, universities, laboratories, automotive service centers and electronic repair shops.\u201d</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"hardware/safety_guide/","title":"Robot Safety Guide","text":"<p>Stretch robots are potentially dangerous machines with safety hazards. If improperly used they can cause injury or death.</p> <ul> <li>All users must carefully read the following safety information before using the robot.</li> <li>Anyone near the robot who has not read this safety information must be closely supervised at all times and made aware that the robot could be dangerous.</li> <li>Only use the robot after inspecting the surrounding environment for potential hazards.</li> </ul>"},{"location":"hardware/safety_guide/#intended-use","title":"Intended Use","text":"<p>The Stretch robots are intended for use by researchers to conduct research in controlled indoor environments. This product is not intended for other uses and lacks the required certifications for other uses, such as use in a home environment by consumers.</p>"},{"location":"hardware/safety_guide/#safety-hazards","title":"Safety Hazards","text":"<p>As described later in this document, we have designed Stretch to be safer than previous commercially-available human-scale mobile manipulators, so that researchers can explore the future of mobile manipulation. For example, we have made it smaller and lighter weight with backdrivable torque-sensing joints that can stop when they detect contact.</p> <p>Nonetheless, Stretch is a research robot that can be dangerous. Researchers must use Stretch carefully to avoid damage, injury, or death. Here, we list several safety hazards that researchers must consider before and while using Stretch.</p>"},{"location":"hardware/safety_guide/#stretch-can-put-people-and-animals-at-risk","title":"Stretch Can Put People And Animals At Risk","text":"<p>As described in more detail later, Stretch can put people and animals at risk. People and animals near the robot must be closely supervised at all times. At all times, an experienced researcher must carefully monitor the robot and be prepared to stop it. Any people near the robot must be made aware that the robot could be dangerous. Before any use of the robot near people or animals, researchers must carefully assess and minimize risks.</p> <p>Researchers who use the robot near children, animals, vulnerable adults, or other people do so at their own risk. Researchers must take appropriate precautions and obtain the required approvals from their organizations.</p>"},{"location":"hardware/safety_guide/#stretch-can-topple-onto-a-person","title":"Stretch Can Topple Onto A Person","text":"<p>The robot may drive off stairs, push or pull itself over with its telescoping arm, fall over while attempting to traverse a threshold or encounter obstacles that cause it to fall on or otherwise collide with people, causing injury.</p> <p>Operate the robot only on flat surfaces away from stairs or other obstacles that may cause it to topple, and do not allow the robot to push or pull itself over.</p>"},{"location":"hardware/safety_guide/#stretch-should-not-be-lifted-by-a-single-person","title":"Stretch Should Not Be Lifted By A Single Person","text":"<p>Stretch with the Dex Wrist weighs about 24 kg (53 lb), so two or more people should lift and carry the robot. A single person can move the robot around by enabling the runstop button, tilting it over, and rolling it on flat ground.</p> <p>At least two people should lift and carry the robot when needed.</p>"},{"location":"hardware/safety_guide/#stretch-can-cause-lacerations","title":"Stretch Can Cause Lacerations","text":"<p>The robot's wrist and tool have sharp edges that can cause lacerations or punctures to the skin or the eyes.</p> <p>Operate the robot away from eyes and other sensitive body parts.</p>"},{"location":"hardware/safety_guide/#stretch-can-trap-crush-or-pinch-body-parts","title":"Stretch Can Trap, Crush, Or Pinch Body Parts","text":"<p>The robot has moving joints that can trap, crush or pinch hands, fingers, or other body parts. The robot could also injure a person or animal by driving over a body part.</p> <p>Keep body parts away from the trap, crush, and pinch points during robot motion, including underneath the wheels.</p>"},{"location":"hardware/safety_guide/#stretch-can-entrap-loose-clothing-or-hair","title":"Stretch Can Entrap Loose Clothing Or Hair","text":"<p>The robot's shoulder and telescoping arm have rollers that can pull in and entrap loose clothing or hair.</p> <p>Keep loose clothing and long hair away from the robot's shoulder and telescoping arm when either is in motion.</p>"},{"location":"hardware/safety_guide/#stretch-has-flammable-components","title":"Stretch Has Flammable Components","text":"<p>The robot has polyurethane covers that are flammable and must be kept away from potential ignition sources, such as open flames and hot surfaces. The robot\u2019s head, shoulder, and mobile base have polyurethane covers.</p> <p>Keep the robot away from potential ignition sources and always have a working fire extinguisher nearby.</p>"},{"location":"hardware/safety_guide/#stretch-is-an-electrical-device","title":"Stretch Is An Electrical Device","text":"<p>Stretch has batteries, electronics, wires, and other electrical components throughout its body. It also provides uncovered connectors that provide power. While the robot has fuses to reduce electrical risks, users must be careful.</p> <p>Keep the robot dry and away from liquids, avoid electrical shocks, ensure power cables and wires are in good condition, be careful with the robot\u2019s connectors, and generally exercise caution while working with this electrical device.</p>"},{"location":"hardware/safety_guide/#stretch-can-perform-dangerous-activities","title":"Stretch Can Perform Dangerous Activities","text":"<p>Stretch is a versatile robot capable of performing many actions, including actions that would be dangerous to people. For example, if a dangerous object is held by or affixed to the robot, such as a knife, a heavy object, or breakable glass, the robot can become very dangerous. Likewise, the robot is capable of physically altering the environment in ways that would be dangerous, such as turning a knob that releases gas from a gas stove.</p> <p>Users must be cautious while using the robot to ensure it interacts safely with people and the surrounding environment.</p>"},{"location":"hardware/safety_guide/#stretch-is-an-open-platform-that-can-be-made-more-dangerous","title":"Stretch Is An Open Platform That Can Be Made More Dangerous","text":"<p>Stretch is an open platform with user-modifiable and user-extensible hardware and software. User changes to the hardware or software can entail serious risks. For example, when shipped, the robot has conservative settings that restrict its speed and the forces it applies to reduce the risks associated with the robot. By modifying the robot, users could enable the robot to move at unsafe speeds and apply unsafe forces. As another example, improper electrical connections could result in a fire.</p> <p>Researchers who choose to modify or extend the robot\u2019s hardware or software do so at their own risk and should be careful to understand the implications of their modifications or extensions. Changes to the robot could result in dangerous situations that cause injury or death.</p>"},{"location":"hardware/safety_guide/#additional-risks","title":"Additional Risks","text":"<p>The most important aspects of safety with Stretch are to use good judgment and common sense. Additional important considerations follow:</p> <ul> <li>If the robot appears to be damaged, stop the robot immediately.</li> <li>Always be ready to stop the robot.</li> <li>Do not operate the robot unless an experienced user is present and attentive.</li> <li>Be aware that the robot can move in unexpected ways.</li> <li>Do not put fingers or other objects into the channel that runs along the length of the mast. A belt moves within this channel.</li> <li>Keep an eye on cords, rugs, and any other floor hazards as the robot drives.</li> <li>Keep the robot at least 3 meters from ledges, curbs, stairs, and any other toppling hazard.</li> <li>Do not operate the robot outdoors.</li> <li>Do not attempt to ride the robot.</li> <li>Do not have the robot hold sharp objects.</li> <li>Do not attempt to service the robot without supervision by Hello Robot.</li> </ul>"},{"location":"hardware/safety_guide/#other-problems-will-likely-occur","title":"Other Problems Will Likely Occur","text":"<p>\u201cAnticipate potential problems and hazards. Always imagine what might happen if the robot malfunctions or behaves in a way different from the desired action. Be vigilant.\u201d - PR2 User Manual by Willow Garage from October 5, 2012</p> <p>Stretch is a complex device that includes many mechanical, electrical, and computational systems that have been designed to work together. Be prepared for something to go wrong. For example, a motor control board might fail, software might not operate as anticipated, an unexpected process might still be running on the robot, or the batteries for the gamepad-style controller or the robot itself might run out.</p>"},{"location":"hardware/safety_guide/#safety-features","title":"Safety Features","text":"<p>We have considered safety from the outset in the design of Stretch.</p> <ul> <li>Runstop: The illuminated runstop button on Stretch\u2019s head can be used to pause the operation of the four primary joints (base, lift, and arm) of the robot when it is in motion.</li> <li>Lightweight design: The overall mass of Stretch with the Dex Wrist is 24Kg (53lb), and the majority of the mass is in the base. The carbon fiber arm and aluminum mast make for a remarkably lightweight upper body. While this reduces the risk of crushing, crushing injuries can still occur and should be carefully monitored.</li> <li>Gravity friendly: Due to Stretch\u2019s design, its actuators don't have to counteract gravity on a large lever arm. As a result, the motors and gearboxes are lower torque and lower weight than a conventional mobile manipulator with a comparable reach, avoiding the often dangerously strong shoulder joints of typical robot arms.</li> <li>Low gear ratio: The primary joints of Stretch (base, lift, and arm) have low gear-ratios (approx 5:1), allowing for backdriving of joints when powered off. A low gear-ratio also reduces the effective inertia of each joint, limiting the impacted force during undesired contact with people and the environment.</li> <li>Contact Sensitivity: The four primary joints of Stretch (base, lift, and arm) have contact sensitivity. We measure motor currents to estimate contact forces. Because Stretch is a low gear-ratio robot, current sensing provides a fairly sensitive measure of contact forces.</li> <li>Firmware limits: Motor torques are limited at the lowest level of the firmware to configured bounds.</li> <li>Velocity limits: Fast motions of the base are restricted when the arm is up high and the tool is outside the base footprint. This limits the likelihood of toppling or snagging the tool during base motion.</li> <li>Tilt detection: The robot can detect when its body is tilted beyond a safe threshold. The robot can be configured to trigger a runstop event during an over-tilt event.</li> </ul>"},{"location":"hardware/safety_guide/#safety-markings","title":"Safety Markings","text":"<p>Stretch has the following safety markings:</p> <ul> <li> <p>Top of the shoulder, indicating potential pinch point between rollers and mast.</p> <ul> <li></li> </ul> </li> <li> <p>Top of the base, indicating potential pinch point between arm and base.</p> <ul> <li></li> </ul> </li> <li> <p>Top of the Dex Wrist, indicating potential pinch points between joints of the wrist and arm.</p> <ul> <li></li> </ul> </li> </ul>"},{"location":"hardware/safety_guide/#runstop","title":"Runstop","text":"<p>The runstop allows the user to pause the motion of the four primary actuators (base, lift, and arm) by tapping the illuminated button on the head. An experienced operator should always keep the runstop within reach, allowing them to stop the motion of the robot if it is deemed unsafe.</p> <p>Warning</p> <p>The runstop is not equivalent to an Emergency Stop found on industrial equipment and no safety guarantees are made by its function.</p> <p>When the runstop is enabled, these actuators are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume.</p> <p>The runstop logic is:</p> Action Runstop State Button Illumination Robot startup Motion enabled Solid Tap runstop button Motion disabled Flashing at 1Hz Hold down runstop button for &gt;2s Motion enabled Solid"},{"location":"hardware/safety_guide/#safety-hazard-details","title":"Safety Hazard Details","text":""},{"location":"hardware/safety_guide/#sharp-edges","title":"Sharp Edges","text":"<p>The Stretch robot is a piece of laboratory equipment. As such, its structure has moderately sharp edges and corners that can be unsafe. These edges can get snagged during motion, or they may cause lacerations when sufficient force is applied to a person. Care should be taken when grasping or otherwise making contact with Stretch that a sharp corner or edge is not contacted.</p>"},{"location":"hardware/safety_guide/#toppling","title":"Toppling","text":"<p>Stretch is a relatively lightweight robot. In some kinematic configurations, a high center of gravity can make it prone to toppling. Toppling can occur when:</p> <ul> <li>The mobile base is moving at a moderate or fast speed and hits a bump, threshold, or other change in floor property.</li> <li>The arm is raised high and pushes or pulls on the environment with sufficient force.</li> <li>The robot drives over a drop-off such as a stair or a curb.</li> </ul> <p>Warning</p> <p>While Stretch has cliff sensors, they do not currently inhibit motion of the base. During typical use, the robot will not attempt to stop itself at a cliff, and can fall down stairs and hurt itself or a person.</p>"},{"location":"hardware/safety_guide/#pinch-points","title":"Pinch Points","text":"<p>Pinch points around the robot's head, gripper, and wrist can cause discomfort and care should be taken when handling these joints as they move.</p> <p>The shoulder, which travels up and down on the lift, has a series of rollers that ride along the mast. While the shoulder shells can prevent large objects from getting pinched by the rollers, small and thin objects can be pulled into and crushed.</p> <p>The telescoping arm, which extends and retracts, has rollers that ride along the telescoping elements. While the arm link cuffs can reduce the chance of large objects getting pinched, small and thin objects, such as hair, can be pulled in.</p> <p>The wrist has three rotational joints in close proximity to one another - care should be taken to avoid getting hair, clothing, or fingers trapped in between these joints where they could get pinched. Special attention should be paid to potential pinch points between the wrist pitch and wrist yaw structures during yaw motion, and between the gripper and wrist pitch structures during pitch motion.</p> <p>Extra care should be taken with long hair, clothing, and small fingers around the shoulder rollers.</p>"},{"location":"hardware/safety_guide/#crush-points","title":"Crush Points","text":"<p>The lift degree of freedom is the strongest joint on the robot and as such can apply potentially unsafe forces to a person.</p> <p></p> <p>The lift, while in motion, may trap or crush objects between the \u2018shoulder\u2019 and another surface. As such, best practices for lift safety should always be used when using the lift degree of freedom.</p> <p>The lift has a max theoretical strength of nearly 200N of linear force. In practice, this force is limited by the lift\u2019s Guarded Move function, which places the lift in Safety Mode when the actuator forces exceed a threshold.</p> <p>The diagrams below show the potential crush points at the top and bottom of the lift range of motion.</p> <p></p> <p></p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"installation/add_new_user/","title":"Adding a New User","text":""},{"location":"installation/add_new_user/#why","title":"Why","text":"<p>If you're sharing Stretch with other developers, it can be helpful to create separate accounts for yourself and your team members. Your code and data will be password protected in your own account, and other developers can modify their own code without accidentally affecting yours.</p> <p>Warning</p> <p>User accounts cannot completely insulate your account from changes in another. For example, if someone attaches a new gripper or end-effector tool to the robot, your account's software would have an outdated configuration for what tool is attached to the robot. A non-exhaustive list of changes that could break/affect accounts:</p> <ul> <li>Making hardware changes to the robot</li> <li>Updating the firmware</li> <li>Installing/changing APT packages</li> </ul>"},{"location":"installation/add_new_user/#how","title":"How","text":"<p>Start by logging into the admin Hello Robot user. Go to Users system settings and unlock adminstrator actions.</p> <p></p> <p>Click \"Add User...\" and complete the subsequent form. The new user needs to be an administrator.</p> <p></p> <p>Log out and back in as the new user. Open a terminal and execute the following to pull down the Stretch Install repository:</p> <pre><code>git clone https://github.com/hello-robot/stretch_install ~/stretch_install\n</code></pre> <p>Make sure it's up-to-date:</p> <pre><code>cd ~/stretch_install &amp;&amp; git pull\n</code></pre> <p>Run the new user install script to set up the SDK for this new account:</p> <pre><code>./stretch_new_user_install.sh\n</code></pre> <p>Finally, reboot the robot and run a system check in the new user account to confirm everything was set up correctly.</p> <pre><code>stretch_system_check.py\n</code></pre> <p>Your new user account is now set up successfully!</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"installation/configure_BIOS/","title":"Configuring the BIOS","text":"<p>This documentation describes how to configure the BIOS of an Intel NUC for compatibility with the stretch installation procedure.</p>"},{"location":"installation/configure_BIOS/#accessing-the-nuc-bios-settings","title":"Accessing the NUC BIOS Settings","text":"<p>First plug in the NUC to a 19V DC power supply. Next power on the NUC using the power button on the front of the NUC.</p> <p>When powered on, the NUC should display a welcome screen similar to the picture below:</p> <p></p> <p>When this label becomes visible press 'F2' to enter into the BIOS configuration menu.</p> <p>Note</p> <p>If you're using a Bluetooth keyboard, the BIOS likely won't recognize the F2 keypress.</p> <p>The BIOS Settings page should look like the picture below:</p> <p></p> <p>Select the 'Advanced' drop down menu near the top right of the screen, and then slect the option 'Boot'</p> <p></p> <p>From the 'Boot' settings page select the 'Secure Boot' tab.</p> <p></p> <p>Turn off 'Secure Boot' by toggling the checkbox labeled 'Secure Boot' to unchecked.</p> <p></p> <p>Next Select the 'Power' tab</p> <p></p> <p>From the power settings screen select the 'Power On' option from the 'After Power Failure' drop down selection.</p> <p></p> <p>Next Select the Security tab</p> <p></p> <p>Turn on UEFI third party drivers compatibility by toggling the checkbox labeled 'Allow UEFI Third Party Driver loaded' to checked.</p> <p></p> <p>Now use the F10 key to save BIOS configuration changes and exit.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"installation/contributing/","title":"Contributing to Stretch Install","text":"<p>Thank you for considering contributing to this repository. Stretch Install houses bash scripts and tutorials that enable users to setup/configure their robots. This guide explains the layout of this repo and how best to make and test changes.</p>"},{"location":"installation/contributing/#repo-layout","title":"Repo Layout","text":"<ul> <li><code>README.md</code> &amp; <code>LICENSE.md</code> - includes info about the repo and a table of tutorials available</li> <li><code>stretch_new_*_install.sh</code> - high level scripts meant to be run by the user</li> <li><code>factory/</code> - subscripts and assets not meant to be run by the user</li> <li><code>18.04/</code> - subscripts and assets specific to performing a Ubuntu 18.04 software install<ul> <li><code>stretch_initial_setup.sh</code> - a bunch of checks and initial setup that are run before performing a robot install</li> <li><code>stretch_install_*.sh</code> - helper scripts that install a specific set of packages</li> <li><code>stretch_create_*_workspace.sh</code> - creates a ROS/ROS2 workspace</li> <li><code>stretch_ros*.repos</code> - the ROS packages that are included and compiled in the ROS workspace by the <code>stretch_create_*_workspace.sh</code> script</li> <li><code>hello_robot_*.desktop</code> - autostarts programs to run when the robot boots up</li> </ul> </li> <li><code>&lt;&gt;.04/</code> - Ubuntu &lt;&gt;.04 software install related subscripts/assets. Similar in layout to 18.04/</li> <li><code>docs/</code> - contains tutorials for using the scripts in this repo</li> </ul> <p>Once you're ready to make changes to this repo, you can fork it on Github.</p>"},{"location":"installation/contributing/#contributing-to-the-tutorials","title":"Contributing to the tutorials","text":"<p>The tutorials in the <code>docs/</code> folder are markdown files that get rendered into our https://docs.hello-robot.com site. If you edit them and file a pull request towards this repo, the changes to the tutorials will get reflected on the docs site. In order to live preview changes to these tutorials, first install mkdocs using:</p> <pre><code>python3 -m pip install mkdocs mkdocs-material mkdocstrings==0.17.0 pytkdocs[numpy-style] jinja2=3.0.3\n</code></pre> <p>Then, run the dev server using:</p> <pre><code>python3 -m mkdocs serve\n</code></pre> <p>Now you can make additions or changes to the source markdown files and see the changes reflected live in the browser.</p>"},{"location":"installation/contributing/#contributing-to-the-installation-scripts","title":"Contributing to the installation scripts","text":"<p>If you are looking to change scripts/assets of an existing software installation (e.g. Ubuntu 18.04/20.04), look within the <code>factory/&lt;&gt;.04/</code> directory and make changes to the appriopriate files. If you're looking to add support for a new Ubuntu distro (e.g. Ubuntu 19.04), create <code>factory/19.04</code> with assets from a previous installation and tweak the scripts until they works correctly for the Ubuntu distro you are targeting. Then, edit the high level scripts (e.g. <code>stretch_new_*_install.sh</code>) to call your distro's specific assets correctly. Ensuring that the tutorials in the <code>docs/</code> work for your new distro is a good way to ensure that your <code>factory/&lt;&gt;.04/</code> directory works correctly. Since bash scripts change behavior based on the underlying system, it can be helpful to use containers to create reproducible behaviors while you're developing support for the new distro. Multipass works well on Ubuntu systems. You can create a new container emulating any Ubuntu distro using the command:</p> <pre><code>multipass launch -c 6 -d 30G -m 7G -n &lt;container-name&gt; 19.04\n</code></pre> <p>Swap <code>19.04</code> in the above command with the distro you're targeting. The above command creates a containers with 30GB disk space, 7GB swap, 6 cores, and the name <code>&lt;container-name&gt;</code>. We've found that at least 30GB disk space is needed for the Ubuntu 18.04/20.04 installations.</p> <p>Then, you can access the shell of your new container using:</p> <pre><code>multipass shell &lt;container-name&gt;\n</code></pre> <p>Other helpful multipass subcommand include <code>transfer</code>, which allows you to transfer files to the container, and <code>delete</code>, which allows you to delete the container. See the multipass docs for more details.</p>"},{"location":"installation/contributing/#filing-a-pull-request","title":"Filing a Pull Request","text":"<p>Once your changes are committed to your fork, you can open a pull request towards the Stretch Install master branch. A member of Hello Robot's software team will review your new PR and get it merged and available for all Stretch users.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"installation/install_ubuntu_18.04/","title":"Ubuntu 18.04 Installation","text":"<p>This guide describes how to perform an OS installation of Ubuntu 18.04 LTS onto Stretch.</p>"},{"location":"installation/install_ubuntu_18.04/#ubuntu-image","title":"Ubuntu Image","text":"<p>Download the 18.04.1 amd64 Ubuntu desktop image by clicking this link:</p> <p>http://old-releases.ubuntu.com/releases/18.04.1/ubuntu-18.04.1-desktop-amd64.iso</p> <p>Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive. There is a good video tutorial available here that explains the procedure.</p>"},{"location":"installation/install_ubuntu_18.04/#installation","title":"Installation","text":"<p>Insert a bootable drive into the USB hub in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu.</p> <p></p> <p>Note</p> <p>If you're using a Bluetooth keyboard, the BIOS likely won't recognize the F10 keypress.</p> <p>From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\".</p> <p></p> <p>From here, the monitor should show the grub bootloader and display a menu similar to what is shown below:</p> <p></p> <p>From this menu select 'Install Ubuntu'. The Ubuntu 18.04 installer will be launched.</p> <p>At the first screen you will be prompted to select a language for the system. Select 'English' as shown below</p> <p></p> <p>Next you will be prompted to select a keyboard layout. Select 'English(US)'.</p> <p></p> <p>The next page will show a menu to select a wifi network if you are not already connected.</p> <p></p> <p>It is suggested to use a wired connection if possible for a faster install; your connection status should be visible in the top right of the display.</p> <p></p> <p></p> <p>On the next page titled, 'Updates and other software', select 'Minimal Installation' under 'What apps would you like to install to start with?'</p> <p>Also, check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and, uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'.</p> <p></p>"},{"location":"installation/install_ubuntu_18.04/#erase-reinstall-vs-install-alongside","title":"Erase &amp; Reinstall vs Install Alongside","text":"<p>On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 18.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase &amp; reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB.</p> <p>Here's what the Erase &amp; Reinstall option will look like, and an screenshot of the Install Alongside option is shown below.</p> <p></p> <p></p> <p>There will be a prompt to confirm you wish to create the appropriate partitions for the ubuntu install.</p> <p>If you've chosen the erase &amp; reinstall option, ensure there is nothing on the hard drive you wish to save before selecting continue</p> <p></p> <p>Next, select your timezone.</p> <p></p> <p>Finally, enter the identifying information as written below, replacing 'stretch-yyy-xxxx' with the appropriate name for the robot. <code>yyy</code> is your robot model number (\"re1\" for a Stretch RE1, \"re2\" for a Stretch 2, or \"se3\" for a Stretch 3), and <code>xxxx</code> is your robot's serial number. The robot's serial number can be found on a sticker on the left wall of the robot's trunk.</p> <ul> <li>name: Hello Robot Inc.</li> <li>computer name: stretch-yyy-xxxx</li> <li>username: hello-robot</li> <li>password: choose your own</li> </ul> <p>Also select the 'Log in automatically' option. When finished the 'Who are you' page should look like the picture below.</p> <p></p> <p>Ubuntu will now be installed.</p> <p></p> <p>After the installation is completed, you will be prompted to remove the installation medium and restart.</p> <p></p> <p>Remove the installation medium and turn off the robot.</p> <p>Ubuntu 18.04 is now installed successfully.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"installation/install_ubuntu_20.04/","title":"Ubuntu 20.04 Installation","text":"<p>This guide describes how to perform an OS installation of Ubuntu 20.04 LTS onto Stretch.</p>"},{"location":"installation/install_ubuntu_20.04/#ubuntu-image","title":"Ubuntu Image","text":"<p>Download the 20.04.4 amd64 Ubuntu desktop image.</p> <p>Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive. There is a good video tutorial available here that explains the procedure.</p>"},{"location":"installation/install_ubuntu_20.04/#installation","title":"Installation","text":"<p>Insert a bootable drive into the USB hub in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu.</p> <p></p> <p>Note</p> <p>If you're using a Bluetooth keyboard, the BIOS likely won't recognize the F10 keypress.</p> <p>From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\".</p> <p></p> <p>From here, the monitor should show the grub bootloader and display a menu similar to what is shown below:</p> <p></p> <p>From this menu select 'Ubuntu'. A disk errors checker will start and then the Ubuntu 20.04 installer will be launched.</p> <p></p> <p>At the first screen you will be prompted to select a language for the system. Select 'English' as shown below and click on the \"Install Ubuntu\".</p> <p></p> <p>Next you will be prompted to select a keyboard layout. Select 'English(US)'.</p> <p></p> <p>The next page will show a menu to select a Wifi network if you are not already connected.</p> <p></p> <p>It is suggested to use a wired connection if possible for a faster install; your connection status should be visible in the top right of the display.</p> <p></p> <p></p> <p>On the next page titled, 'Updates and other software', select 'Minimal Installation' under 'What apps would you like to install to start with?'</p> <p>Also, check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and, uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'.</p> <p></p>"},{"location":"installation/install_ubuntu_20.04/#erase-reinstall-vs-install-alongside","title":"Erase &amp; Reinstall vs Install Alongside","text":"<p>On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 20.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase &amp; reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB.</p> <p>Here's what the Erase &amp; Reinstall option will look like, and an screenshot of the Install Alongside option is shown below.</p> <p></p> <p></p> <p>There will be a prompt to confirm you wish to create the appropriate partitions for the ubuntu install.</p> <p>If you've chosen the erase &amp; reinstall option, ensure there is nothing on the hard drive you wish to save before selecting continue</p> <p></p> <p>Next, select your timezone.</p> <p></p> <p>Finally, enter the identifying information as written below, replacing 'stretch-yyy-xxxx' with the appropriate name for the robot. <code>yyy</code> is your robot model number (\"re1\" for a Stretch RE1, \"re2\" for a Stretch 2, or \"se3\" for a Stretch 3), and <code>xxxx</code> is your robot's serial number. The robot's serial number can be found on a sticker on the left wall of the robot's trunk.</p> <ul> <li>name: Hello Robot Inc.</li> <li>computer name: stretch-yyy-xxxx</li> <li>username: hello-robot</li> <li>password: choose your own</li> </ul> <p>Also select the 'Log in automatically' option. When finished the 'Who are you' page should look like the picture below.</p> <p></p> <p>Ubuntu will now be installed.</p> <p></p> <p>After the installation is completed, you will be prompted to remove the installation medium and restart.</p> <p> </p> <p>Remove the installation medium and press ENTER to restart.</p> <p>Ubuntu 20.04 is now installed successfully.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"installation/install_ubuntu_22.04/","title":"Ubuntu 22.04 Installation","text":"<p>This guide describes how to perform an OS installation of Ubuntu 22.04 LTS onto Stretch.</p>"},{"location":"installation/install_ubuntu_22.04/#ubuntu-image","title":"Ubuntu Image","text":"<p>Download the 22.04.3 amd64 Ubuntu desktop image.</p> <p>Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive. There is a good video tutorial available here that explains the procedure.</p>"},{"location":"installation/install_ubuntu_22.04/#installation","title":"Installation","text":"<ol> <li> <p>Insert a bootable drive into a USB port in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu.</p> <ul> <li></li> </ul> <p>Note</p> <p>If you're using a Bluetooth keyboard, the BIOS likely won't recognize the F10 keypress.</p> </li> <li> <p>From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\". This will take you to the grub menu.</p> <ul> <li></li> </ul> </li> <li> <p>From the grub menu, select 'Ubuntu' or look for a similar option that mentions \"Install Ubuntu\".</p> <ul> <li></li> </ul> </li> <li> <p>A disk errors checker will start and then the Ubuntu 22.04 installer will be launched.</p> <ul> <li></li> </ul> </li> <li> <p>The first screen of the installer will prompt you to select a language for the system and choose between trying or installing Ubuntu. Select 'English' and \"Install Ubuntu\".</p> <ul> <li></li> </ul> </li> <li> <p>Next you will be prompted to select a keyboard layout. Select 'English(US)'.</p> <ul> <li></li> </ul> </li> <li> <p>The next page will show a menu to select a Wifi network if you are not already connected. For a faster and more reliable install, we suggest using a wired connection if one is available to you.</p> <ul> <li></li> <li> <p>Your connection status will be visible in the top right of the display.</p> Wifi Ethernet </li> </ul> </li> <li> <p>The next page configures what gets installed. Select 'Minimal Installation' under 'What apps would you like to install to start with?' Check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'.</p> <ul> <li></li> </ul> </li> </ol>"},{"location":"installation/install_ubuntu_22.04/#erase-reinstall-vs-install-alongside","title":"Erase &amp; Reinstall vs Install Alongside","text":"<ol> <li> <p>On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 22.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase &amp; reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB.</p> Erase &amp; Reinstall Install Alongside </li> <li> <p>There will be a prompt to confirm you wish to create the appropriate partitions for the Ubuntu install. Clicking 'Continue' will begin making changes to your robot's hard drive, so ensure there is nothing on the hard drive you wish to save before selecting continue.</p> <ul> <li></li> </ul> </li> <li> <p>Next, select your timezone.</p> <ul> <li></li> </ul> </li> <li> <p>Finally, enter the identifying information as written below, replacing 'stretch-yyy-xxxx' with the appropriate name for the robot. <code>yyy</code> is your robot model number (\"re1\" for a Stretch RE1, \"re2\" for a Stretch 2, or \"se3\" for a Stretch 3), and <code>xxxx</code> is your robot's serial number. The robot's serial number can be found on a sticker on the left wall of the robot's trunk. Also select the 'Log in automatically' option.</p> <ul> <li>name: Hello Robot Inc.</li> <li>computer name: stretch-yyy-xxxx</li> <li>username: hello-robot</li> <li>password: choose your own</li> <li></li> </ul> </li> <li> <p>Ubuntu will now be installed.</p> <ul> <li></li> </ul> </li> <li> <p>After the installation is completed, you will be prompted to restart. Select 'Restart Now'.</p> <ul> <li></li> </ul> </li> <li> <p>Remove the installation medium and press ENTER to restart.</p> <ul> <li></li> </ul> </li> </ol> <p>Ubuntu 22.04 is now installed successfully.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"installation/robot_install/","title":"Upgrading your Operating System","text":""},{"location":"installation/robot_install/#why","title":"Why","text":"<p>This guide will lead you through installing a new robot distribution, which can be used to:</p> <ul> <li>Upgrade Stretch by installing a newer software stack alongside the previous OS</li> <li>Erase the previous OS and set up Stretch with an entirely fresh software stack</li> <li>Erase a corrupted OS and set up Stretch with an entirely fresh software stack</li> </ul> <p>Each OS installs on a separate partition on the hard drive. You can create as many robot-level installs (i.e. new partitions) as will fit in your robot's hard drive.</p> <p>The list of supported/deprecated distributions is available in the Distributions &amp; Roadmap guide.</p>"},{"location":"installation/robot_install/#how","title":"How","text":"<p>There are a few steps to performing a new robot install:</p> <ol> <li>Plug in charger &amp; Attach clip-clamp</li> <li>Backup robot calibration data</li> <li>Setup the BIOS (only necessary for NUCs not previously configured by Hello Robot)</li> <li>Install Ubuntu</li> <li>Run the new robot installation script</li> <li>Post installation steps</li> </ol> <p>It typically takes ~2 hours to go through these steps. Before we get started, you'll need:</p> <ul> <li>1 Stretch robot</li> <li>Keyboard, Mouse, and Monitor<ul> <li>Bluetooth keyboards/mouses will not work because they are not recognized by the BIOS. Use a wired USB or wireless USB dongle keyboard/mouse.</li> <li>Use a computer monitor instead of a TV. Some TVs have trouble displaying Ubuntu when rebooting.</li> </ul> </li> <li>2 USB sticks<ul> <li>The first flashdrive is used to backup robot calibration data and any other important files</li> <li>The second flashdrive will contain the Ubuntu installer image. This flashdrive will need to be &gt;8gb in capacity.</li> </ul> </li> <li>1 NOCO Genius 10 charger</li> <li>1 Clip Clamp</li> <li> <p>A fast connection to the internet</p> <p>Note</p> <p>It's recommended that you use an Ethernet connection to the internet. You can use a slow Wifi connection if it's difficult to obtain a wired connection at your institution, but expect the install process to take longer because the scripts are downloading gigabytes of software/data to the robot.</p> </li> </ul>"},{"location":"installation/robot_install/#plug-in-charger-attach-clip-clamp","title":"Plug in charger &amp; Attach clip-clamp","text":"<p>Since a new robot install can take a few hours, it's important the robot remain on the charger throughout the install. Switch the charger into SUPPLY mode and plug the charger into the robot.</p> <p></p> <p>Next, attach the clip-clamp below the shoulder as shown. This allows the arm to rest on the clamp during the firmware portion of the install.</p>"},{"location":"installation/robot_install/#back-up-robot-calibration-data","title":"Back up robot calibration data","text":"<p>It is a good idea to backup all valuable data beforehand. If your new robot install will replace a previous one, data from the previous robot install will be deleted. Even if your new robot install will live alongside the previous one(s), data from the previous robot install(s) can be lost.</p> <p>In particular, your new robot install will require the old install's robot calibration data. The steps to copy this material from an existing install is:</p> <ol> <li>Boot into the robot's original Ubuntu partition and plug in a USB key.</li> <li>The robot calibration data lives inside of a directory called <code>stretch-&lt;yyy&gt;-&lt;xxxx&gt;</code>, where <code>&lt;yyy&gt;</code> is your robot model number (\"re1\" for a Stretch RE1, \"re2\" for a Stretch 2, or \"se3\" for a Stretch 3), and <code>&lt;xxxx&gt;</code> is your robot's serial number. There's a few versions of this directory and you will need to decide which version to backup. Each Ubuntu user has a version of this directory located at <code>/home/$USER/stretch_user/stretch-&lt;yyy&gt;-&lt;xxxx&gt;</code>. These user versions are updated when the user runs a URDF calibration, swaps out an end effector, updates Stretch parameters, and more. There's also a system version located at <code>/etc/hello-robot/stretch-&lt;yyy&gt;-&lt;xxxx&gt;</code>, which is likely the oldest version since it was created at Hello Robot HQ. If you're not sure which version to backup, use the version at <code>/etc/hello-robot/stretch-&lt;yyy&gt;-&lt;xxxx&gt;</code> for the next step.</li> <li>Copy the <code>stretch-&lt;yyy&gt;-&lt;xxxx&gt;</code> directory to a USB key.<ul> <li>For example, if you're copying the system version, you can run a command similar to <code>cp -r /etc/hello-robot/stretch-&lt;yyy&gt;-&lt;xxxx&gt; /media/$USER/&lt;USBKEY&gt;</code> from the command line, where <code>&lt;USBKEY&gt;</code> and <code>&lt;xxxx&gt;</code> is replaced with the mounted USB key's name and the robot's serial number, respectively.</li> <li>Or, you can open the file explorer to copy the directory.</li> </ul> </li> </ol> <p>If your previous partition is corrupted or inaccessible, contact Hello Robot support and they will be able to supply an older version of the <code>stretch-&lt;yyy&gt;-&lt;xxxx&gt;</code> directory.</p>"},{"location":"installation/robot_install/#setup-the-bios","title":"Setup the BIOS","text":"<p>This step can be skipped if your robot had an existing software install on it. Otherwise, follow the guide to set up the BIOS.</p>"},{"location":"installation/robot_install/#install-ubuntu","title":"Install Ubuntu","text":"<p>Choose between the following guides based on which version of Ubuntu you're installing (see the Distributions &amp; Roadmap guide for info on what software ships with each OS). Within these guides, you'll have the choice of whether to replace the previous OS partition or to install alongside it. If you choose to install alongside it, you'll also be able to choose the size of each partition on the hard drive.</p> <ul> <li>Ubuntu 18.04 Installation guide</li> <li>Ubuntu 20.04 Installation guide</li> <li>Ubuntu 22.04 Installation guide</li> </ul> <p>After the Ubuntu install, the default <code>hello-robot</code> user account will be set up.</p>"},{"location":"installation/robot_install/#run-the-robot-installation-script","title":"Run the robot installation script","text":"<p>Login to the <code>hello-robot</code> user account on your new Ubuntu partition, open a terminal, and run:</p> <pre><code>sudo apt update\nsudo apt install git zip\n</code></pre> <p>Note</p> <p>The system may not be able to run APT immediately after a reboot as the OS may be running automatic updates in the background. Typically, waiting 10-20 minutes will allow you to use APT again.</p> <p>Next, place the robot's calibration data in the home folder using the following steps:</p> <ol> <li>Plug in the USB key that contains the backed up calibration data.</li> <li>Copy the <code>stretch-&lt;yyy&gt;-&lt;xxxx&gt;</code> directory from the USB key into the home folder (i.e. <code>/home/$USER/</code>).<ul> <li>For example, you can run a command similar to <code>cp -r /media/$USER/&lt;USBKEY&gt;/stretch-&lt;yyy&gt;-&lt;xxxx&gt; /home/$USER/</code> from the command line, where <code>&lt;USBKEY&gt;</code> and <code>&lt;xxxx&gt;</code> are replaced with your USB key's name and your robot's serial number, respectively.</li> <li>Or, you can open the file explorer to copy the directory.</li> </ul> </li> </ol> <p>Next, use git to pull down the Stretch Install repository and begin the installation process:</p> <pre><code>cd ~/\ngit clone https://github.com/hello-robot/stretch_install\ncd stretch_install\ngit pull\n./stretch_new_robot_install.sh\n</code></pre> <p>Once the script has started, it will ask you for your robot's serial number, Y/N confirmation, and the password. Then, the script will typically take 20-30 minutes to complete on a wired connection. Once it finishes, it should print out something similar to:</p> <pre><code>#############################################\nDONE! INSTALLATION COMPLETED SUCCESSFULLY.\n[...]\n#############################################\n</code></pre> <p>If it has not printed out 'DONE', then the robot install did not complete successfully. Take a look at the troubleshooting section below for solutions to common issues, or contact Hello Robot support via email or the forum.</p>"},{"location":"installation/robot_install/#post-install-steps","title":"Post install steps","text":"<p>Next, we'll complete the post install steps. First, in order for the many changes to take effect, the robot will need a full reboot. The steps are:</p> <ol> <li>Ensure there's a clamp under the lift</li> <li>Shutdown the Ubuntu OS through the GUI or use <code>sudo shutdown -h now</code> in the terminal</li> <li>Turn off the power switch in the robot's trunk</li> <li>Ensure a keyboard/monitor is plugged into the robot. When the robot powers up, you can use the keyboard to decide which OS to boot into.</li> <li>Turn on the power switch in the robot's trunk</li> <li>Boot into the new Ubuntu partition and log in if necessary</li> </ol> <p>Next, we'll ensure the robot's firmware is upgraded to the latest available.</p> <pre><code>REx_firmware_updater.py --install\n</code></pre> <p>Next, we'll configure the software for the tool attached to your robot.</p> <pre><code>stretch_configure_tool.py\n</code></pre> <p>Next, we'll run Stretch's homing procedure, where every joint's zero is found. This is a ~30 second procedure that must occur everytime the robot wakes up.</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>Next, we'll calibrate the contact thresholds for the robot's arm joint.</p> <pre><code>REx_calibrate_guarded_contact.py --arm\n</code></pre> <p>Next, we'll remove the clip-clamp and calibrate the contact thresholds for the robot's lift joint.</p> <p></p> <pre><code>REx_calibrate_guarded_contact.py --lift\n</code></pre> <p>Next, we'll run the system check to confirm the robot is ready to use. If you see any failures or errors, contact Hello Robot support via email or the forum.</p> <pre><code>stretch_system_check.py\n</code></pre> <p>Finally, this step is optional. The robot can be configured to automatically run the gamepad teleop program when it boots up. To do this, open Startup Applications and enable the \"hello_robot_gamepad_teleop\" program.</p> <p></p> <p>Your robot is now set up with a new operating system! If you're new to Stretch, consider going through the Getting Started tutorials.</p>"},{"location":"installation/robot_install/#troubleshooting","title":"Troubleshooting","text":"<p>This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum.</p>"},{"location":"installation/robot_install/#expecting-var-hello_fleet_id-to-be-undefined-error","title":"'Expecting var HELLO_FLEET_ID to be undefined' error","text":"<p>If you are seeing the following error:</p> <pre><code>[...]\nChecking ~/.bashrc doesn't already define HELLO_FLEET_ID...\nExpecting var HELLO_FLEET_ID to be undefined. Check end of ~/.bashrc file, delete all lines in 'STRETCH BASHRC SETUP' section, and open a new terminal. Exiting.\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>You are performing a new robot install on a robot that has already gone through the robot install process. If this is intentional, you will need to manually delete lines that a previous robot install appended to the <code>~/.bashrc</code> dotfile. Open the <code>~/.bashrc</code> file in an editor and look near the end for a section that looks like:</p> <pre><code>######################\n# STRETCH BASHRC SETUP\n######################\nexport HELLO_FLEET_PATH=/home/ubuntu/stretch_user\nexport HELLO_FLEET_ID=stretch-re1-1000\nexport PATH=${PATH}:~/.local/bin\nexport LRS_LOG_LEVEL=None #Debug\nsource /opt/ros/noetic/setup.bash\nsource /home/ubuntu/catkin_ws/devel/setup.bash\n[...]\n</code></pre> <p>Delete this section from the <code>~/.bashrc</code>. Note that it's common for other programs (e.g. Conda, Ruby) to append to your <code>~/.bashrc</code> as well, and deleting those lines accidentally can impede their functionality. Take care to only delete lines related to 'STRETCH BASHRC SETUP' section. Next, open a new terminal. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) automatically runs the commands in the <code>~/.bashrc</code> dotfile when opened, so the new terminal won't be set up with the lines that were just deleted. Now you can run a new robot install and this error should gone.</p>"},{"location":"installation/robot_install/#expecting-stretch-yyy-xxxx-to-be-present-in-the-home-folder-error","title":"'Expecting stretch-yyy-xxxx to be present in the home folder' error","text":"<p>If you are seeing the following error:</p> <pre><code>[...]\nChecking robot calibration data in home folder...\nExpecting robot calibration stretch-yyy-xxxx to be present in the the home folder. Exiting.\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>The install scripts exited before performing the robot install because it was unable to find the robot's calibration data folder, 'stretch-yyy-xxxx'. Please ensure you have backed up your robot's calibration data to a USB key and copied the 'stretch-yyy-xxxx' folder to the home folder of your new partition. See the Run the robot installation script section for more details. Then, run the install scripts again and the error should be gone.</p>"},{"location":"installation/robot_install/#repo-not-up-to-date-error","title":"'Repo not up-to-date' error","text":"<p>If you are seeing the following error:</p> <pre><code>[...]\nChecking install repo is up-to-date...\nRepo not up-to-date. Please perform a 'git pull'. Exiting.\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>The version of Stretch Install being used is out of date. In a terminal, go to the Stretch Install folder (should be in the home folder: <code>cd ~/stretch_install</code>), and perform a <code>git pull</code> to pull down the latest version. If the git pull fails, ensure Stretch Install has a clean working tree using <code>git status</code>. If you see any red files, save them if important, delete Stretch Install, and reclone it.</p>"},{"location":"installation/robot_install/#failed-to-fetch-error","title":"'Failed to fetch' error","text":"<p>If you are seeing the following error:</p> <pre><code>Install &lt;some package&gt;\nE: Failed to fetch &lt;url to some .deb file&gt;  Connection failed [IP: &lt;some IP address&gt;]\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>Ubuntu's system package manager, Apt, has failed to contact the server that hosts some package that the install scripts need to download. Typically, these issues are transient and waiting some time before rerunning the install script will solve the issue.</p>"},{"location":"installation/robot_install/#dpkg-returned-an-error-code-error","title":"'dpkg returned an error code' error","text":"<p>If you are seeing the following error:</p> <pre><code>Install &lt;some package&gt;\nE: Sub-process /usr/bin/dpkg returned an error code (1)\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>Ubuntu's system package manager, Apt, has failed to complete some step of the install process for a package that the install scripts need to install. Typically, these issues are transient and waiting some time before rerunning the install script will solve the issue. If you continue to see this error, contact Hello Robot support via email or the forum.</p>"},{"location":"installation/robot_install/#firmware-protocol-mismatch-error","title":"'Firmware protocol mismatch' error","text":"<p>If you are seeing the following error: <pre><code>----------------\nFirmware protocol mismatch on hello-&lt;X&gt;.\nProtocol on board is p&lt;X&gt;.\nValid protocol is: p&lt;X&gt;.\nDisabling device.\nPlease upgrade the firmware and/or version of Stretch Body.\n----------------\n</code></pre></p> <p>Your version of Stretch Body does not align with the firmware installed with your robot. It's recommended that Stretch Body is first upgraded to the latest version available (but if you're intentionally running an older version, you can skip this step and the firmware updater will downgrade your firmware appropriately). To upgrade Stretch Body, follow the instructions here.</p> <p>Next, run the firmware updater tool to automatically update the firmware to the required version for your software.</p> <pre><code>REx_firmware_updater.py --install\n</code></pre> <p>The firmware mismatch errors should now be gone.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"installation/ros_workspace/","title":"Updating your ROS Workspace","text":""},{"location":"installation/ros_workspace/#why","title":"Why","text":"<p>ROS1 and ROS2 organize software by \"workspaces\", where ROS packages are developed, compiled, and made available to run from the command line. By default, a ROS1 workspace called <code>catkin_ws</code> is available in the home directory. Similarly, a ROS2 workspace called <code>ament_ws</code> is available in the home directory. The operating system installed (and therefore version of ROS installed) on your robot dictates whether you'll have a <code>catkin_ws</code> or <code>ament_ws</code> folder.</p> <p>This guide will show you how to replace existing or create new ROS1/2 workspaces for developing ROS software.</p>"},{"location":"installation/ros_workspace/#how","title":"How","text":"<p>Open a terminal and execute the following.</p> <pre><code>cd ~\ngit clone https://github.com/hello-robot/stretch_install\ncd stretch_install\ngit pull\n./stretch_update_ros_workspace.sh\n</code></pre>"},{"location":"installation/ros_workspace/#wrap-up","title":"Wrap up","text":"<p>Close your current terminal and open a new one. The new terminal will have automatically activated the ROS workspace(s).</p> <p>Your new ROS workspace is now set up successfully!</p>"},{"location":"installation/ros_workspace/#troubleshooting","title":"Troubleshooting","text":"<p>This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum.</p>"},{"location":"installation/ros_workspace/#rosdep-failure-on-rtabmap","title":"Rosdep failure on RTabMap","text":"<p>After a failure, if you see the following error in your log file (from <code>~/stretch_user/log/stretch_create_ament_workspace.&lt;timestamp&gt;_log.txt</code>):</p> <pre><code>ERROR: the following rosdeps failed to install\n  apt: command [sudo -H apt-get install -y ros-humble-rtabmap-ros] failed\n</code></pre> <p>Open a terminal, run <code>sudo apt install ros-humble-rtabmap-ros</code>, and then try updating the ROS workspace again.</p>"},{"location":"installation/ros_workspace/#conflicting-ros-version-sourced-error","title":"'Conflicting ROS version sourced' error","text":"<p>If you are seeing the following error:</p> <pre><code>###########################################\nCREATING &lt;ROS VERSION&gt; WORKSPACE at &lt;WS DIR&gt;\n###########################################\n[...]\nEnsuring correct version of ROS is sourced...\nCannot create workspace while a conflicting ROS version is sourced. Exiting.\n</code></pre> <p>The ROS workspace is not created because the check that a conflicting ROS version isn't already sourced has failed. For example, if you're creating an ROS2 Ament workspace, but ROS1 Noetic was previously sourced in the same environment, the check will error out since the new ROS2 workspace would fail to find its dependencies correctly in this environment. Sourcing a version of ROS typically happens using the following command: <code>source /opt/ros/&lt;ros version&gt;/setup.bash</code>. If you ran this command to source a conflicting version previously, simply open a new terminal and the new environment won't have the conflicting ROS version sourced. If you didn't run this command and you're still getting the error, it's likely because the command exists in the <code>~/.bashrc</code> dotfile. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) runs the commands in the <code>~/.bashrc</code> dotfile. Look at the bottom of this dotfile for this command, comment it out temporarily, and open a new terminal. This new shell environment should have no trouble creating the ROS workspace.</p>"},{"location":"installation/ros_workspace/#ros_distro-was-set-before-warning","title":"'ROS_DISTRO was set before' warning","text":"<p>If you are seeing the following warning:</p> <pre><code>ROS_DISTRO was set to '&lt;ROS VERSION&gt;' before. Please make sure that the environment does not mix paths from different distributions.\n</code></pre> <p>Multiple versions of ROS are being sourced in the same environment. This is known to cause issues with the <code>rosdep</code> tool, and might cause issues elsewhere as well. If you haven't explicitly sourced conflicting versions by using the <code>source /opt/ros/&lt;ros version&gt;/setup.bash</code> (a variant on this command could look like <code>source ~/&lt;ws dir&gt;/develop/setup.bash</code>) command twice, then it's likely that one or two versions of ROS are implicitly being sourced in the <code>~/.bashrc</code> dofile. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) runs the commands in the <code>~/.bashrc</code> dotfile. Look at the bottom of this dotfile for the <code>source</code> command and ensure conflicting versions aren't being sourced.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"python/audio/","title":"Playing &amp; Recording Audio","text":"<p>tbd</p>"},{"location":"python/ik/","title":"Inverse Kinematics","text":"<p>tbd</p>"},{"location":"python/moving/","title":"Commanding Stretch to Move","text":"<p>This tutorial covers sending various kinds of motion commands to Stretch.</p>"},{"location":"python/moving/#introducing-stretch-body","title":"Introducing Stretch Body","text":"<p>\"Stretch Body\" is a Python library for Stretch. It's already installed on your robot. The library gives you an interface to command Stretch's body to move. We start by creating an instance of the <code>stretch_body.robot.Robot</code> class.</p> <pre><code>import stretch_body.robot\n\nr = stretch_body.robot.Robot()\n</code></pre> <p>The hardware can only talk to one instance at a time, so if you see an <code>Another process is already using Stretch. Try running \"stretch_free_robot_process.py\"</code> error, you'll need to close the other application or use the <code>stretch_free_robot_process.py</code> CLI to free up the hardware.</p> <p>Next, we'll call the <code>startup()</code> method, which opens USB communication with the hardware, loads the parameters that dictate robot behavior, and launches a few helper threads to poll for status in the background.</p> <pre><code>did_startup = r.startup()\nprint(f'Robot connected to hardware: {did_startup}')\n</code></pre> <p>To close down the connection to the hardware, use <code>stop()</code>. Next, we'll check if the robot is \"homed\", i.e. the robot knows the zero position for each joint.</p> <pre><code>is_homed = r.is_homed()\nprint(f'Robot is homed: {is_homed}')\n</code></pre> <p>To execute the homing procedure, use <code>home()</code>. Ensure the robot has space around it.</p>"},{"location":"python/moving/#sending-position-commands","title":"Sending Position Commands","text":"<p>Stretch has the following joints:</p> <ol> <li>Left and right wheels, constituting a differential drive</li> <li>Pristmatic vertical lift</li> <li>Prismatic telescoping horizontal arm</li> <li>Up to three revolute joints in the wrist: yaw, pitch, and roll<ul> <li>Print out <code>r.end_of_arm.joints</code> to see which joints your Stretch has</li> </ul> </li> <li>Gripper</li> <li>Two revolute joints in the head: pan &amp; tilt</li> </ol> <p>Each of these joints are accessible as an attribute of the <code>Robot</code> object.</p> <pre><code># The mobile base:\nprint(r.base)\n\n# The arm/lift:\nprint(r.arm, r.lift)\n\n# The wrist/gripper\nprint(r.end_of_arm)\n\n# The head\nprint(r.head)\n</code></pre> <p>Use the <code>move_to(pos)</code> method under each joint to queue up a command.</p> <pre><code>r.arm.move_to(0.2) # 0.2 meters\nr.lift.move_to(0.6) # 0.6 meters\n</code></pre> <p>The arm and lift won't move yet. The commands have been queued up, but the hardware will wait to begin moving until we call <code>push_command()</code>.</p> <pre><code>r.push_command()\n</code></pre> <p>The mobile base doesn't have a move method. Instead, its methods are <code>translate_by(delta)</code> and <code>rotate_by(delta)</code>.</p> <pre><code>r.base.translate_by(0.2) # 0.2 meters\nr.push_command()\nr.base.rotate_by(0.1) # 0.1 radians\nr.push_command()\n</code></pre> <p>You'll notice the base doesn't translate forward by 0.2m. Instead, it skips to rotating counter-clockwise by 0.1rad. This happens because <code>push_command()</code> is asynchronous (i.e. the method returns immediately, and the program flow continues immediately to execute the rotation command). Use the <code>wait_command()</code> method to wait for the command to complete execution.</p> <pre><code>r.base.translate_by(0.2) # 0.2 meters\nr.push_command()\nr.wait_command()\n\nr.base.rotate_by(0.1) # 0.1 radians\nr.push_command()\nr.wait_command()\n</code></pre> <p>The wrist and head joints are Dynamixel servos. Their API is <code>move_to(name, pos)</code>, and their commands aren't queued. They execute immediately.</p> <pre><code>print(f'Wrist joints: {r.end_of_arm.joints}') # ['wrist_yaw', 'wrist_pitch', 'wrist_roll', 'stretch_gripper']\nr.end_of_arm.move_to('wrist_yaw', 1.57)\n</code></pre> <p><code>wait_command()</code> will still work here. The gripper uses the same <code>move_to(name, pos)</code> API, and accepts values between -100 to 100, where the gripper is fully closed at -100 and fully open at 100.</p> <pre><code>r.end_of_arm.move_to('stretch_gripper', 100)\n</code></pre>"},{"location":"python/moving/#querying-stretchs-current-joint-state","title":"Querying Stretch's current joint state","text":"<p>The current state of each joint is available in a Python dictionary called <code>status</code> under each joint.</p> <pre><code># Wheel odometry\nprint(r.base.status['x'], r.base.status['y'], r.base.status['theta']) # (x meters, y meters, theta radians)\n\n# Arm/lift positions\nprint(r.arm.status['pos'], r.lift.status['pos']) # (arm meters, lift meters)\n</code></pre> <p>The wrist and head joints are Dynamixel servos. Their <code>status</code> dictionary is under the <code>get_joint(name)</code> method.</p> <pre><code># Wrist positions\nprint(f'Wrist joints: {r.end_of_arm.joints}')\nprint(r.end_of_arm.get_joint('wrist_yaw').status['pos']) # radians\n\n# Head positions\nprint(f'Head joints: {r.head.joints}')\nprint(r.head.get_joint('head_pan').status['pos']) # radians\n</code></pre>"},{"location":"python/moving/#querying-stretchs-joint-limits","title":"Querying Stretch's joint limits","text":"<p>Whereas the mobile base is unbounded (i.e. <code>translate_by(delta)</code> can take any value for delta), the rest of the joints have joint limits. They are available as part of the <code>soft_motion_limits</code> dictionary.</p>"},{"location":"python/moving/#arm-lift","title":"Arm &amp; Lift","text":"<pre><code>print(r.lift.soft_motion_limits['hard']) # (lower bound in meters, upper bound in meters)\nprint(r.arm.soft_motion_limits['hard'])\n</code></pre> <p><code>soft_motion_limits</code> is a dictionary with four keys: \"collision\", \"user\", \"hard\", and \"current\". Each key maps to a <code>[lower bound, upper bound]</code> tuple. \"collision\" is used by Stretch Body's self collision avoidance algorithm to set software limits on the joint. \"user\" is used by you, the user, to set application specific software joint limits. \"hard\" is the hardware limits of the joint. And \"current\" is the aggregate of the previous three limits into the final limits enforced by the software.</p>"},{"location":"python/moving/#wrist-joints","title":"Wrist Joints","text":"Yaw Pitch Roll <pre><code>print(r.end_of_arm.get_joint('wrist_yaw').soft_motion_limits['hard']) # (lower bound in radians, upper bound in radians)\nprint(r.end_of_arm.get_joint('wrist_pitch').soft_motion_limits['hard'])\nprint(r.end_of_arm.get_joint('wrist_roll').soft_motion_limits['hard'])\n</code></pre>"},{"location":"python/moving/#head-joints","title":"Head Joints","text":"<pre><code>print(r.head.get_joint('head_pan').soft_motion_limits['hard']) # (lower bound in radians, upper bound in radians)\nprint(r.head.get_joint('head_tilt').soft_motion_limits['hard'])\n</code></pre>"},{"location":"python/moving/#sending-velocity-commands","title":"Sending Velocity Commands","text":"<p>Each joint accepts velocity commands via the <code>set_velocity(vel)</code> API.</p> <pre><code>r.arm.set_velocity(0.01) # extends 1 cm/s\nr.push_command()\n</code></pre> <p>Once again, Dynamixel joints push their commands immediately.</p> <pre><code>r.end_of_arm.get_joint('wrist_yaw').set_velocity(0.1) # rotate CCW at 0.1 rad/s\n</code></pre> <p>The base can execute translational (<code>v</code>) and rotational (<code>\u03c9</code>) velocity simultaneously via <code>set_velocity(v, \u03c9)</code>. You will often see navigation motion planners choose to control mobile bases with <code>v</code> and <code>\u03c9</code> velocities.</p> <pre><code>r.base.set_velocity(0.01, 0.05) # follow a circular path\nr.push_command()\n</code></pre>"},{"location":"python/moving/#following-cubicquintic-splines","title":"Following Cubic/quintic Splines","text":"<p>Stretch's joints can follow trajectories defined as splines. You will often see geometric motion planners, graphics software (e.g. Blender, Unity3D), and motion capture systems choose to formulate robot trajectories as splines. Spline trajectories are a series of position waypoints, associated with a time at which the joint should reach the position, and optionally associated with a velocity/acceleration that the joint should be traveling at when it passes the waypoint.</p> <p>Each joint has a <code>trajectory</code> attribute that is an instance of either <code>stretch_body.trajectories.RevoluteTrajectory</code> (for the wrist &amp; head joints), <code>stretch_body.trajectories.PrismaticTrajectory</code> (for the arm &amp; lift), or <code>stretch_body.trajectories.DiffDriveTrajectory</code> (for the base). Adding waypoints looks like:</p> <pre><code>r.arm.trajectory.add(t_s=0.0, x_m=0.0, v_m=0.0)\nr.arm.trajectory.add(t_s=10.0, x_m=0.5, v_m=0.0)\nr.arm.trajectory.add(t_s=20.0, x_m=0.0, v_m=0.0) # 20 second trajectory extending &amp; retracting the arm\n</code></pre> <p>Then begin tracking the trajectory using:</p> <pre><code>r.follow_trajectory()\n</code></pre> <p>Trajectories are preemptive. You can edit the future waypoints in a trajectory while the robot is actively tracking it. For example, geometric planners can use this adapt to changing environments.</p> <p>You can stop a trajectory early using <code>stop_trajectory()</code>.</p>"},{"location":"python/moving/#coordinated-motion-through-trajectories","title":"Coordinated Motion through Trajectories","text":"<p>Check out the Creating smooth motion using trajectories tutorial for more info on creating coordinated joint movements.</p>"},{"location":"python/other/","title":"Other Stretch Python Libraries","text":"<p>tbd</p>"},{"location":"python/pyfunmap/","title":"Using PyFUNMAP","text":"<p>tbd</p>"},{"location":"python/sensors/","title":"Working with Sensors","text":""},{"location":"python/sensors/#realsense-cameras","title":"Realsense Cameras","text":"<p>Use the <code>pyrealsense2</code> library to interact with the two Intel Realsense depth cameras on Stretch 3. The library can configure camera parameters, collect imagery, calculate point clouds, and much more. Start by creating a \"pipeline\", which is an interface for streaming data from the camera.</p> <pre><code>import pyrealsense2 as rs\n\ncam = rs.pipeline()\ncam.start()\ncam.stop()\n</code></pre> <p><code>start()</code> returns a \"profile\", which returns information about the Realsense camera.</p> <pre><code>profile = cam.start()\nprint(profile.get_device().get_info()) # \"D435if\"\n</code></pre>"},{"location":"python/sensors/#configuration","title":"Configuration","text":"<p>We pass a \"config\" to <code>start()</code> to choose which camera we want to stream from, what resolution the imagery should be, what frames per second the data should arrive at, and more.</p> <pre><code>config = rs.config()\nconfig.enable_device(d405_info['serial_number'])\nwidth, height, fps = 640, 480, 15\nconfig.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)\nconfig.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\nprofile = cam.start(config)\n</code></pre>"},{"location":"python/sensors/#frames","title":"Frames","text":""},{"location":"python/sensors/#api-and-docs","title":"API and Docs","text":"<p>https://github.com/IntelRealSense/librealsense/tree/master/wrappers/python#python-wrapper</p> <p>https://intelrealsense.github.io/librealsense/python_docs/_generated/pyrealsense2.html</p>"},{"location":"python/visual_servoing/","title":"Visual Servoing","text":"<p>tbd</p>"},{"location":"ros2/","title":"Index","text":""},{"location":"ros2/#tutorial-track-stretch-ros-2-beta","title":"Tutorial Track: Stretch ROS 2 (Beta)","text":""},{"location":"ros2/#robot-operating-system-2-ros-2","title":"Robot Operating System 2 (ROS 2)","text":"<p>ROS 2 is the successor to ROS,  The ROS in ROS 2 stands for \"robot operating system\", but despite the name, ROS is not an operating system. It's a middleware framework that is a collection of transport protocols, development and debugging tools, and open-source packages.</p> <p>As a transport protocol, ROS enables distributed communication via messages between nodes. As a development and debugging toolkit, ROS provides build systems that allow for writing applications in a wide variety of languages (Python and C++ are used in this tutorial track), a launch system to manage the execution of multiple nodes simultaneously, and command line tools to interact with the running system. Finally, as a popular ecosystem, there are many open-source ROS packages that allow users to quickly prototype with new sensors, actuators, planners, perception stacks, and more.</p> <p>Despite the name, ROS is not an operating system. ROS is a middleware framework that is a collection of transport protocols, development and debugging tools, and open-source packages. As a transport protocol, ROS enables distributed communication via messages between nodes. As a development and debugging toolkit, ROS provides build systems that allow for writing applications in a wide variety of languages (Python and C++ are used in this tutorial track), a launch system to manage the execution of multiple nodes simultaneously, and command line tools to interact with the running system. Finally, as a popular ecosystem, there are many open-source ROS packages that allow users to quickly prototype with new sensors, actuators, planners, perception stacks, and more.</p> <p>This tutorial track is for users looking to get familiar with programming Stretch robots via ROS 2. We recommend going through the tutorials in the following order:</p>"},{"location":"ros2/#basics","title":"Basics","text":"Tutorial Description 1 Getting Started Setup instructions for ROS 2 on Stretch 2 Introduction to ROS 2 Explore the client library used in ROS2 3 Introduction to HelloNode Explore the Hello Node class to create a ROS2 node for Stretch 4 Teleoperating Stretch Control Stretch with a Keyboard or a Gamepad controller. 5 Internal State of Stretch Monitor the joint states of Stretch. 6 RViz Basics Visualize topics in Stretch. 7 Nav2 Stack Motion planning and control for mobile base. 8 Follow Joint Trajectory Commands Control joints using joint trajectory server. 9 Perception Use the Realsense D435i camera to visualize the environment. 10 ArUco Marker Detection Localize objects using ArUco markers. 11 ReSpeaker Microphone Array Learn to use the ReSpeaker Microphone Array. 12 FUNMAP Fast Unified Navigation, Manipulation and Planning."},{"location":"ros2/#other-examples","title":"Other Examples","text":"<p>To help get you started on your software development, here are examples of nodes to have Stretch perform simple tasks.</p> Tutorial Description 1 Mobile Base Velocity Control Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Print Joint States Print the joint states of Stretch. 6 Store Effort Values Print, store, and plot the effort values of the Stretch robot. 7 Capture Image Capture images from the RealSense camera data. 8 Voice to Text Interpret speech and save transcript to a text file. 9 Voice Teleoperation of Base Use speech to teleoperate the mobile base. 10 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 11 ArUco Tag Locator Actuate the head to locate a requested ArUco marker tag and return a transform. 12 Obstacle Avoider Avoid obstacles using the planar lidar. 13 Align to ArUco Detect ArUco fiducials using OpenCV and align to them. 14 Deep Perception Use YOLOv5 to detect 3D objects in a point cloud. 15 Avoiding Race Conditions and Deadlocks Learn how to avoid Race Conditions and Deadlocks"},{"location":"ros2/align_to_aruco/","title":"Align to ArUco","text":"<p>ArUco markers are a type of fiducials that are used extensively in robotics for identification and pose estimation. In this tutorial we will learn how to identify ArUco markers with the ArUco detection node and enable Stretch to navigate and align itself with respect to the marker.</p>"},{"location":"ros2/align_to_aruco/#aruco-detection","title":"ArUco Detection","text":"<p>Stretch uses the OpenCV ArUco detection library and is configured to identify a specific set of ArUco markers belonging to the 6x6, 250 dictionary. To understand why this is important, please refer to this handy guide provided by OpenCV.</p> <p>Stretch comes preconfigured to identify ArUco markers. The ROS node that enables this is the detect_aruco_markers node in the stretch_core package. Thanks to this node, identifying and estimating the pose of a marker is as easy as pointing the camera at the marker and running the detection node. It is also possible and quite convenient to visualize the detections with RViz.</p>"},{"location":"ros2/align_to_aruco/#computing-transformations","title":"Computing Transformations","text":"<p>If you have not already done so, now might be a good time to review the tf listener tutorial. Go on, we can wait\u2026 Now that we know how to program stretch to return the transform between known reference frames, we can use this knowledge to compute the transform between the detected marker and the robot base_link. From its current pose, for Stretch to align itself in front of the marker, we need to command it to reach there. But even before that, we need to program Stretch to know the goal pose. We define the goal pose to be 0.5 metre outward from the marker in the marker negative y-axis (Green axis). This is easier to visualize through the figure below.</p> <p> </p> <p>By monitoring the /aruco/marker_array and /aruco/axes topics, we can visualize the markers in RViz. The detection node also publishes the tf pose of the detected markers. This can be visualized by using the TF plugin and selecting the detected marker to inspect the pose. Next, we will use exactly that to compute the transform between the detected marker and the base_link of the robot.</p> <p>Now, we can compute the transformation from the robot base_link frame to the goal pose and pass this as an SE2 pose to the mobile base.</p> <p>Since we want Stretch to stop at a fixed distance with respect to the marker, we define a 0.5m offset in the marker y-axis where Stretch would come to a stop. At the same time, we also want Stretch to align its orientation to point its arm towards the marker so as to make the subsequent manipulation tasks easier to accomplish. This would result in the end pose of the base_link as shown in the above figure. Sweet! The next task is to generate a simple motion plan for the mobile base to reach this end pose. We do this in three steps: 1. Turn theta degrees towards the goal position. This would be the angle formed between the robot x-axis and the line connecting the start and the goal positions. 2. Travel straight to the goal position. This would be the euclidean distance between the start and the goal positions. 3. Turn phi degrees to attain the goal orientation. This would be the correction angle necessary to align the robot y-axis with the marker x-axis.</p> <p>Luckily, we know how to command Stretch to execute a trajectory using the joint trajectory server. If you are just starting, have a look at the Follow Joint Trajectory Commands tutorial to know how to command Stretch using the Joint trajectory Server.</p> <p>Warning</p> <p>Since we won't be using the arm for this demo, it's safer to stow Stretch's arm in.</p> <pre><code>stretch_robot_stow.py\n</code></pre>"},{"location":"ros2/align_to_aruco/#see-it-in-action","title":"See It In Action","text":"<p>First, we need to point the camera towards the marker. To do this, you could use the keyboard teleop node. To do this, run: <pre><code>ros2 launch stretch_core keyboard_teleop.launch.py\n</code></pre></p> <p>When you are ready, execute the following command: <pre><code>ros2 launch stretch_core align_to_aruco.launch.py\n</code></pre></p> <p> </p>"},{"location":"ros2/align_to_aruco/#code-breakdown","title":"Code Breakdown","text":"<p>Let's jump into the code to see how things work under the hood. Follow along here to have a look at the entire script.</p> <p>We make use of two separate Python classes for this demo. The FrameListener class is derived from the Node class and is the place where we compute the TF transformations. For an explantion of this class, you can refer to the TF listener tutorial. <pre><code>class FrameListener(Node):\n</code></pre></p> <p>The AlignToAruco class is where we command Stretch to the pose goal. This class is derived from the FrameListener class so that they can both share the node instance. <pre><code>class AlignToAruco(FrameListener):\n</code></pre></p> <p>The constructor initializes the Joint trajectory action client. It also initializes the attribute called offset that determines the end distance between the marker and the robot. <pre><code>    def __init__(self, node, offset=0.75):\n        self.trans_base = TransformStamped()\n        self.trans_camera = TransformStamped()\n        self.joint_state = JointState()\n        self.offset = offset\n        self.node = node\n\n        self.trajectory_client = ActionClient(self.node, FollowJointTrajectory, '/stretch_controller/follow_joint_trajectory')\n        server_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\n        if not server_reached:\n            self.node.get_logger().error('Unable to connect to arm action server. Timeout exceeded.')\n            sys.exit()\n</code></pre></p> <p>The joint_states_callback is the callback method that receives the most recent joint state messages published on the /stretch/joint_states topic. <pre><code>    def joint_states_callback(self, joint_state):\n        self.joint_state = joint_state\n</code></pre></p> <p>The compute_difference() method is where we call the get_transform() method from the FrameListener class to compute the difference between the base_link and base_right frame with an offset of 0.5 m in the negative y-axis. <pre><code>    def compute_difference(self):\n        self.trans_base, self.trans_camera = self.node.get_transforms()\n</code></pre></p> <p>To compute the (x, y) coordinates of the SE2 pose goal, we compute the transformation here. <pre><code>        R = quaternion_matrix((x, y, z, w))\n        P_dash = np.array([[0], [-self.offset], [0], [1]])\n        P = np.array([[self.trans_base.transform.translation.x], [self.trans_base.transform.translation.y], [0], [1]])\n\n        X = np.matmul(R, P_dash)\n        P_base = X + P\n\n        base_position_x = P_base[0, 0]\n        base_position_y = P_base[1, 0]\n</code></pre></p> <p>From this, it is relatively straightforward to compute the angle phi and the euclidean distance dist. We then compute the angle z_rot_base to perform the last angle correction. <pre><code>        phi = atan2(base_position_y, base_position_x)\n\n        dist = sqrt(pow(base_position_x, 2) + pow(base_position_y, 2))\n\n        x_rot_base, y_rot_base, z_rot_base = euler_from_quaternion([x, y, z, w])\n        z_rot_base = -phi + z_rot_base + 3.14159\n</code></pre></p> <p>The align_to_marker() method is where we command Stretch to the pose goal in three steps using the Joint Trajectory action server. For an explanation on how to form the trajectory goal, you can refer to the Follow Joint Trajectory Commands tutorial. <pre><code>    def align_to_marker(self):\n</code></pre></p> <p>If you want to work with a different ArUco marker than the one we used in this tutorial, you can do so by changing line 44 in the code to the one you wish to detect. Also, don't forget to add the marker in the stretch_marker_dict.yaml ArUco marker dictionary.</p>"},{"location":"ros2/aruco_marker_detection/","title":"Aruco marker detection","text":""},{"location":"ros2/aruco_marker_detection/#aruco-marker-detector","title":"ArUco Marker Detector","text":"<p>For this tutorial, we will go over how to detect Stretch's ArUco markers and review the files that hold the information for the tags.</p>"},{"location":"ros2/aruco_marker_detection/#visualize-aruco-markers-in-rviz","title":"Visualize ArUco Markers in RViz","text":"<p>Begin by running the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>ros2 launch stretch_core d435i_high_resolution.launch.py\n</code></pre> <p>Next, in a new terminal, run the stretch ArUco launch file which will bring up the detect_aruco_markers node.</p> <pre><code>ros2 launch stretch_core stretch_aruco.launch.py\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>ros2 run rviz2 rviz2 -d /home/hello-robot/ament_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz\n</code></pre> <p>You are going to need to teleoperate Stretch's head to detect the ArUco marker tags. Run the following command in a new terminal and control the head to point the camera toward the markers.   </p> <pre><code>ros2 run stretch_core keyboard_teleop\n</code></pre> <p> </p>"},{"location":"ros2/aruco_marker_detection/#the-aruco-marker-dictionary","title":"The ArUco Marker Dictionary","text":"<p>When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml, that holds the information about the markers.</p> <p>If detect_aruco_markers node doesn\u2019t find an entry in stretch_marker_dict.yaml for a particular ArUco marker ID number, it uses the default entry. For example, most robots have shipped with the following default entry:</p> <pre><code>'default':\n  'length_mm': 24\n  'use_rgb_only': False\n  'name': 'unknown'\n  'link': None\n</code></pre> <p>and the following entry for the ArUco marker on the top of the wrist</p> <pre><code>'133':\n  'length_mm': 23.5\n  'use_rgb_only': False\n  'name': 'wrist_top'\n  'link': 'link_aruco_top_wrist'\n</code></pre> <p>Dictionary Breakdown</p> <pre><code>'133':\n</code></pre> <p>The dictionary key for each entry is the ArUco marker\u2019s ID number or <code>default</code>. For example, the entry shown above for the ArUco marker on the top of the wrist assumes that the marker\u2019s ID number is <code>133</code>.</p> <pre><code>'length_mm': 23.5\n</code></pre> <p>The <code>length_mm</code> value used by detect_aruco_markers is important for estimating the pose of an ArUco marker.</p> <p>Note</p> <p>If the actual width and height of the marker do not match this value, then pose estimation will be poor. Thus, carefully measure custom Aruco markers.</p> <pre><code>'use_rgb_only': False\n</code></pre> <p>If <code>use_rgb_only</code> is <code>True</code>, detect_aruco_markers will ignore depth images from the Intel RealSense D435i depth camera when estimating the pose of the marker and will instead only use RGB images from the D435i.</p> <pre><code>'name': 'wrist_top'\n</code></pre> <p><code>name</code> is used for the text string of the ArUco marker\u2019s ROS Marker in the ROS MarkerArray Message published by the detect_aruco_markers ROS node.</p> <pre><code>'link': 'link_aruco_top_wrist'\n</code></pre> <p><code>link</code> is currently used by stretch_calibration. It is the name of the link associated with a body-mounted ArUco marker in the robot\u2019s URDF.</p> <p>It\u2019s good practice to add an entry to stretch_marker_dict.yaml for each ArUco marker you use.</p>"},{"location":"ros2/aruco_marker_detection/#create-a-new-aruco-marker","title":"Create a New ArUco Marker","text":"<p>At Hello Robot, we\u2019ve used the following guide when generating new ArUco markers.</p> <p>We generate ArUco markers using a 6x6-bit grid (36 bits) with 250 unique codes. This corresponds with DICT_6X6_250 defined in OpenCV. We generate markers using this online ArUco marker generator by setting the Dictionary entry to 6x6 and then setting the Marker ID and Marker size, mm as appropriate for the specific application. We strongly recommend measuring the actual marker by hand before adding an entry for it to stretch_marker_dict.yaml.</p> <p>We select marker ID numbers using the following ranges.</p> <ul> <li>0 - 99: reserved for users</li> <li>100 - 249: reserved for official use by Hello Robot Inc.</li> <li>100 - 199: reserved for robots with distinct sets of body-mounted markers<ul> <li>Allows different robots near each other to use distinct sets of body-mounted markers to avoid confusion. This could be valuable for various uses of body-mounted markers, including calibration, visual servoing, visual motion capture, and multi-robot tasks.</li> <li>5 markers per robot = 2 on the mobile base + 2 on the wrist + 1 on the shoulder</li> <li>20 distinct sets = 100 available ID numbers / 5 ID numbers per robot</li> </ul> </li> <li>200 - 249: reserved for official accessories<ul> <li>245 for the prototype docking station</li> <li>246-249 for large floor markers</li> </ul> </li> </ul> <p>When coming up with this guide, we expected the following:</p> <ul> <li>Body-mounted accessories with the same ID numbers mounted to different robots could be disambiguated using the expected range of 3D locations of the ArUco markers on the calibrated body.</li> <li>Accessories in the environment with the same ID numbers could be disambiguated using a map or nearby observable features of the environment.</li> </ul>"},{"location":"ros2/avoiding_deadlocks_race_conditions/","title":"Avoiding Race Conditions and Deadlocks","text":"<p>ROS 2 was created with real time safety in mind. Some of the features of ROS 2, although exposed for the convenience of users, can get in the way if one is not aware about them.</p>"},{"location":"ros2/avoiding_deadlocks_race_conditions/#race-conditions","title":"Race conditions","text":"<p>In parallel programming, race conditions can occur when data is being written to and read from a shared memory space concurrently by two threads. This creates a condition where a thread that\u2019s reading from the shared memory gets conflicting data. Such conditions are often difficult to debug as it requires one to introspect multiple threads. Hence, it is always better to recognize and avoid common pitfalls that lead to race conditions.</p>"},{"location":"ros2/avoiding_deadlocks_race_conditions/#deadlocks","title":"Deadlocks","text":"<p>In parallel programming, deadlocks occur when multiple threads try to gain access to the same system resource at the same time. In the context of ROS 2, this can occur when a resource like the executor which is shared by multiple callback methods is asked to service more than one callback concurrently. Again, this is another pitfall that\u2019s difficult to debug as it often manifests itself with nodes becoming unresponsive without any apparent error message. Hence, it is always better to recognize and avoid common pitfalls that lead to deadlocks.</p>"},{"location":"ros2/avoiding_deadlocks_race_conditions/#executor","title":"Executor","text":"<p>We had a brief introduction to the ROS 2 executor in the previous tutorial on Introduction to ROS 2. We looked at ways to invoke the executor with various spin methods. At a high level, the job of the executor is to service callback messages as they arrive and process and relay messages in the callback. In parallel programming, sometimes you might want parts of a process to run concurrently to avoid deadlocks, or in some cases, you might not want certain parts of a process to run concurrently to avoid race conditions. Fortunately, you can control this by defining the executor model.</p> <p>An executor is defined as a SingleThreadedExecutor by default which is perfect for servicing fast running callbacks sequentially. However, if parallelism is desired, an executor can be defined as a MultiThreadedExecutor. This allows long running callbacks to run in parallel with fast running ones. A custom executor model can also be defined, but that is beyond the scope of this tutorial.</p>"},{"location":"ros2/avoiding_deadlocks_race_conditions/#callback-groups","title":"Callback groups","text":"<p>A callback is the method that receives a ROS message and is where a ROS message is processed. Depending on the kind of data that\u2019s being worked upon, a callback method could finish executing in virtually no time or take several seconds to process. Several seconds is a long time in programming and the longer a callback method takes, the longer the next callback has to wait for the executor to service it.</p> <p>With a MultiThreadedExecutor, callbacks can be serviced in parallel. However, this makes the process prone to race conditions as multiple threads work on the same shared memory. Fortunately, this can be avoided by defining callback groups. Grouping callbacks such that the ones that deal with the same shared memory space never execute concurrently ensures that data doesn\u2019t get corrupted due to race conditions.</p> <p>There are two different kinds of callback groups available. A MutuallyExclusiveCallbackGroup, the default, ensures that the callbacks belonging to this group never execute in parallel. You would use this when two callbacks access and write to the same shared memory space and having them execute them together would result in a race condition. A ReentrantCallbackGroup ensures that callbacks belonging to this group are able to execute parallelly. You would use this when a long running callback occupies the bulk of the executors time and you want shorter fast running callbacks to run in parallel.</p> <p>Now, let\u2019s explore what we have learned so far in the form of a real example. You can try them by creating a Python file and run it in your terminal as <code>python3 FILE_NAME</code>.</p>"},{"location":"ros2/avoiding_deadlocks_race_conditions/#race-condition-example","title":"Race Condition Example","text":"<p>It is instructive to see an example code that generates a race condition. The below code simulates a race condition by defining two subscriber callbacks that write and read from shared memory simultaneously.</p> <p>Note</p> <p>Before executing the Race Condition Example you first need to launch the stretch core driver as ros2 launch stretch_core stretch_driver.launch.py</p> <pre><code>import rclpy\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import JointState\nfrom rclpy.executors import MultiThreadedExecutor\nfrom rclpy.executors import SingleThreadedExecutor\nfrom rclpy.callback_groups import MutuallyExclusiveCallbackGroup\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.node import Node\nimport time\n\nclass ParallelExec(Node):\n    def __init__(self, cb_group):\n        super().__init__('parallel_execution')\n        self.shared_memory = True\n        self.joint_state = JointState()\n        self.mode = String()\n        self.cb_group = cb_group\n\n    def callback_one(self, msg):\n        self.joint_states = msg\n        if not self.shared_memory:\n            time.sleep(0.2)\n            self.get_logger().info(\"Switching from {} to True\".format(self.shared_memory))\n            self.shared_memory = True\n\n    def callback_two(self, msg):\n        self.mode = msg\n        if self.shared_memory:\n            time.sleep(0.2)\n            self.get_logger().info(\"Switching from {} to False\".format(self.shared_memory))\n            self.shared_memory = False\n\n    def main(self):         \n        sub_joint_states = self.create_subscription(JointState, '/stretch/joint_states', self.callback_one, 1, callback_group=self.cb_group)\n        sub_mode = self.create_subscription(String, 'mode', self.callback_two, 1, callback_group=self.cb_group)\n\nif __name__ == '__main__':\n    rclpy.init()\n\n    executor = None\n    cb_group = None\n\n    x = input(\"Select the executor model: Press 1 for SingleThreadedExecutor(); Press 2 for MultiThreadedExecutor()\")\n    if x == '1':\n        print(\"Using single-threaded execution\")\n        executor = SingleThreadedExecutor()\n    elif x == '2':\n        print(\"Using multi-threaded execution\")\n        executor = MultiThreadedExecutor(num_threads=2)\n\n    y = input(\"Select the callback group: Press 1 for MutuallyExclusiveCallbackGroup(); Press 2 for ReentrantCallbackGroup()\")\n    if y == '1':\n        print(\"Processing callbacks one after the other\")\n        cb_group = MutuallyExclusiveCallbackGroup()\n    elif y == '2':\n        print(\"Processing callbacks in parallel\")\n        cb_group = ReentrantCallbackGroup()\n\n    node = ParallelExec(cb_group)\n    node.main()\n\n    executor.add_node(node)\n    executor.spin()\n\n    node.destroy_node()\n    rclpy.shutdown()\n</code></pre> <p>Executing the above script, the expected behavior is to see the logged statements conform to the conditional statements in the callback methods. To elaborate, callback_one() receives joint_state messages and should print \"Switching from False to True\" only if shared_memory is False. Whereas callback_two() receives mode messages and should print \"Switching from True to False\" only if shared_memory is True. However, without setting up protection against race conditions we see that this is not what ends up happening.</p> <p>Executing the above code, you are presented with two prompts, first to select the executor, either a SingleThreadedExecutor or a MultiThreadedExecutor; and then to select a callback group type, either a MutuallyExclusiveCallbackGroup or a ReentrantCallbackGroup.</p> <p>Selecting a SingleThreadedExecutor, irrespective of which callback group is selected, results in callbacks being executed sequentially. This is because the executor is spun using a single thread that can only service one callback at a time. In this case, we see that there is no memory corruption and the observed behavior is the same as the expected behavior.</p> <p>Things get interesting when we choose the MultiThreadedExecutor along with a ReentrantCallbackGroup. Multiple threads are used by the executor to service callbacks, while callbacks are allowed to execute in parallel. This allows multiple threads to access the same memory space and execute read/write operations. The observed behavior is that, sometimes you see the callbacks print statements like  \"Switching from True to True\" or \"Switching from False to False\" which go against the conditions set in the callbacks. This is a race condition.</p> <p>Selecting a MultiThreadedExecutor along with a MutuallyExclusiveCallbackGroup allows us to circumvent this problem by using parallelism but still protecting shared memory from race conditions.</p>"},{"location":"ros2/avoiding_deadlocks_race_conditions/#deadlock-example","title":"Deadlock Example","text":"<p>A great example of a deadlock is provided in the official ROS 2 documentation on sync deadlock, so this example will directly build off of the same code. The server side defines a callback method add_two_ints_callback() which returns the sum of two requested numbers. Notice the call to spin in the main() method which persistently executes the callback method as a service request arrives. For the requested numbers you will need to input them in the terminal manually.</p> <pre><code>from example_interfaces.srv import AddTwoInts\n\nimport rclpy\nfrom rclpy.node import Node\n\nclass MinimalService(Node):\n    def __init__(self):\n        super().__init__('minimal_service')\n        self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\n\n    def add_two_ints_callback(self, request, response):\n        response.sum = request.a + request.b\n        self.get_logger().info('Incoming request\\na: %d b: %d' % (request.a, request.b))\n        return response\n\ndef main():\n    rclpy.init()\n    minimal_service = MinimalService()\n    rclpy.spin(minimal_service)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Now let's look at the client side. This script makes a synchronous service call to the add_two_ints service. </p> <pre><code>import sys\nfrom threading import Thread\n\nfrom example_interfaces.srv import AddTwoInts\nimport rclpy\nfrom rclpy.node import Node\n\nclass MinimalClientSync(Node):\n    def __init__(self):\n        super().__init__('minimal_client_sync')\n        self.cli = self.create_client(AddTwoInts, 'add_two_ints')\n        while not self.cli.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info('service not available, waiting again...')\n        self.req = AddTwoInts.Request()\n\n    def send_request(self):\n        self.req.a = int(sys.argv[1])\n        self.req.b = int(sys.argv[2])\n        return self.cli.call(self.req)\n\ndef main():\n    rclpy.init()\n    minimal_client = MinimalClientSync()\n    response = minimal_client.send_request()\n    minimal_client.get_logger().info(\n        'Result of add_two_ints: for %d + %d = %d' %\n        (minimal_client.req.a, minimal_client.req.b, response.sum))\n\n    rclpy.spin()\n    minimal_client.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Every call to a service expects a response which is a ROS message that requires a callback to listen to. Although not explicitly defined, this callback method is created when the client is initialized. Like any other callback method, the executor needs to call the callback method for the response to be received. Notice that the above example only invokes the executor (with rclpy.spin()) after the synchronous service call has been issued through <code>self.cli.call(self.req)</code> in the send_request() method. This creates a deadlock because the execution can't move forward until a response is received, but the executor has not been invoked yet to service the response callback method.</p> <p>The way to get around this is to invoke the executor in a separate thread before the synchronous service call is made. This way, the response callback is called in a separate thread from the main execution thread. Here's how to do this:</p> <pre><code>def main():\n    rclpy.init()\n    minimal_client = MinimalClientSync()\n\n    # Invoke the executor in a separate thread\n    spin_thread = Thread(target=rclpy.spin, args=(minimal_client,))\n    spin_thread.start()\n\n    response = minimal_client.send_request()\n    minimal_client.get_logger().info(\n        'Result of add_two_ints: for %d + %d = %d' %\n        (minimal_client.req.a, minimal_client.req.b, response.sum))\n    minimal_client.destroy_node()\n    rclpy.shutdown()\n</code></pre> <p>An alternative to this, a feature that's new to ROS 2, is to use an asynchronous service call. This allows one to monitor the response through what's called a future object in ROS 2. The future holds the status of whether a service call was accepted by the server and also the returned response. Since the future is returned immediately on making an async service call, the execution is not blocked and the executor can be invoked in the main execution thread. Here's how to do it:</p> <pre><code>class MinimalClientAsync(Node):\n\n    ...\n\n    def send_request(self, a, b):\n        self.req.a = a\n        self.req.b = b\n\n        # Make an async call and wait until the executor receives a response\n        self.future = self.cli.call_async(self.req)\n        rclpy.spin_until_future_complete(self, self.future)\n        return self.future.result()\n\ndef main():\n    rclpy.init()\n\n    minimal_client = MinimalClientAsync()\n\n    # The response is a future object which can be queried to get the result\n    response = minimal_client.send_request(int(sys.argv[1]), int(sys.argv[2]))\n    minimal_client.get_logger().info(\n        'Result of add_two_ints: for %d + %d = %d' %\n        (int(sys.argv[1]), int(sys.argv[2]), response.sum))\n\n    minimal_client.destroy_node()\n    rclpy.shutdown()\n</code></pre>"},{"location":"ros2/coming_soon/","title":"Coming soon","text":"<p> ROS 2 tutorials are still under active development. Coming soon.</p>"},{"location":"ros2/deep_perception/","title":"Deep Perception","text":"<p>Ever wondered if there is a way to make a robot do awesome things without explicitly having to program it to do so? Deep Perception is a branch of Deep Learning that enables sensing the elements that make up an environment with the help of artificial neural networks without writing complicated code. Well, almost. The most wonderful thing about Stretch is that it comes preloaded with software that makes it a breeze to get started with topics such as Deep Learning. In this tutorial, we will deploy deep neural networks on Stretch using two popular Deep Learning frameworks, namely, PyTorch and OpenVino.</p>"},{"location":"ros2/deep_perception/#yolov5-with-pytorch","title":"YOLOv5 with PyTorch","text":"<p>PyTorch is an open-source end-to-end machine learning framework that makes many pretrained production-quality neural networks available for general use. In this tutorial, we will use the YOLOv5s model trained on the COCO dataset.</p> <p>YOLOv5 is a popular object detection model that divides a supplied image into a grid and detects objects in each cell of the grid recursively. The YOLOv5s model that we have deployed on Stretch has been pretrained on the COCO dataset which allows Stretch to detect a wide range of day-to-day objects. However, that\u2019s not all, in this demo we want to go a step further and use this extremely versatile object detection model to extract useful information about the scene.</p>"},{"location":"ros2/deep_perception/#extracting-bounding-boxes-and-depth-information","title":"Extracting Bounding Boxes and Depth Information","text":"<p>Often, it\u2019s not enough to simply identify an object. Stretch is a mobile manipulator and its job is to manipulate objects in its environment. But before it can do that, it needs information of where exactly the object is located with respect to itself so that a motion plan to reach the object can be generated. This is possible by knowing which pixels correspond to the object of interest in the image frame and then using that to extract the depth information in the camera frame. Once we have this information, it is possible to compute a transform of these points in the end effector frame for Stretch to generate a motion plan. </p> <p>For the sake of brevity, we will limit the scope of this tutorial to drawing bounding boxes around objects of interest to point to pixels in the image frame and drawing a detection plane corresponding to depth pixels in the camera frame.</p> <p>Warning</p> <p>Running inference on Stretch results in a continuously high current draw by the CPU. Please ensure proper ventilation with the onboard fan. It is recommended to run the demo in tethered mode.</p>"},{"location":"ros2/deep_perception/#see-it-in-action","title":"See It In Action","text":"<p>Go ahead and execute the following command to run the inference and visualize the detections in RViz:</p> <pre><code>ros2 launch stretch_deep_perception stretch_detect_objects.launch.py\n</code></pre> <p></p> <p>Voila! You just executed your first deep-learning model on Stretch!</p>"},{"location":"ros2/deep_perception/#code-breakdown","title":"Code Breakdown","text":"<p>Luckily, the stretch_deep_pereption package is extremely modular and is designed to work with a wide array of detectors. Although most of the heavy lifting in this tutorial is being done by the neural network, let's attempt to break down the code into functional blocks to understand the detection pipeline.</p> <p>The control flow begins with executing the detect_objects.py script. In the main() function, we create an instance of the ObjectDetector class from the object_detect_pytorch.py script where we configure the YOLOv5s model. Next, we pass this detector to an instance of the DetectionNode class from the detection_node.py script and call the main function. <pre><code>def main():\n    confidence_threshold = 0.0\n    detector = od.ObjectDetector(confidence_threshold=confidence_threshold)\n    default_marker_name = 'object'\n    node_name = 'DetectObjectsNode'\n    topic_base_name = 'objects'\n    fit_plane = False\n    node = dn.DetectionNode(detector, default_marker_name, node_name, topic_base_name, fit_plane)\n    node.main()\n</code></pre></p> <p>Let's skim through the object_detect_pytorch.py script to understand the configuration. The constructor is where we load the pretrained YOLOv5s model using the torch.hub.load() PyTorch wrapper. We set the confidence threshold to be 0.2, which says that a detection is only considered valid if the probability is higher than 0.2. This can be tweaked, although lower numbers often result in false positives and higher numbers often disregard blurry or smaller valid objects. <pre><code>class ObjectDetector:\n    def __init__(self, confidence_threshold=0.2):\n        # Load the models\n        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom\n        self.confidence_threshold = confidence_threshold\n</code></pre></p> <p>The apply_to_image() method passes the stream of RGB images from the realsense camera to the YOLOv5s model and returns detections in the form of a dictionary consisting of class_id, label, confidence and bouding box coordinates. The last part is exactly what we need for further computations. <pre><code>def apply_to_image(self, rgb_image, draw_output=False):\n        results = self.model(rgb_image)\n\n        ...\n\n        if draw_output:\n            output_image = rgb_image.copy()\n            for detection_dict in results:\n                self.draw_detection(output_image, detection_dict)\n\n        return results, output_image\n</code></pre></p> <p>This method calls the draw_detection() method to draw bounding boxes with the object labels and confidence thresholds over detected objects in the image using OpenCV. <pre><code>def draw_detection(self, image, detection_dict):\n        ...\n\n        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, rectangle_line_thickness)\n\n        ...\n\n        cv2.rectangle(image, (label_x_min, label_y_min), (label_x_max, label_y_max), (255, 255, 255), cv2.FILLED)\n        cv2.putText(image, output_string, (text_x, text_y), font, font_scale, line_color, line_width, cv2.LINE_AA)\n</code></pre></p> <p>Next, the script detection_node.py contains the class DetectionNode which is the main ROS node that subscribes to the RGB and depth images from the realsense camera and feeds them to the detector to run inference. The image_callback() method runs in a loop to subscribe to synchronized RGB and depth images. The RGB images are then rotated 90 degrees and passed to the apply_to_image() method. The returned output image is published on the visualize_object_detections_pub publisher, while the detections_2d dictionary is passed to the detections_2d_to_3d() method for further processing and drawing the detection plane. For detectors that also return markers and axes, it also publishes this information. <pre><code>def image_callback(self, ros_rgb_image, ros_depth_image, rgb_camera_info):\n        ...\n\n        detection_box_image = cv2.rotate(self.rgb_image, cv2.ROTATE_90_CLOCKWISE)\n\n        ...\n\n        detections_2d, output_image = self.detector.apply_to_image(detection_box_image, draw_output=debug_output)\n\n        ...\n\n        if output_image is not None:\n            output_image = ros2_numpy.msgify(Image, output_image, encoding='rgb8')\n            if output_image is not None:\n                self.visualize_object_detections_pub.publish(output_image)\n\n        detections_3d = d2.detections_2d_to_3d(detections_2d, self.rgb_image, self.camera_info, self.depth_image, fit_plane=self.fit_plane, min_box_side_m=self.min_box_side_m, max_box_side_m=self.max_box_side_m)\n\n        ...\n\n        if self.publish_marker_point_clouds: \n            for marker in self.marker_collection:\n                marker_points = marker.get_marker_point_cloud()\n                self.add_point_array_to_point_cloud(marker_points)\n                publish_plane_points = False\n                if publish_plane_points: \n                    plane_points = marker.get_plane_fit_point_cloud()\n                    self.add_point_array_to_point_cloud(plane_points)\n            self.publish_point_cloud()\n        self.visualize_markers_pub.publish(marker_array)\n        if axes_array is not None: \n            self.visualize_axes_pub.publish(axes_array)\n</code></pre></p>"},{"location":"ros2/deep_perception/#face-detection-facial-landmarks-detection-and-head-pose-estimation-with-openvino-and-opencv","title":"Face Detection, Facial Landmarks Detection and Head Pose Estimation with OpenVINO and OpenCV","text":"<p>Detecting objects is just one thing Stretch can do well, it can do much more using pretrained models. For this part of the tutorial, we will be using Intel\u2019s OpenVINO toolkit with OpenCV. The cool thing about this demo is that it uses three different models in tandem to not just detect human faces, but also important features of the human face such as the eyes, nose and the lips with head pose information. This is important in the context of precise manipulation tasks such as assisted feeding where we want to know the exact location of the facial features the end effector must reach.</p> <p>OpenVINO is a toolkit popularized by Intel to optimize and deploy machine learning inference that can utilize hardware acceleration dongles such as the Intel Neural Compute Stick with Intel based compute architectures. More convenient is the fact that most of the deep learning models in the Open Model Zoo are accessible and configurable using the familiar OpenCV API with the opencv-python-inference-engine library.</p> <p>With that, let\u2019s jump right into it!</p> <p>Warning</p> <p>Running inference on Stretch results in a continuously high current draw by the CPU. Please ensure proper ventilation with the onboard fan. It is recommended to run the demo in tethered mode.</p>"},{"location":"ros2/deep_perception/#see-it-in-action_1","title":"See It In Action","text":"<p>First, let\u2019s execute the following command to see what it looks like:</p> <pre><code>ros2 launch stretch_deep_perception stretch_detect_faces.launch.py\n</code></pre> <p></p>"},{"location":"ros2/deep_perception/#code-breakdown_1","title":"Code Breakdown","text":"<p>Ain't that something! If you followed the breakdown in object detection, you'll find that the only change if you are looking to detect faces, facial landmarks or estimate head pose instead of detecting objects is in using a different deep learning model that does just that. For this, we will explore how to use the OpenVINO toolkit. Let's head to the detect_faces.py node to begin.</p> <p>In the main() method, we see a similar structure as with the object detction node. We first create an instance of the detector using the HeadPoseEstimator class from the head_estimator.py script to configure the deep learning models. Next, we pass this to an instance of the DetectionNode class from the detection_node.py script and call the main function. <pre><code>    ...\n\n    detector = he.HeadPoseEstimator(models_directory,\n                                    use_neural_compute_stick=use_neural_compute_stick)\n    default_marker_name = 'face'\n    node_name = 'DetectFacesNode'\n    topic_base_name = 'faces'\n    fit_plane = False\n    node = dn.DetectionNode(detector,\n                            default_marker_name,\n                            node_name,\n                            topic_base_name,\n                            fit_plane,\n                            min_box_side_m=min_head_m,\n                            max_box_side_m=max_head_m)\n    node.main()\n</code></pre></p> <p>In addition to detecting faces, this class also enables detecting facial landmarks as well as estimating head pose. The constructor initializes and configures three separate models, namely head_detection_model, head_pose_model and landmarks_model,  with the help of the renamed_cv2.dnn.readNet() wrappers. Note that renamed_cv2 is simply the opencv_python_inference_engine library compiled under a different namespace for use with Stretch so as not to conflict with the regular OpenCV library and have functionalities from both available to users concurrently. <pre><code>class HeadPoseEstimator:\n    def __init__(self, models_directory, use_neural_compute_stick=False):\n        ...\n\n        self.head_detection_model = renamed_cv2.dnn.readNetFromCaffe(head_detection_model_prototxt_filename, head_detection_model_caffemodel_filename)\n        dm.print_model_info(self.head_detection_model, 'head_detection_model')\n\n        ...\n\n        self.head_pose_model = renamed_cv2.dnn.readNet(head_pose_weights_filename, head_pose_config_filename)\n\n        ...\n\n        self.landmarks_model = renamed_cv2.dnn.readNet(landmarks_weights_filename, landmarks_config_filename)\n</code></pre></p> <p>The apply_to_image() method calls individual methods like detect_faces(), estimate_head_pose() and detect_facial_landmarks() that each runs the inference using the models we configured above. The bounding_boxes from the face detection model are used to supply the cropped image of the faces to head pose and facial landmark models to make their job way more efficient. <pre><code> def apply_to_image(self, rgb_image, draw_output=False):\n        ...\n\n        boxes = self.detect_faces(rgb_image)\n\n        facial_landmark_names = self.landmark_names.copy()\n        for bounding_box in boxes:\n            if draw_output: \n                self.draw_bounding_box(output_image, bounding_box)\n            yaw, pitch, roll = self.estimate_head_pose(rgb_image, bounding_box, enlarge_box=True, enlarge_scale=1.15)\n            if yaw is not None: \n                ypr = (yaw, pitch, roll)\n                if draw_output: \n                    self.draw_head_pose(output_image, yaw, pitch, roll, bounding_box)\n            else:\n                ypr = None\n            landmarks, landmark_names = self.detect_facial_landmarks(rgb_image, bounding_box, enlarge_box=True, enlarge_scale=1.15)\n            if (landmarks is not None) and draw_output: \n                self.draw_landmarks(output_image, landmarks)\n            heads.append({'box':bounding_box, 'ypr':ypr, 'landmarks':landmarks})\n\n        return heads, output_image\n</code></pre></p> <p>The detecion_node.py script then takes over as we saw with the object detection tutorial to publish the detections on pertinent topics. </p> <p>Now go ahead and experiment with a few more pretrained models using PyTorch or OpenVINO on Stretch. If you are feeling extra motivated, try creating your own neural networks and training them. Stretch is ready to deploy them!</p>"},{"location":"ros2/demo_grasp_object/","title":"Object Grasping","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore an object-grasping demo using FUNMAP.</p>"},{"location":"ros2/demo_grasp_object/#motivation","title":"Motivation","text":"<p>Through this demo, we demonstrate grasp candidate computation, grasp planning, and navigation using FUNMAP. An object is placed in front of the Stretch at a reasonable distance and height. The arm of the robot is fully retracted to get a good view of the object. We have observed reliable depth inference and object detection whenever the grasp candidate is placed in front of a dark background.</p>"},{"location":"ros2/demo_grasp_object/#workspace-setup","title":"Workspace Setup","text":"<p>Ideally, this demo requires the object to be placed at half the height of a Stretch. The end-effector of the robot is aligned with the target object. The arm is fully retracted to get a good view of the scene and reliable depth computation. The lift position is slightly below the height of the target surface. The demo works best in dim lighting conditions. Finally, ensure that there is no flat vertical surface, such as a wall, close behind the object of interest.</p>"},{"location":"ros2/demo_grasp_object/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the object grasping demo:</p> <pre><code>ros2 launch stretch_demos grasp_object.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, and the grasp_object nodes. </p> <p>In a new terminal window, start keyboard teleoperation:</p> <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p grasp_object_on:=true\n</code></pre> <p>You will be presented with a keyboard teleoperation menu in a new terminal window. Use key commands to get the Stretch configured as per the above workspace setup guidelines. Once the robot is ready, press \u2018\\\u2019 to trigger object grasping.</p>"},{"location":"ros2/demo_grasp_object/#code-explained","title":"Code Explained","text":"<p>The grasp_object node uses the joint_trajectory_server inside stretch_core to send out target joint positions. </p> <pre><code>self.trajectory_client = ActionClient(self,\n    FollowJointTrajectory,\n    '/stretch_controller/follow_joint_trajectory',\n    callback_group=self.callback_group\n)\n\nserver_reached = self.trajectory_client.wait_for_server(\n    timeout_sec=60.0\n)\n\nif not server_reached:\n    self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n    sys.exit()\n</code></pre> <p>Additionally, the node also subscribes to RealSense's depth cloud, tool type, and Stretch\u2019s joint state topics.</p> <pre><code>self.joint_states_subscriber = self.create_subscription(JointState,\n    '/stretch/joint_states',\n    callback=self.joint_states_callback,\n    qos_profile=1,\n    callback_group=self.callback_group\n)\n\nself.point_cloud_subscriber = self.create_subscription(PointCloud2,\n    '/camera/depth/color/points',\n    callback=self.point_cloud_callback,\n    qos_profile=1,\n    callback_group=self.callback_group\n)\n\nself.tool_subscriber = self.create_subscription(String,\n    '/tool',\n    callback=self.tool_callback,\n    qos_profile=1,\n    callback_group=self.callback_group\n)\n</code></pre> <p>Whenever the user triggers the grasp object service, Stretch scans the field of view for potential grasp candidates.</p> <pre><code>self.logger.info('Stow the arm.')\nself.stow_the_robot()\n\n# 1. Scan surface and find grasp target\nself.look_at_surface(scan_time_s = 4.0)\ngrasp_target = self.manipulation_view.get_grasp_target(self.tf2_buffer)\n</code></pre> <p>After determining a suitable grasp candidate, the node generates a pregrasp base and end-effector pose.</p> <pre><code># 2. Move to pregrasp pose\npregrasp_lift_m = self.manipulation_view.get_pregrasp_lift(grasp_target,\n    self.tf2_buffer\n)\n\nif self.tool == \"tool_stretch_dex_wrist\":\n    pregrasp_lift_m += 0.02\nif (self.lift_position is None):\n    return Trigger.Response(\n        success=False,\n        message='lift position unavailable'\n    )\n\nself.logger.info('Raise tool to pregrasp height.')\nlift_to_pregrasp_m = max(self.lift_position + pregrasp_lift_m, 0.1)\nlift_to_pregrasp_m = min(lift_to_pregrasp_m, max_lift_m)\n\npose = {'joint_lift': lift_to_pregrasp_m}\nself.move_to_pose(pose)\n\nif self.tool == \"tool_stretch_dex_wrist\":\n    self.logger.info('Rotate pitch/roll for grasping.')\n    pose = {'joint_wrist_pitch': -0.3, 'joint_wrist_roll': 0.0}\n    self.move_to_pose(pose)\n\npregrasp_yaw = self.manipulation_view.get_pregrasp_yaw(grasp_target,    \n    self.tf2_buffer\n)\n\nself.logger.info('pregrasp_yaw = {0:.2f} rad'.format(pregrasp_yaw))\nself.logger.info('pregrasp_yaw = {0:.2f} deg'.format(pregrasp_yaw * (180.0/np.pi)))\nself.logger.info('Rotate the gripper for grasping.')\n\npose = {'joint_wrist_yaw': pregrasp_yaw}\nself.move_to_pose(pose)\n\nself.logger.info('Open the gripper.')\npose = {'gripper_aperture': 0.125}\nself.move_to_pose(pose)\n\npregrasp_mobile_base_m, pregrasp_wrist_extension_m = self.manipulation_view.get_pregrasp_planar_translation(grasp_target,\n   self.tf2_buffer\n)\nself.logger.info('pregrasp_mobile_base_m = {0:.3f} m'.format(pregrasp_mobile_base_m))\nself.logger.info('pregrasp_wrist_extension_m = {0:.3f} m'.format(pregrasp_wrist_extension_m))\nself.logger.info('Drive to pregrasp location.')\nself.drive(pregrasp_mobile_base_m)\n\nif pregrasp_wrist_extension_m &gt; 0.0:\n    extension_m = max(self.wrist_position + pregrasp_wrist_extension_m, min_extension_m)\n    extension_m = min(extension_m, max_extension_m)\n    self.logger.info('Extend tool above surface.')\n    pose = {'wrist_extension': extension_m}\n    self.move_to_pose(pose)\nelse:\n    self.logger.info('negative wrist extension for pregrasp, so not extending or retracting.')\n</code></pre> <p>A plan to grasp the object is generated using FUNMAP\u2019s ManipulationView class. This class requires the pregrasp pose parameters to generate the final grasp plan.</p> <pre><code># 3. Grasp the object and lift it\ngrasp_mobile_base_m, grasp_lift_m, grasp_wrist_extension_m = self.manipulation_view.get_grasp_from_pregrasp(grasp_target, self.tf2_buffer)\nself.logger.info('grasp_mobile_base_m = {0:3f} m, grasp_lift_m = {1:3f} m, grasp_wrist_extension_m = {2:3f} m'.format(grasp_mobile_base_m, grasp_lift_m, grasp_wrist_extension_m))\nself.logger.info('Move the grasp pose from the pregrasp pose.')\n\nlift_m = max(self.lift_position + grasp_lift_m, 0.1)\nlift_m = min(lift_m, max_lift_m)\nextension_m = max(self.wrist_position + grasp_wrist_extension_m, min_extension_m)\nextension_m = min(extension_m, max_extension_m)\npose = {'translate_mobile_base': grasp_mobile_base_m,\n        'joint_lift': lift_m,\n        'wrist_extension': extension_m}\n</code></pre> <p>The node then proceeds to grasp the object by sending our goal joint positions.</p>"},{"location":"ros2/demo_grasp_object/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore object grasping with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>The grasp object candidate detection often fails due to the quality of the input point cloud.</p> </li> <li> <p>After grasp candidate detection, the segmented point cloud might not cover the entire surface for an object. Hence, the calculated grasp point will have a constant offset from the ideal grasp location.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"ros2/demo_handover_object/","title":"Object Handover Demo","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore an object-handover demo using FUNMAP.</p>"},{"location":"ros2/demo_handover_object/#motivation","title":"Motivation","text":"<p>Through this demo, we demonstrate human mouth detection using the stretch_deep_perception package, and demonstrate object delivery with navigation using FUNMAP. The robot is teleoperated to have a person in the view of its camera. The person requesting the object must face the robot. We use OpenVINO to perform facial recognition.</p>"},{"location":"ros2/demo_handover_object/#workspace-setup","title":"Workspace Setup","text":"<p>Ideally, this demo requires the person requesting the object to be facing the robot\u2019s camera. Use keyboard teleop to place the object in the robot\u2019s gripper.</p>"},{"location":"ros2/demo_handover_object/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the object handover demo:</p> <pre><code>ros2 launch stretch_demos handover_object.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, and the handover_object nodes. </p> <p>In a new terminal, launch keyboard teleoperation:</p> <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p handover_object_on:=true\n</code></pre> <p>You will be presented with a keyboard teleoperation menu in a new terminal window. Use key commands to get the Stretch configured as per the above workspace setup guidelines. Once the robot is ready, press \u2018y\u2019 or \u2018Y\u2019 to trigger object handover.</p>"},{"location":"ros2/demo_handover_object/#code-explained","title":"Code Explained","text":"<p>The object_handover node uses the joint_trajectory_server inside stretch_core to send out target joint positions.</p> <pre><code>self.trajectory_client = ActionClient(self, FollowJointTrajectory, '/stretch_controller/follow_joint_trajectory', callback_group=self.callback_group)\nserver_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\nif not server_reached:\n    self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n    sys.exit()\n</code></pre> <p>Additionally, the node also subscribes to mouth positions detected by stretch_deep_perception, and Stretch\u2019s joint state topics.</p> <pre><code>self.joint_states_subscriber = self.create_subscription(JointState, '/stretch/joint_states', qos_profile=1, callback=self.joint_states_callback, callback_group=self.callback_group)\n\nself.mouth_position_subscriber = self.create_subscription(MarkerArray, '/nearest_mouth/marker_array', qos_profile=1, callback=self.mouth_position_callback, callback_group=self.callback_group)\n</code></pre> <p>Whenever the node receives a mouth position message, it computes a handoff XYZ coordinate depending upon the current wrist and mouth positions:</p> <pre><code>def mouth_position_callback(self, marker_array):\n    with self.move_lock:\n\n        for marker in marker_array.markers:\n            if marker.type == self.mouth_marker_type:\n                mouth_position = marker.pose.position\n                self.mouth_point = PointStamped()\n                self.mouth_point.point = mouth_position\n                header = self.mouth_point.header\n                header.stamp = marker.header.stamp\n                header.frame_id = marker.header.frame_id\n                # header.seq = marker.header.seq\n                self.logger.info('******* new mouth point received *******')\n\n                lookup_time = Time(seconds=0) # return most recent transform\n                timeout_ros = Duration(seconds=0.1)\n\n                old_frame_id = self.mouth_point.header.frame_id\n                new_frame_id = 'base_link'\n                stamped_transform = self.tf2_buffer.lookup_transform(new_frame_id, old_frame_id, lookup_time, timeout_ros)\n                points_in_old_frame_to_new_frame_mat = rn.numpify(stamped_transform.transform)\n                camera_to_base_mat = points_in_old_frame_to_new_frame_mat\n\n                grasp_center_frame_id = 'link_grasp_center'\n                stamped_transform = self.tf2_buffer.lookup_transform(new_frame_id,\n                    grasp_center_frame_id,\n                    lookup_time,\n                    timeout_ros\n                )\n                grasp_center_to_base_mat = rn.numpify(stamped_transform.transform)\n\n                mouth_camera_xyz = np.array([0.0, 0.0, 0.0, 1.0])\n                mouth_camera_xyz[:3] = rn.numpify(self.mouth_point.point)[:3]\n\n                mouth_xyz = np.matmul(camera_to_base_mat, mouth_camera_xyz)[:3]\n                fingers_xyz = grasp_center_to_base_mat[:,3][:3]\n\n                handoff_object = True\n\n                if handoff_object:\n                    # attempt to handoff the object at a location below\n                    # the mouth with respect to the world frame (i.e.,\n                    # gravity)\n                    target_offset_xyz = np.array([0.0, 0.0, -0.2])\n                else:\n                    object_height_m = 0.1\n                    target_offset_xyz = np.array([0.0, 0.0, -object_height_m])\n                target_xyz = mouth_xyz + target_offset_xyz\n\n                fingers_error = target_xyz - fingers_xyz\n                self.logger.info(f'fingers_error = {str(fingers_error)}')\n\n                delta_forward_m = fingers_error[0]\n                delta_extension_m = -fingers_error[1]\n                delta_lift_m = fingers_error[2]\n\n                max_lift_m = 1.0\n                lift_goal_m = self.lift_position + delta_lift_m\n                lift_goal_m = min(max_lift_m, lift_goal_m)\n                self.lift_goal_m = lift_goal_m\n\n                self.mobile_base_forward_m = delta_forward_m\n\n                max_wrist_extension_m = 0.5\n                wrist_goal_m = self.wrist_position + delta_extension_m\n\n                if handoff_object:\n                    # attempt to handoff the object by keeping distance\n                    # between the object and the mouth distance\n                    wrist_goal_m = wrist_goal_m - 0.25 # 25cm from the mouth\n                    wrist_goal_m = max(0.0, wrist_goal_m)\n\n                self.wrist_goal_m = min(max_wrist_extension_m, wrist_goal_m)\n\n                self.handover_goal_ready = True\n</code></pre> <p>The delta between the wrist XYZ and mouth XYZ is used to calculate the lift position, base forward translation, and wrist extension.</p> <p>Once the user triggers the handover object service, the node sends out joint goal positions for the base, lift, and the wrist, to deliver the object near the person\u2019s mouth:</p> <pre><code>self.logger.info(\"Starting object handover!\")\nwith self.move_lock:\n    # First, retract the wrist in preparation for handing out an object.\n    pose = {'wrist_extension': 0.005}\n    self.move_to_pose(pose)\n\n    if self.handover_goal_ready:\n        pose = {'joint_lift': self.lift_goal_m}\n        self.move_to_pose(pose)\n        tolerance_distance_m = 0.01\n        at_goal = self.move_base.forward(self.mobile_base_forward_m,\n            detect_obstacles=False,\n            tolerance_distance_m=tolerance_distance_m\n        )\n        pose = {'wrist_extension': self.wrist_goal_m}\n        self.move_to_pose(pose)\n        self.handover_goal_ready = False\n</code></pre>"},{"location":"ros2/demo_handover_object/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore object delivery with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>The node requires the target user's face to be in the camera view while triggering the demo. As it stands, it does not keep any past face detections in its memory.</p> </li> <li> <p>Facial landmarks detection might not work well for some faces and is highly variable to the deviation from the faces that the algorithm was originally trained on.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"ros2/demo_hello_world/","title":"Hello World Demo","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore a hello world writing demo using FUNMAP.</p>"},{"location":"ros2/demo_hello_world/#motivation","title":"Motivation","text":"<p>Stretch is a contacts-sensitive robot, making it compliant in human spaces. This contact sensitivity can be leveraged for surface and obstacles detection. Through this demo we demonstrate one specific application of contacts detection - surface writing. In this demo, Stretch writes out the letters 'H', 'E', 'L', 'L', and 'O' in a sequential manner.</p>"},{"location":"ros2/demo_hello_world/#workspace-setup","title":"Workspace Setup","text":"<p>Things that you will need  - Whiteboard  - Marker</p> <p>Ideally, this demo requires a whiteboard that is placed at the same height as the Stretch. The writing surface must be flat and reachable by the robot's arm.</p>"},{"location":"ros2/demo_hello_world/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the hello world demo:</p> <pre><code>ros2 launch stretch_demos hello_world.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, and the hello_world nodes. </p> <p>Next, in a separate terminal, run:</p> <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p hello_world_on:=true\n</code></pre> <p>You will be presented with a keyboard teleoperation menu. Teleoperate the robot so that the arm is perpendicular to the writing surface, about 30 cm away. Place a marker pointing outward in the robot's gripper. Once the robot is ready, press \u2018`\u2019 or '~\u2019 to trigger hello world writing.</p>"},{"location":"ros2/demo_hello_world/#code-explained","title":"Code Explained","text":"<p>The clean_surface node uses the joint_trajectory_server inside stretch_core to send out target joint positions.</p> <pre><code>self.trajectory_client = ActionClient(self,\n    FollowJointTrajectory,\n    '/stretch_controller/follow_joint_trajectory',\n    callback_group=self.callback_group\n)\nserver_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\nif not server_reached:\n    self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n    sys.exit()\n</code></pre> <p>Additionally, the node also subscribes to Stretch\u2019s joint states topic.</p> <pre><code>self.joint_states_subscriber = self.create_subscription(JointState,\n    '/stretch/joint_states',\n    callback=self.joint_states_callback,\n    qos_profile=10,\n    callback_group=self.callback_group\n)\n</code></pre> <p>Whenever the user triggers the hello world write service, the robot moves to an initial joint configuration:</p> <pre><code>def move_to_initial_configuration(self):\n    initial_pose = {'wrist_extension': 0.01,\n                    'joint_lift': self.letter_top_lift_m,\n                    'joint_wrist_yaw': 0.0}\n\n    self.logger.info('Move to initial arm pose for writing.')\n    self.move_to_pose(initial_pose)\n</code></pre> <p>Thereafter, the node uses FUNMAP's <code>align_to_nearest_cliff</code> service to align the Stretch w.r.t the whiteboard.</p> <pre><code>def align_to_surface(self):\n    self.logger.info('align_to_surface')\n    trigger_request = Trigger.Request() \n    trigger_result = self.trigger_align_with_nearest_cliff_service.call_async(trigger_request)\n    self.logger.info('trigger_result = {0}'.format(trigger_result))\n</code></pre> <p>After aligning with the whiteboard, the node then proceeds to execute joint position goals to draw out each letter of the word \"hello\":</p> <pre><code>self.letter_h()\nself.space()\nself.letter_e()\nself.space()\nself.letter_l()\nself.space()\nself.letter_l()\nself.space()\nself.letter_o()\nself.space()\n\nreturn Trigger.Response(\n    success=True,\n    message='Completed writing hello!'\n)\n</code></pre>"},{"location":"ros2/demo_hello_world/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore whiteboard writing with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>Each Stretch has unique calibration thresholds. The contacts thresholds defined in the stock demo code might not work for your Stretch. Additional tuning might be necessary.</p> </li> <li> <p>The quality of the written text structure is also highly dependent on mobile base movements. Currently, navigation is open-loop and does not account for accumulated error terms.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"ros2/demo_open_drawer/","title":"Drawer Opening Demo","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore an drawer-opening demo using FUNMAP.</p>"},{"location":"ros2/demo_open_drawer/#motivation","title":"Motivation","text":"<p>Through this demo, we demonstrate opening of common drawers that have handles, and contact sensing with navigation using FUNMAP. FUNMAP provides APIs that allow users to extend the arm or adjust the lift until a contact is detected. We leverage this contact detection API in the demo. Users can trigger the drawer opening demo through an upward or downward motion of the hook attached to the end-effector, explained below.</p>"},{"location":"ros2/demo_open_drawer/#workspace-setup","title":"Workspace Setup","text":"<p>The robot is teleoperated so as to keep its arm perpendicular to the drawer\u2019s frontal surface, slightly above or below the handle. The workspace must be clear of any obstacles between the end-effector and the drawer. Finally, ensure that the wrist can reach the drawer through extension.</p>"},{"location":"ros2/demo_open_drawer/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the drawer opening demo:</p> <pre><code>ros2 launch stretch_demos open_drawer.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, and the open_drawer nodes. </p> <p>In a new terminal, launch keyboard teleoperation:</p> <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p open_drawer_on:=true\n</code></pre> <p>You will be presented with a keyboard teleoperation menu in a new terminal window. Use key commands to get the Stretch configured as per the above workspace setup guidelines. Once the robot is ready, press \u2018.\u2019 or \u2018&gt;\u2019 to trigger the drawer opening with an upward motion. If you want to run the demo with a downward motion, press \u2018z\u2019 or \u2018Z\u2019.</p>"},{"location":"ros2/demo_open_drawer/#code-explained","title":"Code Explained","text":"<p>The open_drawer node uses the joint_trajectory_server inside stretch_core to send out target joint positions. </p> <pre><code>self.trajectory_client = ActionClient(self, FollowJointTrajectory, '/stretch_controller/follow_joint_trajectory', callback_group=self.callback_group)\n        server_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\n        if not server_reached:\n            self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n            sys.exit()\n</code></pre> <p>The node then proceeds to connect to the contact detection services provided by FUNMAP:</p> <pre><code>self.trigger_open_drawer_service = self.create_service(Trigger, '/open_drawer/trigger_open_drawer_down',                                                self.trigger_open_drawer_down_callback, callback_group=self.callback_group)\n\nself.trigger_open_drawer_service = self.create_service(Trigger, '/open_drawer/trigger_open_drawer_up',                                                self.trigger_open_drawer_up_callback, callback_group=self.callback_group)\n\nself.trigger_align_with_nearest_cliff_service = self.create_client(Trigger, '/funmap/trigger_align_with_nearest_cliff', callback_group=self.callback_group)\n\nself.trigger_align_with_nearest_cliff_service.wait_for_service()\nself.logger.info('Node ' + self.get_name() + ' connected to /funmap/trigger_align_with_nearest_cliff.')\n\nself.trigger_reach_until_contact_service = self.create_client(Trigger, '/funmap/trigger_reach_until_contact', callback_group=self.callback_group)\nself.trigger_reach_until_contact_service.wait_for_service()\nself.logger.info('Node ' + self.get_name() + ' connected to /funmap/trigger_reach_until_contact.')\n\nself.trigger_lower_until_contact_service = self.create_client(Trigger, '/funmap/trigger_lower_until_contact', callback_group=self.callback_group)\nself.trigger_lower_until_contact_service.wait_for_service()\nself.logger.info('Node ' + self.get_name() + ' connected to /funmap/trigger_lower_until_contact.')\n</code></pre> <p>Once the user triggers the drawer opening demo, the robot is commanded to move to an initial configuration:</p> <pre><code>initial_pose = {'wrist_extension': 0.01,\n                'joint_wrist_yaw': 1.570796327,\n                'gripper_aperture': 0.0}\nself.logger.info('Move to the initial configuration for drawer opening.')\nself.move_to_pose(initial_pose)\n</code></pre> <p>The robot then proceeds to extend its wrist until it detects a contact, hopefully caused by a drawer surface.</p> <pre><code>self.extend_hook_until_contact()\n</code></pre> <p>It then backs off from the surface by about 5 cm so as to not touch it. </p> <pre><code>success = self.backoff_from_surface()\nif not success:\n    return Trigger.Response(\n        success=False,\n        message='Failed to backoff from the surface.'\n    )\n</code></pre> <p>Depending upon the user\u2019s choice, the robot will either raise or lower its lift so as to hook the handle. </p> <pre><code>if direction == 'down':\n    self.lower_hook_until_contact()\nelif direction == 'up':\n    self.raise_hook_until_contact()\n</code></pre> <p>Finally, the robot will pull open the drawer.</p> <pre><code>success = self.pull_open()\nif not success:\n     return Trigger.Response(\n         success=False,\n         message='Failed to pull open the drawer.'\n     )\n\npush_drawer_closed = False\nif push_drawer_closed:\n    time.sleep(3.0)\n    self.push_closed()\n\nreturn Trigger.Response(\n    success=True,\n    message='Completed opening the drawer!'\n)\n</code></pre>"},{"location":"ros2/demo_open_drawer/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore drawer opening with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>The demo performs a blind hook operation using contacts sensitivity. Special care must be taken so as to not damage the target surface.</p> </li> <li> <p>The code might command the Stretch to lift its arm by a large amount which might cause collisions in the workspace. The code also does not take into account the configuration space of the robot.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"ros2/demo_surface_cleaning/","title":"Surface Cleaning Demo","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore a surface cleaning demo using FUNMAP.</p>"},{"location":"ros2/demo_surface_cleaning/#motivation","title":"Motivation","text":"<p>Through this demo, we demonstrate flat surface detection using the head camera, and demonstrate surface cleaning with navigation using FUNMAP. The robot is teleoperated to have a flat surface at a reasonable height in the view of the robot\u2019s camera. The arm of the robot is fully retracted to get a good and entire view of the surface of interest. We have observed reliable depth inference and object detection whenever the surface to be cleaned is placed in front of a dark background.</p>"},{"location":"ros2/demo_surface_cleaning/#workspace-setup","title":"Workspace Setup","text":"<p>Ideally, this demo requires the surface to be present at half the height of a Stretch. Teleoperate your robot so as to get a good view of it. The end-effector of the robot is aligned with the target object. The arm is fully retracted to get a good view of the scene and reliable depth computation. The lift position is slightly below the height of the target surface. The demo works best in dim lighting conditions. Finally, ensure that there is no flat vertical surface, such as a wall, close behind the surface to be cleaned. You can use RViz to visualize the pointcloud data being captured by the camera.</p>"},{"location":"ros2/demo_surface_cleaning/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the surface cleaning demo:</p> <pre><code>ros2 launch stretch_demos clean_surface.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, keyboard_teleop, and the clean_surface nodes. </p> <p>In a new terminal, launch keyboard teleoperation: <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p clean_surface_on:=true\n</code></pre></p> <p>You will be presented with a keyboard teleoperation menu in a new terminal window. Use key commands to get the Stretch configured as per the above workspace setup guidelines. Once the robot is ready, press \u2018/\u2019 or \u2018?\u2019 to trigger surface cleaning.</p>"},{"location":"ros2/demo_surface_cleaning/#code-explained","title":"Code Explained","text":"<p>The clean_surface node uses the joint_trajectory_server inside stretch_core to send out target joint positions.</p> <pre><code>self.trajectory_client = ActionClient(self,\n    FollowJointTrajectory,\n    '/stretch_controller/follow_joint_trajectory',\n    callback_group=self.callback_group\n)\nserver_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\nif not server_reached:\n    self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n    sys.exit()\n</code></pre> <p>Additionally, the node also subscribes to RealSense's depth cloud, and Stretch\u2019s joint state topics.</p> <pre><code>self.joint_states_subscriber = self.create_subscription(JointState,\n    '/stretch/joint_states',\n    callback=self.joint_states_callback,\n    qos_profile=10,\n    callback_group=self.callback_group\n)\n\nself.point_cloud_subscriber = self.create_subscription(PointCloud2,\n    '/camera/depth/color/points',\n    callback=self.point_cloud_callback,\n    qos_profile=10, \n    callback_group=self.callback_group\n)\n</code></pre> <p>Whenever the user triggers the clean surface service, Stretch scans the field of view for candidate flat surfaces and generates a surface wiping plan represented by a list of joint position goals. The wiping plan is generated using the ManipulationView class:</p> <pre><code>self.log.info(\"Cleaning initiating!\")\n\ntool_width_m = 0.08\ntool_length_m = 0.08\nstep_size_m = 0.04\nmin_extension_m = 0.01\nmax_extension_m = 0.5\n\nself.look_at_surface()\nstrokes, simple_plan, lift_to_surface_m = self.manipulation_view.get_surface_wiping_plan(self.tf2_buffer,\n    tool_width_m,\n    tool_length_m,\n    step_size_m\n)\nself.log.info(\"Plan:\" + str(simple_plan))\n\nself.log.info('********* lift_to_surface_m = {0} **************'.format(lift_to_surface_m))\n</code></pre> <p>The ManipulationView class holds a max-height image as an internal representation of the surroundings. It creates a segmentation mask to generate a set of linear forward-backward strokes that cover the entire detected surface. See <code>stretch_funmap/segment_max_height_image.py</code> to understand how a flat surface is detected. Obstacles, if present on the surface, are inflated using dilation and erosion to create a margin of safety. These strokes, in the image frame, are then transformed into the base_link frame using tf2. This is how we obtain the wiping plan.</p> <p>After generating the wiping plan, the node validates this plan by asserting that the required strokes are greater than zero and the surface is reachable by adjusting the lift and wrist positions:</p> <pre><code>if True and (strokes is not None) and (len(strokes) &gt; 0):\n    if (self.lift_position is not None) and (self.wrist_position is not None):\n        above_surface_m = 0.1\n        lift_above_surface_m = self.lift_position + lift_to_surface_m + above_surface_m\n</code></pre> <p>After validation, the node then proceeds to execute the wiping plan in a sequential manner.</p>"},{"location":"ros2/demo_surface_cleaning/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore surface cleaning with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>The area of surface being cleaned is dependent on the fill rate of the RealSense point cloud. If the segemented cleaning surface has too many holes, Stretch might only attempt to clean a fraction of the total area.</p> </li> <li> <p>As each Stretch is unique in its contacts thresholds after calibration, your robot might attempt too aggressively to press down the cloth upon the surface. Additonal tuning might be necessary.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"ros2/example_1/","title":"Example 1","text":""},{"location":"ros2/example_1/#example-1","title":"Example 1","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p> </p> <p>The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending <code>Twist</code> messages to a Stretch robot.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To drive the robot in circles with the move node, type the following in a new terminal.</p> <p><pre><code>ros2 run stetch_ros_tutorials move\n</code></pre> To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"ros2/example_1/#the-code","title":"The Code","text":"<p>Below is the code which will send Twist messages to drive the robot in circles.</p> <pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nclass Move(Node):\n    def __init__(self):\n        super().__init__('stretch_base_move')\n        self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 10)\n\n        self.get_logger().info(\"Starting to move in circle...\")\n        timer_period = 0.5  # seconds\n        self.timer = self.create_timer(timer_period, self.move_around)\n    def move_around(self):\n        command = Twist()\n        command.linear.x = 0.0\n        command.linear.y = 0.0\n        command.linear.z = 0.0\n        command.angular.x = 0.0\n        command.angular.y = 0.0\n        command.angular.z = 0.5\n        self.publisher_.publish(command)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    base_motion = Move()\n    rclpy.spin(base_motion)\n    base_motion.destroy_node()  \n    rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_1/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\n</code></pre> You need to import rclpy if you are writing a ROS 2 Node. The geometry_msgs.msg import is so that we can send velocity commands to the robot.</p> <p><pre><code>class Move(Node):\n    def __init__(self):\n        super().__init__('stretch_base_move')\n        self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 10)\n</code></pre> This section of code defines the talker's interface to the rest of ROS. self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 10) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist. The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough.</p> <p><pre><code>        timer_period = 0.5  # seconds\n        self.timer = self.create_timer(timer_period, self.move_around)\n</code></pre> We create a timer with a period of 0.5 seconds. This timer ensures that the function move_around is called every 0.5 seconds. This ensures a constant rate of 2Hz for the execution loop.</p> <p><pre><code>command = Twist()\n</code></pre> Make a Twist message.  We're going to set all of the elements since we can't depend on them defaulting to safe values.</p> <p><pre><code>        command.linear.x = 0.0\n        command.linear.y = 0.0\n        command.linear.z = 0.0\n</code></pre> A Twist has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y direction or the z-direction. We set the linear velocities to 0.</p> <p><pre><code>        command.angular.x = 0.0\n        command.angular.y = 0.0\n        command.angular.z = 0.5\n</code></pre> A Twist also has three rotational velocities (in radians per second). The Stretch will only respond to rotations around the z (vertical) axis. We set this to a non-zero value.</p> <p><pre><code>self.publisher_.publish(command)\n</code></pre> Publish the Twist commands in the previously defined topic name /stretch/cmd_vel.</p> <p><pre><code>def main(args=None):\n    rclpy.init(args=args)\n    base_motion = Move()\n    rclpy.spin(base_motion)\n    base_motion.destroy_node()  \n    rclpy.shutdown()\n</code></pre> The next line, rclpy.init(args=args), is very important as it tells ROS to initialize the node. Until rclpy has this information, it cannot start communicating with the ROS Master. In this case, your node will take on the name 'stretch_base_move'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". We then create an instance called base_motion of the class Move(). This is then spun using the spin function in rclpy to call the callback functions, in our case the timer that ensures the move_around function is called at a steady rate of 2Hz.</p> <p>To stop the node from sending twist messages, type <code>Ctrl + c</code>.</p>"},{"location":"ros2/example_10/","title":"Example 10","text":""},{"location":"ros2/example_10/#example-10","title":"Example 10","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>This tutorial provides you with an idea of what tf2 can do in the Python track. We will elaborate on how to create a tf2 static broadcaster and listener.</p>"},{"location":"ros2/example_10/#tf2-static-broadcaster","title":"tf2 Static Broadcaster","text":"<p>For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the link_mast, link_lift, and link_wrist_yaw frames.</p> <p>Begin by starting up the stretch driver launch file.</p> <p><pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> Open RViz in another terminal and add the RobotModel and TF plugins in the left hand panel</p> <p><pre><code>ros2 run rviz2 rviz2\n</code></pre> Then run the tf2 broadcaster node to visualize three static frames.</p> <pre><code>ros2 run stretch_ros_tutorials tf_broadcaster\n</code></pre> <p>The GIF below visualizes what happens when running the previous node.</p> <p> </p> <p>OPTIONAL: If you would like to see how the static frames update while the robot is in motion, run the stow command node and observe the tf frames in RViz.</p> <pre><code>ros2 run stretch_ros_tutorials stow_command\n</code></pre> <p> </p>"},{"location":"ros2/example_10/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rclpy\nfrom rclpy.node import Node\nfrom tf2_ros import TransformBroadcaster\nimport tf_transformations\nfrom geometry_msgs.msg import TransformStamped\n\n# This node publishes three child static frames in reference to their parent frames as below:\n# parent -&gt; link_mast            child -&gt; fk_link_mast\n# parent -&gt; link_lift            child -&gt; fk_link_lift\n# parent -&gt; link_wrist_yaw       child -&gt; fk_link_wrist_yaw\n\nclass FixedFrameBroadcaster(Node):\n    def __init__(self):\n        super().__init__('stretch_tf_broadcaster')\n        self.br = TransformBroadcaster(self)\n        time_period = 0.1 # seconds\n        self.timer = self.create_timer(time_period, self.broadcast_timer_callback)\n\n        self.mast = TransformStamped()\n        self.mast.header.frame_id = 'link_mast'\n        self.mast.child_frame_id = 'fk_link_mast'\n        self.mast.transform.translation.x = 0.0\n        self.mast.transform.translation.y = 0.0\n        self.mast.transform.translation.z = 0.0\n        q = tf_transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.mast.transform.rotation.x = q[0]\n        self.mast.transform.rotation.y = q[1]\n        self.mast.transform.rotation.z = q[2]\n        self.mast.transform.rotation.w = q[3]\n\n        self.lift = TransformStamped()\n        self.lift.header.stamp = self.get_clock().now().to_msg()\n        self.lift.header.frame_id = 'link_lift'\n        self.lift.child_frame_id = 'fk_link_lift'\n        self.lift.transform.translation.x = 0.0\n        self.lift.transform.translation.y = 2.0\n        self.lift.transform.translation.z = 0.0\n        q = tf_transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.lift.transform.rotation.x = q[0]\n        self.lift.transform.rotation.y = q[1]\n        self.lift.transform.rotation.z = q[2]\n        self.lift.transform.rotation.w = q[3]\n        self.br.sendTransform(self.lift)\n\n        self.wrist = TransformStamped()\n        self.wrist.header.stamp = self.get_clock().now().to_msg()\n        self.wrist.header.frame_id = 'link_wrist_yaw'\n        self.wrist.child_frame_id = 'fk_link_wrist_yaw'\n        self.wrist.transform.translation.x = 0.0\n        self.wrist.transform.translation.y = 2.0\n        self.wrist.transform.translation.z = 0.0\n        q = tf_transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.wrist.transform.rotation.x = q[0]\n        self.wrist.transform.rotation.y = q[1]\n        self.wrist.transform.rotation.z = q[2]\n        self.wrist.transform.rotation.w = q[3]\n        self.br.sendTransform(self.wrist)\n\n        self.get_logger().info(\"Publishing Tf frames. Use RViz to visualize.\")\n\n    def broadcast_timer_callback(self):\n        self.mast.header.stamp = self.get_clock().now().to_msg()\n        self.br.sendTransform(self.mast)\n\n        self.lift.header.stamp = self.get_clock().now().to_msg()\n        self.br.sendTransform(self.lift)\n\n        self.wrist.header.stamp = self.get_clock().now().to_msg()\n        self.br.sendTransform(self.wrist)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    tf_broadcaster = FixedFrameBroadcaster()\n\n    rclpy.spin(tf_broadcaster)\n\n    tf_broadcaster.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_10/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom tf2_ros import TransformBroadcaster\nimport tf_transformations\nfrom geometry_msgs.msg import TransformStamped\n</code></pre> <p>You need to import rclpy if you are writing a ROS 2 node. Import <code>tf_transformations</code> to get quaternion values from Euler angles. Import the <code>TransformStamped</code> from the <code>geometry_msgs.msg</code> package because we will be publishing static frames and it requires this message type. The <code>tf2_ros</code> package provides an implementation of a <code>TransformBroadcaster.</code> to help make the task of publishing transforms easier.</p> <pre><code>class FixedFrameBroadcaster(Node):\n    def __init__(self):\n        super().__init__('stretch_tf_broadcaster')\n        self.br = TransformBroadcaster(self)\n</code></pre> <p>Here we create a <code>TransformStamped</code> object which will be the message we will send over once populated.</p> <pre><code>        self.lift = TransformStamped()\n        self.lift.header.stamp = self.get_clock().now().to_msg()\n        self.lift.header.frame_id = 'link_lift'\n        self.lift.child_frame_id = 'fk_link_lift'\n</code></pre> <p>We need to give the transform being published a timestamp, we'll just stamp it with the current time, <code>self.get_clock().now().to_msg()</code>. Then, we need to set the name of the parent frame of the link we're creating, in this case link_lift. Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is fk_link_lift.</p> <pre><code>        self.mast.transform.translation.x = 0.0\n        self.mast.transform.translation.y = 0.0\n        self.mast.transform.translation.z = 0.0\n</code></pre> <p>Set the translation values for the child frame.</p> <pre><code>        q = tf_transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.lift.transform.rotation.x = q[0]\n        self.lift.transform.rotation.y = q[1]\n        self.lift.transform.rotation.z = q[2]\n        self.lift.transform.rotation.w = q[3]\n</code></pre> <p>The <code>quaternion_from_euler()</code> function takes in a Euler angle argument and returns a quaternion values. Then set the rotation values to the transformed quaternions.</p> <p>This process will be completed for the link_mast and link_wrist_yaw as well.</p> <pre><code>self.br.sendTransform(self.lift)\n</code></pre> <p>Send the three transforms using the <code>sendTransform()</code> function.</p> <pre><code>def main(args=None):\n    rclpy.init(args=args)\n    tf_broadcaster = FixedFrameBroadcaster()\n</code></pre> <p>Instantiate the <code>FixedFrameBroadcaster()</code> class.</p> <pre><code>rclpy.spin(tf_broadcaster)\n</code></pre> <p>Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"ros2/example_10/#tf2-static-listener","title":"tf2 Static Listener","text":"<p>In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section we will create a tf2 listener that will find the transform between fk_link_lift and link_grasp_center.</p> <p>Begin by starting up the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then run the tf2 broadcaster node to create the three static frames.</p> <pre><code>ros2 run stretch_ros_tutorials tf_broadcaster\n</code></pre> <p>Finally, run the tf2 listener node to print the transform between two links.</p> <pre><code>ros2 run stretch_ros_tutorials tf_listener\n</code></pre> <p>Within the terminal the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames.</p> <pre><code>[INFO] [1659551318.098168]: The pose of target frame link_grasp_center with reference from fk_link_lift is:\ntranslation:\n  x: 1.08415191335\n  y: -0.176147838153\n  z: 0.576720021135\nrotation:\n  x: -0.479004489528\n  y: -0.508053545368\n  z: -0.502884087254\n  w: 0.509454501243\n</code></pre> <p> </p>"},{"location":"ros2/example_10/#the-code_1","title":"The Code","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom rclpy.time import Time\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\n\nclass FrameListener(Node):\n\n    def __init__(self):\n        super().__init__('stretch_tf_listener')\n\n        self.declare_parameter('target_frame', 'link_grasp_center')\n        self.target_frame = self.get_parameter(\n            'target_frame').get_parameter_value().string_value\n\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        time_period = 1.0 # seconds\n        self.timer = self.create_timer(time_period, self.on_timer)\n\n    def on_timer(self):\n        from_frame_rel = self.target_frame\n        to_frame_rel = 'fk_link_mast'\n\n        try:\n            now = Time()\n            trans = self.tf_buffer.lookup_transform(\n                to_frame_rel,\n                from_frame_rel,\n                now)\n        except TransformException as ex:\n            self.get_logger().info(\n                f'Could not transform {to_frame_rel} to {from_frame_rel}: {ex}')\n            return\n\n        self.get_logger().info(\n                        f'the pose of target frame {from_frame_rel} with reference to {to_frame_rel} is: {trans}')\n\n\ndef main():\n    rclpy.init()\n    node = FrameListener()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_10/#the-code-explained_1","title":"The Code Explained","text":"<pre><code>        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n</code></pre> <p>Here, we create a <code>TransformListener</code> object. Once the listener is created, it starts receiving tf2 transformations over the wire, and buffers them for up to 10 seconds.</p> <pre><code>        from_frame_rel = self.target_frame\n        to_frame_rel = 'fk_link_mast'\n</code></pre> <p>Store frame names in variables that will be used to compute transformations.</p> <pre><code>        try:\n            now = Time()\n            trans = self.tf_buffer.lookup_transform(\n                to_frame_rel,\n                from_frame_rel,\n                now)\n        except TransformException as ex:\n            self.get_logger().info(\n                f'Could not transform {to_frame_rel} to {from_frame_rel}: {ex}')\n            return\n</code></pre> <p>Try to look up the transform we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between from_frame_rel and to_frame_rel frames with the <code>lookup_transform()</code> function.</p>"},{"location":"ros2/example_12/","title":"Example 12","text":"<p>For this example, we will send follow joint trajectory commands for the head camera to search and locate an ArUco tag. In this instance, a Stretch robot will try to locate the docking station's ArUco tag.</p>"},{"location":"ros2/example_12/#modifying-stretch-marker-dictionary-yaml-file","title":"Modifying Stretch Marker Dictionary YAML File","text":"<p>When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml, that holds the information about the markers. A further breakdown of the YAML file can be found in our Aruco Marker Detection tutorial.</p> <p>Below is what needs to be included in the stretch_marker_dict.yaml file so the detect_aruco_markers node can find the docking station's ArUco tag.</p> <pre><code>'245':\n  'length_mm': 88.0\n  'use_rgb_only': False\n  'name': 'docking_station'\n  'link': None\n</code></pre>"},{"location":"ros2/example_12/#getting-started","title":"Getting Started","text":"<p>Begin by running the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>ros2 launch stretch_core d435i_high_resolution.launch.py\n</code></pre> <p>Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. In a new terminal, execute:</p> <pre><code>ros2 launch stretch_core stretch_aruco.launch.py\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>ros2 run rviz2 rviz2 -d /home/hello-robot/ament_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz\n</code></pre> <p>Then run the aruco_tag_locator.py node. In a new terminal, execute:</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 aruco_tag_locator.py\n</code></pre> <p> </p>"},{"location":"ros2/example_12/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\n# Import modules\nimport rclpy\nimport time\nimport tf2_ros\nfrom tf2_ros import TransformException\nfrom rclpy.time import Time\nfrom math import pi\n\n# Import hello_misc script for handling trajectory goals with an action client\nimport hello_helpers.hello_misc as hm\n\n# We're going to subscribe to a JointState message type, so we need to import\n# the definition for it\nfrom sensor_msgs.msg import JointState\n\n# Import the FollowJointTrajectory from the control_msgs.action package to\n# control the Stretch robot\nfrom control_msgs.action import FollowJointTrajectory\n\n# Import JointTrajectoryPoint from the trajectory_msgs package to define\n# robot trajectories\nfrom trajectory_msgs.msg import JointTrajectoryPoint\n\n# Import TransformStamped from the geometry_msgs package for the publisher\nfrom geometry_msgs.msg import TransformStamped\n\nclass LocateArUcoTag(hm.HelloNode):\n    \"\"\"\n    A class that actuates the RealSense camera to find the docking station's\n    ArUco tag and returns a Transform between the `base_link` and the requested tag.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes the subscriber and other needed variables.\n        :param self: The self reference.\n        \"\"\"\n        # Initialize the inhereted hm.Hellonode class\n        hm.HelloNode.__init__(self)\n        hm.HelloNode.main(self, 'aruco_tag_locator', 'aruco_tag_locator', wait_for_first_pointcloud=False)\n        # Initialize subscriber\n        self.joint_states_sub = self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)\n        # Initialize publisher\n        self.transform_pub = self.create_publisher(TransformStamped, 'ArUco_transform', 10)\n\n        # Initialize the variable that will store the joint state positions\n        self.joint_state = None\n\n        # Provide the min and max joint positions for the head pan. These values\n        # are needed for sweeping the head to search for the ArUco tag\n        self.min_pan_position = -3.8\n        self.max_pan_position =  1.50\n\n        # Define the number of steps for the sweep, then create the step size for\n        # the head pan joint\n        self.pan_num_steps = 10\n        self.pan_step_size = abs(self.min_pan_position - self.max_pan_position)/self.pan_num_steps\n\n        # Define the min tilt position, number of steps, and step size\n        self.min_tilt_position = -0.75\n        self.tilt_num_steps = 3\n        self.tilt_step_size = pi/16\n\n        # Define the head actuation rotational velocity\n        self.rot_vel = 0.5 # radians per sec\n\n    def joint_states_callback(self, msg):\n        \"\"\"\n        A callback function that stores Stretch's joint states.\n        :param self: The self reference.\n        :param msg: The JointState message type.\n        \"\"\"\n        self.joint_state = msg\n\n    def send_command(self, command):\n        '''\n        Handles single joint control commands by constructing a FollowJointTrajectoryGoal\n        message and sending it to the trajectory_client created in hello_misc.\n        :param self: The self reference.\n        :param command: A dictionary message type.\n        '''\n        if (self.joint_state is not None) and (command is not None):\n\n            # Extract the string value from the `joint` key\n            joint_name = command['joint']\n\n            # Set trajectory_goal as a FollowJointTrajectory.Goal and define\n            # the joint name\n            trajectory_goal = FollowJointTrajectory.Goal()\n            trajectory_goal.trajectory.joint_names = [joint_name]\n\n            # Create a JointTrajectoryPoint message type\n            point = JointTrajectoryPoint()\n\n            # Check to see if `delta` is a key in the command dictionary\n            if 'delta' in command:\n                # Get the current position of the joint and add the delta as a\n                # new position value\n                joint_index = self.joint_state.name.index(joint_name)\n                joint_value = self.joint_state.position[joint_index]\n                delta = command['delta']\n                new_value = joint_value + delta\n                point.positions = [new_value]\n\n            # Check to see if `position` is a key in the command dictionary\n            elif 'position' in command:\n                # extract the head position value from the `position` key\n                point.positions = [command['position']]\n\n            # Set the rotational velocity\n            point.velocities = [self.rot_vel]\n\n            # Assign goal position with updated point variable\n            trajectory_goal.trajectory.points = [point]\n\n            # Specify the coordinate frame that we want (base_link) and set the time to be now.\n            trajectory_goal.trajectory.header.stamp = self.get_clock().now().to_msg()\n            trajectory_goal.trajectory.header.frame_id = 'base_link'\n\n            # Make the action call and send the goal. The last line of code waits\n            # for the result\n            self.trajectory_client.send_goal(trajectory_goal)\n\n    def find_tag(self, tag_name='docking_station'):\n        \"\"\"\n        A function that actuates the camera to search for a defined ArUco tag\n        marker. Then the function returns the pose.\n        :param self: The self reference.\n        :param tag_name: A string value of the ArUco marker name.\n\n        :returns transform: The docking station's TransformStamped message.\n        \"\"\"\n        # Create dictionaries to get the head in its initial position\n        pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n        self.send_command(pan_command)\n        tilt_command = {'joint': 'joint_head_tilt', 'position': self.min_tilt_position}\n        self.send_command(tilt_command)\n\n        # Nested for loop to sweep the joint_head_pan and joint_head_tilt in increments\n        for i in range(self.tilt_num_steps):\n            for j in range(self.pan_num_steps):\n                # Update the joint_head_pan position by the pan_step_size\n                pan_command = {'joint': 'joint_head_pan', 'delta': self.pan_step_size}\n                self.send_command(pan_command)\n\n                # Give time for system to do a Transform lookup before next step\n                time.sleep(0.2)\n\n                # Use a try-except block\n                try:\n                    now = Time()\n                    # Look up transform between the base_link and requested ArUco tag\n                    transform = self.tf_buffer.lookup_transform('base_link',\n                                                            tag_name,\n                                                            now)\n                    self.get_logger().info(f\"Found Requested Tag: \\n{transform}\")\n\n                    # Publish the transform\n                    self.transform_pub.publish(transform)\n\n                    # Return the transform\n                    return transform\n                except TransformException as ex:\n                    continue\n\n            # Begin sweep with new tilt angle\n            pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n            self.send_command(pan_command)\n            tilt_command = {'joint': 'joint_head_tilt', 'delta': self.tilt_step_size}\n            self.send_command(tilt_command)\n            time.sleep(0.25)\n\n        # Notify that the requested tag was not found\n        self.get_logger().info(\"The requested tag '%s' was not found\", tag_name)\n\n    def main(self):\n        \"\"\"\n        Function that initiates the issue_command function.\n        :param self: The self reference.\n        \"\"\"\n        # Create a StaticTranformBoradcaster Node. Also, start a Tf buffer that\n        # will store the tf information for a few seconds.Then set up a tf listener, which\n        # will subscribe to all of the relevant tf topics, and keep track of the information\n        self.static_broadcaster = tf2_ros.StaticTransformBroadcaster(self)\n        self.tf_buffer = tf2_ros.Buffer()\n        self.listener = tf2_ros.TransformListener(self.tf_buffer, self)\n\n        # Give the listener some time to accumulate transforms\n        time.sleep(1.0)\n\n        # Notify Stretch is searching for the ArUco tag with `get_logger().info()`\n        self.get_logger().info('Searching for docking ArUco tag')\n\n        # Search for the ArUco marker for the docking station\n        pose = self.find_tag(\"docking_station\")\n\ndef main():\n    try:\n        # Instantiate the `LocateArUcoTag()` object\n        node = LocateArUcoTag()\n        # Run the `main()` method\n        node.main()\n        node.new_thread.join()\n    except:\n        node.get_logger().info('Interrupt received, so shutting down')\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_12/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport time\nimport tf2_ros\nfrom tf2_ros import TransformException\nfrom rclpy.time import Time\nfrom math import pi\n\nimport hello_helpers.hello_misc as hm\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom geometry_msgs.msg import TransformStamped\n</code></pre> <p>You need to import <code>rclpy</code> if you are writing a ROS Node. Import other python modules needed for this node. Import the <code>FollowJointTrajectory</code> from the control_msgs.action package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.</p> <pre><code>def __init__(self):\n        # Initialize the inhereted hm.Hellonode class\n        hm.HelloNode.__init__(self)\n        hm.HelloNode.main(self, 'aruco_tag_locator', 'aruco_tag_locator', wait_for_first_pointcloud=False)\n        # Initialize subscriber\n        self.joint_states_sub = self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)\n        # Initialize publisher\n        self.transform_pub = self.create_publisher(TransformStamped, 'ArUco_transform', 10)\n\n        # Initialize the variable that will store the joint state positions\n        self.joint_state = None\n</code></pre> <p>The <code>LocateArUcoTag</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is instantiated.</p> <p>Set up a subscriber with <code>self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)</code>.  We're going to subscribe to the topic <code>stretch/joint_states</code>, looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function <code>joint_states_callback()</code> automatically.</p> <p><code>self.create_publisher(TransformStamped, 'ArUco_transform', 10)</code> declares that your node is publishing to the <code>ArUco_transform</code> topic using the message type <code>TransformStamped</code>. The <code>10</code> argument limits the amount of queued messages if any subscriber is not receiving them fast enough.</p> <pre><code>self.min_pan_position = -4.10\nself.max_pan_position =  1.50\nself.pan_num_steps = 10\nself.pan_step_size = abs(self.min_pan_position - self.max_pan_position)/self.pan_num_steps\n</code></pre> <p>Provide the minimum and maximum joint positions for the head pan. These values are needed for sweeping the head to search for the ArUco tag. We also define the number of steps for the sweep, then create the step size for the head pan joint.</p> <pre><code>self.min_tilt_position = -0.75\nself.tilt_num_steps = 3\nself.tilt_step_size = pi/16\n</code></pre> <p>Set the minimum position of the tilt joint, the number of steps, and the size of each step.</p> <pre><code>self.rot_vel = 0.5 # radians per sec\n</code></pre> <p>Define the head actuation rotational velocity.</p> <pre><code>def joint_states_callback(self, msg):\n    self.joint_state = msg\n</code></pre> <p>The <code>joint_states_callback()</code> function stores Stretch's joint states.</p> <pre><code>def send_command(self, command):\n    if (self.joint_state is not None) and (command is not None):\n        joint_name = command['joint']\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = [joint_name]\n        point = JointTrajectoryPoint()\n</code></pre> <p>Assign <code>trajectory_goal</code> as a <code>FollowJointTrajectory.Goal</code> message type. Then extract the string value from the <code>joint</code> key. Also, assign <code>point</code> as a <code>JointTrajectoryPoint</code> message type.</p> <pre><code>if 'delta' in command:\n    joint_index = self.joint_state.name.index(joint_name)\n    joint_value = self.joint_state.position[joint_index]\n    delta = command['delta']\n    new_value = joint_value + delta\n    point.positions = [new_value]\n</code></pre> <p>Check to see if <code>delta</code> is a key in the command dictionary. Then get the current position of the joint and add the delta as a new position value.</p> <pre><code>elif 'position' in command:\n    point.positions = [command['position']]\n</code></pre> <p>Check to see if <code>position</code> is a key in the command dictionary. Then extract the position value.</p> <pre><code>point.velocities = [self.rot_vel]\ntrajectory_goal.trajectory.points = [point]\ntrajectory_goal.trajectory.header.stamp = self.get_clock().now().to_msg()\ntrajectory_goal.trajectory.header.frame_id = 'base_link'\nself.trajectory_client.send_goal(trajectory_goal)\n</code></pre> <p>Then <code>trajectory_goal.trajectory.points</code> is defined by the positions set in <code>point</code>. Specify the coordinate frame that we want (base_link) and set the time to be now. Make the action call and send the goal.</p> <pre><code>def find_tag(self, tag_name='docking_station'):\n    pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n    self.send_command(pan_command)\n    tilt_command = {'joint': 'joint_head_tilt', 'position': self.min_tilt_position}\n    self.send_command(tilt_command)\n</code></pre> <p>Create a dictionary to get the head in its initial position for its search and send the commands with the <code>send_command()</code> function.</p> <pre><code>for i in range(self.tilt_num_steps):\n    for j in range(self.pan_num_steps):\n        pan_command = {'joint': 'joint_head_pan', 'delta': self.pan_step_size}\n        self.send_command(pan_command)\n        time.sleep(0.5)\n</code></pre> <p>Utilize a nested for loop to sweep the pan and tilt in increments. Then update the <code>joint_head_pan</code> position by the <code>pan_step_size</code>. Use <code>time.sleep()</code> function to give time to the system to do a Transform lookup before the next step.</p> <pre><code>try:\n    now = Time()\n    transform = self.tf_buffer.lookup_transform('base_link',\n                                                tag_name,\n                                                now)\n    self.get_logger().info(f\"Found Requested Tag: \\n{transform}\")\n    self.transform_pub.publish(transform)\n    return transform\nexcept TransformException as ex:\n    continue\n</code></pre> <p>Use a try-except block to look up the transform between the base_link and the requested ArUco tag. Then publish and return the <code>TransformStamped</code> message.</p> <pre><code>pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\nself.send_command(pan_command)\ntilt_command = {'joint': 'joint_head_tilt', 'delta': self.tilt_step_size}\nself.send_command(tilt_command)\ntime.sleep(.25)\n</code></pre> <p>Begin sweep with new tilt angle.</p> <pre><code>def main(self):\n    self.static_broadcaster = tf2_ros.StaticTransformBroadcaster(self)\n    self.tf_buffer = tf2_ros.Buffer()\n    self.listener = tf2_ros.TransformListener(self.tf_buffer, self)\n    time.sleep(1.0)\n</code></pre> <p>Create a StaticTranformBoradcaster Node. Also, start a tf buffer that will store the tf information for a few seconds. Then set up a tf listener, which will subscribe to all of the relevant tf topics, and keep track of the information. Include <code>time.sleep(1.0)</code> to give the listener some time to accumulate transforms.</p> <pre><code>self.get_logger().info('Searching for docking ArUco tag')\npose = self.find_tag(\"docking_station\")\n</code></pre> <p>Notice Stretch is searching for the ArUco tag with a <code>self.get_logger().info()</code> function. Then search for the ArUco marker for the docking station.</p> <p><pre><code>def main():\n    try:\n        node = LocateArUcoTag()\n        node.main()\n        node.new_thread.join()\n    except:\n        node.get_logger().info('Interrupt received, so shutting down')\n        node.destroy_node()\n        rclpy.shutdown()\n</code></pre> Instantiate the <code>LocateArUcoTag()</code> object and run the <code>main()</code> method.</p>"},{"location":"ros2/example_2/","title":"Filter Laser Scans","text":""},{"location":"ros2/example_2/#example-2","title":"Example 2","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>This example aims to provide instructions on how to filter scan messages.</p> <p>For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specification itself:</p> <pre><code># Laser scans angles are measured counter clockwise,\n# with Stretch's LiDAR having both angle_min and angle_max facing forward\n# (very closely along the x-axis) of the device frame\n#\nstd_msgs/Header header   # timestamp data in a particular coordinate frame\nfloat32 angle_min        # start angle of the scan [rad]\nfloat32 angle_max        # end angle of the scan [rad]\nfloat32 angle_increment  # angular distance between measurements [rad]\nfloat32 time_increment   # time between measurements [seconds]\nfloat32 scan_time        # time between scans [seconds]\nfloat32 range_min        # minimum range value [m]\nfloat32 range_max        # maximum range value [m]\nfloat32[] ranges         # range data [m] (Note: values &lt; range_min or &gt; range_max should be discarded)\nfloat32[] intensities    # intensity data [device-specific units]\n</code></pre> <p>The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type.</p> <p> </p> <p>For a Stretch robot the start angle of the scan, <code>angle_min</code>, and end angle, <code>angle_max</code>, are closely located along the x-axis of Stretch's frame. <code>angle_min</code> and <code>angle_max</code> are set at -3.1416 and 3.1416, respectively. This is illustrated by the images below.</p> <p> </p> <p>Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot.</p> <p>First, open a terminal and run the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then in a new terminal run the rplidar launch file from <code>stretch_core</code>. <pre><code>ros2 launch stretch_core rplidar.launch.py\n</code></pre></p> <p>To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan filter node by typing the following in a new terminal.</p> <pre><code>ros2 run stretch_ros_tutorials scan_filter\n</code></pre> <p>Then run the following command to bring up a simple RViz configuration of the Stretch robot. <pre><code>ros2 run rviz2 rviz2 -d `ros2 pkg prefix stretch_calibration`/share/stretch_calibration/rviz/stretch_simple_test.rviz\n</code></pre></p> <p>Note</p> <p>If the laser scan points published by the scan or the scan_filtered topic are not visible in RViz, you can visualize them by adding them using the 'Add' button in the left panel, selecting the 'By topic' tab, and then selecting the scan or scan_filtered topic.</p> <p>Change the topic name from the LaserScan display from /scan to /filter_scan.</p> <p> </p>"},{"location":"ros2/example_2/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom numpy import linspace, inf\nfrom math import sin\nfrom sensor_msgs.msg import LaserScan\nclass ScanFilter(Node):\n    def __init__(self):\n        super().__init__('stretch_scan_filter')\n        self.pub = self.create_publisher(LaserScan, '/filtered_scan', 10)\n        self.sub = self.create_subscription(LaserScan, '/scan', self.scan_filter_callback, 10)\n\n        self.width = 1\n        self.extent = self.width / 2.0\n        self.get_logger().info(\"Publishing the filtered_scan topic. Use RViz to visualize.\")\n    def scan_filter_callback(self,msg):\n        angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n        msg.ranges = new_ranges\n        self.pub.publish(msg)\ndef main(args=None):\n    rclpy.init(args=args)\n    scan_filter = ScanFilter()\n    rclpy.spin(scan_filter)\n    scan_filter.destroy_node()\n    rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_2/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom numpy import linspace, inf\nfrom math import sin\nfrom sensor_msgs.msg import LaserScan\n</code></pre> <p>You need to import rclpy if you are writing a ROS Node. There are functions from numpy and math that are required within this code, that's why linspace, inf, and sin are imported. The sensor_msgs.msg import is so that we can subscribe and publish LaserScan messages.</p> <p><pre><code>self.width = 1\nself.extent = self.width / 2.0\n</code></pre> We're going to assume that the robot is pointing up the x-axis, so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered.</p> <pre><code>self.sub = self.create_subscription(LaserScan, '/scan', self.scan_filter_callback, 10)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic \"/scan\", looking for LaserScan messages.  When a message comes in, ROS is going to pass it to the function \"callback\" automatically.</p> <pre><code>self.pub = self.create_publisher(LaserScan, '/filtered_scan', 10)\n</code></pre> <p>This declares that your node is publishing to the filtered_scan topic using the message type LaserScan. This lets any nodes listening on filtered_scan that we are going to publish data on that topic.</p> <p><pre><code>angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n</code></pre> This line of code utilizes linspace to compute each angle of the subscribed scan.</p> <p><pre><code>points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n</code></pre> Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference.</p> <p><pre><code>new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n</code></pre> If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\".</p> <p><pre><code>msg.ranges = new_ranges\nself.pub.publish(msg)\n</code></pre> Substitute the new ranges in the original message, and republish it.</p> <p><pre><code>def main(args=None):\n    rclpy.init(args=args)\n    scan_filter = ScanFilter()\n</code></pre> The next line, rclpy.init_node initializes the node. In this case, your node will take on the name 'stretch_scan_filter'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Setup Scanfilter class with <code>scan_filter = Scanfilter()</code></p> <pre><code>rclpy.spin(scan_filter)\n</code></pre> <p>Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"ros2/example_3/","title":"Collision Avoidance","text":""},{"location":"ros2/example_3/#example-3","title":"Example 3","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>The aim of example 3 is to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py mode:=navigation\n</code></pre> <p>Then in a new terminal type the following to activate the LiDAR sensor.</p> <pre><code>ros2 launch stretch_core rplidar.launch.py\n</code></pre> <p>To activate the avoider node, type the following in a new terminal.</p> <pre><code>ros2 run stretch_ros_tutorials avoider\n</code></pre> <p>To stop the node from sending twist messages, type Ctrl + c in the terminal running the avoider node.</p> <p> </p>"},{"location":"ros2/example_3/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom numpy import linspace, inf, tanh\nfrom math import sin\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nclass Avoider(Node):\n    def __init__(self):\n        super().__init__('stretch_avoider')\n        self.width = 1\n        self.extent = self.width / 2.0\n        self.distance = 0.5\n        self.twist = Twist()\n        self.twist.linear.x = 0.0\n        self.twist.linear.y = 0.0\n        self.twist.linear.z = 0.0\n        self.twist.angular.x = 0.0\n        self.twist.angular.y = 0.0\n        self.twist.angular.z = 0.0\n        self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 1)\n        self.subscriber_ = self.create_subscription(LaserScan, '/scan', self.set_speed, 10)\n    def set_speed(self, msg):\n        angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n        error = min(new_ranges) - self.distance\n        self.twist.linear.x = tanh(error) if (error &gt; 0.05 or error &lt; -0.05) else 0\n        self.publisher_.publish(self.twist)\ndef main(args=None):\n    rclpy.init(args=args)\n    avoider = Avoider()\n    rclpy.spin(avoider)\n    avoider.destroy_node()\n    rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_3/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom numpy import linspace, inf, tanh\nfrom math import sin\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\n</code></pre> <p>You need to import rclpy if you are writing a ROS Node. There are functions from numpy and math that are required within this code, thus linspace, inf, tanh, and sin are imported. The sensor_msgs.msg import is so that we can subscribe to LaserScan messages. The geometry_msgs.msg import is so that we can send velocity commands to the robot.</p> <pre><code>self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 1)\n</code></pre> <p>This declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist.</p> <pre><code>self.subscriber_ = self.create_subscription(LaserScan, '/scan', self.set_speed, 10)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic \"/scan\", looking for LaserScan messages.  When a message comes in, ROS is going to pass it to the function \"set_speed\" automatically.</p> <pre><code>self.width = 1\nself.extent = self.width / 2.0\nself.distance = 0.5\n</code></pre> <p>self.width is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing in the x-axis, any points with y coordinates further than half of the defined width (self.extent) from the x-axis are not considered. self.distance defines the stopping distance from an object.</p> <pre><code>self.twist = Twist()\nself.twist.linear.x = 0.0\nself.twist.linear.y = 0.0\nself.twist.linear.z = 0.0\nself.twist.angular.x = 0.0\nself.twist.angular.y = 0.0\nself.twist.angular.z = 0.0\n</code></pre> <p>Allocate a Twist to use, and set everything to zero.  We're going to do this when the class is initiating. Redefining this within the callback function, <code>set_speed()</code> can be more computationally taxing.</p> <pre><code>angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\npoints = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\nnew_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n</code></pre> <p>This line of code utilizes linspace to compute each angle of the subscribed scan. Here we  compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\".</p> <pre><code>error = min(new_ranges) - self.distance\n</code></pre> <p>Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as error.</p> <pre><code>self.twist.linear.x = tanh(error) if (error &gt; 0.05 or error &lt; -0.05) else 0\n</code></pre> <p>Set the speed according to a tanh function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1</p> <pre><code>self.publisher_.publish(self.twist)\n</code></pre> <p>Publish the Twist message.</p> <pre><code>def main(args=None):\n    rclpy.init(args=args)\n    avoider = Avoider()\n    rclpy.spin(avoider)\n    avoider.destroy_node()\n    rclpy.shutdown()\n</code></pre> <p>The next line, rclpy.init() method initializes the node. In this case, your node will take on the name 'stretch_avoider'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Setup Avoider class with <code>avoider = Avioder()</code></p> <p>Give control to ROS with <code>rclpy.spin()</code>. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"ros2/example_4/","title":"Rviz Markers","text":""},{"location":"ros2/example_4/#example-4","title":"Example 4","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p></p> <p>Let's bring up Stretch in RViz by using the following command.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\nros2 run rviz2 rviz2 -d `ros2 pkg prefix stretch_calibration`/share/stretch_calibration/rviz/stretch_simple_test.rviz\n</code></pre> <p>In a new terminal run the following commands to create a marker.</p> <pre><code>ros2 run stretch_ros_tutorials marker\n</code></pre> <p>The gif below demonstrates how to add a new Marker display type, and change the topic name from <code>visualization_marker</code> to <code>balloon</code>. A red sphere Marker should appear above the Stretch robot.</p> <p></p>"},{"location":"ros2/example_4/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom visualization_msgs.msg import Marker\nclass Balloon(Node):\n    def __init__(self):\n        super().__init__('stretch_marker')\n        self.publisher_ = self.create_publisher(Marker, 'balloon', 10)  \n\n        self.marker = Marker()\n        self.marker.header.frame_id = '/base_link'\n        self.marker.header.stamp = self.get_clock().now().to_msg()\n        self.marker.type = self.marker.SPHERE\n        self.marker.id = 0\n        self.marker.action = self.marker.ADD\n        self.marker.scale.x = 0.5\n        self.marker.scale.y = 0.5\n        self.marker.scale.z = 0.5\n        self.marker.color.r = 1.0\n        self.marker.color.g = 0.0\n        self.marker.color.b = 0.0\n        self.marker.color.a = 1.0\n        self.marker.pose.position.x = 0.0\n        self.marker.pose.position.y = 0.0\n        self.marker.pose.position.z = 2.0\n        self.get_logger().info(\"Publishing the balloon topic. Use RViz to visualize.\")\n    def publish_marker(self):\n        self.publisher_.publish(self.marker)\ndef main(args=None):\n    rclpy.init(args=args)\n    balloon = Balloon()\n    while rclpy.ok():\n        balloon.publish_marker()\n    balloon.destroy_node()  \n    rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_4/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom visualization_msgs.msg import Marker\n</code></pre> <p>You need to import rclpy if you are writing a ROS 2 Node. Import the <code>Marker</code> type from the visualization_msgs.msg package. This import is required to publish a Marker, which will be visualized in RViz.</p> <pre><code>self.publisher_ = self.create_publisher(Marker, 'balloon', 10)  \n</code></pre> <p>This declares that your node is publishing to the /ballon topic using the message type Twist.</p> <pre><code>self.marker = Marker()\nself.marker.header.frame_id = '/base_link'\nself.marker.header.stamp = self.get_clock().now().to_msg()\nself.marker.type = self.marker.SPHERE\n</code></pre> <p>Create a maker. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers</p> <pre><code>self.marker.id = 0\n</code></pre> <p>Each marker has a unique ID number. If you have more than one marker that you want displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number of an existing marker, it will replace the existing marker with that ID number.</p> <pre><code>self.marker.action = self.marker.ADD\n</code></pre> <p>This line of code sets the action. You can add, delete, or modify markers.</p> <pre><code>self.marker.scale.x = 0.5\nself.marker.scale.y = 0.5\nself.marker.scale.z = 0.5\n</code></pre> <p>These are the size parameters for the marker. These will vary by marker type.</p> <pre><code>self.marker.color.r = 1.0\nself.marker.color.g = 0.0\nself.marker.color.b = 0.0\n</code></pre> <p>Color of the object, specified as r/g/b/a, with values in the range of [0, 1].</p> <pre><code>self.marker.color.a = 1.0\n</code></pre> <p>The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible.</p> <pre><code>self.marker.pose.position.x = 0.0\nself.marker.pose.position.y = 0.0\nself.marker.pose.position.z = 2.0\n</code></pre> <p>Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in frame_id. In this case, the position will always be directly 2 meters above the frame_id (base_link), and will move with it.</p> <pre><code>def publish_marker(self):\n        self.publisher_.publish(self.marker)\n</code></pre> <p>Publish the Marker data structure to be visualized in RViz.</p> <pre><code>def main(args=None):\n    rclpy.init(args=args)\n    balloon = Balloon()\n</code></pre> <p>The next line, rospy.init. In this case, your node will take on the name talker. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Setup Balloon class with <code>balloon = Balloon()</code></p> <pre><code>while rclpy.ok():\n    balloon.publish_marker()\n    balloon.destroy_node()  \n    rclpy.shutdown()\n</code></pre> <p>This loop is a fairly standard rclpy construct: checking the rclpy.ok() flag and then doing work. You have to run this check to see if your program should exit (e.g. if there is a Ctrl-C or otherwise).</p>"},{"location":"ros2/example_5/","title":"Example 5","text":""},{"location":"ros2/example_5/#example-5","title":"Example 5","text":"<p>In this example, we will review a Python script that prints out the positions of a selected group of Stretch joints. This script is helpful if you need the joint positions after you teleoperated Stretch with the Xbox controller or physically moved the robot to the desired configuration after hitting the run stop button.</p> <p>If you are looking for a continuous print of the joint states while Stretch is in action, then you can use the ros2 topic command-line tool shown in the Internal State of Stretch Tutorial.</p> <p>Begin by starting up the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>You can then hit the run-stop button (you should hear a beep and the LED light in the button blink) and move the robot's joints to a desired configuration. Once you are satisfied with the configuration, hold the run-stop button until you hear a beep. Then run the following command to execute the joint_state_printer.py which will print the joint positions of the lift, arm, and wrist. In a new terminal, execute:</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 joint_state_printer.py\n</code></pre> <p>Your terminal will output the <code>position</code> information of the previously mentioned joints shown below. <pre><code>name: ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\nposition: [0.6043133175850597, 0.19873586673129257, 0.017257283863713464]\n</code></pre></p> <p>Note</p> <p>Stretch's arm has four prismatic joints and the sum of their positions gives the wrist_extension distance. The wrist_extension is needed when sending joint trajectory commands to the robot. Further, you can not actuate an individual arm joint. Here is an image of the arm joints for reference:</p> <p> </p>"},{"location":"ros2/example_5/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport sys\nimport time\n\n# We're going to subscribe to a JointState message type, so we need to import\n# the definition for it\nfrom sensor_msgs.msg import JointState\n\nclass JointStatePublisher(Node):\n    \"\"\"\n    A class that prints the positions of desired joints in Stretch.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber.\n        :param self: The self reference\n        \"\"\"\n        super().__init__('stretch_joint_state')\n\n        # Set up a subscriber. We're going to subscribe to the topic \"joint_states\"\n        self.sub = self.create_subscription(JointState, 'joint_states', self.callback, 1)\n\n\n    def callback(self, msg):\n        \"\"\"\n        Callback function to deal with the incoming JointState messages.\n        :param self: The self reference.\n        :param msg: The JointState message.\n        \"\"\"\n        # Store the joint messages for later use\n        self.get_logger().info('Receiving JointState messages')\n        self.joint_states = msg\n\n\n    def print_states(self, joints):\n        \"\"\"\n        print_states function to deal with the incoming JointState messages.\n        :param self: The self reference.\n        :param joints: A list of string values of joint names.\n        \"\"\"\n        # Create an empty list that will store the positions of the requested joints\n        joint_positions = []\n\n        # Use of forloop to parse the names of the requested joints list.\n        # The index() function returns the index at the first occurrence of\n        # the name of the requested joint in the self.joint_states.name list\n        for joint in joints:\n            if joint == \"wrist_extension\":\n                index = self.joint_states.name.index('joint_arm_l0')\n                joint_positions.append(4*self.joint_states.position[index])\n                continue\n\n            index = self.joint_states.name.index(joint)\n            joint_positions.append(self.joint_states.position[index])\n\n        # Print the joint position values to the terminal\n        print(\"name: \" + str(joints))\n        print(\"position: \" + str(joint_positions))\n\n        # Sends a signal to rclpy to shutdown the ROS interfaces\n        rclpy.shutdown()\n\n        # Exit the Python interpreter\n        sys.exit(0)\n\ndef main(args=None):\n    # Initialize the node\n    rclpy.init(args=args)\n    joint_publisher = JointStatePublisher()\n    time.sleep(1)\n    rclpy.spin_once(joint_publisher)\n\n    # Create a list of the joints and name them joints. These will be an argument\n    # for the print_states() function\n    joints = [\"joint_lift\", \"wrist_extension\", \"joint_wrist_yaw\"]\n    joint_publisher.print_states(joints)\n\n    # Give control to ROS.  This will allow the callback to be called whenever new\n    # messages come in.  If we don't put this line in, then the node will not work,\n    # and ROS will not process any messages\n    rclpy.spin(joint_publisher)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_5/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport sys\nimport time\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\n</code></pre> <p>You need to import rclpy if you are writing a ROS 2 Node. Import <code>sensor_msgs.msg</code> so that we can subscribe to <code>JointState</code> messages.</p> <pre><code>self.sub = self.create_subscription(JointState, 'joint_states', self.callback, 1)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>joint_states</code>, looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function \"callback\" automatically</p> <pre><code>def callback(self, msg):\n    self.joint_states = msg\n</code></pre> <p>This is the callback function where the <code>JointState</code> messages are stored as <code>self.joint_states</code>. Further information about this message type can be found here: JointState Message</p> <pre><code>def print_states(self, joints):\n    joint_positions = []\n</code></pre> <p>This is the <code>print_states()</code> function which takes in a list of joints of interest as its argument. the is also an empty list set as <code>joint_positions</code> and this is where the positions of the requested joints will be appended.</p> <pre><code>for joint in joints:\n  if joint == \"wrist_extension\":\n    index = self.joint_states.name.index('joint_arm_l0')\n    joint_positions.append(4*self.joint_states.position[index])\n    continue\n  index = self.joint_states.name.index(joint)\n  joint_positions.append(self.joint_states.position[index])\n</code></pre> <p>In this section of the code, a for loop is used to parse the names of the requested joints from the <code>self.joint_states</code> list. The <code>index()</code> function returns the index of the name of the requested joint and appends the respective position to the <code>joint_positions</code> list.</p> <pre><code>rclpy.shutdown()\nsys.exit(0)\n</code></pre> <p>The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter.</p> <pre><code>rclpy.init(args=args)\njoint_publisher = JointStatePublisher()\ntime.sleep(1)\n</code></pre> <p>The next line, rclpy.init_node initializes the node. In this case, your node will take on the name 'stretch_joint_state'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Declare object, joint_publisher, from the <code>JointStatePublisher</code> class.</p> <p>The use of the <code>time.sleep()</code> function is to allow the joint_publisher class to initialize all of its features before requesting to publish joint positions of desired joints (running the <code>print_states()</code> method).</p> <pre><code>joints = [\"joint_lift\", \"wrist_extension\", \"joint_wrist_yaw\"]\n#joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"]\njoint_publisher.print_states(joints)\n</code></pre> <p>Create a list of the desired joints that you want positions to be printed. Then use that list as an argument for the <code>print_states()</code> method.</p> <pre><code>rclpy.spin(joint_publisher)\n</code></pre> <p>Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"ros2/example_6/","title":"Example 6","text":""},{"location":"ros2/example_6/#example-6","title":"Example 6","text":"<p>In this example, we will review a Python script that prints and stores the effort values from a specified joint. If you are looking for a continuous print of the joint state efforts while Stretch is in action, then you can use the ros2 topic command-line tool shown in the Internal State of Stretch Tutorial.</p> <p> </p> <p>Begin by running the following command in a terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>There's no need to switch to the position mode in comparison with ROS1 because the default mode of the launcher is this position mode. Then run the effort_sensing.py node. In a new terminal, execute:</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 effort_sensing.py\n</code></pre> <p>This will send a <code>FollowJointTrajectory</code> command to move Stretch's arm or head while also printing the effort of the lift.</p>"},{"location":"ros2/example_6/#the-code","title":"The Code","text":"<pre><code>##!/usr/bin/env python3\n\nimport rclpy\nimport hello_helpers.hello_misc as hm\nimport os\nimport csv\nimport time\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('tkagg')\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom datetime import datetime\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\n\nclass JointActuatorEffortSensor(hm.HelloNode):\n    def __init__(self, export_data=True, animate=True):\n        hm.HelloNode.__init__(self)\n        hm.HelloNode.main(self, 'issue_command', 'issue_command', wait_for_first_pointcloud=False)\n        self.joints = ['joint_lift']\n        self.joint_effort = []\n        self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n        self.export_data = export_data\n        self.result = False\n        self.file_name = datetime.now().strftime(\"effort_sensing_tutorial_%Y%m%d%I\")\n\n\n    def issue_command(self):\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = self.joints\n\n        point0 = JointTrajectoryPoint()\n        point0.positions = [0.9]\n\n        trajectory_goal.trajectory.points = [point0]\n        trajectory_goal.trajectory.header.stamp = self.get_clock().now().to_msg()\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n\n        while self.joint_state is None:\n            time.sleep(0.1)\n        self._send_goal_future = self.trajectory_client.send_goal_async(trajectory_goal, self.feedback_callback)\n        self.get_logger().info('Sent position goal = {0}'.format(trajectory_goal))\n        self._send_goal_future.add_done_callback(self.goal_response_callback)\n\n    def goal_response_callback(self, future):\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info('Failed')\n            return\n\n        self.get_logger().info('Succeded')\n\n        self._get_result_future = goal_handle.get_result_async()\n        self._get_result_future.add_done_callback(self.get_result_callback)\n\n    def get_result_callback(self, future):\n        self.result = future.result().result\n        self.get_logger().info('Sent position goal = {0}'.format(self.result))\n\n    def feedback_callback(self, feedback_msg):\n        if 'wrist_extension' in self.joints:\n            self.joints.remove('wrist_extension')\n            self.joints.append('joint_arm_l0')\n\n        current_effort = []\n        for joint in self.joints:\n            index = self.joint_state.name.index(joint)\n            current_effort.append(self.joint_state.effort[index])\n\n        if not self.export_data:\n            print(\"name: \" + str(self.joints))\n            print(\"effort: \" + str(current_effort))\n        else:\n            self.joint_effort.append(current_effort)\n\n        if self.export_data:\n            file_name = self.file_name\n            completeName = os.path.join(self.save_path, file_name)\n            with open(completeName, \"w\") as f:\n                writer = csv.writer(f)\n                writer.writerow(self.joints)\n                writer.writerows(self.joint_effort)\n\n    def plot_data(self, animate = True):\n        while not self.result:\n            time.sleep(0.1)\n        file_name = self.file_name\n        self.completeName = os.path.join(self.save_path, file_name)\n        self.data = pd.read_csv(self.completeName)\n        self.y_anim = []\n        self.animate = animate\n\n        for joint in self.data.columns:\n\n            # Create figure, labels, and title\n            fig = plt.figure()\n            plt.title(joint + ' Effort Sensing')\n            plt.ylabel('Effort')\n            plt.xlabel('Data Points')\n\n            # Conditional statement for animation plotting\n            if self.animate:\n                self.effort = self.data[joint]\n                frames = len(self.effort)-1\n                anim = animation.FuncAnimation(fig=fig,func=self.plot_animate, repeat=False,blit=False,frames=frames, interval =75)\n                plt.show()\n\n                ## If you want to save a video, make sure to comment out plt.show(),\n                ## right before this comment.\n                # save_path = str(self.completeName + '.mp4')\n                # anim.save(save_path, writer=animation.FFMpegWriter(fps=10))\n\n                # Reset y_anim for the next joint effort animation\n                del self.y_anim[:]\n\n            # Conditional statement for regular plotting (No animation)\n            else:\n                self.data[joint].plot(kind='line')\n                # save_path = str(self.completeName + '.png')\n                # plt.savefig(save_path, bbox_inches='tight')\n                plt.show()\n\n    def plot_animate(self,i):\n        # Append self.effort values for given joint\n        self.y_anim.append(self.effort.values[i])\n        plt.plot(self.y_anim, color='blue')\n\n    def main(self):\n        self.get_logger().info('issuing command')\n        self.issue_command()\n\ndef main():\n    try:\n        node = JointActuatorEffortSensor(export_data=True, animate=True)\n        node.main()\n        node.plot_data()\n        node.new_thread.join()\n\n    except KeyboardInterrupt:\n        node.get_logger().info('interrupt received, so shutting down')\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_6/#the-code-explained","title":"The Code Explained","text":"<p>This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport hello_helpers.hello_misc as hm\nimport os\nimport csv\nimport time\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('tkagg')\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom datetime import datetime\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\n</code></pre> <p>You need to import rclpy if you are writing a ROS Node. Import the <code>FollowJointTrajectory</code> from the <code>control_msgs.action</code> package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the <code>trajectory_msgs</code> package to define robot trajectories. The <code>hello_helpers</code> package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.</p> <pre><code>class JointActuatorEffortSensor(hm.HelloNode):\n    def __init__(self, export_data=False):\n        hm.HelloNode.__init__(self)\n        hm.HelloNode.main(self, 'issue_command', 'issue_command', wait_for_first_pointcloud=False)\n</code></pre> <p>The <code>JointActuatorEffortSensor</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is initialized also the HelloNode class already have the topic joint_states, thanks to this we don't need to create a subscriber.</p> <pre><code>self.joints = ['joint_lift']\n</code></pre> <p>Create a list of the desired joints you want to print.</p> <pre><code>self.joint_effort = []\nself.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\nself.export_data = export_data\nself.result = False\nself.file_name = datetime.now().strftime(\"effort_sensing_tutorial_%Y%m%d%I\")\n</code></pre> <p>Create an empty list to store the joint effort values. The <code>self.save_path</code> is the directory path where the .txt file of the effort values will be stored. You can change this path to a preferred directory. The <code>self.export_data</code> is a boolean and its default value is set to <code>True</code>. If set to <code>False</code>, then the joint values will be printed in the terminal, otherwise, it will be stored in a .txt file and that's what we want to see the plot graph. Also we want to give our text file a name since the beginning and we use the <code>self.file_name</code> to access this later.</p> <pre><code>self._send_goal_future = self.trajectory_client.send_goal_async(trajectory_goal, self.feedback_callback)\nself._send_goal_future.add_done_callback(self.goal_response_callback)\n</code></pre> <p>The ActionClient.send_goal_async() method returns a future to a goal handle. Include the goal and <code>feedback_callback</code> functions in the send goal function. Also we need to register a <code>goal_response_callback</code> for when the future is complete </p> <p><pre><code>def goal_response_callback(self,future):\n    goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info('Failed')\n            return\n\n        self.get_logger().info('Succeded')\n</code></pre> Looking at the <code>goal_response_callback</code> in more detail we can see if the future is complete with the messages that will appear.</p> <p><pre><code>        self._get_result_future = goal_handle.get_result_async()\n        self._get_result_future.add_done_callback(self.get_result_callback)\n</code></pre> We need the goal_handle to request the result with the method get_result_async. With this we will get a future that will complete when the result is ready so we need a callback for this result.</p> <p><pre><code>def get_result_callback(self, future):\n        self.result = future.result().result\n        self.get_logger().info('Sent position goal = {0}'.format(result))\n</code></pre> In the result callback we log the result of our poistion goal </p> <pre><code>def feedback_callback(self,feedback_msg):\n</code></pre> <p>The feedback callback function takes in the <code>FollowJointTrajectoryActionFeedback</code> message as its argument.</p> <pre><code>if 'wrist_extension' in self.joints:\n    self.joints.remove('wrist_extension')\n    self.joints.append('joint_arm_l0')\n</code></pre> <p>Use a conditional statement to replace <code>wrist_extenstion</code> with <code>joint_arm_l0</code>. This is because <code>joint_arm_l0</code> has the effort values that the <code>wrist_extension</code> is experiencing.</p> <pre><code>current_effort = []\nfor joint in self.joints:\n    index = self.joint_states.name.index(joint)\n    current_effort.append(self.joint_states.effort[index])\n</code></pre> <p>Create an empty list to store the current effort values. Then use a for loop to parse the joint names and effort values.</p> <pre><code>if not self.export_data:\n    print(\"name: \" + str(self.joints))\n    print(\"effort: \" + str(current_effort))\nelse:\n    self.joint_effort.append(current_effort)\n</code></pre> <p>Use a conditional statement to print effort values in the terminal or store values into a list that will be used for exporting the data in a .txt file.</p> <p><pre><code>if self.export_data:\n            file_name = self.file_name\n            completeName = os.path.join(self.save_path, file_name)\n            with open(completeName, \"w\") as f:\n                writer = csv.writer(f)\n                writer.writerow(self.joints)\n                writer.writerows(self.joint_effort)\n</code></pre> A conditional statement is used to export the data to a .txt file. The file's name is set to the one we created at the beginning.</p> <p><pre><code>def plot_data(self, animate = True):\n        while not self.result:\n            time.sleep(0.1)\n        file_name = self.file_name\n        self.completeName = os.path.join(self.save_path, file_name)\n        self.data = pd.read_csv(self.completeName)\n        self.y_anim = []\n        self.animate = animate\n</code></pre> This function will help us initialize some values to plot our data, we need to wait until we get the results to start plotting, because the file could still be storing values and we want to plot every point also we need to create an empty list for the animation.</p> <p><pre><code>for joint in self.data.columns:\n\n            # Create figure, labels, and title\n            fig = plt.figure()\n            plt.title(joint + ' Effort Sensing')\n            plt.ylabel('Effort')\n            plt.xlabel('Data Points')\n</code></pre> Create a for loop to print each joint's effort writing the correct labels for x and y</p> <p><pre><code>if self.animate:\n    self.effort = self.data[joint]\n    frames = len(self.effort)-1\n    anim = animation.FuncAnimation(fig=fig,func=self.plot_animate, repeat=False,blit=False,frames=frames, interval =75)\n    plt.show()\n    del self.y_anim[:]\n\nelse:\n    self.data[joint].plot(kind='line')\n    # save_path = str(self.completeName + '.png')\n    # plt.savefig(save_path, bbox_inches='tight')\n    plt.show()\n</code></pre> This is a conditional statement for the animation plotting</p> <p><pre><code>def plot_animate(self,i):\n        self.y_anim.append(self.effort.values[i])\n        plt.plot(self.y_anim, color='blue')\n</code></pre> We will create another function that will plot every increment in the data frame</p> <p> </p>"},{"location":"ros2/example_7/","title":"Capture Image","text":""},{"location":"ros2/example_7/#example-7","title":"Example 7","text":"<p>In this example, we will review a Python script that captures an image from the RealSense camera.</p> <p> </p> <p>Begin by running the stretch <code>driver.launch</code> file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>ros2 launch stretch_core d435i_low_resolution.launch.py\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>ros2 run rviz2 rviz2 -d /home/hello-robot/ament_ws/src/stretch_tutorials/rviz/perception_example.rviz\n</code></pre>"},{"location":"ros2/example_7/#capture-image-with-python-script","title":"Capture Image with Python Script","text":"<p>In this section, we will use a Python node to capture an image from the RealSense camera. Execute the capture_image.py node to save a .jpeg image of the image topic <code>/camera/color/image_raw</code>. In a terminal, execute:</p> <p><pre><code>cd ~/ament_ws/src/stretch_tutorials/stretch_ros_tutorials\npython3 capture_image.py\n</code></pre> An image named camera_image.jpeg is saved in the stored_data folder in this package, if you don't have this folder you can create it yourself.</p>"},{"location":"ros2/example_7/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nimport sys\nimport os\nimport cv2\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nclass CaptureImage(Node):\n    \"\"\"\n    A class that converts a subscribed ROS image to a OpenCV image and saves\n    the captured image to a predefined directory.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes a CvBridge class, subscriber, and save path.\n        :param self: The self reference.\n        \"\"\"\n        super().__init__('stretch_capture_image')\n        self.bridge = CvBridge()\n        self.sub = self.create_subscription(Image, '/camera/color/image_raw', self.image_callback, 10)\n        self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n        self.br = CvBridge()\n\n    def image_callback(self, msg):\n        \"\"\"\n        A callback function that converts the ROS image to a CV2 image and stores the\n        image.\n        :param self: The self reference.\n        :param msg: The ROS image message type.\n        \"\"\"\n\n        try:\n            image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        except CvBridgeError as e:\n            self.get_logger().warn('CV Bridge error: {0}'.format(e))\n\n        file_name = 'camera_image.jpeg'\n        completeName = os.path.join(self.save_path, file_name)\n        cv2.imwrite(completeName, image)\n        rclpy.shutdown()\n        sys.exit(0)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    capture_image = CaptureImage()\n    rclpy.spin(capture_image)\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_7/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport sys\nimport os\nimport cv2\n</code></pre> <p>You need to import <code>rclpy</code> if you are writing a ROS Node. There are functions from <code>sys</code>, <code>os</code>, and <code>cv2</code> that are required within this code. <code>cv2</code> is a library of Python functions that implements computer vision algorithms. Further information about cv2 can be found here: OpenCV Python.</p> <pre><code>from rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n</code></pre> <p>The <code>sensor_msgs.msg</code> is imported so that we can subscribe to ROS <code>Image</code> messages. Import CvBridge to convert between ROS <code>Image</code> messages and OpenCV images and the <code>Node</code> is neccesary to create a node in ROS2.</p> <pre><code>def __init__(self):\n    \"\"\"\n    A function that initializes a CvBridge class, subscriber, and save path.\n    :param self: The self reference.\n    \"\"\"\n    super().__init__('stretch_capture_image')\n    self.bridge = CvBridge()\n    self.sub = self.create_subscription(Image, '/camera/color/image_raw', self.image_callback, 10)\n    self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n    self.br = CvBridge()\n</code></pre> <p>Initialize the node, CvBridge class, the subscriber, and the directory where the captured image will be stored.</p> <pre><code>def image_callback(self, msg):\n    \"\"\"\n    A callback function that converts the ROS image to a cv2 image and stores the\n    image.\n    :param self: The self reference.\n    :param msg: The ROS image message type.\n    \"\"\"\n    try:\n        image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n    except CvBridgeError as e:\n        rospy.logwarn('CV Bridge error: {0}'.format(e))\n</code></pre> <p>Try to convert the ROS Image message to a cv2 Image message using the <code>imgmsg_to_cv2()</code> function.  </p> <pre><code>file_name = 'camera_image.jpeg'\ncompleteName = os.path.join(self.save_path, file_name)\ncv2.imwrite(completeName, image)\n</code></pre> <p>Join the directory and file name using the <code>path.join()</code> function. Then use the <code>imwrite()</code> function to save the image.</p> <pre><code>rclpy.shutdown()\nsys.exit(0)\n</code></pre> <p>The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter.</p> <pre><code>    rclpy.init(args=args)\n    capture_image = CaptureImage()\n</code></pre> <p>The next line, rclpy.init_node initializes the node. In this case, your node will take on the name 'stretch_capture_image'. Also setup CaptureImage class with <code>capture_image = CaptureImage()</code>.</p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p><pre><code>rclpy.spin(capture_image)\n</code></pre> Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"ros2/example_7/#edge-detection","title":"Edge Detection","text":"<p>In this section, we highlight a node that utilizes the Canny Edge filter algorithm to detect the edges from an image and convert it back as a ROS image to be visualized in RViz. In a terminal, execute:</p> <pre><code>cd ~/ament_ws/src/stretch_tutorials/stretch_ros_tutorials\npython3 edge_detection.py\n</code></pre> <p>The node will publish a new Image topic named <code>/image_edge_detection</code>. This can be visualized in RViz and a gif is provided below for reference.</p> <p> </p>"},{"location":"ros2/example_7/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport cv2\n\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nclass EdgeDetection(Node):\n    \"\"\"\n    A class that converts a subscribed ROS image to a OpenCV image and saves\n    the captured image to a predefined directory.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes a CvBridge class, subscriber, and other\n        parameter values.\n        :param self: The self reference.\n        \"\"\"\n        super().__init__('stretch_edge_detection')\n        self.bridge = CvBridge()\n        self.sub = self.create_subscription(Image, '/camera/color/image_raw', self.callback, 1)\n        self.pub = self.create_publisher(Image, '/image_edge_detection', 1)\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n        self.lower_thres = 100\n        self.upper_thres = 200\n        self.get_logger().info(\"Publishing the CV2 Image. Use RViz to visualize.\")\n\n    def callback(self, msg):\n        \"\"\"\n        A callback function that converts the ROS image to a CV2 image and goes\n        through the Canny Edge filter in OpenCV for edge detection. Then publishes\n        that filtered image to be visualized in RViz.\n        :param self: The self reference.\n        :param msg: The ROS image message type.\n        \"\"\"\n        try:\n            image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        except CvBridgeError as e:\n            self.get_logger().warn('CV Bridge error: {0}'.format(e))\n\n        image = cv2.Canny(image, self.lower_thres, self.upper_thres)\n        image_msg = self.bridge.cv2_to_imgmsg(image, 'passthrough')\n        image_msg.header = msg.header\n        self.pub.publish(image_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    edge_detection = EdgeDetection()\n    rclpy.spin(edge_detection)\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_7/#the-code-explained_1","title":"The Code Explained","text":"<p>Since there are similarities in the capture image node, we will only break down the different components of the edge detection node.</p> <p>Define the lower and upper bounds of the Hysteresis Thresholds.</p> <pre><code>image = cv2.Canny(image, self.lower_thres, self.upper_thres)\n</code></pre> <p>Run the Canny Edge function to detect edges from the cv2 image.</p> <pre><code>image_msg = self.bridge.cv2_to_imgmsg(image, 'passthrough')\n</code></pre> <p>Convert the cv2 image back to a ROS image so it can be published.</p> <pre><code>image_msg.header = msg.header\nself.pub.publish(image_msg)\n</code></pre> <p>Publish the ROS image with the same header as the subscribed ROS message.</p>"},{"location":"ros2/example_8/","title":"Example 8","text":"<p>This example will showcase how to save the interpreted speech from Stretch's ReSpeaker Mic Array v2.0 to a text file.</p> <p> </p> <p>Begin by running the <code>respeaker.launch.py</code> file in a terminal.</p> <p><pre><code>ros2 launch respeaker_ros2 respeaker.launch.py\n</code></pre> Then run the speech_text.py node. In a new terminal, execute:</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 speech_text.py\n</code></pre> <p>The ReSpeaker will be listening and will start to interpret speech and save the transcript to a text file.  To shut down the node, type <code>Ctrl</code> + <code>c</code> in the terminal.</p>"},{"location":"ros2/example_8/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\n# Import modules\nimport rclpy\nimport os\nfrom rclpy.node import Node\n\n# Import SpeechRecognitionCandidates from the speech_recognition_msgs package\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n\nclass SpeechText(Node):\n    def __init__(self):\n        super().__init__('stretch_speech_text')\n        # Initialize subscriber\n        self.sub = self.create_subscription(SpeechRecognitionCandidates, \"speech_to_text\", self.callback, 1)\n\n        # Create path to save captured images to the stored data folder\n        self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n\n        # Create log message\n        self.get_logger().info(\"Listening to speech\")\n\n    def callback(self,msg):\n        # Take all items in the iterable list and join them into a single string\n        transcript = ' '.join(map(str,msg.transcript))\n\n        # Define the file name and create a complete path name\n        file_name = 'speech.txt'\n        completeName = os.path.join(self.save_path, file_name)\n\n        # Append 'hello' at the end of file\n        with open(completeName, \"a+\") as file_object:\n            file_object.write(\"\\n\")\n            file_object.write(transcript)\n\ndef main(args=None):\n    # Initialize the node and name it speech_text\n    rclpy.init(args=args)\n\n    # Instantiate the SpeechText class\n    speech_txt = SpeechText()\n\n    # Give control to ROS.  This will allow the callback to be called whenever new\n    # messages come in.  If we don't put this line in, then the node will not work,\n    # and ROS will not process any messages\n    rclpy.spin(speech_txt)\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_8/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport os\nfrom rclpy.node import Node\n</code></pre> <p>You need to import rclpy if you are writing a ROS Node.</p> <pre><code>from speech_recognition_msgs.msg import SpeechRecognitionCandidates\n</code></pre> <p>Import <code>SpeechRecognitionCandidates</code> from the <code>speech_recgonition_msgs.msg</code> so that we can receive the interpreted speech.</p> <pre><code>def __init__(self):\n    super().__init__('stretch_speech_text')\n    self.sub = self.create_subscription(SpeechRecognitionCandidates, \"speech_to_text\", self.callback, 1)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>speech_to_text</code>, looking for <code>SpeechRecognitionCandidates</code> messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically.</p> <pre><code>self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n</code></pre> <p>Define the directory to save the text file.</p> <pre><code>transcript = ' '.join(map(str,msg.transcript))\n</code></pre> <p>Take all items in the iterable list and join them into a single string named transcript.</p> <pre><code>file_name = 'speech.txt'\ncompleteName = os.path.join(self.save_path, file_name)\n</code></pre> <p>Define the file name and create a complete path directory.</p> <pre><code>with open(completeName, \"a+\") as file_object:\n    file_object.write(\"\\n\")\n    file_object.write(transcript)\n</code></pre> <p>Append the transcript to the text file.</p> <pre><code>def main(args=None):\n    rclpy.init(args=args)\n    speech_txt = SpeechText()\n</code></pre> <p>The next line, rclpy.init() method initializes the node. In this case, your node will take on the name 'stretch_speech_text'. Instantiate the <code>SpeechText()</code> class.</p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <pre><code>rclpy.spin(speech_txt)\n</code></pre> <p>Give control to ROS with <code>rclpy.spin()</code>. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"ros2/example_9/","title":"Voice Teleop","text":""},{"location":"ros2/example_9/#example-9","title":"Example 9","text":"<p>This example aims to combine the ReSpeaker Microphone Array and Follow Joint Trajectory tutorials to voice teleoperate the mobile base of the Stretch robot.</p> <p>Begin by running the following command in a new terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then run the <code>respeaker.launch.py</code> file. In a new terminal, execute:</p> <pre><code>ros2 launch respeaker_ros2 respeaker.launch.py\n</code></pre> <p>Then run the voice_teleoperation_base.py node in a new terminal.</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 voice_teleoperation_base.py\n</code></pre> <p>In terminal 3, a menu of voice commands is printed. You can reference this menu layout below.  </p> <pre><code>------------ VOICE TELEOP MENU ------------\n\nVOICE COMMANDS              \n\"forward\": BASE FORWARD                   \n\"back\"   : BASE BACK                      \n\"left\"   : BASE ROTATE LEFT               \n\"right\"  : BASE ROTATE RIGHT              \n\"stretch\": BASE ROTATES TOWARDS SOUND     \n\nSTEP SIZE                 \n\"big\"    : BIG                            \n\"medium\" : MEDIUM                         \n\"small\"  : SMALL                          \n\n\n\"quit\"   : QUIT AND CLOSE NODE            \n\n-------------------------------------------\n</code></pre> <p>To stop the node from sending twist messages, type <code>Ctrl</code> + <code>c</code> or say \"quit\".</p>"},{"location":"ros2/example_9/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\n# Import modules\nimport math\nimport rclpy\nimport sys\nfrom rclpy.duration import Duration\n\n# We're going to subscribe to 64-bit integers, so we need to import the definition\n# for them\nfrom sensor_msgs.msg import JointState\n\n# Import Int32 message typs from the std_msgs package\nfrom std_msgs.msg import Int32\n\n# Import the FollowJointTrajectory from the control_msgs.msg package to\n# control the Stretch robot\nfrom control_msgs.action import FollowJointTrajectory\n\n# Import JointTrajectoryPoint from the trajectory_msgs package to define\n# robot trajectories\nfrom trajectory_msgs.msg import JointTrajectoryPoint\n\n# Import hello_misc script for handling trajectory goals with an action client\nimport hello_helpers.hello_misc as hm\n\n# Import SpeechRecognitionCandidates from the speech_recognition_msgs package\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n\nclass GetVoiceCommands:\n    def __init__(self, node):\n        self.node = node\n        # Set step size as medium by default\n        self.step_size = 'medium'\n        self.rad_per_deg = math.pi/180.0\n\n        # Small step size parameters\n        self.small_deg = 5.0\n        self.small_rad = self.rad_per_deg * self.small_deg\n        self.small_translate = 0.025\n\n        # Medium step size parameters\n        self.medium_deg = 10.0\n        self.medium_rad = self.rad_per_deg * self.medium_deg\n        self.medium_translate = 0.05\n\n        # Big step size parameters\n        self.big_deg = 20.0\n        self.big_rad = self.rad_per_deg * self.big_deg\n        self.big_translate = 0.1\n\n        # Initialize the voice command\n        self.voice_command = None\n\n        # Initialize the sound direction\n        self.sound_direction = 0\n\n        # Initialize subscribers\n        self.speech_to_text_sub = self.node.create_subscription(SpeechRecognitionCandidates, \"/speech_to_text\", self.callback_speech, 1)\n        self.sound_direction_sub = self.node.create_subscription(Int32, \"/sound_direction\", self.callback_direction, 1)\n\n    def callback_direction(self, msg):\n        self.sound_direction = msg.data * -self.rad_per_deg\n\n    def callback_speech(self,msg):\n        self.voice_command = ' '.join(map(str,msg.transcript))\n\n    def get_inc(self):\n        if self.step_size == 'small':\n            inc = {'rad': self.small_rad, 'translate': self.small_translate}\n        if self.step_size == 'medium':\n            inc = {'rad': self.medium_rad, 'translate': self.medium_translate}\n        if self.step_size == 'big':\n            inc = {'rad': self.big_rad, 'translate': self.big_translate}\n        return inc\n\n    def print_commands(self):\n        \"\"\"\n        A function that prints the voice teleoperation menu.\n        :param self: The self reference.\n        \"\"\"\n        print('                                           ')\n        print('------------ VOICE TELEOP MENU ------------')\n        print('                                           ')\n        print('               VOICE COMMANDS              ')\n        print(' \"forward\": BASE FORWARD                   ')\n        print(' \"back\"   : BASE BACK                      ')\n        print(' \"left\"   : BASE ROTATE LEFT               ')\n        print(' \"right\"  : BASE ROTATE RIGHT              ')\n        print(' \"stretch\": BASE ROTATES TOWARDS SOUND     ')\n        print('                                           ')\n        print('                 STEP SIZE                 ')\n        print(' \"big\"    : BIG                            ')\n        print(' \"medium\" : MEDIUM                         ')\n        print(' \"small\"  : SMALL                          ')\n        print('                                           ')\n        print('                                           ')\n        print(' \"quit\"   : QUIT AND CLOSE NODE            ')\n        print('                                           ')\n        print('-------------------------------------------')\n\n    def get_command(self):\n        command = None\n        # Move base forward command\n        if self.voice_command == 'forward':\n            command = {'joint': 'translate_mobile_base', 'inc': self.get_inc()['translate']}\n\n        # Move base back command\n        if self.voice_command == 'back':\n            command = {'joint': 'translate_mobile_base', 'inc': -self.get_inc()['translate']}\n\n        # Rotate base left command\n        if self.voice_command == 'left':\n            command = {'joint': 'rotate_mobile_base', 'inc': self.get_inc()['rad']}\n\n        # Rotate base right command\n        if self.voice_command == 'right':\n            command = {'joint': 'rotate_mobile_base', 'inc': -self.get_inc()['rad']}\n\n        # Move base to sound direction command\n        if self.voice_command == 'stretch':\n            command = {'joint': 'rotate_mobile_base', 'inc': self.sound_direction}\n\n        # Set the step size of translational and rotational base motions\n        if (self.voice_command == \"small\") or (self.voice_command == \"medium\") or (self.voice_command == \"big\"):\n            self.step_size = self.voice_command\n            self.node.get_logger().info('Step size = {0}'.format(self.step_size))\n\n        if self.voice_command == 'quit':\n            # Sends a signal to ros to shutdown the ROS interfaces\n            self.node.get_logger().info(\"done\")\n\n            # Exit the Python interpreter\n            sys.exit(0)\n\n        # Reset voice command to None\n        self.voice_command = None\n\n        # return the updated command\n        return command\n\n\nclass VoiceTeleopNode(hm.HelloNode):\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n        self.rate = 10.0\n        self.joint_state = None\n        hm.HelloNode.main(self, 'voice_teleop', 'voice_teleop', wait_for_first_pointcloud=False)\n        self.speech = GetVoiceCommands(self)\n\n\n    def joint_states_callback(self, msg):\n        self.joint_state = msg\n\n    def send_command(self, command):\n        joint_state = self.joint_state\n        # Conditional statement to send  joint trajectory goals\n        if (joint_state is not None) and (command is not None):\n            # Assign point as a JointTrajectoryPoint message type\n            point = JointTrajectoryPoint()\n            point.time_from_start = Duration(seconds=0).to_msg()\n\n            # Assign trajectory_goal as a FollowJointTrajectoryGoal message type\n            trajectory_goal = FollowJointTrajectory.Goal()\n            trajectory_goal.goal_time_tolerance = Duration(seconds=0).to_msg()\n\n            # Extract the joint name from the command dictionary\n            joint_name = command['joint']\n            trajectory_goal.trajectory.joint_names = [joint_name]\n\n            # Extract the increment type from the command dictionary\n            inc = command['inc']\n            self.get_logger().info('inc = {0}'.format(inc))\n            new_value = inc\n\n            # Assign the new_value position to the trajectory goal message type\n            point.positions = [new_value]\n            trajectory_goal.trajectory.points = [point]\n            trajectory_goal.trajectory.header.stamp = self.get_clock().now().to_msg()\n            self.get_logger().info('joint_name = {0}, trajectory_goal = {1}'.format(joint_name, trajectory_goal))\n\n            # Make the action call and send goal of the new joint position\n            self.trajectory_client.send_goal(trajectory_goal)\n            self.get_logger().info('Done sending command.')\n\n            # Reprint the voice command menu\n            self.speech.print_commands()\n\n    def timer_get_command(self):\n        # Get voice command\n            command = self.speech.get_command()\n\n            # Send voice command for joint trajectory goals\n            self.send_command(command)\n    def main(self):\n        self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)\n        rate = self.create_rate(self.rate)\n        self.speech.print_commands()\n\n        self.sleep = self.create_timer(1/self.rate, self.timer_get_command)\n\ndef main(args=None):\n    try:\n        #rclpy.init(args=args)\n        node = VoiceTeleopNode()\n        node.main()\n        node.new_thread.join()\n    except KeyboardInterrupt:\n        node.get_logger().info('interrupt received, so shutting down')\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/example_9/#the-code-explained","title":"The Code Explained","text":"<p>This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import math\nimport rclpy\nimport sys\nfrom rclpy.duration import Duration\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Int32\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n</code></pre> <p>You need to import <code>rclpy</code> if you are writing a ROS Node. Import the <code>FollowJointTrajectory</code> from the <code>control_msgs.action</code> package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the <code>trajectory_msgs</code> package to define robot trajectories. The <code>hello_helpers</code> package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.  Import <code>sensor_msgs.msg</code> so that we can subscribe to JointState messages.</p> <pre><code>class GetVoiceCommands:\n</code></pre> <p>Create a class that subscribes to the <code>speech-to-text</code> recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion.</p> <pre><code>self.node = node\nself.step_size = 'medium'\nself.rad_per_deg = math.pi/180.0\n</code></pre> <p>Set the default step size as medium and create a float value, <code>self.rad_per_deg</code>, to convert degrees to radians.</p> <pre><code>self.small_deg = 5.0\nself.small_rad = self.rad_per_deg * self.small_deg\nself.small_translate = 0.025\n\nself.medium_deg = 10.0\nself.medium_rad = self.rad_per_deg * self.medium_deg\nself.medium_translate = 0.05\n\nself.big_deg = 20.0\nself.big_rad = self.rad_per_deg * self.big_deg\nself.big_translate = 0.1\n</code></pre> <p>Define the three rotation and translation step sizes.</p> <pre><code>self.voice_command = None\nself.sound_direction = 0\nself.speech_to_text_sub = self.node.create_subscription(SpeechRecognitionCandidates, \"/speech_to_text\", self.callback_speech, 1)\nself.sound_direction_sub = self.node.create_subscription(Int32, \"/sound_direction\", self.callback_direction, 1)\n</code></pre> <p>Initialize the voice command and sound direction to values that will not result in moving the base.</p> <p>Set up two subscribers.  The first one subscribes to the topic <code>/speech_to_text</code>, looking for <code>SpeechRecognitionCandidates</code> messages.  When a message comes in, ROS is going to pass it to the function <code>callback_speech</code> automatically. The second subscribes to <code>/sound_direction</code> message and passes it to the <code>callback_direction</code> function.</p> <pre><code>def callback_direction(self, msg):\n    self.sound_direction = msg.data * -self.rad_per_deg\n</code></pre> <p>The <code>callback_direction</code> function converts the <code>sound_direction</code> topic from degrees to radians.</p> <pre><code>if self.step_size == 'small':\n    inc = {'rad': self.small_rad, 'translate': self.small_translate}\nif self.step_size == 'medium':\n    inc = {'rad': self.medium_rad, 'translate': self.medium_translate}\nif self.step_size == 'big':\n    inc = {'rad': self.big_rad, 'translate': self.big_translate}\nreturn inc\n</code></pre> <p>The <code>callback_speech</code> stores the increment size for translational and rotational base motion in <code>inc</code>. The increment size is contingent on the <code>self.step_size</code> string value.</p> <pre><code>command = None\nif self.voice_command == 'forward':\n    command = {'joint': 'translate_mobile_base', 'inc': self.get_inc()['translate']}\nif self.voice_command == 'back':\n    command = {'joint': 'translate_mobile_base', 'inc': -self.get_inc()['translate']}\nif self.voice_command == 'left':\n    command = {'joint': 'rotate_mobile_base', 'inc': self.get_inc()['rad']}\nif self.voice_command == 'right':\n    command = {'joint': 'rotate_mobile_base', 'inc': -self.get_inc()['rad']}\nif self.voice_command == 'stretch':\n    command = {'joint': 'rotate_mobile_base', 'inc': self.sound_direction}\n</code></pre> <p>In the <code>get_command()</code> function, the <code>command</code> is initialized as <code>None</code>, or set as a dictionary where the <code>joint</code> and <code>inc</code> values are stored. The <code>command</code> message type is dependent on the <code>self.voice_command</code> string value.</p> <pre><code>if (self.voice_command == \"small\") or (self.voice_command == \"medium\") or (self.voice_command == \"big\"):\n    self.step_size = self.voice_command\n    self.node.get_logger().info('Step size = {0}'.format(self.step_size))\n</code></pre> <p>Based on the <code>self.voice_command</code> value set the step size for the increments.</p> <pre><code>if self.voice_command == 'quit':\n    self.node.get_logger().info(\"done\")\n    sys.exit(0)\n</code></pre> <p>If the <code>self.voice_command</code> is equal to <code>quit</code>, then initiate a clean shutdown of ROS and exit the Python interpreter.</p> <pre><code>class VoiceTeleopNode(hm.HelloNode):\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n        self.rate = 10.0\n        self.joint_state = None\n        hm.HelloNode.main(self, 'voice_teleop', 'voice_teleop', wait_for_first_pointcloud=False)\n        self.speech = GetVoiceCommands(self)\n</code></pre> <p>A class that inherits the <code>HelloNode</code> class from <code>hm</code> declares object from the <code>GetVoiceCommands</code> class and sends joint trajectory commands. The main function instantiates the <code>HelloNode</code> class.</p> <pre><code>def send_command(self, command):\n    joint_state = self.joint_state\n    if (joint_state is not None) and (command is not None):\n        point = JointTrajectoryPoint()\n        point.time_from_start = Duration(seconds=0).to_msg()\n</code></pre> <p>The <code>send_command</code> function stores the joint state message and uses a conditional statement to send joint trajectory goals. Also, assign <code>point</code> as a <code>JointTrajectoryPoint</code> message type.</p> <pre><code>trajectory_goal = FollowJointTrajectory.Goal()\ntrajectory_goal.goal_time_tolerance = Duration(seconds=0).to_msg()\n</code></pre> <p>Assign <code>trajectory_goal</code> as a <code>FollowJointTrajectory.Goal</code> message type.</p> <pre><code>joint_name = command['joint']\ntrajectory_goal.trajectory.joint_names = [joint_name]\n</code></pre> <p>Extract the joint name from the command dictionary.</p> <pre><code>inc = command['inc']\nself.get_logger().info('inc = {0}'.format(inc))\nnew_value = inc\n</code></pre> <p>Extract the increment type from the command dictionary.</p> <pre><code>point.positions = [new_value]\ntrajectory_goal.trajectory.points = [point]\n</code></pre> <p>Assign the new value position to the trajectory goal message type.</p> <pre><code>self.trajectory_client.send_goal(trajectory_goal)\nself.get_logger().info('Done sending command.')\n</code></pre> <p>Make the action call and send the goal of the new joint position.</p> <pre><code>self.speech.print_commands()\n</code></pre> <p>Reprint the voice command menu after the trajectory goal is sent.</p> <pre><code>def main(self):\n      self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)\n      rate = self.create_rate(self.rate)\n      self.speech.print_commands()\n</code></pre> <p>The main function initializes the subscriber and we are going to use the publishing rate that we set before.</p> <pre><code>try:\n    #rclpy.init(args=args)\n    node = VoiceTeleopNode()\n    node.main()\n    node.new_thread.join()\nexcept KeyboardInterrupt:\n    node.get_logger().info('interrupt received, so shutting down')\n    node.destroy_node()\n    rclpy.shutdown()\n</code></pre> <p>Declare a <code>VoiceTeleopNode</code> object. Then execute the <code>main()</code> method.</p>"},{"location":"ros2/feature_comparison/","title":"ROS 1 v/s ROS 2 Feature Comparison","text":"<p>This document will help you better understand our progress of bringing parity between our ROS 1 and ROS 2 software offerings. Additional comments indicate changes in the API and/or behavior for a component.</p> Package Node/Launch/Component Item ROS 1 ROS 2 Comments Stretch Install Humble Installation scripts NA \u2713 Stretch Install Humble Mesh and URDF files NA \u2713 Stretch Install Iron Installation scripts NA \u2713 Stretch Install Iron Mesh and URDF files NA \u2713 Stretch Core stretch_driver JointTrajectoryAction Server, Position mode \u2713 \u2713 Stretch Core stretch_driver JointTrajectoryAction Server, Navigation mode \u2713 \u2713 Stretch Core stretch_driver JointTrajectoryAction Server, Trajectory mode \u2715 \u2713 Smooth preemption missing Stretch Core stretch_driver Mode topic \u2715 \u2713 Stretch Core stretch_driver Mode services \u2713 \u2713 ROS 1 does not have trajectory mode Stretch Core stretch_driver Home/stop/stow services \u2713 \u2713 Stretch Core stretch_driver Magnetometer and BatteryState topic \u2713 \u2713 Stretch Core stretch_driver Joint states, joint limits \u2713 \u2713 Stretch Core stretch_driver TF2 \u2713 \u2713 Stretch Core keyboard_teleop Teleoperation \u2713 \u2713 Stretch Core keyboard_teleop Service triggers for demos \u2713 \u2713 Stretch Core detect_aruco_markers Aruco marker detection \u2713 \u2713 Stretch Core rplidar Driver \u2713 \u2713 Stretch Core rplidar LaserScan filtering \u2713 \u2713 Stretch Core d435i Driver \u2713 \u2713 Stretch Core d435i High/Low resolution launch \u2713 \u2713 Stretch Core d435i IMU \u2713 \u2713 Stretch Core d435i Aligned depth \u2713 \u2715 ROS 2 pending Stretch Core respeaker Driver \u2713 \u2715 ROS 2 pending Stretch Core API Docs Documentation \u2715 \u2715 ROS 1 and ROS 2 pending Hello Helpers hello_misc HelloNode \u2713 \u2713 Hello Helpers hello_misc Preprocess trajectories \u2713 \u2713 Hello Helpers hello_ros_viz Create markers \u2713 \u2713 Hello Helpers simple_command_groups SimpleCommandGroup \u2713 \u2713 Hello Helpers gripper_conversion GripperConversion \u2713 \u2713 Hello Helpers fit_plane Fit plane helper functions \u2713 \u2713 Hello Helpers configure_wrist Switch between standard and dex wrist \u2713 \u2713 Hello Helpers API Docs Documentation \u2715 \u2715 ROS 1 and 2 pending Stretch Description stretch_description Stretch URDF \u2713 \u2713 Stretch Description stretch_description Standard Gripper xacro \u2713 \u2713 Stretch Description stretch_description Dex wrist xacro \u2713 \u2713 Stretch Description stretch_description Batch-specific mesh files \u2713 \u2713 Stretch Calibration collect_head_calibration_data CollectHeadCalibrationDataNode \u2713 \u2713 Stretch Calibration process_head_calibration_data HeadCalibrator, ProcessHeadCalibrationNode \u2713 \u2713 Stretch Calibration check_head_calibration check_head_calibration \u2713 \u2713 Stretch Calibration revert_to_previous_calibration revert_to_previous_calibration \u2713 \u2713 Stretch Calibration update_uncalibrated_urdf update_uncalibrated_urdf \u2713 \u2713 Stretch Calibration update_urdf_after_xacro_change update_urdf_after_xacro_change \u2713 \u2713 Stretch Calibration update_with_most_recent_calibration update_with_most_recent_calibration \u2713 \u2713 Stretch Calibration visualize_most_recent_head_calibration visualize_most_recent_head_calibration \u2713 \u2713 Stretch Calibration Revert to position mode Revert to position mode \u2713 \u2713 Stretch Tutorials Getting started Instructions \u2713 \u2713 Stretch Tutorials Modes tutorial Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Tutorials ROS 2 with rclpy Instructions \u2713 \u2713 Stretch Tutorials Race conditions and deadlocks in ROS 2 Instructions \u2713 \u2713 Stretch Tutorials HelloNode Tutorial Instructions \u2713 \u2713 Stretch Tutorials HFollow Joint Trajectory Commands Trajectory mode \u2713 \u2713 Stretch Tutorials HFollow Joint Trajectory Commands Joint Trajectory Server \u2713 \u2713 Stretch Tutorials Internal State of Stretch Command line and RQT graph \u2713 \u2713 Stretch Tutorials RViz basics Robot visualizationa and TF tree \u2713 \u2713 Stretch Tutorials Teleoperation Velocity command of mobile base \u2713 \u2713 Stretch Tutorials Tf2 broadcaster and listener TF2 static broadcaster and listener \u2713 \u2713 Stretch Tutorials Respeaker voice to text Instructions \u2713 \u2715 ROS 2 pending Stretch Tutorials API docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Tutorials Migration guides From ROS1 Noetic &amp; from ROS2 Galactic NA \u2715 ROS 2 pending Stretch Tutorials Nav2 stack Mapping \u2713 \u2713 Stretch Tutorials Nav2 stack Navigation \u2713 \u2713 Stretch Tutorials Filter laser scans RPLidar and laserscan filtering \u2713 \u2713 Stretch Tutorials Deep perception Objects and faces detection \u2713 \u2713 Stretch Tutorials FUNMAP demos Instructions \u2713 \u2713 Stretch Tutorials Align to ArUco ArUco detection, tf transforms and trajectory server \u2713 \u2713 Stretch Tutorials Obstacle avoider RPLidar based sensing and avoidance \u2713 \u2713 Stretch Tutorials Mobile base collision avoidance RPLidar based sensing and avoidance \u2713 \u2715 Stretch FUNMAP manipulation_planning plan_surface_coverage, detect_cliff \u2713 \u2713 Stretch FUNMAP manipulation_planning PlanarRobotModel, ManipulationPlanner \u2713 \u2713 Stretch FUNMAP manipulation_planning ManipulationView \u2713 \u2713 Stretch FUNMAP mapping robot stowing, scanning, &amp; localizing methods \u2713 \u2713 Stretch FUNMAP mapping HeadScan \u2713 \u2713 Stretch FUNMAP navigate ForwardMotionObstacleDetector \u2713 \u2713 Stretch FUNMAP navigate FastSingleViewPlanner \u2713 \u2713 Stretch FUNMAP navigate MoveBase \u2713 \u2713 Stretch FUNMAP ros_max_height_image ROSVolumeOfInterest \u2713 \u2713 Stretch FUNMAP ros_max_height_image ROSMaxHeightImage \u2713 \u2713 Stretch FUNMAP funmap ContactDetector, FunMapNode, services \u2713 \u2713 Stretch FUNMAP API Docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Deep Perception detection_node DetectionNode \u2713 \u2713 Stretch Deep Perception detect_faces OpenVINO Face Detection \u2713 \u2713 Stretch Deep Perception detect_objects PyTorch YOLO object detection \u2713 \u2713 Stretch Deep Perception detect_nearest_mouth OpenVINO Mouth Detection \u2713 \u2713 Stretch Deep Perception detect_nearest_mouth OpenVINO Body Landmark Detection \u2713 \u2713 Stretch Deep Perception API Docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Demos hello_world HelloWorldNode, services \u2713 \u2713 Stretch Demos clean_surface CleanSurfaceNode, services \u2713 \u2713 Stretch Demos grasp_object GraspObjectNode, services \u2713 \u2713 Stretch Demos handover_object HandoverObjectNode, services \u2713 \u2713 Stretch Demos open_drawer OpenDrawerNode, services \u2713 \u2713 Stretch Demos HelloNode API Switch demos to HelloNode API \u2713 \u2713 Stretch Demos autodocking_behaviors MoveBaseActionClient, CheckTF \u2713 \u2715 ROS 2 pending Stretch Demos autodocking_behaviors VisualServoing \u2713 \u2715 ROS 2 pending Stretch Demos API docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Nav2 offline_mapping SLAM \u2713 \u2713 Stretch Nav2 navigation Pose, Waypoints, Obstacle avoidance \u2713 \u2713 Stretch Nav2 Patrolling Demo Python API, Autonomous waypoint nav \u2713 \u2713 Stretch Nav2 API Docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch MoveIt 2 MoveIt Config Files YAML/XML/SRDF/URDF files \u2715 \u2713 Stretch MoveIt 2 moveit RViz plugin \u2715 \u2713 Stretch MoveIt 2 moveit_py Joint Space Goals \u2715 \u2713 In review Stretch MoveIt 2 moveit_py Pose Goals \u2715 \u2715 ROS 2 pending Stretch MoveIt 2 moveit_py Multiplanning pipeline \u2715 \u2713 In review Stretch MoveIt 2 moveit_py Octomap plugin \u2715 \u2713 In review Stretch MoveIt 2 moveit End effector pose goals \u2715 \u2715 ROS 2 pending Stretch MoveIt 2 moveit Hybrid planning \u2715 \u2715 ROS 2 pending Web-based Teleoperation web_interface operator \u2713 \u2715 ROS 2 pending Web-based Teleoperation web_interface robot \u2713 \u2715 ROS 2 pending"},{"location":"ros2/follow_joint_trajectory/","title":"Follow joint trajectory","text":""},{"location":"ros2/follow_joint_trajectory/#followjointtrajectory-commands","title":"FollowJointTrajectory Commands","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. For this exercise you'll need to have Ubuntu 22.04 and ROS Iron for it to work completly.</p> <p>Stretch driver offers a <code>FollowJointTrajectory</code> action service for its arm. Within this tutorial, we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute.</p>"},{"location":"ros2/follow_joint_trajectory/#stow-command-example","title":"Stow Command Example","text":"<p>Begin by launching stretch_driver in a terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py mode:=trajectory\n</code></pre> <p>In a new terminal type the following commands.</p> <pre><code>ros2 run stretch_ros_tutorials stow_command\n</code></pre> <p>This will send a FollowJointTrajectory command to stow Stretch's arm.</p>"},{"location":"ros2/follow_joint_trajectory/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.duration import Duration\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom hello_helpers.hello_misc import HelloNode\nimport time\nclass StowCommand(HelloNode):\n    def __init__(self):\n        HelloNode.__init__(self)\n        HelloNode.main(self, 'stow_command', 'stow_command', wait_for_first_pointcloud=False)\n    def issue_stow_command(self):\n        while not self.joint_state.position:\n            self.get_logger().info(\"Waiting for joint states message to arrive\")\n            time.sleep(0.1)\n            continue\n        self.get_logger().info('Stowing...')\n        joint_state = self.joint_state\n        stow_point1 = JointTrajectoryPoint()\n        stow_point2 = JointTrajectoryPoint()\n        duration1 = Duration(seconds=0.0)\n        duration2 = Duration(seconds=4.0)\n        stow_point1.time_from_start = duration1.to_msg()\n        stow_point2.time_from_start = duration2.to_msg()\n        lift_index = joint_state.name.index('joint_lift')\n        arm_index = joint_state.name.index('wrist_extension')\n        wrist_yaw_index = joint_state.name.index('joint_wrist_yaw')\n        joint_value1 = joint_state.position[lift_index]\n        joint_value2 = joint_state.position[arm_index]\n        joint_value3 = joint_state.position[wrist_yaw_index]\n\n        stow_point1.positions = [joint_value1, joint_value2, joint_value3]\n        stow_point2.positions = [0.2, 0.0, 3.14]\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n        trajectory_goal.trajectory.points = [stow_point1, stow_point2]\n        self.trajectory_client.send_goal_async(trajectory_goal)\n        self.get_logger().info(\"Goal sent\")\n    def main(self):\n        self.issue_stow_command()\ndef main():\n    try:\n        node = StowCommand()\n        node.main()\n        node.new_thread.join()\n    except:\n        node.get_logger().info(\"Exiting\")\n        node.destroy_node()\n        rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/follow_joint_trajectory/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.duration import Duration\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom hello_helpers.hello_misc import HelloNode\nimport time\n</code></pre> <p>You need to import rclpy if you are writing a ROS 2 Node. Import the FollowJointTrajectory from the control_msgs.action package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories.</p> <pre><code>class StowCommand(HelloNode):\n    def __init__(self):\n        HelloNode.__init__(self)\n        HelloNode.main(self, 'stow_command', 'stow_command', wait_for_first_pointcloud=False)\n</code></pre> <p>The <code>StowCommand</code> class inherits from the <code>HelloNode</code> class and is initialized with the main method in HelloNode by passing the arguments node_name as 'stow_command', node_namespace as 'stow_command' and wait_for_first_pointcloud as False. Refer to the Introduction to HelloNode tutorial if you haven't already to understand how this works.</p> <pre><code>def issue_stow_command(self):\n</code></pre> <p>The <code>issue_stow_command()</code> method will stow Stretch's arm. Within the function, we set stow_point as a <code>JointTrajectoryPoint</code>and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined in the next set of the code.</p> <pre><code>        while not self.joint_state.position:\n            self.get_logger().info(\"Waiting for joint states message to arrive\")\n            time.sleep(0.1)\n            continue\n        self.get_logger().info('Stowing...')\n        joint_state = self.joint_state\n        stow_point1 = JointTrajectoryPoint()\n        stow_point2 = JointTrajectoryPoint()\n        duration1 = Duration(seconds=0.0)\n        duration2 = Duration(seconds=4.0)\n        stow_point1.time_from_start = duration1.to_msg()\n        stow_point2.time_from_start = duration2.to_msg()\n        lift_index = joint_state.name.index('joint_lift')\n        arm_index = joint_state.name.index('wrist_extension')\n        wrist_yaw_index = joint_state.name.index('joint_wrist_yaw')\n        joint_value1 = joint_state.position[lift_index]\n        joint_value2 = joint_state.position[arm_index]\n        joint_value3 = joint_state.position[wrist_yaw_index]\n\n        stow_point1.positions = [joint_value1, joint_value2, joint_value3]\n        stow_point2.positions = [0.2, 0.0, 3.14]\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n        trajectory_goal.trajectory.points = [stow_point1, stow_point2]\n</code></pre> <p>Set trajectory_goal as a <code>FollowJointTrajectory.Goal()</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> is defined by the positions set in stow_point1 and stow_point2.</p> <p><pre><code>        self.trajectory_client.send_goal_async(trajectory_goal)\n        self.get_logger().info(\"Goal sent\")\n</code></pre> Make the action call and send the goal.</p> <pre><code>def main(args=None):\n    try:\n        node = StowCommand()\n        node.main()\n        node.new_thread.join()\n    except:\n        node.get_logger().info(\"Exiting\")\n        node.destroy_node()\n        rclpy.shutdown()\n</code></pre> <p>Create a funcion, <code>main()</code>, to do all of the setup in the class and issue the stow command. Initialize the <code>StowCommand()</code> class and set it to node and run the <code>main()</code> function.</p> <pre><code>if __name__ == '__main__':\n    main()\n</code></pre> <p>To make the script executable call the main() function like above.</p>"},{"location":"ros2/follow_joint_trajectory/#multipoint-command-example","title":"Multipoint Command Example","text":"<p>If you have killed the above instance of stretch_driver relaunch it again through the terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py mode:=trajectory\n</code></pre> <p>In a new terminal type the following commands.</p> <pre><code>ros2 run stretch_ros_tutorials multipoint_command\n</code></pre> <p>This will send a list of JointTrajectoryPoint's to move Stretch's arm.</p>"},{"location":"ros2/follow_joint_trajectory/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.duration import Duration\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom hello_helpers.hello_misc import HelloNode\nimport time\nclass MultiPointCommand(HelloNode):\n    def __init__(self):\n        HelloNode.__init__(self)\n        HelloNode.main(self, 'multipoint_command', 'multipoint_command', wait_for_first_pointcloud=False)\n    def issue_multipoint_command(self):\n        while not self.joint_state.position:\n            self.get_logger().info(\"Waiting for joint states message to arrive\")\n            time.sleep(0.1)\n            continue\n\n        self.get_logger().info('Issuing multipoint command...')\n        joint_state = self.joint_state\n        duration0 = Duration(seconds=0.0)\n        duration1 = Duration(seconds=6.0)\n        duration2 = Duration(seconds=9.0)\n        duration3 = Duration(seconds=12.0)\n        duration4 = Duration(seconds=16.0)\n        duration5 = Duration(seconds=20.0)\n        lift_index = joint_state.name.index('joint_lift')\n        arm_index = joint_state.name.index('wrist_extension')\n        wrist_yaw_index = joint_state.name.index('joint_wrist_yaw')\n        gripper_index = joint_state.name.index('joint_gripper_finger_left')\n        joint_value1 = joint_state.position[lift_index]\n        joint_value2 = joint_state.position[arm_index]\n        joint_value3 = joint_state.position[wrist_yaw_index]\n        joint_value4 = joint_state.position[gripper_index]\n        point0 = JointTrajectoryPoint()\n        point0.positions = [joint_value1, joint_value2, joint_value3, joint_value4]\n        point0.velocities = [0.0, 0.0, 0.0, 0.0]\n        point0.time_from_start = duration0.to_msg()\n        point1 = JointTrajectoryPoint()\n        point1.positions = [0.9, 0.0, 0.0, 0.0] \n        point1.time_from_start = duration1.to_msg()\n        point2 = JointTrajectoryPoint()\n        point2.positions = [0.9, 0.2, 0.0, -0.3]\n        point2.time_from_start = duration2.to_msg()\n        point3 = JointTrajectoryPoint()\n        point3.positions = [0.9, 0.4, 0.0, -0.3]\n        point3.time_from_start = duration3.to_msg()\n        point4 = JointTrajectoryPoint()\n        point4.positions = [0.9, 0.4, 0.0, 0.0]\n        point4.time_from_start = duration4.to_msg()\n        point5 = JointTrajectoryPoint()\n        point5.positions = [0.4, 0.0, 1.54, 0.0]\n        point5.time_from_start = duration5.to_msg()\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw', 'joint_gripper_finger_left']\n        trajectory_goal.trajectory.points = [point0, point1, point2, point3, point4, point5]\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n        self.trajectory_client.send_goal_async(trajectory_goal)\n        self.get_logger().info(\"Goal sent\")\n\n    def main(self):\n        self.issue_multipoint_command()\ndef main():\n    try:\n        node = MultiPointCommand()\n        node.main()\n        node.new_thread.join()\n    except KeyboardInterrupt:\n        node.get_logger().info(\"Exiting\")\n        node.destroy_node()\n        rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"ros2/follow_joint_trajectory/#the-code-explained_1","title":"The Code Explained.","text":"<p>Seeing that there are similarities between the multipoint and stow command nodes, we will only breakdown the distinct components of the multipoint_command node.</p> <pre><code>        point1 = JointTrajectoryPoint()\n        point1.positions = [0.9, 0.0, 0.0, 0.0] \n        point1.time_from_start = duration1.to_msg()\n</code></pre> <p>Set point1 as a <code>JointTrajectoryPoint</code>and provide desired positions (in meters). These are the positions of the lift, wrist_extension, wrist_yaw and gripper_aperture joints, respectively.</p> <p>Note</p> <p>The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated.</p> <pre><code>        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw', 'joint_gripper_finger_left']\n        trajectory_goal.trajectory.points = [point0, point1, point2, point3, point4, point5]\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n        self.trajectory_client.send_goal_async(trajectory_goal)\n        self.get_logger().info(\"Goal sent\")\n</code></pre> <p>Set trajectory_goal as a <code>FollowJointTrajectory.Goal()</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> is defined by a list of the 6 points.</p>"},{"location":"ros2/gazebo_basics/","title":"Spawning Stretch in Simulation (Gazebo)","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>Note</p> <p>Simulation support for Stretch in ROS 2 is under active development. Please reach out to us if you want to work with Stretch in a simulated environment like Gazebo/Ignition in ROS 2.</p> <p>Refer to the instructions below if you want to test this functionality in ROS 1.</p>"},{"location":"ros2/gazebo_basics/#empty-world-simulation","title":"Empty World Simulation","text":"<p>To spawn the Stretch in gazebo's default empty world run the following command in your terminal.</p> <pre><code>roslaunch stretch_gazebo gazebo.launch\n</code></pre> <p>This will bringup the robot in the gazebo simulation similar to the image shown below.</p> <p></p>"},{"location":"ros2/gazebo_basics/#custom-world-simulation","title":"Custom World Simulation","text":"<p>In gazebo, you are able to spawn Stretch in various worlds. First, source the gazebo world files by running the following command in a terminal</p> <pre><code>echo \"source /usr/share/gazebo/setup.sh\"\n</code></pre> <p>Then using the world argument, you can spawn the stretch in the willowgarage world by running the following</p> <pre><code>roslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world\n</code></pre> <p></p>"},{"location":"ros2/getting_started/","title":"Getting Started","text":"<p>This tutorial series covers writing ROS 2 software for Stretch. ROS 2 programs can be written in a variety of programming languages, but this series uses Python. We'll write programs that enable Stretch to navigate autonomously in its environment, manipulate objects with Stretch's gripper, perceive its environment, and much more.</p>"},{"location":"ros2/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Ensure that:</p> <ol> <li>Your Stretch has the latest robot distribution installed<ul> <li>These tutorials were written for the latest robot distribution. Take a look at the Distributions &amp; Roadmap guide to identify your current distribution and upgrade if necessary.</li> </ul> </li> <li>You are comfortable developing with Stretch<ul> <li>If you've never developed with Stretch before or are new to programming, check out the Developing with Stretch tutorial series. In particular, the Using ROS 2 with Stretch tutorial from that series is a good resource for those new to ROS 2.</li> </ul> </li> </ol>"},{"location":"ros2/internal_state_of_stretch/","title":"Internal state of stretch","text":""},{"location":"ros2/internal_state_of_stretch/#getting-the-state-of-the-robot","title":"Getting the State of the Robot","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>Begin by starting up the stretch driver launch file by typing the following in a terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then utilize the ROS command-line tool, ros2 topic, to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a terminal.</p> <pre><code>ros2 topic echo /stretch/joint_states\n</code></pre> <p>Your terminal will then output the information associated with the <code>/stretch/joint_states</code> topic. Your <code>header</code>, <code>position</code>, <code>velocity</code>, and <code>effort</code> information may vary from what is printed below.</p> <pre><code>header:\n  seq: 70999\n  stamp:\n    secs: 1420\n    nsecs:   2000000\n  frame_id: ''\nname: [joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left,\n  joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift,\n  joint_right_wheel, joint_wrist_yaw]\nposition: [-1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2.9291786329821434e-07, 1.3802900147297237e-06, 0.08154086954434359, 1.4361499260374905e-07, 0.4139061738340768, 9.32603306580404e-07]\nvelocity: [0.00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1.322424459109634e-05, -0.00035084643762840415, 0.0012164337445918797, 0.0002138814988808099, 0.00010419792027496809, 4.0575263146426684e-05, 0.00022487596895736357, -0.0007751929074042957, 0.0002451588607332439]\neffort: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n---\n</code></pre> <p>Additionally, if you type <code>ros2 topic list</code> in the terminal, you will see the list of active topics being published.</p> <p>A powerful tool to visualize the ROS communication is through the rqt_graph package. You can see a graph of topics being communicated between nodes by typing the following.</p> <pre><code>ros2 run rqt_graph rqt_graph\n</code></pre> <p></p> <p>The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.</p>"},{"location":"ros2/intro_to_hellonode/","title":"Introduction to HelloNode","text":"<p>HelloNode is a convenience class for creating a ROS 2 node for Stretch. The most common way to use this class is to extend it. In your extending class, the main funcion would call <code>HelloNode</code>'s main function. This would look like:</p> <pre><code>import hello_helpers.hello_misc as hm\n\nclass MyNode(hm.HelloNode):\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n\n    def main(self):\n        hm.HelloNode.main(self, 'my_node', 'my_node', wait_for_first_pointcloud=False)\n        # my_node's main logic goes here\n\nnode = MyNode()\nnode.main()\n</code></pre> <p>There is also a one-liner class method for instantiating a <code>HelloNode</code> for easy prototyping. One example where this is handy in sending pose commands from iPython:</p> <pre><code># roslaunch the stretch launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.move_to_pose({'joint_lift': 0.4})\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#attributes","title":"Attributes","text":""},{"location":"ros2/intro_to_hellonode/#dryrun","title":"<code>dryrun</code>","text":"<p>This attribute allows you to control whether the robot actually moves when calling <code>move_to_pose()</code>, <code>home_the_robot()</code>, <code>stow_the_robot()</code>, or other motion methods in this class. When <code>dryrun</code> is set to True, these motion methods return immediately. This attribute is helpful when you want to run just the perception/planning part of your node without actually moving the robot. For example, you could replace the following verbose snippet:</p> <pre><code># launch the stretch driver launch file beforehand\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\nactually_move = False\n[...]\nif actually_move:\n    temp.move_to_pose({'translate_mobile_base': 1.0})\n</code></pre> <p>to be more consise:</p> <pre><code># launch the stretch driver launch file beforehand\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\n[...]\ntemp.dryrun = True\ntemp.move_to_pose({'translate_mobile_base': 1.0})\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#methods","title":"Methods","text":""},{"location":"ros2/intro_to_hellonode/#move_to_posepose-blockingfalse-custom_contact_thresholdsfalse-duration20","title":"<code>move_to_pose(pose, blocking=False, custom_contact_thresholds=False, duration=2.0)</code>","text":"<p>This method takes in a dictionary that describes a desired pose for the robot and communicates with stretch_driver to execute it. The basic format of this dictionary is string/number key/value pairs, where the keys are joint names and the values are desired position goals. For example, <code>{'joint_lift': 0.5}</code> would put the lift at 0.5m in its joint range. A full list of command-able joints is published to the <code>/stretch/joint_states</code> topic. Used within a node extending <code>HelloNode</code>, calling this method would look like:</p> <pre><code>self.move_to_pose({'joint_lift': 0.5})\n</code></pre> <p>Internally, this dictionary is converted into a JointTrajectory message that is sent to a FollowJointTrajectory action server in stretch_driver. This method waits by default for the server to report that the goal has completed executing. However, you can return before the goal has completed by setting the <code>blocking</code> argument to False. This can be useful for preempting goals.</p> <p>When the robot is in <code>position</code> mode, if you set <code>custom_contact_thresholds</code> to True, this method expects a different format dictionary: string/tuple key/value pairs, where the keys are still joint names, but the values are <code>(position_goal, effort_threshold)</code>. The addition of a effort threshold enables you to detect when a joint has made contact with something in the environment, which is useful for manipulation or safe movements. For example, <code>{'joint_arm': (0.5, 20)}</code> commands the telescoping arm fully out (the arm is nearly fully extended at 0.5 meters) but with a low enough effort threshold (20% of the arm motor's max effort) that the motor will stop when the end of arm has made contact with something. Again, in a node, this would look like:</p> <pre><code>self.move_to_pose({'joint_arm': (0.5, 40)}, custom_contact_thresholds=True)\n</code></pre> <p>When the robot is in <code>trajectory</code> mode, if you set argument <code>duration</code> as <code>ts</code>, this method will ensure that the target joint positions are achieved over <code>ts</code> seconds. For example, the below would put the lift at 0.5m from its current position in <code>5.0</code> seconds:</p> <pre><code>self.move_to_pose({'joint_lift': 0.5}, duration=5.0)\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#home_the_robot","title":"<code>home_the_robot()</code>","text":"<p>This is a convenience method to interact with the driver's <code>/home_the_robot</code> service.</p>"},{"location":"ros2/intro_to_hellonode/#stow_the_robot","title":"<code>stow_the_robot()</code>","text":"<p>This is a convenience method to interact with the driver's <code>/stow_the_robot</code> service.</p>"},{"location":"ros2/intro_to_hellonode/#stop_the_robot","title":"<code>stop_the_robot()</code>","text":"<p>This is a convenience method to interact with the driver's <code>/stop_the_robot</code> service.</p>"},{"location":"ros2/intro_to_hellonode/#get_tffrom_frame-to_frame","title":"<code>get_tf(from_frame, to_frame)</code>","text":"<p>Use this method to get the transform (geometry_msgs/TransformStamped) between two frames. This method is blocking. For example, this method can do forward kinematics from the base_link to the link between the gripper fingers, link_grasp_center, using:</p> <pre><code># launch the stretch driver launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\nt = temp.get_tf('base_link', 'link_grasp_center')\nprint(t.transform.translation)\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#get_robot_floor_pose_xyafloor_frameodom","title":"<code>get_robot_floor_pose_xya(floor_frame='odom')</code>","text":"<p>Returns the current estimated x, y position and angle of the robot on the floor. This is typically called with respect to the odom frame or the map frame. x and y are in meters and the angle is in radians.</p> <p>Note</p> <p>To get the robot pose with respect to the odom frame we need to launch stretch_driver along with the broadcast_odom_tf parameter set to True. To do this execute the command:  <code>ros2 launch stretch_core stretch_driver.launch.py broadcast_odom_tf:=True</code></p> <pre><code># launch the stretch driver launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\nt = temp.get_robot_floor_pose_xya(floor_frame='odom')\nprint(t)\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#mainnode_name-node_topic_namespace-wait_for_first_pointcloudtrue","title":"<code>main(node_name, node_topic_namespace, wait_for_first_pointcloud=True)</code>","text":"<p>When extending the <code>HelloNode</code> class, call this method at the very beginning of your <code>main()</code> method. This method handles setting up a few ROS components, including registering the node with the ROS server, creating a TF listener, creating a FollowJointTrajectory client for the <code>move_to_pose()</code> method, subscribing to depth camera point cloud topic, and connecting to the quick-stop service.</p> <p>Since it takes up to 30 seconds for the head camera to start streaming data, the <code>wait_for_first_pointcloud</code> argument will get the node to wait until it has seen camera data, which is helpful if your node is processing camera data.</p>"},{"location":"ros2/intro_to_hellonode/#quick_createname-wait_for_first_pointcloudfalse","title":"<code>quick_create(name, wait_for_first_pointcloud=False)</code>","text":"<p>A class level method for quick testing. This allows you to avoid having to extend <code>HelloNode</code> to use it.</p> <pre><code># launch the stretch driver launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.move_to_pose({'joint_lift': 0.4})\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#subscribed-topics","title":"Subscribed Topics","text":""},{"location":"ros2/intro_to_hellonode/#cameradepthcolorpoints-sensor_msgspointcloud2","title":"/camera/depth/color/points (sensor_msgs/PointCloud2)","text":"<p>Provides a point cloud as currently seen by the Realsense depth camera in Stretch's head. Accessible from the <code>self.point_cloud</code> attribute.</p> <pre><code># launch the stretch driver launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp', wait_for_first_pointcloud=True)\nprint(temp.point_cloud)\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#stretchjoint_states-sensor_msgsjointstate","title":"/stretch/joint_states (sensor_msgs/JointState)","text":"<p>Provides the current state of robot joints that includes joint names, positions, velocities, efforts. Accessible from the <code>self.joint_state</code> attribute. <pre><code>print(temp.joint_state)\n</code></pre></p>"},{"location":"ros2/intro_to_hellonode/#mode-std_msgsstring","title":"/mode (std_msgs/String)","text":"<p>Provides the mode the stretch driver is currently in. Possible values include <code>position</code>, <code>trajectory</code>, <code>navigation</code>, <code>homing</code>, <code>stowing</code>. <pre><code>print(temp.mode)\n</code></pre></p>"},{"location":"ros2/intro_to_hellonode/#tool-std_msgsstring","title":"/tool (std_msgs/String)","text":"<p>Provides the end of arm tool attached to the robot. <pre><code>print(temp.tool)\n</code></pre></p>"},{"location":"ros2/intro_to_hellonode/#subscribed-services","title":"Subscribed Services","text":""},{"location":"ros2/intro_to_hellonode/#stop_the_robot-std_srvstrigger","title":"/stop_the_robot (std_srvs/Trigger)","text":"<p>Provides a service to quickly stop any motion currently executing on the robot.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.stop_the_robot_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#stow_the_robot-std_srvstrigger","title":"/stow_the_robot (std_srvs/Trigger)","text":"<p>Provides a service to stow the robot arm.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.stow_the_robot_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#home_the_robot-std_srvstrigger","title":"/home_the_robot (std_srvs/Trigger)","text":"<p>Provides a service to home the robot joints.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.home_the_robot_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#switch_to_trajectory_mode-std_srvstrigger","title":"/switch_to_trajectory_mode (std_srvs/Trigger)","text":"<p>Provides a service to quickly stop any motion currently executing on the robot.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.switch_to_trajectory_mode_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#switch_to_position_mode-std_srvstrigger","title":"/switch_to_position_mode (std_srvs/Trigger)","text":"<p>Provides a service to quickly stop any motion currently executing on the robot.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.switch_to_position_mode_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"ros2/intro_to_hellonode/#switch_to_navigation_mode-std_srvstrigger","title":"/switch_to_navigation_mode (std_srvs/Trigger)","text":"<p>Provides a service to quickly stop any motion currently executing on the robot.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.switch_to_navigation_mode_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"ros2/intro_to_ros2/","title":"Introduction to ROS 2","text":"<p>In this tutorial we will explore rclpy, the client library for interacting with ROS 2 using the Python API. The rclpy library forms the base of ROS 2 and you will notice that all tutorials in the following sections will use it. In this section we will focus on a few common constructs of rclpy and then follow some examples using the IPython interpreter to get familiar with them.</p>"},{"location":"ros2/intro_to_ros2/#ipython","title":"IPython","text":"<p>It is not always necessary to write a functional Python script while prototyping or exploring a new library. It's instructive and helpful to use what\u2019s called an REPL (Read-Eval-Print Loop), which quickly allows us to execute Python instructions and see their output immediately. This allows better understanding of what each instruction does and is often a great way to debug unexpected behavior. IPython is a command line based interactive interpreter for Python that uses REPL and can be used to run Python snippets for quick prototyping.</p> <p>To run IPython in a terminal, simply execute: <pre><code>python3 -m IPython\n</code></pre></p> <p>Try out the following snippets for a ROS 2 quickstart:</p>"},{"location":"ros2/intro_to_ros2/#initialization-and-shutdown","title":"Initialization and Shutdown","text":""},{"location":"ros2/intro_to_ros2/#rclpyinit","title":"rclpy.init()","text":"<p>All rclpy functionality can be exposed after initialization: <pre><code>import rclpy\n\nrclpy.init()\n</code></pre></p>"},{"location":"ros2/intro_to_ros2/#rclpycreate_node","title":"rclpy.create_node()","text":"<p>To create a new ROS 2 node, one can use the create_node method with the node name as the argument: <pre><code>node = rclpy.create_node('temp')\n</code></pre></p>"},{"location":"ros2/intro_to_ros2/#rclpyloggingget_logger","title":"rclpy.logging.get_logger()","text":"<p>The rclpy library also provides a logger to print messages with different severity levels to stdout. Here\u2019s how you can use it: <pre><code>import rclpy.logging\nlogger = rclpy.logging.get_logger('temp')\nlogger.info(\"Hello\")\nlogger.warn(\"Robot\")\nlogger.error(\"Stretch\")\n</code></pre></p>"},{"location":"ros2/intro_to_ros2/#rclpyok","title":"rclpy.ok()","text":"<p>If you want to check whether rclpy has been initialized, you can run the following snippet. This is especially useful to simulate an infinite loop based on whether rclpy has been shutdown. <pre><code>import time\n\nwhile rclpy.ok():\n    print(\"Hello\")\n    time.sleep(1.0)\n</code></pre></p> <p>Press ctrl+c to get out of the infinite loop.</p>"},{"location":"ros2/intro_to_ros2/#rclpyshutdown","title":"rclpy.shutdown()","text":"<p>Finally, to destroy a node safely and shutdown the instance of rclpy you can run:</p> <pre><code>node.destroy_node()\nrclpy.shutdown()\n</code></pre>"},{"location":"ros2/intro_to_ros2/#publishing-and-subscribing","title":"Publishing and Subscribing","text":""},{"location":"ros2/intro_to_ros2/#create_publisher","title":"create_publisher()","text":"<p>ROS 2 is a distributed communication system and one way to send data is through a publisher. It takes the following arguments: msg_type, msg_topic and a history depth (formerly queue_size): <pre><code>from std_msgs.msg import String\nimport rclpy\n\nrclpy.init()\nnode = rclpy.create_node('temp')\npub = node.create_publisher(String, 'hello', 10)\n</code></pre></p>"},{"location":"ros2/intro_to_ros2/#create_subscription","title":"create_subscription()","text":"<p>To receive a message, we need to create a subscriber with a callback function that listens to the arriving messages. Let's create a subscriber and define a callback called hello_callback() that logs the a message as soon as one is received: <pre><code>def hello_callback(msg):\n    print(\"Received message: {}\".format(msg.data))\n\nsub = node.create_subscription(String, 'hello', hello_callback, 10)\n</code></pre></p>"},{"location":"ros2/intro_to_ros2/#publish","title":"publish()","text":"<p>Now that you have defined a publisher and a subscriber, let\u2019s send a message and see if it gets printed to the console: <pre><code>msg = String()\nmsg.data = \"Hello\"\npub.publish(msg)\n</code></pre></p>"},{"location":"ros2/intro_to_ros2/#rclpyspin_once","title":"rclpy.spin_once()","text":"<p>That didn\u2019t do it! Although the message was sent, it didn't get printed to the console. Why? Because the hello_callback() method was never called to print the message. In ROS, we don\u2019t call this method manually, but rather leave it to what\u2019s called the executor. The executor can be invoked by calling the spin_once() method. We pass the node object and a timeout of 2 seconds as the arguments. The timeout is important because the spin_once() method is blocking and it will wait for a message to arrive indefinitely if a timeout is not defined. It returns immediately once a message is received. <pre><code>rclpy.spin_once(node, timeout_sec=2.0)\n</code></pre></p>"},{"location":"ros2/intro_to_ros2/#rclpyspin","title":"rclpy.spin()","text":"<p>The spin_once() method only does work equivalent to a single message callback. What if you want the executor to process callbacks continuously? This can be achieved using the spin() method. While retaining the current interpreter instance, let\u2019s open a new terminal window with a new instance of IPython and execute the following:</p> <p>Terminal 2: <pre><code>import rclpy\nfrom std_msgs.msg import String\nrclpy.init()\nnode = rclpy.create_node('temp2')\ndef hello_callback(msg):\n    print(\"I heard: {}\".format(msg.data))\nsub = node.create_subscription(String, 'hello', hello_callback, 10)\nrclpy.spin(node)\n</code></pre></p> <p>Now, from the first IPython instance, send a series of messages and see what happens:</p> <p>Terminal 1: <pre><code>for i in range(10):\n    msg.data = \"Hello {}\".format(i)\n    pub.publish(msg)\n</code></pre></p> <p>Voila! Finally, close both the terminals to end the session.</p>"},{"location":"ros2/intro_to_ros2/#service-server-and-client","title":"Service Server and Client","text":""},{"location":"ros2/intro_to_ros2/#create_service","title":"create_service()","text":"<p>Let\u2019s explore another common way of using ROS 2. Imagine a case where you need to request some information from a node and you expect to receive a response. This can be achieved using the service client paradigm in ROS 2. Let\u2019s fire up IPython again and create a quick service: <pre><code>import rclpy\nfrom example_interfaces.srv import AddTwoInts\nrclpy.init()\n\ndef add_ints(req, res):\n    print(\"Received request\")\n    res.sum = req.a + req.b\n    return res\n\nnode = rclpy.create_node('temp')\nsrv = node.create_service(AddTwoInts, 'add_ints', add_ints)\n\n# you need to spin to receive the request\nrclpy.spin_once(node, timeout_sec=60.0)\n</code></pre></p> <p>Note</p> <p>You need to execute the next section of this tutorial within 60 seconds as the timeout defined for the spin_once() method to receive incoming requests is defined as 60 seconds. If the time elapses, you can execute the spin_once() method again before issuing a service request in the next section. Alternatively, you can call the spin() method to listen for incoming requests indefinitely.</p> <p>The add_ints() method is the callback method for the service server. Once a service request is received, this method will act on it to generate the response. Since a service request is a ROS message, we need to invoke the executor with a spin method to receive the message.</p>"},{"location":"ros2/intro_to_ros2/#create_client","title":"create_client()","text":"<p>Now, while retaining the current IPython session, open another session of the IPython interpreter in another terminal to write the service client: <pre><code>import rclpy\nfrom example_interfaces.srv import AddTwoInts\nrclpy.init()\nnode = rclpy.create_node('temp2')\n\ncli = node.create_client(AddTwoInts, 'add_ints')\n\nreq = AddTwoInts.Request()\nreq.a = 10\nreq.b = 20\n\nreq_future = cli.call_async(req)\nrclpy.spin_until_future_complete(node, req_future, timeout_sec=2.0)\n\nprint(\"Received response: {}\".format(req_future.result().sum))\n</code></pre></p>"},{"location":"ros2/intro_to_ros2/#rclpyspin_until_future_complete","title":"rclpy.spin_until_future_complete()","text":"<p>Notice that the spin method manifests itself as the spin_until_future_complete() method which takes the node, future and timeout_sec as the arguments. The future is an object in ROS 2 that\u2019s returned immediately after an async service call has been made. We can then wait on the result of this future. This way the call to the service is not blocking and the code execution can continue as soon as the service call is issued.</p>"},{"location":"ros2/jogging/","title":"Motion Commands in ROS2","text":""},{"location":"ros2/jogging/#quickstart","title":"Quickstart","text":"<p>Sending motion commands is as easy as:</p> <ol> <li>Launch the ROS2 driver in a terminal:     <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre></li> <li>Open iPython and type the following code, one line at a time:     <pre><code>import hello_helpers.hello_misc as hm\nnode = hm.HelloNode.quick_create('temp')\nnode.move_to_pose({'joint_lift': 0.4}, blocking=True)\nnode.move_to_pose({'joint_wrist_yaw': 0.0, 'joint_wrist_roll': 0.0}, blocking=True)\n</code></pre></li> </ol>"},{"location":"ros2/jogging/#writing-a-node","title":"Writing a node","text":"<p>You can also write a ROS2 node to send motion commands:</p> <pre><code>import hello_helpers.hello_misc as hm\n\nclass MyNode(hm.HelloNode):\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n\n    def main(self):\n        hm.HelloNode.main(self, 'my_node', 'my_node', wait_for_first_pointcloud=False)\n\n        # my_node's main logic goes here\n        self.move_to_pose({'joint_lift': 0.6}, blocking=True)\n        self.move_to_pose({'joint_wrist_yaw': -1.0, 'joint_wrist_pitch': -1.0}, blocking=True)\n\nnode = MyNode()\nnode.main()\n</code></pre> <p>Copy the above into a file called \"example.py\" and run it using:</p> <pre><code>python3 example.py\n</code></pre>"},{"location":"ros2/jogging/#retrieving-joint-limits","title":"Retrieving joint limits","text":"<p>In a terminal, echo the <code>/joint_limits</code> topic:</p> <pre><code>ros2 topic echo /joint_limits\n</code></pre> <p>In a second terminal, request the driver publish the joint limits:</p> <pre><code>ros2 service call /get_joint_states std_srvs/srv/Trigger {}\n</code></pre> <p>In the first terminal, you'll see a single message get published. It'll look like this:</p> <pre><code>header:\n  stamp:\n    sec: 1725388967\n    nanosec: 818893747\n  frame_id: ''\nname:\n- joint_head_tilt\n- joint_wrist_pitch\n- joint_wrist_roll\n- joint_wrist_yaw\n- joint_head_pan\n- joint_lift\n- joint_arm\n- gripper_aperture\n- joint_gripper_finger_left\n- joint_gripper_finger_right\nposition:\n- -2.0171847360696185\n- -1.5707963267948966\n- -2.9114955354069467\n- -1.3933658823294575\n- -4.035903452927122\n- 0.0\n- 0.0\n- -0.1285204486235414\n- -0.3757907854489514\n- -0.3757907854489514\nvelocity:\n- 0.4908738521234052\n- 0.45099035163837853\n- 2.9176314585584895\n- 4.416586351787409\n- 1.7303303287350031\n- 1.0966833704348709\n- 0.5197662863936018\n- 0.34289112948906764\n- 1.0026056417808995\n- 1.0026056417808995\neffort: []\n</code></pre> <p>We're misusing the sensor_msgs/JointState message to publish the joint limits. The <code>name</code> array lists out each ranged joint. The <code>position</code> array lists the lower bound for each joint. The <code>velocity</code> array lists the upper bound. The length of these 3 arrays will be equal, because the index of the joint in the <code>name</code> array determines which index the corresponding limits will be in the other two arrays.</p> <p>The revolute joints will have their limits published in radians, and the prismatic joints will have them published in meters. See the Hardware Overview to see the ranges represented visually.</p>"},{"location":"ros2/modes/","title":"Stretch Modes","text":"<p>Stretch Driver allows the robot to be commanded in several modes of operation. Understanding what these modes are and what behaviors they enable is central to using the robot effectively.</p>"},{"location":"ros2/modes/#position-mode","title":"Position mode","text":"<p>Position mode enables position control of the arm, head and mobile base with sequential incremental positions achieved using the move_by() method in the underlying Python interface to the robot (Stretch Body). It disables velocity control of the mobile base through the /cmd_vel topic. The position commands to various joints are honored by the Joint Trajectory Server through the /stretch_controller/follow_joint_trajectory action interface.</p> <p>To understand how to command joints using the Joint Trajectory Server, refer to the Follow Joint Trajectory Commands tutorial.</p>"},{"location":"ros2/modes/#trajectory-mode","title":"Trajectory mode","text":"<p>Trajectory mode is able to execute plans from high level planners like MoveIt2. These planners are able to generate waypoint trajectories for each joint for smooth motion profiles in velocity and acceleration space. The joint trajectory action server, and the underlying Python interface to the robot (Stretch Body) execut the trajectory respecting each waypoints' time_from_start attribute of the trajectory_msgs/JointTrajectoryPoint message. This allows coordinated motion of the base + arm. To understand better how this is achieved, it might be instructive to look at the Trajectory API tutorial in Stretch Body.</p> <p>To understand how to command joints using the Joint Trajectory Server, refer to the Follow Joint Trajectory Commands tutorial.</p>"},{"location":"ros2/modes/#navigation-mode","title":"Navigation mode","text":"<p>Navigation mode enables mobile base velocity control via the /cmd_vel topic, and disables position-based control of the mobile base as in position mode. The arm and head joints remain commandable in position mode.</p> <p>To understand how to command the mobile base using navigation mode, refer to the Mobile Base Velocity Control tutorial.</p>"},{"location":"ros2/moveit_basics/","title":"MoveIt! Basics","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p>"},{"location":"ros2/moveit_basics/#overview","title":"Overview","text":"<p>MoveIt 2 is a whole-body motion planning framework for mobile manipulators that allows planning pose and joint goals in environments with and without obstacles. Stretch being a mobile manipulator is uniquely well-suited to utilize the planning capabilities of MoveIt 2 in different scenarios.</p>"},{"location":"ros2/moveit_basics/#motivation","title":"Motivation","text":"<p>Stretch has a kinematically simple 3 DoF arm (+2 with DexWrist) that is suitable for pick and place tasks of varied objects. Its mobile base provides it with 2 additional degrees of freedom that afford it more manipulability and also the ability to move around freely in its environment. To fully utilize these capabilities, we need a planner that can plan for both the arm and the mobile base at the same time. With MoveIt 2 and ROS 2, it is now possible to achieve this, empowering users to plan more complicated robot trajectories in difficult and uncertain environments.</p>"},{"location":"ros2/moveit_basics/#demo-with-stretch-robot","title":"Demo with Stretch Robot","text":"<p>Before we proceed, it's always a good idea to home the robot first by running the following script so that we have the correct joint positions being published on the /joint_states topic. This is necessary for planning trajectories on Stretch with MoveIt.</p> <pre><code>stretch_robot_home.py\n</code></pre>"},{"location":"ros2/moveit_basics/#planning-with-moveit-2-using-rviz","title":"Planning with MoveIt 2 Using RViz","text":"<p>The easiest way to run MoveIt 2 on your robot is through RViz. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To launch RViz with MoveIt 2, run the following command. (Press Ctrl+C in the terminal to terminate)</p> <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre> <p>Follow the instructions in this tutorial to plan and execute trajectories using the interactive markers in RViz.</p> <p>Use the interactive markers to drag joints to desired positions or go to the manipulation tab in the Motion Planning pane to fine-tune joint values using the sliders. Next, click the 'Plan' button to plan the trajectory. If the plan is valid, you should be able to execute the trajectory by clicking the 'Execute' button. Below we see Stretch raising its arm without any obstacle in the way.</p> <p></p> <p>To plan with obstacles, you can insert objects like a box, cyclinder or sphere, in the planning scene to plan trajectories around the object. This can be done by adding an object using the Scene Objects tab in the Motion Planning pane. Below we see Stretch raising its arm with a flat cuboid obstacle in the way. The mobile base allows Stretch to move forward and then back again while raising the arm to avoid the obstacle.</p> <p></p>"},{"location":"ros2/moveit_basics/#planning-with-moveit-2-using-the-movegroup-c-api","title":"Planning with MoveIt 2 Using the MoveGroup C++ API","text":"<p>If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. Execute the launch file again and go through the comments in the code to understand what's going on. (Press Ctrl+C in the terminal to terminate)</p> <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre> <p>Follow the instructions in this tutorial to plan and execute trajectories using the MoveGroup C++ API.</p> <p></p>"},{"location":"ros2/moveit_movegroup_demo/","title":"Moveit movegroup demo","text":""},{"location":"ros2/moveit_movegroup_demo/#planning-with-moveit-2-using-the-movegroup-c-api","title":"Planning with MoveIt 2 Using the MoveGroup C++ API","text":"<p>If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. For this tutorial, we are going to use the RViz Visual Tools plugin to execute the C++ source code part by part to explore more sophisticated functionalities.</p> <p>Execute the launch file again to begin the tutorial. You can follow along in the C++ code to inspect finer details. (Press Ctrl+C in the terminal to terminate) (Ensure you have enough room around the robot before running the script)</p> <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre> <p>To execute the script and interact with the robot, all you need to do is press the Next button in the RViz Visual Tools window at the bottom left. Follow the prompts on the terminal to run through the tutorial. While executing the script, it's also a good idea to study and understand the script that is being executed. Find it here.</p> <p></p> <ol> <li>Let's begin by jogging the camera pan and tilt joints. For having a complete 3D representation of its environment, Stretch needs to point its head in all directions, up, down, left, right, you name it! Luckily, we have a planning group that allows you to do just that - the stretch_head planning group. Go ahead and press the Next button to jog the camera.</li> </ol> <p></p> <ol> <li>What good is a robot that can't hold your hand on your worst days. We gave Stretch a gripper to do just that and more! Let's exercise it using the stretch_gripper planning group. All you have to do is press Next.</li> </ol> <p></p> <ol> <li>What about the good days you ask? Stretch always wants to reach out to you, no matter what. Speaking of reaching out, let's make Stretch exercise its arm for the next time you need it. Press Next.</li> </ol> <p></p> <ol> <li>Stretch doesn't like sitting in a corner fretting about the future. It is the future. Stretch wants to explore and in style. What better way to do it than by rolling around? Press Next and you'll see. That's the mobile_base planning group.</li> </ol> <p></p> <ol> <li>All that exploring does get tiring and sometimes Stretch just wants to relax and dream about its next adventure. Stretch prefers to relax with its arm down, lest someone trips over it and disturb Stretch's peaceful slumber. Press Next to see the mobile_base_arm planning group.</li> </ol> <p></p> <ol> <li>Did someone say adventure? How about dodging some pesky obstacles? They're everywhere, but they don't bother Stretch a lot. It can go around them. Nothing stops Stretch! You know what to do.</li> </ol> <p></p> <ol> <li>Stretch is smart, you don't need to tell it which joint goes where. Just say what you want done and it does it. How about planning a pose goal to see it in action? Press Next.</li> </ol> <p></p> <p>Press Ctrl+C to end this demo.</p> <ol> <li>To wrap it up, the final act! This one is a surprise that's only a click away. Go on and execute the following command: <pre><code>ros2 launch stretch_moveit2 moveit_draw.launch.py\n</code></pre></li> </ol> <p></p>"},{"location":"ros2/moveit_rviz_demo/","title":"Moveit rviz demo","text":""},{"location":"ros2/moveit_rviz_demo/#planning-with-moveit-2-using-rviz","title":"Planning with MoveIt 2 Using RViz","text":"<ol> <li>The easiest way to run MoveIt 2 on your robot is through the RViz plugin. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To get started using MoveIt 2 with RViz, execute the following command in a terminal. (Press Ctrl+C in the terminal to terminate)</li> </ol> <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre> <ol> <li>You should see Stretch visualized in RViz with joint positions exactly as they appear on the actual robot (If not, home the robot and start from step 1!). You can select a Planning Group from the drop down menu that allows you to choose a group of joints to plan for and control using MoveIt. When you select a Planning Group the joints that can be controlled are highlighted with interactive markers. Let\u2019s go ahead and select the stretch_arm planning group.</li> </ol> <ol> <li>Now, click and drag the arrow down to slide the arm lift downwards and then use the wheel to turn the gripper inwards so that it fits squarely over the robot base. At this point if the robot base glows red in RViz, it means the robot arm is in collision with the base. You should move the lift upwards slightly until the red highlight disappears.</li> </ol> <ol> <li>Now click on the Plan button to see the simulated motion of the robot in RViz</li> </ol> <ol> <li>Before proceeding, ensure that the robot is in an open space without obstalces. Click the Execute button to execute the plan on the actual robot. Congratulations, you just stowed the robot arm using MoveIt! (Alternatively, if you do not want to review the simulated plan, you can click \u2018Plan and Execute\u2019 to execute the planned trajectory directly)</li> </ol> <ol> <li>Now, let\u2019s move Stretch\u2019s mobile base! Select the mobile_base_arm planning_group from the drop down menu. You should see the base interactive marker appear in RViz. Use the arrow to drag the base forward or backward for about 1m. Click Plan and Execute when you are done. Voila!</li> </ol> <p>The mobile_base_arm planning group also allows you to execute a coordinated base and arm motion plan. Go ahead and move the markers around to plan some fun trajectories, maybe make Stretch do a Pirouette! Similarly, the stretch_gripper and stretch_head planning groups allow opening/closing the gripper and panning/tilting the camera.</p> <ol> <li>The interactive markers are just one way to control the joints. If you want a finer control, you can switch to the Joints tab of the plugin and use the sliders to adjust the desired end state of the joints.</li> </ol> <p></p> <ol> <li>MoveIt allows you to plan not just simple trajectories but also avoid obstacles. Let\u2019s add an obstacle to the planning scene. Click on the Scene Objects tab and select the Box object. Define a cube of dimensions 0.1x0.1x0.1m and add it to the scene using the green + button next to it. Now, place it just in front of the mobile base using the fine controls in the Change object pose/scale buttons to the right. Click on the Publish button for MoveIt to account for the object while planning.</li> </ol> <p></p> <ol> <li>Now return back to the Planning tab and define an end state such that the Box is in between the robot start and end states. Again, ensure that the robot has enough space around it. Plan and Execute!</li> </ol> <p></p> <p>With a fully functional perception pipeline, the planning scene can represent Stretch\u2019s surroundings accurately and allow Stretch to manipulate and navigate in a cluttered environment</p> <ol> <li>Feel free to explore more sophisticated planners shipped along with MoveIt 2 in the Context tab. End!</li> </ol>"},{"location":"ros2/navigation_overview/","title":"Nav2 Overview","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p>"},{"location":"ros2/navigation_overview/#overview","title":"Overview","text":"<p>The ROS 2 Navigation Stack or Nav2 is a motion planning and control framework for the mobile base. It is a stack because it encompasses several ROS packages that help with mapping, localization, planning and navigation. Stretch's mobile base has been integrated and works well right out of the box with Nav2.</p>"},{"location":"ros2/navigation_overview/#motivation","title":"Motivation","text":"<p>Stretch has a differential drive mobile base that enables navigation. However, before Stretch can navigate, it must be able to generate a map, localize itself in the environment and plan a path between points. This is challenging when the environment could present complexities such as static or dynamic obstacles that can get in the way of the robot, or floor surfaces with different frictional properties that can trip a controller.</p> <p>Fortunately, the Nav2 stack enables these capabilities through various packages and provides a simple Python API to interact with them. Let\u2019s take a quick tour!</p>"},{"location":"ros2/navigation_overview/#demo-with-stretch","title":"Demo with Stretch","text":""},{"location":"ros2/navigation_overview/#mapping","title":"Mapping","text":"<p>It is possible to generate a high-fidelity 2D map of your environment using the slam_toolbox package. For you, it is as easy as teleoperating Stretch in your home, laboratory or office. Stretch does the rest.</p> <p> </p>"},{"location":"ros2/navigation_overview/#navigation","title":"Navigation","text":"<p>Once a map has been generated, Stretch can localize itself and find its way in its environment. Just point on the map and Stretch will get there, without your help.</p> <p> </p>"},{"location":"ros2/navigation_overview/#simple-commander-python-api","title":"Simple Commander Python API","text":"<p>It is as simple to interact with Nav2 programmatically, thanks to the Python API. For example, have a look at this neat patrol demo.</p> <p> </p> <p>Have a look at the following tutorials to explore the above capabilities on your own Stretch! If you stumble upon something unexpected, please let us know.</p>"},{"location":"ros2/navigation_simple_commander/","title":"Nav2 Stack Using Simple Commander Python API","text":"<p>In this tutorial, we will work with Stretch to explore the Simple Commander Python API to enable autonomous navigation programmatically. We will also demonstrate a security patrol routine for Stretch developed using this API. If you just landed here, it might be a good idea to first review the previous tutorial which covered mapping and navigation using RViz as an interface.</p>"},{"location":"ros2/navigation_simple_commander/#the-simple-commander-python-api","title":"The Simple Commander Python API","text":"<p>To develop complex behaviors with Stretch where navigation is just one aspect of the autonomy stack, we need to be able to plan and execute navigation routines as part of a bigger program. Luckily, the Nav2 stack exposes a Python API that abstracts the ROS layer and the Behavior Tree framework (more on that later!) from the user through a pre-configured library called the robot navigator. This library defines a class called BasicNavigator which wraps the planner, controller and recovery action servers and exposes methods such as <code>goToPose()</code>, <code>goToPoses()</code> and <code>followWaypoints()</code> to execute navigation behaviors.</p> <p>Let's first see the demo in action and then explore the code to understand how this works!</p> <p>Warning</p> <p>We will not be using the arm for this demo. We recommend stowing the arm to avoid inadvertently bumping it into walls while the robot is navigating. </p> <p>Execute:</p> <pre><code>stretch_robot_stow.py\n</code></pre>"},{"location":"ros2/navigation_simple_commander/#setup","title":"Setup","text":"<p>Let's set the patrol route up before you can execute this demo in your map. This requires reading the position of the robot at various locations in the map and entering the co-ordinates in the array called <code>security_route</code> in the simple_commander_demo.py file. </p> <p>First, execute the following command while passing the correct map YAML. Then, press the 'Startup' button:</p> <pre><code>ros2 launch stretch_nav2 navigation.launch.py map:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p>Since we expect the first point in the patrol route to be at the origin of the map, the first coordinates should be (0.0, 0.0). Next, to define the route, the easiest way to define the waypoints in the <code>security_route</code> array is by setting the robot at random locations in the map using the '2D Pose Estimate' button in RViz as shown below. For each location, note the x, and y coordinates in the position field of the base_footprint frame and add it to the <code>security_route</code> array in simple_commander_demo.py.</p> <p> </p> <p>Finally, Press Ctrl+C to exit out of navigation and save the simple_commander_demo.py file. Now, build the workspace to make the updated file available for the next launch command. </p> <pre><code>cd ~/ament_ws/\ncolcon build\n</code></pre>"},{"location":"ros2/navigation_simple_commander/#see-it-in-action","title":"See It In Action","text":"<p>Go ahead and execute the following command to run the demo and visualize the result in RViz. Be sure to pass the correct path to the map YAML: Terminal 1:</p> <pre><code>ros2 launch stretch_nav2 demo_security.launch.py map:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p> </p>"},{"location":"ros2/navigation_simple_commander/#code-breakdown","title":"Code Breakdown","text":"<p>Now, let's jump into the code to see how things work under the hood. Follow along in the code to have a look at the entire script.</p> <p>First, we import the <code>BasicNavigator</code> class from the robot_navigator library which comes standard with the Nav2 stack. This class wraps around the planner, controller and recovery action servers.</p> <pre><code>from stretch_nav2.robot_navigator import BasicNavigator, TaskResult\n</code></pre> <p>In the main method, we initialize the node and create an instance of the BasicNavigator class called navigator.</p> <pre><code>def main():\n    rclpy.init()\n\n    navigator = BasicNavigator()\n</code></pre> <p>Then, we set up a path for Stretch to patrol consisting of the coordinates in the map reference frame. These coordinates are specific to the map generated for this tutorial and would not be suitable for your robot. To define coordinates that work with your robot, first command the robot to at least three random locations in the map you have generated of your environment, then read the base_link x and y coordinates for each of them from the RViz TF plugin. Plug them in the <code>security_route</code> list. Keep in mind that for this demo, the robot is starting from [0.0, 0.0] which is the origin of the map. This might not be the case for you.</p> <pre><code>    security_route = [\n        [0.0, 0.0],\n        [1.057, 1.3551],\n        [1.5828, 5.0823],\n        [-0.5390, 5.6623],\n        [0.8975, 9.7033]]\n</code></pre> <p> </p> <p>Next, we set an initial pose for the robot which would help AMCL localize the robot by providing an initial estimate of the robot's location. For this, we pass a PoseStamped message in the map reference frame with the robot's pose to the <code>setInitialPose()</code> method. The Nav2 stack recommends this before starting the lifecycle nodes using the \"Startup\" button in RViz. The <code>waitUntilNav2Active()</code> method waits until precisely this event.</p> <pre><code>    initial_pose = PoseStamped()\n    initial_pose.header.frame_id = 'map'\n    initial_pose.header.stamp = navigator.get_clock().now().to_msg()\n    initial_pose.pose.position.x = 0.0\n    initial_pose.pose.position.y = 0.0\n    initial_pose.pose.orientation.z = 0.0\n    initial_pose.pose.orientation.w = 1.0\n    navigator.setInitialPose(initial_pose)\n\n    navigator.waitUntilNav2Active()\n</code></pre> <p>Once the nodes are active, the navigator is ready to receive pose goals either through the <code>goToPose()</code>, <code>goToPoses()</code> or <code>followWaypoints()</code> methods. For this demo, we will be using the <code>followWaypoints()</code> method which takes a list of poses as an argument. Since we intend for the robot to patrol the route indefinitely or until the node is killed (or the robot runs out of battery!), we wrap the method in an infinite while loop with <code>rclpy.ok()</code>. Then, we generate pose goals with the <code>security_route</code> list and append them to a new list called <code>route_poses</code> which is passed to the <code>followWaypoints()</code> method.</p> <pre><code>    while rclpy.ok():\n\n        route_poses = []\n        pose = PoseStamped()\n        pose.header.frame_id = 'map'\n        pose.header.stamp = navigator.get_clock().now().to_msg()\n        pose.pose.orientation.w = 1.0\n        for pt in security_route[1:]:\n            pose.pose.position.x = pt[0]\n            pose.pose.position.y = pt[1]\n            route_poses.append(deepcopy(pose))\n\n        nav_start = navigator.get_clock().now()\n        navigator.followWaypoints(route_poses)\n</code></pre> <p>Since we are utilizing an action server built into Nav2, it's possible to seek feedback on this long running task through the action interface. The <code>isTaskComplete()</code> method returns a boolean depending on whether the patrolling task is complete. For the follow waypoints action server, the feedback message tells us which waypoint is currently being executed through the <code>feedback.current_waypoint</code> attribute. It is possible to cancel a goal using the <code>cancelTask()</code> method if the robot gets stuck. For this demo, we have set the timeout at 600 seconds to allow sufficient time for the robot to succeed. However, if you wish to see it in action, you can reduce the timeout to 30 seconds.</p> <pre><code>        i = 0\n        while not navigator.isTaskComplete():\n            i += 1\n            feedback = navigator.getFeedback()\n            if feedback and i % 5 == 0:\n                navigator.get_logger().info('Executing current waypoint: ' +\n                    str(feedback.current_waypoint + 1) + '/' + str(len(route_poses)))\n                now = navigator.get_clock().now()\n\n                if now - nav_start &gt; Duration(seconds=600.0):\n                    navigator.cancelTask()\n</code></pre> <p>Once the robot reaches the end of the route, we reverse the <code>security_route</code> list to generate the goal pose list that would be used by the <code>followWaypoints()</code> method in the next iteration of this loop.</p> <pre><code>        security_route.reverse()\n</code></pre> <p>Finally, after a leg of the patrol route is executed, we call the <code>getResult()</code> method to know whether the task succeeded, canceled or failed to log a message.</p> <pre><code>        result = navigator.getResult()\n        if result == TaskResult.SUCCEEDED:\n            navigator.get_logger().info('Route complete! Restarting...')\n        elif result == TaskResult.CANCELED:\n            navigator.get_logger().info('Security route was canceled, exiting.')\n            rclpy.shutdown()\n        elif result == TaskResult.FAILED:\n            navigator.get_logger().info('Security route failed! Restarting from other side...')\n</code></pre> <p>That's it! Using the Simple Commander API is as simple as that. Be sure to follow more examples in the nav2_simple_commander package if you wish to work with other useful methods exposed by the library.</p>"},{"location":"ros2/navigation_stack/","title":"Nav2 Stack Using RViz","text":"<p>In this tutorial, we will explore the ROS 2 navigation stack using slam_toolbox for mapping an environment and the core Nav2 packages to navigate in the mapped environment. If you want to know more about teleoperating the mobile base or working with the RPlidar 2D scanner on Stretch, we recommend visiting the previous tutorials on Teleoperating stretch and Filtering Laser Scans. These topics are a vital part of how Stretch's mobile base can be velocity controlled using Twist messages, and how the RPlidar's LaserScan messages enable Obstacle Avoidance for autonomous navigation.</p> <p>Navigation is a key aspect of an autonomous agent because, often, to do anything meaningful, the agent needs to traverse an environment to reach a specific spot to perform a specific task. With a robot like Stretch, the task could be anything from delivering water or medicines for the elderly to performing a routine patrol of an establishment for security.</p> <p>Stretch's mobile base enables this capability and this tutorial will explore how we can autonomously plan and execute mobile base trajectories. Running this tutorial will require the robot to be untethered, so please ensure that the robot is adequately charged.</p>"},{"location":"ros2/navigation_stack/#mapping","title":"Mapping","text":"<p>The first step is to map the space that the robot will navigate in. The <code>offline_mapping.launch.py</code> file will enable you to do this. First, run:</p> <pre><code>ros2 launch stretch_nav2 offline_mapping.launch.py\n</code></pre> <p>Rviz will show the robot and the map that is being constructed. Now, use the Xbox controller (see instructions below for using a keyboard) to teleoperate the robot around. To teleoperate the robot using the Xbox controller, keep the front left (LB) button pressed while using the right joystick for translation and rotation.</p> <p>Avoid sharp turns and revisit previously visited spots to form loop closures.</p> <p> </p> <p> </p> <p>In Rviz, once you see a map that has reconstructed the space well enough, open a new terminal and run the following commands to save the map to the <code>stretch_user/</code> directory.</p> <pre><code>mkdir ${HELLO_FLEET_PATH}/maps\nros2 run nav2_map_server map_saver_cli -f ${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;\n</code></pre> <p>Note</p> <p>The <code>&lt;map_name&gt;</code> does not include an extension. The map_saver node will save two files as <code>&lt;map_name&gt;.pgm</code> and <code>&lt;map_name&gt;.yaml</code>.</p> <p>Tip</p> <p>For a quick sanity check, you can inspect the saved map using a pre-installed tool called Eye of Gnome (eog) by running the following command:</p> <pre><code>eog ${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.pgm\n</code></pre>"},{"location":"ros2/navigation_stack/#navigation","title":"Navigation","text":"<p>Next, with <code>&lt;map_name&gt;.yaml</code>, we can navigate the robot around the mapped space. Run:</p> <pre><code>ros2 launch stretch_nav2 navigation.launch.py map:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p>A new RViz window should pop up with a <code>Startup</code> button in a menu at the bottom left of the window. Press the <code>Startup</code> button to kick-start all navigation related lifecycle nodes. Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location on the map does not match the robot's location in the real space. To correct this, from the top bar of Rviz, use <code>2D Pose Estimate</code> to lay an arrow down roughly where the robot is located in real space. This gives an initial estimate of the robot's location to AMCL, the localization package. AMCL will better localize the robot once we pass the robot a <code>2D Nav Goal</code>. </p> <p>In the top bar of Rviz, use <code>2D Nav Goal</code> to lay down an arrow where you'd like the robot to navigate. In the terminal, you'll see Nav2 go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior - spinning around 180 degrees in place or backing up.</p> <p> </p> <p>Tip</p> <p>If navigation fails or the robot becomes unresponsive to subsequent goals through RViz, you can still teleoperate the robot using an Xbox controller.</p>"},{"location":"ros2/navigation_stack/#note","title":"Note","text":"<p>The launch files expose the launch argument \"teleop_type\". By default, this argument is set to \"joystick\", which launches joystick teleop in the terminal with the Xbox controller that ships with Stretch. The Xbox controller utilizes a dead man's switch safety feature to avoid unintended movement of the robot. This is the switch located on the front left side of the controller marked \"LB\". Keep this switch pressed while translating or rotating the base using the joystick located on the right side of the Xbox controller.</p> <p>If the Xbox controller is not available, the following commands will launch mapping or navigation, respectively, with keyboard teleop:</p> <pre><code>ros2 launch stretch_nav2 offline_mapping.launch.py teleop_type:=keyboard\n</code></pre> <p>or</p> <pre><code>ros2 launch stretch_nav2 navigation.launch.py teleop_type:=keyboard map:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre>"},{"location":"ros2/navigation_stack/#simple-commander-api","title":"Simple Commander API","text":"<p>It is also possible to send 2D Pose Estimates and Nav Goals programmatically. In your own launch file, you may include <code>navigation.launch</code> to bring up the navigation stack. Then, you can send pose goals using the Nav2 simple commander API to navigate the robot programatically. We will explore this in the next tutorial.</p>"},{"location":"ros2/obstacle_avoider/","title":"Obstacle Avoider","text":"<p>In this tutorial, we will work with Stretch to detect and avoid obstacles using the onboard RPlidar A1 laser scanner and learn how to filter laser scan data. If you want to know more about the laser scanner setup on Stretch and how to get it up and running, we recommend visiting the previous tutorials on Filtering Laser Scans and Mobile Base Collision Avoidance.</p> <p>A major drawback of using any ToF (Time of Flight) sensor is the inherent inaccuracies as a result of occlusions and weird reflection and diffraction phenomena the light pulses are subject to in an unstructured environment. This results in unexpected and undesired noise that can get in the way of an otherwise extremely useful sensor. Fortunately, it is easy to account for and eliminate these inaccuracies to a great extent by filering out the noise. We will do this with a ROS package called laser_filters that comes prebuilt with some pretty handy laser scan message filters.</p> <p>By the end of this tutorial, you will be able to tweak them for your particular use case and publish and visualize them on the /scan_filtered topic using RViz. So let\u2019s jump in! We will look at three filters from this package that have been tuned to work well with Stretch in an array of scenarios.</p>"},{"location":"ros2/obstacle_avoider/#laserscan-filtering","title":"LaserScan Filtering","text":"<p>LaserScanAngularBoundsFilterInPlace - This filter removes laser scans belonging to an angular range. For Stretch, we use this filter to discount points that are occluded by the mast because it is a part of Stretch\u2019s body and not really an object we need to account for as an obstacle while navigating the mobile base.</p> <p>LaserScanSpeckleFilter - We use this filter to remove phantom detections in the middle of empty space that are a result of reflections around corners. These disjoint speckles can be detected as false positives and result in jerky motion of the base through empty space. Removing them returns a relatively noise-free scan.</p> <p>LaserScanBoxFilter - Stretch is prone to returning false detections right over the mobile base. While navigating, since it\u2019s safe to assume that Stretch is not standing right above an obstacle, we filter out any detections that are in a box shape over the mobile base.</p> <p>Beware that filtering laser scans comes at the cost of a sparser scan that might not be ideal for all applications. If you want to tweak the values for your end application, you could do so by changing the values in the laser_filter_params.yaml file and by following the laser_filters package wiki. Also, if you are feeling zany and want to use the raw unfiltered scans from the laser scanner, simply subscribe to the /scan topic instead of the /scan_filtered topic.</p> <p></p>"},{"location":"ros2/obstacle_avoider/#avoidance-logic","title":"Avoidance logic","text":"<p>Now, let\u2019s use what we have learned so far to upgrade the collision avoidance demo in a way that Stretch is able to scan an entire room autonomously without bumping into things or people. To account for dynamic obstacles getting too close to the robot, we will define a keepout distance of 0.4 m - detections below this value stop the robot. To keep Stretch from getting too close to static obstacles, we will define another variable called turning distance of 0.75 m - frontal detections below this value make Stretch turn to the left until it sees a clear path ahead.</p> <p>Building up on the teleoperation using velocity commands tutorial, let's implement a simple logic for obstacle avoidance. The logic can be broken down into three steps:</p> <ol> <li>If the minimum value from the frontal scans is greater than 0.75 m, then continue to move forward</li> <li>If the minimum value from the frontal scans is less than 0.75 m, then turn to the right until this is no longer true</li> <li>If the minimum value from the overall scans is less than 0.4 m, then stop the robot</li> </ol> <p>Warning</p> <pre><code>If you see Stretch try to run over your lazy cat or headbutt a wall, just press the bright runstop button on Stretch's head to calm it down. For pure navigation tasks, it's also safer to stow Stretch's arm in.\n</code></pre> <p>Execute the command: <pre><code>stretch_robot_stow.py\n</code></pre></p>"},{"location":"ros2/obstacle_avoider/#see-it-in-action","title":"See It In Action","text":"<p>Alright, let's see it in action! Execute the following command to run the scripts: <pre><code>ros2 launch stretch_core rplidar_keepout.launch.py\n</code></pre></p> <p></p>"},{"location":"ros2/obstacle_avoider/#code-breakdown","title":"Code Breakdown:","text":"<p>Let's jump into the code to see how things work under the hood. Follow along here to have a look at the entire script.</p> <p>The turning distance is defined by the distance attribute and the keepout distance is defined by the keepout attribute.</p> <pre><code>        self.distance = 0.75 # robot turns at this distance\n        self.keepout = 0.4 # robot stops at this distance\n</code></pre> <p>To pass velocity commands to the mobile base, we publish the translational and rotational velocities to the /stretch/cmd_vel topic. To subscribe to the filtered laser scans from the laser scanner, we subscribe to the /scan_filtered topic. While you are at it, go ahead and check the behavior by switching to the /scan topic instead. See why filtering is necessary?</p> <pre><code>        self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 1) #/stretch_diff_drive_controller/cmd_vel for gazebo\n        self.subscriber_ = self.create_subscription(LaserScan, '/scan_filtered', self.lidar_callback, 10)\n</code></pre> <p>lidar_callback() is the callback function for the laser scanner that gets called every time a new message is received.</p> <pre><code>def lidar_callback(self, msg):\n</code></pre> <p>When the scan message is filtered, all the ranges that are filtered out are assigned the nan (not a number) value. This can get in the way of computing the minimum. Therefore, we reassign these values to inf (infinity).</p> <pre><code>        all_points = [r if (not isnan(r)) else inf for r in msg.ranges]\n</code></pre> <p>Next, we compute the two minimums that are necessary for the avoidance logic to work - the overall minimum and the frontal minimum named min_all and min_front respectively.</p> <pre><code>        front_points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        front_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, front_points)]\n\n        min_front = min(front_ranges)\n        min_all = min(all_points)\n</code></pre> <p>Finally, we check the minimum values against the distance and keepout attributes to set the rotational and linear velocities of the mobile base with the set_speed() method.</p> <pre><code>        if(min_all &lt; self.keepout):\n            lin_vel = 0.0\n            rot_vel = 0.0\n        elif(min_front &lt; self.distance):\n            lin_vel = 0.0\n            rot_vel = 0.25\n        else:\n            lin_vel = 0.5\n            rot_vel = 0.0\n\n        self.set_speed(lin_vel, rot_vel)\n</code></pre> <p>That wasn't too hard, was it? Now, feel free to play with this code and change the attributes to see how it affects Stretch's behavior.</p>"},{"location":"ros2/perception/","title":"Perception","text":""},{"location":"ros2/perception/#perception-introduction","title":"Perception Introduction","text":"<p>The Stretch robot is equipped with the Intel RealSense D435i camera, an essential component that allows the robot to measure and analyze the world around it. In this tutorial, we are going to showcase how to visualize the various topics published by the camera.</p> <p>Begin by running the stretch <code>driver.launch.py</code> file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>ros2 launch stretch_core d435i_low_resolution.launch.py\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>ros2 run rviz2 rviz2 -d /home/hello-robot/ament_ws/src/stretch_tutorials/rviz/perception_example.rviz\n</code></pre>"},{"location":"ros2/perception/#pointcloud2-display","title":"PointCloud2 Display","text":"<p>A list of displays on the left side of the interface can visualize the camera data. Each display has its properties and status that notify a user if topic messages are received.</p> <p>For the <code>PointCloud2</code> display, a sensor_msgs/pointCloud2 message named <code>/camera/depth/color/points</code> is received and the GIF below demonstrates the various display properties when visualizing the data.</p> <p> </p>"},{"location":"ros2/perception/#image-display","title":"Image Display","text":"<p>The <code>Image</code> display when toggled creates a new rendering window that visualizes a sensor_msgs/Image messaged, /camera/color/image_raw. This feature shows the image data from the camera; however, the image comes out sideways.</p> <p> </p>"},{"location":"ros2/perception/#depthcloud-display","title":"DepthCloud Display","text":"<p>The <code>DepthCloud</code> display is visualized in the main RViz window. This display takes in the depth image and RGB image provided by RealSense to visualize and register a point cloud.</p> <p> </p>"},{"location":"ros2/perception/#deep-perception","title":"Deep Perception","text":"<p>Hello Robot also has a ROS package that uses deep learning models for various detection demos. A link to the tutorials is provided: stretch_deep_perception.</p>"},{"location":"ros2/remote_compute/","title":"Offloading Heavy Robot Compute to Remote Workstation","text":"<p>In this tutorial, we will explore the method for offloading computationally intensive processes, such as running computer vision models, to a remote workstation computer. This approach offers several advantages such as: - Saving robot's processing power. - Increasing robot's efficiency by offloading high-power consuming processes.  - Utilizing available GPU hardware on powerful workstations to run large deep learning models.</p> <p>We will delve into the process of offloading Stretch Deep Perception ROS2 nodes. These nodes are known for their demanding computational requirements and are frequently used in Stretch Demos. </p> <p>NOTE: All Stretch ROS2 packages are developed with Humble distro.</p>"},{"location":"ros2/remote_compute/#1-setting-a-ros_domain_id","title":"1. Setting a ROS_DOMAIN_ID","text":"<p>ROS2 utilizes DDS as the default middleware for communication. DDS enables nodes within the same physical network to seamlessly discover one another and establish communication, provided they share the same ROS_DOMAIN_ID. This powerful mechanism ensures secure message passing between remote nodes as intended.</p> <p>By default, all ROS 2 nodes are configured with domain ID 0. To avoid conflicts, select a domain ID from the range of 0 to 101, and then set this chosen domain ID as the value for the <code>ROS_DOMAIN_ID</code> environment variable in both the Workstation and the Robot. <pre><code>export ROS_DOMAIN_ID=&lt;ID&gt;\n</code></pre></p>"},{"location":"ros2/remote_compute/#2-setup-the-workstation-to-work-with-stretch","title":"2. Setup the Workstation to work with Stretch","text":"<p>The workstation needs to be installed with the stretch related ros2 packages to have access to robot meshes for Visualization in Rviz, custom interfaces dependencies and essential perception packages.</p> <p>This section assumes the workstation already has an active ROS2 distro and colcon dependencies pre-installed. You can find ROS2 Installation step for Ubuntu here.</p>"},{"location":"ros2/remote_compute/#setup-essential-stretch_ros2-packages","title":"Setup Essential stretch_ros2 Packages","text":"<p>Make sure the ROS2 distro is sourced. <pre><code>source /opt/ros/humble/setup.bash\n</code></pre></p> <p>Create workspace directory and clone stretch_ros2 packages along with it's dependency packages to <code>src</code> folder. <pre><code>mkdir -p ~/ament_ws/src\ncd ~/ament_ws/src/\ngit clone https://github.com/hello-robot/stretch_ros2\ngit clone https://github.com/hello-binit/ros2_numpy -b humble\ngit clone https://github.com/IntelRealSense/realsense-ros.git -b ros2-development\ngit clone https://github.com/Slamtec/sllidar_ros2.git -b main\ngit clone https://github.com/hello-binit/respeaker_ros2.git -b humble\ngit clone https://github.com/hello-binit/audio_common.git -b humble\n</code></pre></p> <p>Build and install all the packages present in source folder.  <pre><code>cd ~/ament_ws\nrosdep install --rosdistro=humble -iyr --skip-keys=\"librealsense2\" --from-paths src\ncolcon build --cmake-args -DCMAKE_BUILD_TYPE=Release\n</code></pre></p> <p>Make sure to source the workspace to discover the packages in it. <pre><code>source ~/ament_ws/install/setup.bash\n</code></pre></p>"},{"location":"ros2/remote_compute/#setup-robot-urdf-and-meshes","title":"Setup Robot URDF and Meshes","text":"<p>All the robots will have calibrated URDF with pre-configured mesh files in the stretch_description package directory that is specific to your actual robot. So we recommend you to copy the <code>stretch_description</code> directory that exists inside your robot and replace it with the one existing in the workstation. The Stretch Description directory exists in the path <code>~/ament_ws/src/stretch_ros2/stretch_description</code>.</p> <p>If you dont want to use the URDFs from the robot, you can manually generate the uncalibrated URDF w.r.t your robot configuration using the following commands: <pre><code>cd ~/ament_ws/src/stretch_ros2/stretch_description/urdf/\n\n#if Dex-Wrist Installed\ncp stretch_description_dex.xacro stretch_description.xacro\n\n#if Standard Gripper\ncp stretch_description_standard.xacro stretch_description.xacro\n\nros2 run stretch_calibration update_uncalibrated_urdf\ncp stretch_uncalibrated.urdf stretch.urdf\n</code></pre></p> <p>After setting up the stretch_description folder, re-build the workspace to update the package with latest changes. <pre><code>cd ~/ament_ws\ncolcon build\n</code></pre></p>"},{"location":"ros2/remote_compute/#download-stretch-deep-perception-models","title":"Download Stretch Deep Perception Models","text":"<p>stretch_deep_perception_models provides open deep learning models from third parties for use. We are cloning this directory to the home folder in the workstation. <pre><code>cd ~/ \ngit clone https://github.com/hello-robot/stretch_deep_perception_models\n</code></pre></p>"},{"location":"ros2/remote_compute/#3-start-core-nodes-on-the-robot-compute","title":"3. Start core Nodes on the Robot Compute","text":"<p>Start the core driver nodes for controlling the robot, streaming the Lidar and realsense depth camera/s data using the stretch_core package.</p> <pre><code># Terminal 1: Start the Stretch Driver Node\nros2 launch stretch_core stretch_driver.launch.py\n# Terminal 2: Start the realsense D435i stream.\nros2 launch stretch_core d435i_high_resolution.launch.py\n# Terminal 3: Start lidar.\nros2 launch stretch_core rplidar.launch.py\n</code></pre>"},{"location":"ros2/remote_compute/#4-verify-remote-workstation-is-able-to-discover-stretch-nodes","title":"4. Verify Remote Workstation is able to discover Stretch Nodes","text":"<p>After launching the above core nodes, all the robot control interfaces and sensor data streams should be exposed to all the other nodes in the same physical network with common ROS_DOMAIN_ID set.</p> <p>From the remote workstation try the following test commands: <pre><code># Check if all robot topics are visible.\nros2 topic list\n\n# Check if able to receive a sensor data by printing from Joint States topic.\nros2 topic echo /joint_states\n\n# Check if able to send commands to robot by triggering stow_the_robot service\nros2 service call /stow_the_robot std_srvs/srv/Trigger\n</code></pre></p>"},{"location":"ros2/remote_compute/#5-offload-object-detection-node-to-remote-workstation","title":"5. Offload Object Detection Node to Remote Workstation","text":"<p>From the workstation, run the object detection node which runs a YoloV5 model. <pre><code>ros2 run stretch_deep_perception detect_objects\n</code></pre> The node would start printing out the detected objects. <pre><code>Fusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\nAdding AutoShape... \n[INFO] [1698379209.925727618] [DetectObjectsNode]: DetectObjectsNode started\ntv  detected\nkeyboard  detected\nchair  detected\nmouse  detected\nmouse  detected\ntv  detected\nkeyboard  detected\nchair  detected\nmouse  detected\nmouse  detected\nbowl  detected\ntv  detected\nkeyboard  detected\nchair  detected\nmouse  detected\nmouse  detected\n</code></pre></p>"},{"location":"ros2/remote_compute/#visualize-in-rviz","title":"Visualize in Rviz","text":"<p><pre><code>rviz2 -d ~/ament_ws/install/stretch_deep_perception/share/stretch_deep_perception/rviz/object_detection.rviz\n</code></pre> </p>"},{"location":"ros2/remote_compute/#6-offload-face-detection-node-to-remote-workstation","title":"6. Offload Face  detection Node to Remote Workstation","text":"<p>From the workstation, run the face detection node. The face-detection node uses model parameters loaded from the stretch_deep_perception_models directory, whose path is pulled from HELLO_FLEET_PATH environment variable. In our case, we will set the HELLO_FLEET_PATH environment variable to point to the home folder where the stretch_deep_perception_models directory was cloned. <pre><code>export HELLO_FLEET_PATH=~/\nros2 run stretch_deep_perception detect_faces\n</code></pre> The node will load the face detection model network and start poblishing the detection. <pre><code>head_detection_model.getUnconnectedOutLayers() = [112]\nhead_detection_model output layer names = ['detection_out']\nhead_detection_model output layer names = ('detection_out',)\nhead_detection_model input layer = &lt;dnn_Layer 0x7f7d1e695cd0&gt;\n.\n.\n.\nlandmarks_model input layer name = align_fc3\nlandmarks_model out_layer = &lt;dnn_Layer 0x7f7d1e695d30&gt;\n[INFO] [1698383830.671699923] [DetectFacesNode]: DetectFacesNode started\n</code></pre></p>"},{"location":"ros2/remote_compute/#visualize-in-rviz_1","title":"Visualize in Rviz","text":"<p><pre><code>rviz2 -d ~/ament_ws/install/stretch_deep_perception/share/stretch_deep_perception/rviz/face_detection.rviz\n</code></pre> </p>"},{"location":"ros2/remote_compute/#troubleshooting-notes","title":"Troubleshooting Notes","text":"<ul> <li>Using a dedicated Wi-Fi router would increase the data transmission speeds significantly.</li> <li>Realtime PointCloud visualization in Rviz commonly lags because of subscribing to a large message data stream. We recommend turning off the point-cloud visualization in remote workstations when possible to decrease network overhead.</li> <li>If the nodes in the remote network are unable to discover robot running nodes, here are two debug steps:</li> <li>Check if you can ping between the robot and remote workstation computer.</li> <li>Use <code>ifconfig</code> command and compare the Network assigned IP addresses of both the robot and workstation. The first two parts of the IP address should normally match for both computers to discover each other in the network.</li> </ul>"},{"location":"ros2/respeaker_mic_array/","title":"ReSpeaker Microphone Array","text":"<p>For this tutorial, we will get a high-level view of how to use Stretch's ReSpeaker Mic Array v2.0.  </p> <p> </p>"},{"location":"ros2/respeaker_mic_array/#stretch-body-package","title":"Stretch Body Package","text":"<p>In this section we will use command line tools in the Stretch_Body package, a low-level Python API for Stretch's hardware, to directly interact with the ReSpeaker.</p> <p>Begin by typing the following command in a new terminal.</p> <pre><code>stretch_respeaker_test.py\n</code></pre> <p>The following will be displayed in your terminal:</p> <pre><code>For use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n* waiting for audio...\n* recording 3 seconds\n* done\n* playing audio\n* done\n</code></pre> <p>The ReSpeaker Mico Array will wait until it hears audio loud enough to trigger its recording feature. Stretch will record audio for 3 seconds and then replay it through its speakers. This command line is a good method to see if the hardware is working correctly.</p> <p>To stop the python script, type <code>Ctrl</code> + <code>c</code> in the terminal.</p>"},{"location":"ros2/respeaker_mic_array/#respeaker_ros-package","title":"ReSpeaker_ROS Package","text":"<p>A ROS package for the ReSpeaker is utilized for this section.</p> <p>Begin by running the <code>sample_respeaker.launch.py</code> file in a terminal.</p> <pre><code>ros2 launch respeaker_ros2 respeaker.launch.py\n</code></pre> <p>This will bring up the necessary nodes that will allow the ReSpeaker to implement a voice and sound interface with the robot.</p>"},{"location":"ros2/respeaker_mic_array/#respeaker-topics","title":"ReSpeaker Topics","text":"<p>Below are executables you can run to see the ReSpeaker results.</p> <pre><code>ros2 topic echo /sound_direction\nros2 topic echo /sound_localization\nros2 topic echo /is_speeching\nros2 topic echo /audio\nros2 topic echo /speech_audio\nros2 topic echo /speech_to_text\n</code></pre> <p>There's also another topic called <code>/status_led</code>, with this topic you can change the color of the LEDs in the ReSpeaker, you need to publish the desired color in the terminal using <code>ros2 topic pub</code>. We will explore this topics in the next tutorial.</p> <p>You can also set various parameters via <code>dynamic_reconfigure</code> by running the following command in a new terminal.</p> <pre><code>ros2 run rqt_reconfigure rqt_reconfigure\n</code></pre>"},{"location":"ros2/respeaker_topics/","title":"ReSpeaker Microphone Array Topics","text":"<p>In this tutorial we will see the topics more in detail and have an idea of what the ReSpeaker can do. If you just landed here, it might be a good idea to first review the previous tutorial which covered the basics of the ReSpeaker and the information about the package used</p>"},{"location":"ros2/respeaker_topics/#respeaker-topics","title":"ReSpeaker Topics","text":"<p>Begin by running the <code>sample_respeaker.launch.py</code> file in a terminal.</p> <pre><code>ros2 launch respeaker_ros respeaker.launch.py\n</code></pre> <p>This will bring up the necessary nodes that will allow the ReSpeaker to implement a voice and sound interface with the robot. To see the topics that are available for us you can run the command <code>ros2 topic list -t</code> and search the topics that we are looking for. Don't worry these are the executables you can run to see the ReSpeaker results.</p> <p><pre><code>ros2 topic echo /sound_direction    # Result of Direction (in Radians) of Audio\nros2 topic echo /sound_localization # Result of Direction as Pose (Quaternion values)\nros2 topic echo /is_speeching       # Result of Voice Activity Detector\nros2 topic echo /audio              # Raw audio data\nros2 topic echo /speech_audio       # Raw audio data when there is speech\nros2 topic echo /speech_to_text     # Voice recognition\nros2 topic pub  /status_led ...     # Modify LED color\n</code></pre> Let's go one by one and see what we can expect of each topic, the first is the <code>sound_direction</code> topic, in the terminal execute the command that you learned earlier:</p> <p><pre><code>ros2 topic echo /sound_direction    # Result of Direction (in Radians) of Audio\n</code></pre> This will give you the direction of the sound detected by the ReSpeaker in radians</p> <p><pre><code>data: 21\n---\ndata: 138\n---\ndata: -114\n---\ndata: -65\n---\n</code></pre> The Direction of Arrival (DOA) for the ReSpeaker goes from -180 to 180, to know more about how is it in Stretch watch this DOA diagram:</p> <p> </p> <p>The next topic is the <code>sound_localization</code>, this is similar to the <code>sound_direction</code> topic but now the result it's as pose (Quaternion Values), try it out, execute the command:</p> <pre><code>ros2 topic echo /sound_localization # Result of Direction as Pose (Quaternion values)\n</code></pre> <p>With this you will have in your terminal this:</p> <pre><code>---\nheader:\n  stamp:\n    sec: 1695325677\n    nanosec: 882383094\n  frame_id: respeaker_base\npose:\n  position:\n    x: -0.0\n    y: 0.0\n    z: 0.0\n  orientation:\n    x: 0.0\n    y: 0.0\n    z: 0.43051109680829525\n    w: 0.9025852843498605\n---\n</code></pre> <p>The next one on the list is the <code>is_speeching</code> topic, with this you will have the result of Voice Activity Detector, let's try it out:</p> <pre><code>ros2 topic echo /is_speeching   # Result of Voice Activity Detector\n</code></pre> <p>The result will be a true or false in the data but it can detect sounds as true so be careful with this <pre><code>data: false\n---\ndata: true\n---\ndata: false\n---\n</code></pre></p> <p>The <code>audio</code> topic is goint to output all the Raw audio data, if you want to see what this does execute the command:</p> <p><pre><code>ros2 topic echo /audio  # Raw audio data\n</code></pre> You will expect a lot of data from this, you will see this output: <pre><code>---\ndata:\n- 229\n- 0\n- 135\n- 0\n- 225\n- 0\n- 149\n- 0\n- 94\n- 0\n- 15\n</code></pre></p> <p>For the <code>speech_audio</code> topic you can expect the same result as the <code>audio</code> topic but this time you are going to have the raw data when there is a speech, execute the next command and speak near the microphone array: <pre><code>ros2 topic echo /speech_audio   # Raw audio data when there is speech\n</code></pre> So if it's almost the same topic but now is going to ouput the data when you are talking then you guessed right, the result will look like the same as before. <pre><code>---\ndata:\n- 17\n- 254\n- 70\n- 254\n</code></pre></p> <p>Passing to the <code>speech_to_text</code> topic, with this you can say a small sentence and it will output what you said. In the terminal, execute the next command and speak near the microphone array again:</p> <pre><code>ros2 topic echo /speech_to_text   # Voice recognition\n</code></pre> <p>In this instance, \"hello robot\" was said. The following will be displayed in your terminal:</p> <p><pre><code>transcript:\n  - hello robot\nconfidence:\n- ######\n---\n</code></pre> And for the final topic, the <code>status_led</code>, with this you can setup custom LED patterns and effects. There are 3 ways to do it, the first one is using <code>rqt_publisher</code>, in the terminal input:</p> <p><pre><code>ros2 run rqt_publisher rqt_publisher\n</code></pre> With this the rqt_publisher window will open, there you need to add the topic manually, search for the <code>/status_led</code> topic, then click in the plus button, this is the add new publisher button and the topic will be added, then you can start moving the RGBA values between 0 to 1 and that's it, you can try it with the next example:</p> <p> </p> <p>You will see that there's a purple light coming out from the ReSpeaker, you can change the rate and color if you want.</p> <p>Now for the next way you can do it in the terminal, let's try again with the same values that we had so input this command in the terminal: <pre><code>ros2 topic pub /status_led std_msgs/msg/ColorRGBA \"r: 1.0\ng: 0.0\nb: 1.0\na: 1.0\"\n</code></pre> And you can see that we have the same result as earlier, good job!</p> <p>And for the final way it's going to be with a python code, here you can modify the lights just as we did before but now you have color patterns that you can create, let's try it so that you can see yourself, input in the terminal: <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 led_color_change.py\n</code></pre> With this we can change the colors as well but the difference is that we are able to create our own patterns, in the ReSpeaker Documentation there are more options to customize and control de LEDs.</p>"},{"location":"ros2/robot_drivers/","title":"Robot Drivers","text":"<p>This tutorial introduces the ROS2 drivers for Stretch and its sensors. These drivers reside in the <code>stretch_core</code> package, which is part of a suite of ROS2 packages available open-source on Github.</p>"},{"location":"ros2/robot_drivers/#stretch-driver","title":"Stretch Driver","text":"<p>The robot's driver is called \"Stretch Driver\". You can use it to send motion commands and read joint info. The command to launch Stretch Driver is:</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py rviz:=True\n</code></pre>"},{"location":"ros2/robot_drivers/#modes","title":"Modes","text":"<p>Stretch Driver has a few modes that change how the driver behaves. They are:</p> <ul> <li>Position: The default and simplest mode. In this mode, you can control every joint on the robot using position commands. For example, you could move the telescoping arm 25cm out by sending the driver a command of 0.25m for the \"joint_arm\" joint. Two kinds of position commands are available for the mobile base: translation and rotation, with joint names \"translate_mobile_base\" and \"rotate_mobile_base\". They are mutually exlusive and these joints have no limits since the wheels can spin continuously.<ul> <li>Position commands are tracked by a trapezoidal motion profile.</li> <li>Position commands are contact sensitive. This is helpful for manipulating objects in the world. For example, Stretch can open a cabinet by reaching out with the telescoping arm and detecting contact with the door and its handle.</li> <li>Position commands are preemptable, so you can issue a new position command before the previous one has finished executing and the robot will smoothly transition to executing the new command. This feature is helpful for reactive control (e.g visual servo-ing).</li> </ul> </li> <li>Navigation: In this mode, every joint behaves identically to \"position\" mode except for the mobile base, which takes velocity commands.<ul> <li>Velocity control of the base is a common way to move mobile robots around. For example, Nav2 is a piece of software that uses this mode to navigate Stretch within its environment.</li> <li>Velocity control has the potential to cause unsafe \"running away\" behavior if your program were to command a velocity motion and neglect to stop the motion later. Stretch has a few safety features to prevent this \"running away\" behavior. The driver has a 0.5 second timeout, which means that if the driver doesn't receive a new command within 0.5s, the base will stop smoothly. Additionally, the firmware has a 1 second timeout for the wheels, which means that even if the robot's onboard computer were to crash, the robot will stop if the firmware doesn't receive a new command within 1 second.</li> </ul> </li> <li>Trajectory: In this mode, every joint follows a trajectory that is modeled as a spline. The key benefit of this mode is control over the timing at which the robot achieves positions (a.k.a waypoints), enabling smooth and coordinated motion through a preplanned trajectory.<ul> <li>Trajectory commands are contact sensitive.</li> <li>The waypoints in trajectory commands are preemptable.</li> </ul> </li> <li>Homing: This is the mode reported by the driver when a homing sequence is happening. While this mode is active, no commands will be accepted by the driver and the mode cannot be switched. After the robot has completed its 30 second homing sequence, it will return to the mode it was in before.</li> <li>Stowing: This is the mode reported by the driver when a stowing sequence is happening. While this mode is active, no commands will be accepted by the driver and this mode cannot be switched. After the robot has completed its stowing sequence, it will return to the mode it was in before.</li> <li>Runstopped: This is the mode reported by the driver when the robot is in runstop. You can runstop Stretch by pressing the runstop button (i.e. the glowing white button in Stretch's head). While this mode is active, no commands will be accepted by the driver and the mode cannot be switched. After the robot has been taken out of runstop, it will return to the mode it was in before, or \"position\" mode if the driver was launched while the robot was runstopped.</li> </ul> <p>The driver publishes its current mode at the <code>/stretch/mode</code> topic, so you can see the driver's current mode using:</p> <pre><code>ros2 topic echo /stretch/mode\n# TODO - include output\n</code></pre>"},{"location":"ros2/robot_drivers/#keyboard-teleop","title":"Keyboard Teleop","text":"<p>The keyboard teleop node is an easy way to send position commands. First, switch the driver into \"position\" mode using the <code>/stetch/switch_to_position_mode</code> service:</p> <pre><code>ros2 service call /stretch/switch_to_position_mode\n# TODO - verify command and include output\n</code></pre> <p>Then, start the keyboard teleop node:</p> <pre><code>ros2 run stretch_core keyboard_teleop\n</code></pre>"},{"location":"ros2/robot_drivers/#joints-ranges","title":"Joints &amp; Ranges","text":"<p>You can read the robot's current joint state by echo-ing the <code>/stretch/joint_states</code> topic:</p> <pre><code>ros2 topic echo /stretch/joint_states\n# TODO - verify output\nheader:\n  seq: 70999\n  stamp:\n    secs: 1420\n    nsecs:   2000000\n  frame_id: ''\nname: [joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left,\n  joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift,\n  joint_right_wheel, joint_wrist_yaw]\nposition: [-1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2.9291786329821434e-07, 1.3802900147297237e-06, 0.08154086954434359, 1.4361499260374905e-07, 0.4139061738340768, 9.32603306580404e-07]\nvelocity: [0.00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1.322424459109634e-05, -0.00035084643762840415, 0.0012164337445918797, 0.0002138814988808099, 0.00010419792027496809, 4.0575263146426684e-05, 0.00022487596895736357, -0.0007751929074042957, 0.0002451588607332439]\neffort: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n---\n</code></pre> <p>To stop the stream of joint states, press Ctrl+C. The position, velocity, and effort arrays match the length of the joint names array, so each joint's state is found by indexing into the arrays at the same index that the joint has in the joint names array. E.g. \"joint_wrist_yaw\" has position 9.32603306580404e-07, velocity 0.0002451588607332439, and effort 0.0.</p>"},{"location":"ros2/robot_drivers/#motion-profiles","title":"Motion Profiles","text":"<ul> <li>Position commands are tracked in the firmware by a trapezoidal motion profile, and specifing the optional velocity and acceleration in JointTrajectoryPoint changes the shape of the trapezoid. More details about the motion profile in the tutorial.</li> </ul>"},{"location":"ros2/robot_drivers/#contact-sensitivity","title":"Contact Sensitivity","text":"<ul> <li>In order to specify contact thresholds for a position command, the optional effort in JointTrajectoryPoint is misused to mean a threshold for the effort the robot will apply while executing the position command.</li> </ul>"},{"location":"ros2/robot_drivers/#preemption","title":"Preemption","text":""},{"location":"ros2/robot_drivers/#other-services","title":"Other Services","text":"<pre><code>ros2 service call /stretch/home_the_robot\nros2 service call /stretch/stow_the_robot\nros2 service call /stretch/stop_the_robot\n# TODO - verify commands and include outputs\n</code></pre>"},{"location":"ros2/robot_drivers/#lidar-driver","title":"Lidar Driver","text":"<p>Rplidar Spinning Lidar</p>"},{"location":"ros2/robot_drivers/#camera-drivers","title":"Camera Drivers","text":"<p>Realsense Cameras</p>"},{"location":"ros2/robot_drivers/#head-camera","title":"Head Camera","text":""},{"location":"ros2/robot_drivers/#eye-in-hand-camera","title":"Eye in Hand Camera","text":""},{"location":"ros2/robot_drivers/#fisheye-navigation-camera","title":"Fisheye Navigation Camera","text":""},{"location":"ros2/robot_drivers/#mic-array-driver","title":"Mic Array Driver","text":"<p>Respeaker Microphone Array</p>"},{"location":"ros2/robot_drivers/#imus","title":"IMUs","text":""},{"location":"ros2/robot_drivers/#cliff-sensors","title":"Cliff Sensors","text":""},{"location":"ros2/rviz_basics/","title":"Rviz basics","text":""},{"location":"ros2/rviz_basics/#visualizing-with-rviz","title":"Visualizing with RViz","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>You can utilize RViz to visualize Stretch's sensor information. To begin, run the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then run the following command to bring up a simple RViz configuration of the Stretch robot.</p> <pre><code>ros2 run rviz2 rviz2 -d `ros2 pkg prefix --share stretch_calibration`/rviz/stretch_simple_test.rviz\n</code></pre> <p>An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window.</p> <p></p> <p>If you want to visualize Stretch's tf transform tree, you need to add the display type to the RViz window. First, click on the Add button and include the TF  type to the display. You will then see all of the transform frames of the Stretch robot and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference.</p> <p></p> <p>There are further tutorials for RViz that can be found here.</p>"},{"location":"ros2/teleoperating_stretch/","title":"Teleoperating stretch","text":""},{"location":"ros2/teleoperating_stretch/#teleoperating-stretch","title":"Teleoperating Stretch","text":"<p>Note</p> <p>Teleoperation support for Stretch in ROS 2 is under active development. Please reach out to us if you want to teleoperate Stretch in ROS 2.</p>"},{"location":"ros2/teleoperating_stretch/#xbox-controller-teleoperating","title":"Xbox Controller Teleoperating","text":"<p>If you have not already had a look at the Xbox Controller Teleoperation section in the Quick Start guide, now might be a good time to try it.</p>"},{"location":"ros2/teleoperating_stretch/#keyboard-teleoperating-full-body","title":"Keyboard Teleoperating: Full Body","text":"<p>For full-body teleoperation with the keyboard, you first need to run the <code>stretch_driver.launch.py</code> in a terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then in a new terminal, type the following command</p> <pre><code>ros2 run stretch_core keyboard_teleop\n</code></pre> <p>Below are the keyboard commands that allow a user to control all of Stretch's joints.</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------\n\n              i HEAD UP                    \n j HEAD LEFT            l HEAD RIGHT       \n              , HEAD DOWN                  \n\n\n 7 BASE ROTATE LEFT     9 BASE ROTATE RIGHT\n home                   page-up            \n\n\n              8 LIFT UP                    \n              up-arrow                     \n 4 BASE FORWARD         6 BASE BACK        \n left-arrow             right-arrow        \n              2 LIFT DOWN                  \n              down-arrow                   \n\n\n              w ARM OUT                    \n a WRIST FORWARD        d WRIST BACK       \n              x ARM IN                     \n\n\n              5 GRIPPER CLOSE              \n              0 GRIPPER OPEN               \n\n  step size:  b BIG, m MEDIUM, s SMALL     \n\n              q QUIT                       \n\n-------------------------------------------\n</code></pre> <p>To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"ros2/teleoperating_stretch/#keyboard-teleoperating-mobile-base","title":"Keyboard Teleoperating: Mobile Base","text":"<p>Begin by running the following command in your terminal:</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py mode:=navigation\n</code></pre> <p>To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to nagivation for the robot to receive Twist messages. In comparison with ROS1 that we needed to use the rosservice command, we can do it in the same driver launch as you can see in the command you just input! Now in other terminal run the teleop_twist_keyboard node with the argument remapping the cmd_vel topic name to stretch/cmd_vel.</p> <pre><code>ros2 run teleop_twist_keyboard teleop_twist_keyboard cmd_vel:=stretch/cmd_vel\n</code></pre> <p>Below are the keyboard commands that allow a user to move Stretch's base.  </p> <pre><code>Reading from the keyboard  and Publishing to Twist!\n---------------------------\nMoving around:\n   u    i    o\n   j    k    l\n   m    ,    .\n\nFor Holonomic mode (strafing), hold down the shift key:\n---------------------------\n   U    I    O\n   J    K    L\n   M    &lt;    &gt;\n\nt : up (+z)\nb : down (-z)\n\nanything else : stop\n\nq/z : increase/decrease max speeds by 10%\nw/x : increase/decrease only linear speed by 10%\ne/c : increase/decrease only angular speed by 10%\n\nCTRL-C to quit\n\ncurrently:  speed 0.5   turn 1.0\n</code></pre> <p>To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"ros2/teleoperating_stretch/#create-a-node-for-mobile-base-teleoperating","title":"Create a node for Mobile Base Teleoperating","text":"<p>To move Stretch's mobile base using a python script, please look at example 1 for reference.</p>"},{"location":"ros2/writing_nodes/","title":"Writing Nodes","text":"<p>TODO</p> <p>You would publish geometry_msgs/Twist messages to the /stretch/cmd_vel topic. Since Twist messages are generalized to robots that can move with velocity in any direction, only the <code>Twist.linear.x</code> (translational velocity) and <code>Twist.angular.z</code> (rotational velocity) fields apply for differential drive mobile bases.</p>"},{"location":"software/architecture/","title":"Software Architecture","text":"<p>TODO</p>"},{"location":"software/changelogs/","title":"Changelogs","text":"<p>A \"changelog\" keeps track of the notable changes for each new version of a project that is released. The changelogs for Stretch's software projects can be found below.</p>"},{"location":"software/changelogs/#changelog-for-stretch-body","title":"Changelog for Stretch Body","text":"<p>The changes between releases of Stretch Body are documented here.</p>"},{"location":"software/changelogs/#0728-january-27-2025","title":"0.7.28 - January 27, 2025","text":"<ul> <li>Drops Numba as a dependency</li> </ul>"},{"location":"software/changelogs/#0713-may-7-2024","title":"0.7.13 - May 7, 2024","text":"<ul> <li>This PR robot process protection to multi-user setup. Currently, if you have an ongoing connection to the robot, and attempt to launch another application that needs to connect to the USB hardware, the second attempt will not connect and will let the user know that an ongoing connection already exists. This is because only one connection to the hardware can safely communicate at a time. Therefore, this protects the first connection from experiencing dropouts or interruptions. The warning also informs the user that they can kill the first connection using the <code>stretch_free_robot_process.py</code> CLI.</li> <li>All of the above protections work well on single user set-ups, which is the default Stretch ships with. However, it's common in developer teams that are sharing the robot to want to create multiple Unix users on the robot. This PR enables all of the same protections and CLIs that worked previously to now work with multi-user setups.</li> </ul>"},{"location":"software/changelogs/#0711-feburary-20-2024","title":"0.7.11 - Feburary 20, 2024","text":"<ul> <li>Introduce the <code>stretch_configure_tool.py</code> tool (pr #287)<ul> <li>This PR introduces a new CLI called stretch_configure_tool.py. It eliminates a pain point around changing the robot.tool parameter and updating the URDF (and exported URDF).</li> <li>This tool supports all three models, all officially supported tools, Ubuntu 20.04 and 22.04.</li> </ul> </li> <li>Add \"Tool\" section to the system check</li> <li>Add the ability to change the formatter for the logging params</li> </ul>"},{"location":"software/changelogs/#072-january-26-2024","title":"0.7.2 - January 26, 2024","text":"<ul> <li>Fix for wait_command() timeout not being respected (issue #255)</li> <li>Improvements to the System Check tool, including:<ul> <li>Check if hello stepper self recognize correctly</li> <li>Check if OV9782 camera seen for Stretch 3s</li> <li>Update pip recommendations</li> <li>Hide verbose printout</li> <li>Add battery section to hardware check</li> </ul> </li> </ul>"},{"location":"software/changelogs/#070-january-25-2024","title":"0.7.0 - January 25, 2024","text":"<ul> <li>Introduces P5 firmware protocol support</li> <li>Revamped system check tool</li> <li>Revamped wrist/tool management system</li> <li>Started development on a self collision avoidance system</li> <li>Utilities and CLI to introspect UVC cameras</li> <li>Support for Dex Wrist 3</li> </ul>"},{"location":"software/changelogs/#068-december-7-2023","title":"0.6.8 - December 7, 2023","text":"<ul> <li>Deprecate <code>robot.is_calibrated()</code> and introduce <code>robot.is_homed()</code> to match homing nomenclature used elsewhere (details)</li> <li>Added a <code>robot.wait_command()</code> method to easily block process execution until motion is completed (details)</li> <li>Added Dex Wrist URDF visualization to the <code>stretch_robot_urdf_visualizer.py</code> tool</li> <li>Automatic checking for updates in background while homing + user notification about out-of-date software in <code>stretch_robot_system_check.py</code> (details)</li> <li>Add mutex locking on Stretch Body + introduce <code>stretch_free_robot_process.py</code> tool (details)</li> </ul>"},{"location":"software/changelogs/#062-september-11-2023","title":"0.6.2 - September 11, 2023","text":"<ul> <li>Introduces the new set of Gamepad modules to teleop stretch using the provided physical gamepad or any other UI input devices. This gamepad controller primarily uses velocity control.</li> <li>The stretch_gamepad_teleop.py tool is added and we are deprecating the old position control based stretch_xbox_teleop_controller.py</li> <li>The new controller now has a precision mode, allowing users to make fine-grain robot motions.</li> </ul>"},{"location":"software/changelogs/#060-october-7-2023","title":"0.6.0 - October 7, 2023","text":"<p>This is the initial productioon release that supports Prince batch. - Introduces the P4 firmware protocol support - Adds Prince batch params and new pimu IMU support  </p>"},{"location":"software/changelogs/#050-july-11-2023","title":"0.5.0 - July 11, 2023","text":"<ul> <li>Introduces the use_asyncio  mode that will enable using asynchronous IO call methods to perform robot push/pull commands and RPC transactions with the help of <code>asyncio</code> to speed up the USB device communications. This mode can be toggled back to use the regular non-async IO calls by changing the stretch params <code>use_asyncio</code>.</li> <li>By default, the asyncio mode is enabled.</li> <li>Adds the P3 protocol support for all the Arduino devices (stepper, pimu, wacc).</li> <li>Also, for the asynchronous transport layer to work, the devices firmware will need to support V1 transport protocol that is supported with firmware only above v0.5.0p3 for all hello-* arduino devices. For older firmwares, the <code>async_io</code> would be disabled automatically</li> <li>The  DXLStatusThread of robot class is now separated into two threads: DXLHeadStatusThread and DXLEndofArmStatusThread (Both threads run at 15Hz).</li> <li>Now the robot monitor, trace, sentry, and collision manager handles stepping is moved out of the NonDXLStatusThread. Instead, it is moved to a new separate thread called SystemMonitorThread is used to step these handles, which also runs at 25Hz.</li> <li>The Robot class can be <code>startup()</code> with the optional parameters to turn off some threads to save system resources.</li> <li>Added status_aux pull status RPCs feature for all the devices. <code>motor_sync_cnt</code> and <code>motor_sync_queues</code> status messages are populated with AUX pull status</li> <li>Now the RPC transactions queues is deprecated, instead <code>do_pull_transaction_vX()</code> and <code>do_push_transaction_vX()</code> methods are used from the SyncTransactionHandler.</li> <li>Asynchournous RPC transactions are handled by AsyncTransactionHandler that create analougus async methods to the one present in SyncTransactionHandler.</li> </ul>"},{"location":"software/changelogs/#048-sept-14-2022","title":"0.4.8 - Sept 14, 2022","text":"<p>This is the initial production release that supports the Stretch RE2 (Mitski batch).</p> <ul> <li> <p>This includes the <code>robot_params_RE2V0.py</code> which are the initial robot settings for the RE2 version of the product.</p> </li> <li> <p>It introduces the PrismaticJoint class which consolidates the common Arm and Lift functionality.</p> </li> <li> <p>It changes the units for guarded contact motion from approximate Newtons (suffix _N) to <code>effort_pct</code> - the pecentage [-100,100] of maximum current (A) that a joint should stop at. This change requires RE1 users to migrate their code and robot parameters. See the forum post for more details.</p> </li> <li> <p>It introduces <code>mkdocs.yaml</code> to support serving the repository documenation via MKDocs.</p> </li> </ul> <p>It introduces several new features and fixes several bugs, notably:</p> <ul> <li>Adds <code>wait_until_at_setpoint()</code> to the Arm and Lift classes</li> <li>Adds use of argparse with all tools</li> <li>Moves Robot thread rates to YAML</li> <li>Cleans up the splined trajectory interface, enables velocity controlled splined trajectories for the Dynamixels</li> <li>Flags a warning for users incorrectly setting the homing offset on DXL servos</li> </ul>"},{"location":"software/changelogs/#034-july-20-2022","title":"0.3.4 - July 20, 2022","text":"<p>Release to add minor features and fix minor bugs:</p> <ul> <li>Add a <code>range_pad_t</code> parameter to allow for padding of hardstops for joint homing</li> <li>Clean up tools and warnings to more consistent and legible #140</li> </ul>"},{"location":"software/changelogs/#030-june-21-2022","title":"0.3.0 - June 21, 2022","text":"<p>This release moves Stretch Body to use a new parameter management format. This change will require older systems to migrate their parameters to the new format. For systems that haven't yet migrated, Stretch Body will exit with a warning that they must migrate first by running <code>RE1_migrate_params.py</code>. See the forum post for more details.</p> <p>Features:</p> <ul> <li>New param management and RE1.5 support #135</li> </ul>"},{"location":"software/changelogs/#021-january-6-2022","title":"0.2.1 - January 6, 2022","text":"<p>Release to fix two bugs:</p> <ul> <li>Fix goto commands in the head jog tool #128 - Fixes goto commands in the head jog tool</li> <li>Fix port_handler location #121 - Fixes dxl buffer resetting under a serial communication failure</li> </ul>"},{"location":"software/changelogs/#020-december-28-2021","title":"0.2.0 - December 28, 2021","text":"<p>This release brings support for waypoint trajectories into master. Support for waypoint trajectories was built up over the last year in the feature/waypoint_trajectories_py3 branch, however, this branch couldn't be merged because the new functionality had flaky performance due to subtle bugs. This branch also attempted to introduce support for Python3 and timestamp synchronization. Support for Python3 and other features were merged in v0.1.0. The remaining features from this branch have been broken into 7 PRs, each targeting a specific device and squashing any previous bugs through functional and performance testing. They are:</p> <ul> <li>Introduce waypoint trajectory RPCs #98</li> <li>Add individual device threading #105</li> <li>Trajectory management classes #106</li> <li>Lift and arm trajectories #110</li> <li>Dynamixel trajectories #113</li> <li>Mobile base trajectories #114</li> <li>Whole body trajectories #115</li> </ul> <p>This release also fixes several bugs. They are:</p> <ul> <li>fixed baud map bug #117</li> <li>fix contact thresh bug #116</li> <li>Fixed EndOfArm tools unittest #104</li> </ul> <p>Testing:</p> <p>Each PR in this release was tested on multiple robots, but was primarily tested on G2, on Python 2.7/Ubuntu 18.04.</p>"},{"location":"software/changelogs/#0111-october-4-2021","title":"0.1.11 - October 4, 2021","text":"<p>This release gives Stretch Body the ability to support multiple firmware protocols, which at this moment is P0 and P1 firmware. P1 firmware builds on P0 to add waypoint trajectory support and a refactoring of controller functionality into classes. Additionally, this PR fixes how Dynamixel motors calculate velocity from encoder ticks.</p> <p>Features:</p> <ul> <li>Support new firmware protocol (P1) #97</li> </ul> <p>Bugfixes:</p> <ul> <li>Fix how negative dynamixel velocities are calculated #60</li> </ul>"},{"location":"software/changelogs/#0110-september-23-2021","title":"0.1.10 - September 23, 2021","text":"<p>This release introduces 3 features:</p> <ol> <li>#68 and #95: Introduces two new Jupyter notebooks that can be used to interactively explore working with Stretch. See forum post for details.</li> <li>#94: Introduces Github Action files and docs. Will be enabled in the future to automatically test PRs.</li> <li>#66: Improves the statistics captured on Stretch Body's performance. Will be used to measure improvements to Stretch Body's communication with low level devices.</li> </ol> <p>Bugfixes:</p> <ol> <li>#90: Patches a bug where triggering <code>pimu.trigger_motor_sync()</code> at to high of a rate puts the robot into Runstop mode.</li> <li>#101: Fixes bugs on startup of Dynamixel devices, ensures status is populated on startup of all devices, and add bool to <code>robot.startup()</code></li> </ol> <p>Testing:</p> <p>All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1.</p>"},{"location":"software/changelogs/#016-august-26-2021","title":"0.1.6 - August 26, 2021","text":"<p>This release introduces these features:</p> <ul> <li>Revised soft limits and collision avoidance</li> <li>Added velocity interfaces for arm and lift</li> </ul> <p>Bugfixes:</p> <ul> <li>Better error handling for DXL servos and their tools</li> <li>Fix bug where dxls maintain previous motion profile</li> </ul>"},{"location":"software/changelogs/#014-july-20-2021","title":"0.1.4 - July 20, 2021","text":"<p>This release introduces six features and several bugfixes. The features are:</p> <ul> <li>Robot self-collision model and tutorial</li> <li>Add ability to runstop individual DXL servos</li> <li>Merged py2 and py3 tools</li> <li>Added instructions for developing/testing Stretch Body</li> <li>Collision avoidance tutorial</li> <li>Improve realsense visualizer</li> </ul> <p>Bugfixes:</p> <ul> <li>Fix issue where user soft limits overwritten by collision models</li> <li>Pin py2 deps to older version</li> </ul>"},{"location":"software/changelogs/#010-may-30-2021","title":"0.1.0 - May 30, 2021","text":"<p>This release introduces eight major features and several bugfixes. The features are:</p> <ol> <li>Python param management</li> <li>Configurable baud rate/GroupRead on Dynamixels</li> <li>Pluggable end effector tools</li> <li>Pluggable end effector support in Xbox Teleop</li> <li>Python logging</li> <li>Soft motion limit</li> <li>Self collision management</li> <li>Unit testing framework - as part of each PR</li> </ol> <p>Bugfixes:</p> <ol> <li>Multiturn enable_pos bug (#12) &amp; unit tests</li> <li>Misc bugs</li> <li>Other - as part of the features</li> </ol> <p>Testing:</p> <p>All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1.</p>"},{"location":"software/changelogs/#changelog-for-stretch-factory","title":"Changelog for Stretch Factory","text":"<p>The changes between releases of Stretch Factory is documented here.</p>"},{"location":"software/changelogs/#054-feburary-7-2024","title":"0.5.4 - Feburary 7, 2024","text":"<p>This release adds a new tool <code>REx_xrandr_display.py</code> that allows users to programmatically change and revert the display resolution.</p> <p>Example Usage: <pre><code>$ REx_xrandr_display.py --current\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nDisplay Name:       HDMI-1\nDisplay Resolution: 1920x1080x60.00\n$ REx_xrandr_display.py --set-720p\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\n$ REx_xrandr_display.py --current\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nDisplay Name:       HDMI-1\nDisplay Resolution: 1280x720x60.00\n$ REx_xrandr_display.py --revert\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n</code></pre></p>"},{"location":"software/changelogs/#053-feburary-3-2024","title":"0.5.3 - Feburary 3, 2024","text":"<p>This release adds a new tool <code>REx_camera_set_symlink.py</code> that allows the user create a USB symlink to any USB camera that is plugged into the robot. The symlink is assigned by generating an Udev rule that records the following USB attributes of the plugged in camera: <code>[idVendor,idProduct,serial]</code>.</p> <p>Example Usage: By addressing a camera port: <pre><code>$ REx_camera_set_symlink.py --port /dev/video6 --symlink hello-new-camera\n</code></pre> <pre><code>For use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nLinking usb port: /dev/video6 to symlink port: /dev/hello-new-camera\nSuccessfully generated udev rule at path: /etc/udev/rules.d/86-hello-new-camera.rules\nSuccessfully Identified device at port: /dev/hello-new-camera\n</code></pre></p>"},{"location":"software/changelogs/#052-feburary-2-2024","title":"0.5.2 - Feburary 2, 2024","text":"<p>There was a bug with the firmware updater tool, instead of before looking for the orginal dev/ttyACM port of the device when it is in bootloader state, it's been changed to look for the arduino zero device and use the associated dev/ttyACM port instead.</p>"},{"location":"software/changelogs/#050-january-24-2024","title":"0.5.0 - January 24, 2024","text":"<p>This release introduces the concept of \"stepper types\" to the Stretch Factory package. In P5 firmware, we introduced the ability for the uC on a stepper PCB to know which kind of stepper it is (i.e. arm, lift, right wheel, left wheel). In this release, the firmware updater checks if the stepper already knows its stepper type before erasing the flash memory through a new firmware flash. It saves the stepper_type, flashes the new firmware, then writes the stepper_type to flash memory. This release also introduces the new tool called REx_stepper_type.py tool for Hello Robot support members to be able to assist Stretch users in flashing their stepper_type bits.</p> <p>There's a couple benefits to each stepper knowing its stepper_type at the firmware level:</p> <ol> <li>The wheels on newer Stretch robots can take advantage of better runstop by actually disconnecting the H bridge from the motor. This makes it easier to backdrive the robot around by tilting the robot and pushing it like a vacuum cleaner.</li> <li>The system check tool can verify that the UDEV rules for each stepper agree with the stepper_type of the stepper. In case the UDEV rules get corrupted, this provides an additional level of redundancy.</li> </ol>"},{"location":"software/changelogs/#0413-december-13-2023","title":"0.4.13 - December 13, 2023","text":"<p>This release eliminates a failure case in the firmware updater where Hello devices are left in a soft-bricked (i.e. can be hardware reset) state from a failed firmware update attempt. The fix has been tested with 800+ firmware flashes on a variety of Stretch robots.</p>"},{"location":"software/changelogs/#0311-january-17-2023","title":"0.3.11 - January 17, 2023","text":"<p>This release adds the tool <code>REx_discover_hello_devices.py</code>. This tool will enable users to find and map all the robot-specific USB devices (i.e. Lift, Arm, Left wheel, Right wheel, Head, Wrist/End-of-arm) and assign them to the robot by updating UDEV rules and stretch configuration files.  This tool would require Stretch Body v0.4.11 and above.</p>"},{"location":"software/changelogs/#0310-january-16-2023","title":"0.3.10 - January 16, 2023","text":"<p>This release (and previous releases since 0.3.0) makes a number of small improvements to the following tools:</p> <ul> <li><code>REx_dynamixel_jog.py</code> - Jog tool can put dxl in multi-turn, position, pwm, and velocity modes</li> <li><code>REx_calibrate_range.py</code> - Add error checking for failed homing</li> <li><code>REx_calibrate_guarded_contact.py</code> - Support RE1 robots</li> <li><code>REx_base_calibrate_imu_collect.py</code> - Fix bug</li> </ul> <p>There are also organization changes to the docs and READMEs.</p>"},{"location":"software/changelogs/#030-september-1-2022","title":"0.3.0 - September 1, 2022","text":"<p>This release moves Stretch Factory to use a new naming scheme for its tools. The prefix <code>REx</code> is now used instead of <code>RE1</code>. This semantic change is in anticipation of the release of future versions of Stretch (e.g. RE2).</p> <p>In addition, two new tools are introduced:</p> <ul> <li>REx_calibrate_guarded_contact.py: Measure the efforts required to move througout the joint workspace and save contact thresholds to Configuration YAML</li> <li>REx_calibrate_range.py: Measure the range of motion of a joint and save to the Configuration YAML.</li> </ul> <p>These new tools move the the <code>effort_pct</code> contact model as supported by Stretch Body 0.3</p> <p>Additional features;</p> <ul> <li>Firmware updater tested and supports 20.04 #47</li> </ul>"},{"location":"software/changelogs/#020-june-21-2022","title":"0.2.0 - June 21, 2022","text":"<ul> <li>Support new parameter manage scheme #45</li> </ul> <p>Adds new parameter management format. This change will require older systems to migrate their parameters to the new format. For systems that haven't yet migrated, Stretch Body will exit with a warning that they must migrate first by running <code>RE1_migrate_params.py</code>. See the forum post for more details.</p>"},{"location":"software/changelogs/#010","title":"0.1.0","text":"<ul> <li>Introduce the RE1_firmware_update.py tool</li> </ul>"},{"location":"software/changelogs/#002-may-13-2020","title":"0.0.2 - May 13, 2020","text":"<p>This is the initial release of Stretch Factory. It includes tools to support debug and testing of the Stretch Hardware.</p>"},{"location":"software/contributing/","title":"Contributing","text":"<p>Thank you for considering contributing to Stretch's software . All of Stretch's software is open-source and the code is available on Github. This guide covers how to get set up with developing on some part of the software and contributing it back.</p>"},{"location":"software/contributing/#background","title":"Background","text":""},{"location":"software/contributing/#opening-a-github-issue","title":"Opening a Github Issue","text":"<p>TODO</p>"},{"location":"software/contributing/#forking-a-repository","title":"Forking a Repository","text":"<p>TODO</p>"},{"location":"software/contributing/#filing-a-pull-request","title":"Filing a Pull Request","text":"<p>TODO</p>"},{"location":"software/contributing/#contributing-to-stretch-docs","title":"Contributing to Stretch Docs","text":"<p>TODO</p>"},{"location":"software/contributing/#contributing-to-stretch-install","title":"Contributing to Stretch Install","text":"<p>Thank you for considering contributing to this repository. Stretch Install houses bash scripts and tutorials that enable users to setup/configure their robots. This guide explains the layout of this repo and how best to make and test changes.</p>"},{"location":"software/contributing/#repo-layout","title":"Repo Layout","text":"<ul> <li><code>README.md</code> &amp; <code>LICENSE.md</code> - includes info about the repo and a table of tutorials available</li> <li><code>stretch_new_*_install.sh</code> - high level scripts meant to be run by the user</li> <li><code>factory/</code> - subscripts and assets not meant to be run by the user</li> <li><code>18.04/</code> - subscripts and assets specific to performing a Ubuntu 18.04 software install<ul> <li><code>stretch_initial_setup.sh</code> - a bunch of checks and initial setup that are run before performing a robot install</li> <li><code>stretch_install_*.sh</code> - helper scripts that install a specific set of packages</li> <li><code>stretch_create_*_workspace.sh</code> - creates a ROS/ROS2 workspace</li> <li><code>stretch_ros*.repos</code> - the ROS packages that are included and compiled in the ROS workspace by the <code>stretch_create_*_workspace.sh</code> script</li> <li><code>hello_robot_*.desktop</code> - autostarts programs to run when the robot boots up</li> </ul> </li> <li><code>&lt;&gt;.04/</code> - Ubuntu &lt;&gt;.04 software install related subscripts/assets. Similar in layout to 18.04/</li> <li><code>docs/</code> - contains tutorials for using the scripts in this repo</li> </ul> <p>Once you're ready to make changes to this repo, you can fork it on Github.</p>"},{"location":"software/contributing/#contributing-to-the-tutorials","title":"Contributing to the tutorials","text":"<p>The tutorials in the <code>docs/</code> folder are markdown files that get rendered into our https://docs.hello-robot.com site. If you edit them and file a pull request towards this repo, the changes to the tutorials will get reflected on the docs site. In order to live preview changes to these tutorials, first install mkdocs using:</p> <pre><code>python3 -m pip install mkdocs mkdocs-material mkdocstrings==0.17.0 pytkdocs[numpy-style] jinja2=3.0.3\n</code></pre> <p>Then, run the dev server using:</p> <pre><code>python3 -m mkdocs serve\n</code></pre> <p>Now you can make additions or changes to the source markdown files and see the changes reflected live in the browser.</p>"},{"location":"software/contributing/#contributing-to-the-installation-scripts","title":"Contributing to the installation scripts","text":"<p>If you are looking to change scripts/assets of an existing software installation (e.g. Ubuntu 18.04/20.04), look within the <code>factory/&lt;&gt;.04/</code> directory and make changes to the appriopriate files. If you're looking to add support for a new Ubuntu distro (e.g. Ubuntu 19.04), create <code>factory/19.04</code> with assets from a previous installation and tweak the scripts until they works correctly for the Ubuntu distro you are targeting. Then, edit the high level scripts (e.g. <code>stretch_new_*_install.sh</code>) to call your distro's specific assets correctly. Ensuring that the tutorials in the <code>docs/</code> work for your new distro is a good way to ensure that your <code>factory/&lt;&gt;.04/</code> directory works correctly. Since bash scripts change behavior based on the underlying system, it can be helpful to use containers to create reproducible behaviors while you're developing support for the new distro. Multipass works well on Ubuntu systems. You can create a new container emulating any Ubuntu distro using the command:</p> <pre><code>multipass launch -c 6 -d 30G -m 7G -n &lt;container-name&gt; 19.04\n</code></pre> <p>Swap <code>19.04</code> in the above command with the distro you're targeting. The above command creates a containers with 30GB disk space, 7GB swap, 6 cores, and the name <code>&lt;container-name&gt;</code>. We've found that at least 30GB disk space is needed for the Ubuntu 18.04/20.04 installations.</p> <p>Then, you can access the shell of your new container using:</p> <pre><code>multipass shell &lt;container-name&gt;\n</code></pre> <p>Other helpful multipass subcommand include <code>transfer</code>, which allows you to transfer files to the container, and <code>delete</code>, which allows you to delete the container. See the multipass docs for more details.</p>"},{"location":"software/contributing/#filing-a-pull-request_1","title":"Filing a Pull Request","text":"<p>Once your changes are committed to your fork, you can open a pull request towards the Stretch Install master branch. A member of Hello Robot's software team will review your new PR and get it merged and available for all Stretch users.</p>  All materials are Copyright 2020-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"software/contributing/#contributing-to-stretchs-python-libraries","title":"Contributing to Stretch's Python Libraries","text":"<p>TODO</p>"},{"location":"software/contributing/#contributing-to-stretchs-ros-packages","title":"Contributing to Stretch's ROS Packages","text":"<p>TODO</p>"},{"location":"software/distributions/","title":"Distributions &amp; Roadmap","text":"<p>Stretch's \"robot distributions\" are bundles of software installed onto the robot's onboard computer. A distribution typically includes an Ubuntu operating system, Stretch's Python libraries, a ROS distribution with Stretch's ROS packages, and firmware for Stretch's hardware.</p>"},{"location":"software/distributions/#list-of-distributions","title":"List of Distributions","text":"Distribution Release Date Description EOL Date Ubuntu 22.04 (Recommended) January 2024 The 22.04 distribution comes with ROS2 Humble and Python3.10. It does not come with ROS1. January 2027 Ubuntu 20.04 (Maintenance) October 2022 The 20.04 distribution comes with ROS1 Noetic and Python3.8. This distribution is in maintenance (i.e. only receives bug-fixes, and new features aren't ported back). January 2025"},{"location":"software/distributions/#deprecated-distributions","title":"Deprecated Distributions","text":"Distribution Release Date Description EOL Date Ubuntu 18.04 (Deprecated) May 2020 18.04 was the first distribution for Stretch, and included software for ROS1 Melodic and Python2. This distribution is no longer supported. August 2023"},{"location":"software/distributions/#determining-your-current-distribution","title":"Determining your Current Distribution","text":"<p>Open a terminal and run the following command. The outputs contains a description of the installed distribution.</p> <pre><code>lsb_release -d\nDescription:    Ubuntu 22.04.3 LTS\n</code></pre>"},{"location":"software/distributions/#installing-a-distribution","title":"Installing a Distribution","text":"<p>To install one of the distributions above, check out the Performing a Robot Installation guide.</p>"},{"location":"software/distributions/#roadmap","title":"Roadmap","text":"<p>We may support ROS 2 Jazzy (a future distribution of ROS2) and Ubuntu 24.04 in the future. Reach out to Hello Robot to provide feedback on distributions you'd like to see.</p>"},{"location":"software/updating_software/","title":"Keeping your Software Up-to-date","text":"<p>Your Stretch receives a few different kinds of software updates, each with its own procedure for applying the update. This guide covers a strategy for keeping your Stretch's software up-to-date. First, here are the kinds of updates you can expect:</p> <ul> <li>SDK-level updates: The ROS and Python libraries that constitute Stretch's software development kit (SDK) receive frequent updates. Using the latest SDK ensures access to the latest features and bug-fixes.</li> <li>Operating System-level updates: The onboard computer runs an operating system (typically Ubuntu). The OS bundled with the Stretch SDK is called a 'Robot Distribution'. A list of the available distributions is in the Distributions &amp; Roadmap guide. Using the recommended distribution ensures that you receive SDK-level updates.</li> <li>Firmware-level updates: While the onboard computer inside Stretch receives SDK and OS-level updates, the microcontrollers powering Stretch's motors and sensors receive firmware-level updates. Using the latest firmware ensures correctness and performance from Stretch's hardware controllers.</li> </ul>"},{"location":"software/updating_software/#identifying-your-current-software","title":"Identifying your current software","text":"<p>The system check tool can print out a summary of your current software:</p> <pre><code>stretch_system_check.py -v\n</code></pre> <p>At the bottom of the output, you might see a \"Checking Software\" section (if this doesn't appear, see below):</p> <p> </p> This output tells us that this Stretch is running the Ubuntu 22.04 distribution, with firmware v0.5.1p3, varying versions of Stretch's Python packages, and ROS2 Humble.  If the 'Checking Software' section doesn't appear <p>It's likely because you're using an older version of the tool. Follow these instructions to collect the same information. You can ask for help on the forum if you run into any issues.</p>"},{"location":"software/updating_software/#manually-identifying-your-current-software","title":"Manually identifying your current software","text":"<p>Run the following command to identify the operating system you're running: <pre><code>lsb_release -d\n</code></pre> The output will report your OS: <pre><code>Description:    Ubuntu 22.04.3 LTS\n</code></pre> The list of supported/deprecated distributions is available here.</p> <p>Next, run the following command to identify your firmware: <pre><code>REx_firmware_updater.py --recommended\n</code></pre> The output will report your current firmware and recommended action: <pre><code>Collecting information...\nCollecting information...\nCollecting information........\n\n######################################## Recommended Firmware Updates ########################################\n\n\nDEVICE                    | INSTALLED                 | RECOMMENDED               | ACTION                    \n--------------------------------------------------------------------------------------------------------------\nHELLO-MOTOR-ARM           | Stepper.v0.5.1p3          | Stepper.v0.6.2p4          | Upgrade recommended       \nHELLO-MOTOR-RIGHT-WHEEL   | Stepper.v0.5.1p3          | Stepper.v0.6.2p4          | Upgrade recommended       \nHELLO-MOTOR-LEFT-WHEEL    | Stepper.v0.5.1p3          | Stepper.v0.6.2p4          | Upgrade recommended       \nHELLO-PIMU                | Pimu.v0.5.1p3             | Pimu.v0.6.1p4             | Upgrade recommended       \nHELLO-WACC                | Wacc.v0.5.1p3             | Wacc.v0.5.1p3             | At most recent version    \nHELLO-MOTOR-LIFT          | Stepper.v0.5.1p3          | Stepper.v0.6.2p4          | Upgrade recommended       \n\nRun recommended command: \nREx_firmware_updater.py --install  --arm --right_wheel --left_wheel --pimu --lift\n</code></pre></p> <p>Next, run the following command to identify which versions of the Python libraries you have: <pre><code>pip3 list | grep hello\n</code></pre> The output will report your Python libraries: <pre><code>hello-robot@stretch-re2-2002:~$ pip3 list | grep hello\nhello-helpers                          0.2.0\nhello-robot-stretch-body               0.6.6\nhello-robot-stretch-body-tools         0.6.0\nhello-robot-stretch-diagnostics        0.0.14\nhello-robot-stretch-factory            0.4.12\nhello-robot-stretch-tool-share         0.2.8\nhello-robot-stretch-urdf               0.0.11\n</code></pre> You can see what the latest version of these libraries are by visiting https://pypi.org/project/hello-robot-stretch-body/ and replacing the library name in the URL.</p> <p>Next, run the following command to see if ROS is enabled: <pre><code>echo $ROS_DISTRO\n</code></pre> If the output is empty, no version of ROS has been sourced in your bashrc. Otherwise, the output indicates which version of ROS is enabled: <pre><code>humble\n</code></pre></p> <p>Next, if ROS is enabled, run the following command to identify your Stretch ROS branch and latest commit: <pre><code>cd ~/&lt;workspace_dir&gt;/src/&lt;stretch_ros_version&gt; &amp;&amp; git branch &amp;&amp; git show HEAD --stat\n# replace &lt;workspace_dir&gt; with 'ament_ws' if using ROS2 or 'catkin_ws' if using ROS1\n# replace &lt;stretch_ros_version&gt; with 'stretch_ros2' if using ROS2 or 'stretch_ros' if using ROS1\n</code></pre> The output will report your Stretch ROS branch (<code>* humble</code> below) amd latest commit (<code>99cda32feec0cf67e9b13cc862b4f293dd678c1e</code> below): <pre><code>* humble\ncommit 99cda32feec0cf67e9b13cc862b4f293dd678c1e (HEAD -&gt; humble, origin/humble, origin/HEAD)\nAuthor: Binit Shah &lt;bshah@hello-robot.com&gt;\nDate:   Sun Nov 19 21:46:39 2023 -0500\n\n    Update export_urdf.sh\n\nstretch_description/urdf/export_urdf.sh | 23 +++++++++++++----------\n1 file changed, 13 insertions(+), 10 deletions(-)\n</code></pre> You can see the main branch and its latest commit by visiting the Github page for either Stretch ROS or Stretch ROS2.</p> <p>With the information collected here, you'll be able to follow the rest of this tutorial.</p>"},{"location":"software/updating_software/#updating-software","title":"Updating Software","text":""},{"location":"software/updating_software/#robot-distribution","title":"Robot Distribution","text":"Output Recommendation If your Stretch is running a supported distribution, you can skip to the next section. If your Stretch is running a deprecated distribution, it's a good idea to perform the upgrade now. This will update your SDK and firmware as well. Check out the Upgrading your Operating System guide. If your code relies on software that has not yet been ported to a newer distribution or if you're in the middle of a project, you may want to hold off on upgrading. If you're starting a new project, it's a good idea to upgrade and build on the latest distribution. Check out the Upgrading your Operating System guide."},{"location":"software/updating_software/#python-libraries","title":"Python Libraries","text":"<p>If you see a warning that your Python pkgs are out-of-date, you can run the following command for each package name. <pre><code>pip3 install -U hello-robot-stretch-&lt;pkg_name&gt;\n</code></pre></p>"},{"location":"software/updating_software/#ros-workspace","title":"ROS Workspace","text":"<p>If you see a warning that your ROS workspace is out-of-date, you can create a new workspace to replace it. Check out the Updating your ROS Workspace guide.</p>"},{"location":"software/updating_software/#firmware","title":"Firmware","text":"<p>If you see a warning that your firmware is out-of-date, you can run the following command. <pre><code>REx_firmware_updater.py --install\n</code></pre></p>"},{"location":"software/updating_software/#troubleshooting","title":"Troubleshooting","text":""},{"location":"software/updating_software/#param-migration-error","title":"Param Migration Error","text":"<p>If you see the following error:</p> <pre><code>Please run tool REx_migrate_params.py before continuing. For more details, see https://forum.hello-robot.com/t/425\n</code></pre> <p>This error appears because the organization of Stretch's parameters has changed since Stretch Body v0.3 and requires a migration of these parameters to the new organization system. Executing the following command will automaticaly migrate your parameters over:</p> <pre><code>REx_migrate_params.py\n</code></pre>"},{"location":"software/updating_software/#firmware-mismatch-error","title":"Firmware Mismatch Error","text":"<p>If you see the following error:</p> <pre><code>----------------\nFirmware protocol mismatch on /dev/XXXX.\nProtocol on board is pX.\nValid protocol is: pX.\nDisabling device.\nPlease upgrade the firmware and/or version of Stretch Body.\n----------------\n</code></pre> <p>This error appears because the low level Python SDK, Stretch Body, and the firmware cannot communicate to each other. There is a protocol mismatch preventing communication between the two. Simply upgrade Stretch Body using the instructions above.</p>"},{"location":"stretch-body/","title":"Index","text":""},{"location":"stretch-body/#overview","title":"Overview","text":"<p>The stretch_body repository includes Python packages that allow a developer to interact with the hardware of the Stretch robots. These packages are:</p> Package Description hello-robot-stretch-body Python library to interface with Stretch hello-robot-stretch-body-tools Useful commandline tools for using Stretch <p>Python3 version of packages can be installed by:</p> <pre><code>pip3 install -U hello-robot-stretch-body\npip3 install -U hello-robot-stretch-body-tools\n</code></pre> <p>See docs.hello-robot.com for documentation on using Stretch. In particular, see the Stretch Body Tutorials for additional information on using these packages.</p>"},{"location":"stretch-body/#testing-and-development","title":"Testing and Development","text":"<p>See Stretch Body's README and Stretch Body Commandline Tool's README for information on testing/developing these packages.</p>"},{"location":"stretch-body/#changelog","title":"Changelog","text":"<p>See the changelog for information on what changed with each release.</p>"},{"location":"stretch-body/#license","title":"License","text":"<p>Each subdirectory contains a LICENSE.md file that applies to the directory's contents. This software is intended for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc. For further information including inquiries about dual licensing, please contact Hello Robot Inc.</p>  All materials are Copyright 2022-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-body/CHANGELOG/","title":"Changelog","text":"<p>The changes between releases of Stretch Body are documented here.</p>"},{"location":"stretch-body/CHANGELOG/#0728-january-27-2025","title":"0.7.28 - January 27, 2025","text":"<ul> <li>Drops Numba as a dependency</li> </ul>"},{"location":"stretch-body/CHANGELOG/#0713-may-7-2024","title":"0.7.13 - May 7, 2024","text":"<ul> <li>This PR robot process protection to multi-user setup. Currently, if you have an ongoing connection to the robot, and attempt to launch another application that needs to connect to the USB hardware, the second attempt will not connect and will let the user know that an ongoing connection already exists. This is because only one connection to the hardware can safely communicate at a time. Therefore, this protects the first connection from experiencing dropouts or interruptions. The warning also informs the user that they can kill the first connection using the <code>stretch_free_robot_process.py</code> CLI.</li> <li>All of the above protections work well on single user set-ups, which is the default Stretch ships with. However, it's common in developer teams that are sharing the robot to want to create multiple Unix users on the robot. This PR enables all of the same protections and CLIs that worked previously to now work with multi-user setups.</li> </ul>"},{"location":"stretch-body/CHANGELOG/#0711-feburary-20-2024","title":"0.7.11 - Feburary 20, 2024","text":"<ul> <li>Introduce the <code>stretch_configure_tool.py</code> tool (pr #287)<ul> <li>This PR introduces a new CLI called stretch_configure_tool.py. It eliminates a pain point around changing the robot.tool parameter and updating the URDF (and exported URDF).</li> <li>This tool supports all three models, all officially supported tools, Ubuntu 20.04 and 22.04.</li> </ul> </li> <li>Add \"Tool\" section to the system check</li> <li>Add the ability to change the formatter for the logging params</li> </ul>"},{"location":"stretch-body/CHANGELOG/#072-january-26-2024","title":"0.7.2 - January 26, 2024","text":"<ul> <li>Fix for wait_command() timeout not being respected (issue #255)</li> <li>Improvements to the System Check tool, including:<ul> <li>Check if hello stepper self recognize correctly</li> <li>Check if OV9782 camera seen for Stretch 3s</li> <li>Update pip recommendations</li> <li>Hide verbose printout</li> <li>Add battery section to hardware check</li> </ul> </li> </ul>"},{"location":"stretch-body/CHANGELOG/#070-january-25-2024","title":"0.7.0 - January 25, 2024","text":"<ul> <li>Introduces P5 firmware protocol support</li> <li>Revamped system check tool</li> <li>Revamped wrist/tool management system</li> <li>Started development on a self collision avoidance system</li> <li>Utilities and CLI to introspect UVC cameras</li> <li>Support for Dex Wrist 3</li> </ul>"},{"location":"stretch-body/CHANGELOG/#068-december-7-2023","title":"0.6.8 - December 7, 2023","text":"<ul> <li>Deprecate <code>robot.is_calibrated()</code> and introduce <code>robot.is_homed()</code> to match homing nomenclature used elsewhere (details)</li> <li>Added a <code>robot.wait_command()</code> method to easily block process execution until motion is completed (details)</li> <li>Added Dex Wrist URDF visualization to the <code>stretch_robot_urdf_visualizer.py</code> tool</li> <li>Automatic checking for updates in background while homing + user notification about out-of-date software in <code>stretch_robot_system_check.py</code> (details)</li> <li>Add mutex locking on Stretch Body + introduce <code>stretch_free_robot_process.py</code> tool (details)</li> </ul>"},{"location":"stretch-body/CHANGELOG/#062-september-11-2023","title":"0.6.2 - September 11, 2023","text":"<ul> <li>Introduces the new set of Gamepad modules to teleop stretch using the provided physical gamepad or any other UI input devices. This gamepad controller primarily uses velocity control.</li> <li>The stretch_gamepad_teleop.py tool is added and we are deprecating the old position control based stretch_xbox_teleop_controller.py</li> <li>The new controller now has a precision mode, allowing users to make fine-grain robot motions.</li> </ul>"},{"location":"stretch-body/CHANGELOG/#060-october-7-2023","title":"0.6.0 - October 7, 2023","text":"<p>This is the initial productioon release that supports Prince batch. - Introduces the P4 firmware protocol support - Adds Prince batch params and new pimu IMU support  </p>"},{"location":"stretch-body/CHANGELOG/#050-july-11-2023","title":"0.5.0 - July 11, 2023","text":"<ul> <li>Introduces the use_asyncio  mode that will enable using asynchronous IO call methods to perform robot push/pull commands and RPC transactions with the help of <code>asyncio</code> to speed up the USB device communications. This mode can be toggled back to use the regular non-async IO calls by changing the stretch params <code>use_asyncio</code>.</li> <li>By default, the asyncio mode is enabled.</li> <li>Adds the P3 protocol support for all the Arduino devices (stepper, pimu, wacc).</li> <li>Also, for the asynchronous transport layer to work, the devices firmware will need to support V1 transport protocol that is supported with firmware only above v0.5.0p3 for all hello-* arduino devices. For older firmwares, the <code>async_io</code> would be disabled automatically</li> <li>The  DXLStatusThread of robot class is now separated into two threads: DXLHeadStatusThread and DXLEndofArmStatusThread (Both threads run at 15Hz).</li> <li>Now the robot monitor, trace, sentry, and collision manager handles stepping is moved out of the NonDXLStatusThread. Instead, it is moved to a new separate thread called SystemMonitorThread is used to step these handles, which also runs at 25Hz.</li> <li>The Robot class can be <code>startup()</code> with the optional parameters to turn off some threads to save system resources.</li> <li>Added status_aux pull status RPCs feature for all the devices. <code>motor_sync_cnt</code> and <code>motor_sync_queues</code> status messages are populated with AUX pull status</li> <li>Now the RPC transactions queues is deprecated, instead <code>do_pull_transaction_vX()</code> and <code>do_push_transaction_vX()</code> methods are used from the SyncTransactionHandler.</li> <li>Asynchournous RPC transactions are handled by AsyncTransactionHandler that create analougus async methods to the one present in SyncTransactionHandler.</li> </ul>"},{"location":"stretch-body/CHANGELOG/#048-sept-14-2022","title":"0.4.8 - Sept 14, 2022","text":"<p>This is the initial production release that supports the Stretch RE2 (Mitski batch).</p> <ul> <li> <p>This includes the <code>robot_params_RE2V0.py</code> which are the initial robot settings for the RE2 version of the product.</p> </li> <li> <p>It introduces the PrismaticJoint class which consolidates the common Arm and Lift functionality.</p> </li> <li> <p>It changes the units for guarded contact motion from approximate Newtons (suffix _N) to <code>effort_pct</code> - the pecentage [-100,100] of maximum current (A) that a joint should stop at. This change requires RE1 users to migrate their code and robot parameters. See the forum post for more details.</p> </li> <li> <p>It introduces <code>mkdocs.yaml</code> to support serving the repository documenation via MKDocs.</p> </li> </ul> <p>It introduces several new features and fixes several bugs, notably:</p> <ul> <li>Adds <code>wait_until_at_setpoint()</code> to the Arm and Lift classes</li> <li>Adds use of argparse with all tools</li> <li>Moves Robot thread rates to YAML</li> <li>Cleans up the splined trajectory interface, enables velocity controlled splined trajectories for the Dynamixels</li> <li>Flags a warning for users incorrectly setting the homing offset on DXL servos</li> </ul>"},{"location":"stretch-body/CHANGELOG/#034-july-20-2022","title":"0.3.4 - July 20, 2022","text":"<p>Release to add minor features and fix minor bugs:</p> <ul> <li>Add a <code>range_pad_t</code> parameter to allow for padding of hardstops for joint homing</li> <li>Clean up tools and warnings to more consistent and legible #140</li> </ul>"},{"location":"stretch-body/CHANGELOG/#030-june-21-2022","title":"0.3.0 - June 21, 2022","text":"<p>This release moves Stretch Body to use a new parameter management format. This change will require older systems to migrate their parameters to the new format. For systems that haven't yet migrated, Stretch Body will exit with a warning that they must migrate first by running <code>RE1_migrate_params.py</code>. See the forum post for more details.</p> <p>Features:</p> <ul> <li>New param management and RE1.5 support #135</li> </ul>"},{"location":"stretch-body/CHANGELOG/#021-january-6-2022","title":"0.2.1 - January 6, 2022","text":"<p>Release to fix two bugs:</p> <ul> <li>Fix goto commands in the head jog tool #128 - Fixes goto commands in the head jog tool</li> <li>Fix port_handler location #121 - Fixes dxl buffer resetting under a serial communication failure</li> </ul>"},{"location":"stretch-body/CHANGELOG/#020-december-28-2021","title":"0.2.0 - December 28, 2021","text":"<p>This release brings support for waypoint trajectories into master. Support for waypoint trajectories was built up over the last year in the feature/waypoint_trajectories_py3 branch, however, this branch couldn't be merged because the new functionality had flaky performance due to subtle bugs. This branch also attempted to introduce support for Python3 and timestamp synchronization. Support for Python3 and other features were merged in v0.1.0. The remaining features from this branch have been broken into 7 PRs, each targeting a specific device and squashing any previous bugs through functional and performance testing. They are:</p> <ul> <li>Introduce waypoint trajectory RPCs #98</li> <li>Add individual device threading #105</li> <li>Trajectory management classes #106</li> <li>Lift and arm trajectories #110</li> <li>Dynamixel trajectories #113</li> <li>Mobile base trajectories #114</li> <li>Whole body trajectories #115</li> </ul> <p>This release also fixes several bugs. They are:</p> <ul> <li>fixed baud map bug #117</li> <li>fix contact thresh bug #116</li> <li>Fixed EndOfArm tools unittest #104</li> </ul> <p>Testing:</p> <p>Each PR in this release was tested on multiple robots, but was primarily tested on G2, on Python 2.7/Ubuntu 18.04.</p>"},{"location":"stretch-body/CHANGELOG/#0111-october-4-2021","title":"0.1.11 - October 4, 2021","text":"<p>This release gives Stretch Body the ability to support multiple firmware protocols, which at this moment is P0 and P1 firmware. P1 firmware builds on P0 to add waypoint trajectory support and a refactoring of controller functionality into classes. Additionally, this PR fixes how Dynamixel motors calculate velocity from encoder ticks.</p> <p>Features:</p> <ul> <li>Support new firmware protocol (P1) #97</li> </ul> <p>Bugfixes:</p> <ul> <li>Fix how negative dynamixel velocities are calculated #60</li> </ul>"},{"location":"stretch-body/CHANGELOG/#0110-september-23-2021","title":"0.1.10 - September 23, 2021","text":"<p>This release introduces 3 features:</p> <ol> <li>#68 and #95: Introduces two new Jupyter notebooks that can be used to interactively explore working with Stretch. See forum post for details.</li> <li>#94: Introduces Github Action files and docs. Will be enabled in the future to automatically test PRs.</li> <li>#66: Improves the statistics captured on Stretch Body's performance. Will be used to measure improvements to Stretch Body's communication with low level devices.</li> </ol> <p>Bugfixes:</p> <ol> <li>#90: Patches a bug where triggering <code>pimu.trigger_motor_sync()</code> at to high of a rate puts the robot into Runstop mode.</li> <li>#101: Fixes bugs on startup of Dynamixel devices, ensures status is populated on startup of all devices, and add bool to <code>robot.startup()</code></li> </ol> <p>Testing:</p> <p>All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1.</p>"},{"location":"stretch-body/CHANGELOG/#016-august-26-2021","title":"0.1.6 - August 26, 2021","text":"<p>This release introduces these features:</p> <ul> <li>Revised soft limits and collision avoidance</li> <li>Added velocity interfaces for arm and lift</li> </ul> <p>Bugfixes:</p> <ul> <li>Better error handling for DXL servos and their tools</li> <li>Fix bug where dxls maintain previous motion profile</li> </ul>"},{"location":"stretch-body/CHANGELOG/#014-july-20-2021","title":"0.1.4 - July 20, 2021","text":"<p>This release introduces six features and several bugfixes. The features are:</p> <ul> <li>Robot self-collision model and tutorial</li> <li>Add ability to runstop individual DXL servos</li> <li>Merged py2 and py3 tools</li> <li>Added instructions for developing/testing Stretch Body</li> <li>Collision avoidance tutorial</li> <li>Improve realsense visualizer</li> </ul> <p>Bugfixes:</p> <ul> <li>Fix issue where user soft limits overwritten by collision models</li> <li>Pin py2 deps to older version</li> </ul>"},{"location":"stretch-body/CHANGELOG/#010-may-30-2021","title":"0.1.0 - May 30, 2021","text":"<p>This release introduces eight major features and several bugfixes. The features are:</p> <ol> <li>Python param management</li> <li>Configurable baud rate/GroupRead on Dynamixels</li> <li>Pluggable end effector tools</li> <li>Pluggable end effector support in Xbox Teleop</li> <li>Python logging</li> <li>Soft motion limit</li> <li>Self collision management</li> <li>Unit testing framework - as part of each PR</li> </ol> <p>Bugfixes:</p> <ol> <li>Multiturn enable_pos bug (#12) &amp; unit tests</li> <li>Misc bugs</li> <li>Other - as part of the features</li> </ol> <p>Testing:</p> <p>All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1.</p>"},{"location":"stretch-body/body/","title":"Stretch Body","text":"<p>The stretch_body package provides a low level Python API to the Hello Robot Stretch hardware. This package comes pre-installed on Stretch robots. Tutorials for using this package can be found on the docs.</p>"},{"location":"stretch-body/body/#installing","title":"Installing","text":"<p>This package comes pre-installed on Stretch robots. To install or upgrade to a stable Stretch Body for Python3, run:</p> <pre><code>$ python3 -m pip install --upgrade hello-robot-stretch-body\n</code></pre> <p>To install or upgrade to a pre-release of Stretch Body for Python3, run:</p> <pre><code>$ python3 -m pip install --upgrade --pre hello-robot-stretch-body\n</code></pre> <p>Please report feedback on the Issue Tracker or the Forum.</p>"},{"location":"stretch-body/body/#running-tests","title":"Running tests","text":"<p>There are a number of unit, functional, and performance tests within the <code>test/</code> folder, separated into test suites by different files. Suites are separated by a device or functionality within Stretch Body that is being tested.</p> <p>In Python3, run <code>python3 -m unittest test.test_&lt;suite-name&gt;</code>.</p> <p>For example, to run the <code>stretch_body.robot.Robot</code> functional tests, run:</p> <pre><code>$ git clone https://github.com/hello-robot/stretch_body.git\n$ cd stretch_body/body\n$ python3 -m unittest test.test_robot\n</code></pre>"},{"location":"stretch-body/body/#developing","title":"Developing","text":"<p>The source code for Stretch Body resides within the <code>stretch_body/</code> folder. You can install Stretch Body as \"editable\", and directly edit the source code to test changes.</p> <p>In Python3, run <code>python3 -m pip install -e .</code></p> <p>For example, to test changes to <code>stretch_body.robot.Robot</code>, run:</p> <pre><code>$ python3 -m pip uninstall hello-robot-stretch-body # ensure previous Stretch Body installations are removed\n$ git clone https://github.com/hello-robot/stretch_body.git\n$ cd stretch_body/body\n$ python3 -m pip install -e .\n</code></pre> <p>Now, make desired edits to the stretch_body/body/stretch_body/robot.py file. Software using Stretch Body is now using the modified <code>stretch_body.robot.Robot</code> class.</p>"},{"location":"stretch-body/body/#deploying","title":"Deploying","text":"<p>Increment the version number and run the <code>deploy.sh</code> script. Verify the new release is reflected on PyPI.</p>"},{"location":"stretch-body/body/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>This software is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License v3.0 (GNU LGPLv3) as published by the Free Software Foundation.</p> <p>This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License v3.0 (GNU LGPLv3) for more details, which can be found via the following link: </p> <p>https://www.gnu.org/licenses/lgpl-3.0.en.html</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-body/body/test/","title":"Index","text":"<p>Unit tests can be run by, for example:</p> <p>python -m unittest test_dynamixel_XL430</p>"},{"location":"stretch-body/docs/robot_parameters/","title":"Robot Parameters","text":"<p>The behavior of Stretch's hardware and software is tweakable through \"robot parameters\", which are dozens of key-value pairs stored in YAML files and read into Stretch Body when the Python SDK is used. To learn about the parameter system and tweaking the values, follow the Parameter Management Tutorial. In this document, a description of every parameter and its default value is provided.</p>"},{"location":"stretch-body/docs/robot_parameters/#use_asyncio","title":"use_asyncio","text":"<p>A boolean to toggle the use of Asyncio for coordination serial communication with the stepper motors. Using asyncio enables the program flow to move onto other computation while waiting for a reply from a stepper.</p> Parameter Default Value robot.use_asyncio <code>1</code>"},{"location":"stretch-body/docs/robot_parameters/#use_collision_manager","title":"use_collision_manager","text":"<p>A boolean to toggle the use of the collision manager, which prevents self-collisions between the robot's arm and body. For example, this can be helpful for novice users during gamepad teleop because they can figure out the controls without fear of teleoperating the robot into collisions.</p> Parameter Default Value robot.use_collision_manager <code>0</code>"},{"location":"stretch-body/docs/robot_parameters/#params","title":"params","text":"<p>Additional sources of parameters for Stretch Body to import in when organizing the robot's complete set of parameters. This parameter is an array of strings, where each string is an importable Python module. Therefore, it's important that your additional source of parameters is on the \"Python Path\" (i.e. you can import it from Python).</p> <p>The most common reason to set an additional parameter source is to supply parameters relating to a new tool.</p> Parameter Default Value params <code>[]</code>"},{"location":"stretch-body/docs/robot_parameters/#tool","title":"tool","text":"<p>You can swap the tool (or \"end-effector\") on Stretch. There's a repository of 3D printable tools that includes a USB camera tool, a phone holder tool, a dry erase marker tool, and many more. Many of these tools come with a Python class that extends Stretch Body's <code>EndOfArm</code> class to implement functionality specific to the tool. For example, the USB camera tool comes with a <code>ToolUSBCamWrist</code> class, named <code>tool_usbcam_wrist</code>, to take pictures or return a video stream directly from Stretch Body. You can also create your own custom tools and extend <code>EndOfArm</code> to expose functionality custom to your tool. More information on swapping/creating tools is available in the Tool Change Tutorial.</p> <p>After attaching the hardware to Stretch's wrist, set this parameter to your tool class's name to change which tool class is imported into Stretch Body. See also params for supplying parameters needed by your tool class.</p> Parameter Default Value robot.tool <code>'tool_stretch_gripper'</code>"},{"location":"stretch-body/docs/robot_parameters/#use_multiturn","title":"use_multiturn","text":"<p>The Dynamixel joints on the robot have a \"multiturn\" or \"Extended Position Control\" mode, which allows the Dynamixel servo to rotate many revolutions. This is in contrast to regular \"Position Control\" mode, where the servo is limited to 360\u00b0 rotation. The joints where the servos have gear reductions operate in multiturn mode to achieve the desired range. In the other joints, the servo directly controls the position of the joint, so multiple revolutions are not required. More details on the Dynamixel's control modes are available here.</p> Parameter Default Value head_pan.use_multiturn <code>0</code> * head_tilt.use_multiturn <code>0</code> stretch_gripper.use_multiturn <code>1</code> wrist_yaw.use_multiturn <code>1</code> wrist_pitch.use_multiturn <code>0</code> wrist_roll.use_multiturn <code>0</code> <p>* <code>head_pan.use_multiturn</code> is <code>0</code> for most Stretch robots, except for some early RE1s. For those robots, the parameter is set to <code>1</code> in \"stretch_configuration_params.yaml\". </p>"},{"location":"stretch-body/docs/robot_parameters/#i_feedforward-and-i_safety_feedforward","title":"i_feedforward and i_safety_feedforward","text":"<p>Gravity compensation adds a fixed \u2018feedforward\u2019 current to the motor controller to support the lift against gravity. This allows the lift to \u2018float\u2019 when the runstop is enabled, for example. If the feedforward current is too low, the lift will drift downward. If it is too high, it will drift upward. The <code>i_safety_feedforward</code> is the amount of current (A) applied when the motor is in safety mode (eg, runstop enabled). The <code>i_feedforward</code> term is applied when the lift is in normal operation. Generally the two parameters will be identical.</p> <p>There\u2019s a simple tool to calibrate these values. <code>REx_calibrate_gravity_comp.py</code> will move the lift to a few positions, sampled the applied motor currents, and update your gravity compensation parameters. More details can be found in this knowledge base post.</p>"},{"location":"stretch-body/docs/robot_parameters/#stretch_gripper_overload","title":"stretch_gripper_overload","text":"<p>A boolean to toggle whether a sentry monitors the gripper servo for risk of \"overloading\", a hardware protection state the servo goes into when it cannot provide the torque being asked for, and backs off the commands to reduce the amount of torque being asked for. In effect, this sentry enables the gripper to keep its grip on objects without overloading.</p> Parameter Default Value robot_sentry.stretch_gripper_overload <code>1</code>"},{"location":"stretch-body/docs/robot_parameters/#base_max_velocity","title":"base_max_velocity","text":"<p>A boolean to toggle whether a sentry monitors the robot's center of mass and limits max allowable speed of the mobile base to prevent unstable behavior resulting from fast motion paired with a high center of mass. Disabling this safety feature means the base will not limit its speed and will travel at the speed you've commanded it.</p> Parameter Default Value robot_sentry.base_max_velocity <code>1</code>"},{"location":"stretch-body/docs/robot_parameters/#parameters-for-command-line-tools","title":"Parameters for Command Line Tools","text":""},{"location":"stretch-body/docs/robot_parameters/#show_sw_exc","title":"show_sw_exc","text":"<p>In the <code>stretch_robot_system_check.py</code> tool, the \"Checking Software\" section is wrapped in a try/except because of the experimental nature of the code. The tool is introspecting the system and relying on resources/files to be available in specific places with specific formats. Since the tool's output needs to be easily understood, the error or exception cannot be shown (hence the try/except block). This parameter allows the tool to raise the exception and display it in the terminal. This is useful for debugging purposes.</p> Parameter Default Value system_check.show_sw_exc False"},{"location":"stretch-body/docs/is_thread_safe/","title":"Is Stretch Body thread safe?","text":"<p>The short answer: yes, if you don't use asyncio. This tutorial builds up to a few kinds of threading experiments that prove this, and hopefully will serve as a launching point for further exploration.</p> <p>First, let's define what it means to be thread-safe. If we create an instance of the <code>Robot</code> class, we'd like to know if we can safely use it in Python threads. Using the <code>Robot</code>, there's two ways we interact with the underlying hardware: reading sensor data and writing motion commands. Our experiments will cover:</p> <ol> <li>Single threaded reading and writing</li> <li>Single writer, but multithreaded readers</li> <li>Multithreading for both reading and writing</li> </ol> <p>Having multiple readers, even if only one thread can send commands, would be useful for visualization purposes.</p>"},{"location":"stretch-body/docs/is_thread_safe/#writing","title":"Writing","text":"<p>Let's create an instance of the <code>Robot</code>:</p> <pre><code>import stretch_body.robot\n\nr = stretch_body.robot.Robot()\nassert r.startup()\nassert r.is_homed()\n</code></pre> <p>Then create a \"writer\", that will send motion commands:</p> <pre><code>import random\n\ndef write():\n    r.lift.move_to(random.uniform(0.3, 1.1))\n    r.arm.move_to(random.uniform(0.0, 0.4))\n    r.end_of_arm.move_to('wrist_roll', random.uniform(-1.0, 1.0))\n    r.head.move_to('head_pan', random.uniform(-1.0, 1.0))\n    r.push_command()\n</code></pre> <p>Then let's define a writing-only runner, that sends motion commands at 10hz:</p> <pre><code>def wo_runner():\n    \"\"\"write-only runner\"\"\"\n    while True:\n        write()\n        time.sleep(0.1)\n</code></pre>"},{"location":"stretch-body/docs/is_thread_safe/#results","title":"Results","text":"<p>We can run <code>wo_runner()</code> in the main thread:</p> <pre><code>wo_runner()\n</code></pre> <p>or launch it in a thread:</p> <pre><code>import threading\nthreading.Thread(target=wo_runner).start()\n</code></pre> <p>When AsyncIO is turned on, the program exits with no error. Likely an exception is being thrown in the thread, but isn't being surfaced to the user. Surfacing this exception is the first step towards making this library thread-safe with AsyncIO.</p> <p>However, if we disable AsyncIO, we'll see the robot start to jitter and it tracks random motion goals.</p> <p>Next, we can run multiple threads with no problem:</p> <pre><code>import threading\n\nthreading.Thread(target=wo_runner).start()\nthreading.Thread(target=wo_runner).start()\n</code></pre>"},{"location":"stretch-body/docs/is_thread_safe/#high-low-competition","title":"High-Low Competition","text":"<p>In the previous experiment, it's hard to tell the two-thread jitter apart from the one-thread jitter. So let's write two competing writers:</p> <pre><code>def low_write():\n    r.lift.move_to(0.3)\n    r.arm.move_to(0.0)\n    r.end_of_arm.move_to('wrist_roll', -1.0)\n    r.head.move_to('head_pan', -1.0)\n    r.push_command()\n\ndef high_write():\n    r.lift.move_to(1.1)\n    r.arm.move_to(0.4)\n    r.end_of_arm.move_to('wrist_roll', 1.0)\n    r.head.move_to('head_pan', 1.0)\n    r.push_command()\n</code></pre> <p>and create runners for each:</p> <pre><code>def low_rw_runner():\n    \"\"\"read-and-write runner\"\"\"\n    pl=NBPlot()\n    while True:\n        low_write()\n        read(pl)\n        time.sleep(0.1)\n\ndef high_rw_runner():\n    \"\"\"read-and-write runner\"\"\"\n    pl=NBPlot()\n    s = time.time()\n    while time.time() - s &lt; 10:\n        high_write()\n        read(pl)\n        time.sleep(0.1)\n</code></pre> <p>Then, launch them with:</p> <pre><code>if __name__ == \"__main__\":\n    import threading\n    threading.Thread(target=low_rw_runner).start()\n    threading.Thread(target=high_rw_runner).start()\n</code></pre> <p>Running this, we'll see the threads compete with one another, and the joints will oscillate between their limits, depending on which thread is given more processing time by the operating system at that moment. After 10 seconds, the high writer dies, and the robot moves to its lower joint limits.</p>"},{"location":"stretch-body/docs/is_thread_safe/#reading","title":"Reading","text":"<p>Let's create a \"reader\", that will plot the joint states for the four joints:</p> <pre><code>def read(plotter):\n    lift_pos = r.lift.status['pos']\n    arm_pos = r.arm.status['pos']\n    roll_pos = r.end_of_arm.get_joint('wrist_roll').status['pos']\n    pan_pos = r.head.get_joint('head_pan').status['pos']\n    plotter.plot(lift_pos, arm_pos, roll_pos, pan_pos)\n</code></pre> <p>Then let's define a reading-only runner, that will plot joint state at 10hz:</p> <pre><code>def ro_runner():\n    \"\"\"read-only runner\"\"\"\n    pl=NBPlot()\n    while True:\n        read(pl)\n        time.sleep(0.1)\n</code></pre> <p><code>NBPlot</code> is defined within <code>multiprocessing_plotter.py</code> in this folder. It enables Matplotlib to plot within threads.</p>"},{"location":"stretch-body/docs/is_thread_safe/#results_1","title":"Results","text":"<p>We can run <code>ro_runner()</code> in the main thread without any issues, but if we launch <code>ro_runner()</code> in a thread:</p> <pre><code>import threading\nthreading.Thread(target=ro_runner).start()\n</code></pre> <p>Once again, the program exits with no error if we use asyncio and works fine if we disable it. Surfacing the underlying error will be helpful to debug.</p>"},{"location":"stretch-body/docs/is_thread_safe/#reading-and-writing","title":"Reading and Writing","text":"<p>For completion, we have <code>wr_runner()</code>:</p> <pre><code>def rw_runner():\n    \"\"\"read-and-write runner\"\"\"\n    pl=NBPlot()\n    while True:\n        write()\n        read(pl)\n        time.sleep(0.1)\n</code></pre> <p>We run it with:</p> <pre><code>import threading\n\nthreading.Thread(target=rw_runner).start()\nthreading.Thread(target=rw_runner).start()\n</code></pre> <p>And we'll see two live plots pop up and show the robot tracking the motion commands being sent by the two threads.</p> <p></p>"},{"location":"stretch-body/docs/is_thread_safe/#code","title":"Code","text":"<p>The code for this experiment lives in two files within this same directory:</p> <ul> <li><code>read_write.py</code>: which defines the reading and writing runners</li> <li><code>low_high_competition.py</code>: which defines the low &amp; high writers that compete in two threads</li> <li><code>multiprocessing_plotter.py</code>: which is a helper library for plotting within Python threads. The logic does not impact these experiments.</li> </ul>"},{"location":"stretch-body/docs/is_thread_safe/#takeaway","title":"Takeaway","text":"<p>This document captures a few experiments around Stretch Body's thread safety. It shows that the library is currently thread safe only if you disable AsyncIO, and highlights some possible ways to start debugging how to make the library thread-safe with AsyncIO.</p>"},{"location":"stretch-body/tools/","title":"Stretch Body Command Line Tools","text":"<p>This package provides Python tools that work with the Hello Robot Stretch Body package. These tools perform common tasks when working with Stretch (e.g. homing and stowing), and serve as tutorial code for working on various parts of the robot. This package comes pre-installed on Stretch robots. A tutorial for using this package can be found on the docs.</p>"},{"location":"stretch-body/tools/#installing","title":"Installing","text":"<p>This package comes pre-installed on Stretch robots. To install or upgrade to a stable Stretch Body Command Line Tools for Python3, run:</p> <pre><code>$ python3 -m pip install --upgrade hello-robot-stretch-body-tools\n</code></pre> <p>To install or upgrade to a pre-release of the Command Line Tools for Python3, run:</p> <pre><code>$ python3 -m pip install --upgrade --pre hello-robot-stretch-body-tools\n</code></pre> <p>Please report feedback on the Issue Tracker or the Forum.</p>"},{"location":"stretch-body/tools/#usage","title":"Usage","text":"<p>All of the command-line tools reside within the <code>bin/</code> folder. When this package is installed, they are accessible from anywhere as command-line tools. For example, to perform a robot system check, run:</p> <pre><code>$ stretch_robot_system_check.py\n</code></pre> <p>For more info on these tools, see the tutorial.</p>"},{"location":"stretch-body/tools/#developing","title":"Developing","text":"<p>The source code for the command-line tools resides within the <code>bin/</code> folder. You can install the tools package as \"editable\", and directly edit the source code to test changes.</p> <p>In Python3, run <code>python3 -m pip install -e .</code></p> <p>For example, to test changes to the  <code>stretch_robot_home.py</code> script, run</p> <pre><code>$ python3 -m pip uninstall hello-robot-stretch-body-tools # ensure previous Stretch Body Tools installations are removed\n$ git clone https://github.com/hello-robot/stretch_body.git\n$ cd stretch_body/tools\n$ python3 -m pip install -e .\n</code></pre> <p>Now, make desired edits to the stretch_robot_home.py file. Executing the script on the command-line will now run your modified version.</p>"},{"location":"stretch-body/tools/#deploying","title":"Deploying","text":"<p>Increment the version number and run the <code>deploy.sh</code> script. Verify the new release is reflected on PyPI.</p>"},{"location":"stretch-body/tools/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"}]}