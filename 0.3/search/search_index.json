{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"Welcome","text":"<p>This is your jumping off point for Stretch tutorials, hardware manuals, and APIs. </p> <p>Please contact us at support@hello-robot.com if you don't find what you're looking for. We wish you success in your development journey with Stretch!</p>"},{"location":"#safety","title":"Safety","text":"<p>WARNING: The Stretch robot can potentially be dangerous if used without caution. All users must take the time to learn safe and best practices in operating Stretch prior to using the robot.</p> Resource Description Stretch Safety Guide Safety guide for users of the Stretch RE1 Stretch Best Practices - Powered Off Video How to work with Stretch when its power is off Stretch Best Practices - Powered On Video How to work with Stretch when its power is on"},{"location":"#quick-start","title":"Quick Start","text":"<p>New to Stretch? We recommend following the below resources to get started.</p> Resource Description Stretch Unboxing Video How to unpack your new Stretch Stretch Quick Start Guide Getting started  - Xbox Teleoperation Demo and beyond Stretch Troubleshooting Guide Solutions to common issues Stretch Network Setup Guide to network setup for working with Stretch untethered"},{"location":"#tutorials","title":"Tutorials","text":"Tutorial Track Description Getting to Know Stretch Everything a new user of Stretch needs to get started Stretch Body Python SDK Learn how to program Stretch using its low level Python interface ROS Learn how to program Stretch using its ROS interface ROS 2 (Beta) Learn how to program Stretch using its ROS 2 interface Stretch Tool Share Learn how to update the end of arm tool hardware"},{"location":"#hardware","title":"Hardware","text":"Stretch 2Stretch RE1 Guide Description Safety Guide Safety guide for users of the Stretch Battery Maintenance Guide Guide to care for and charge the Stretch RE2 Batteries Hardware Guide Specification and functional description of the Stretch RE2 Hardware Dex Wrist Guide Installing, configuring, and working with the Stretch RE2 Dex Wrist Guide Description Safety Guide Safety guide for users of the Stretch Battery Maintenance Guide Guide to care for and charge the Stretch RE1 Batteries Hardware Guide Specification and functional description of the Stretch RE1 Hardware Dex Wrist Guide Installing, configuring, and working with the Stretch RE1 Dex Wrist"},{"location":"#software","title":"Software","text":"Repository Description Stretch Body Python SDK that allows you to interact with the hardware. Stretch ROS ROS related code for Stretch. Stretch ROS 2 ROS 2 related code for Stretch. Stretch Factory Factory Python tools for debug, testing and calibration. Stretch Firmware Arduino code for the firmware that drives Stretch. Stretch Tool Share Hardware extensions to extend the capabilities of Stretch. Stretch Install Installation scripts for Stretch."},{"location":"#where-to-find-things","title":"Where to Find Things","text":"<p>All of the documentation is searchable and accessible via the navigation menu on this site. Alternatively, you can view the markdown hosted on the Hello Robot GitHub portal.</p> <p>In addition, it is worth spending some time on the following sites:</p> Resource Description Stretch Community Repository for community shared code Stretch Forum Discourse User Forum"},{"location":"#version","title":"Version","text":"<p>This is version 0.2 of the Stretch User Documentation. It is written with the following system configuration in mind:</p> Descriptor Version Model Stretch RE1 or Stretch 2 OS Ubuntu 20.04 ROS ROS Noetic and ROS 2 Galactic Python Python3 Stretch Body &gt;=0.4 <p>You can access prior documentation suitable for older configurations (eg Ubuntu 18.04, ROS Melodic) here.</p>"},{"location":"#license","title":"License","text":"<p>This documentation is only to be used for an authentic Stretch robot produced and sold by Hello Robot Inc. </p> <p>All Hello Robot documentation and related materials are released under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains documentation exclusively for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license (the \"License\"); you may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>https://creativecommons.org/licenses/by-nd/4.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Patents pending and trademark rights cover the Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\"</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"repos_overview/","title":"Repositories Overview","text":"<p>This is the starting off point for exploring the many code repositories that power Stretch. The table below briefly covers the purpose of each repository.</p>"},{"location":"repos_overview/#repo-descriptions","title":"Repo Descriptions","text":"Repository Description 1 Stretch Body Python SDK that allows you to interact with the hardware. 2 Stretch ROS ROS related code for Stretch. 3 Stretch ROS2 ROS 2 related code for Stretch. 4 Stretch Factory Factory Python tools for debug, testing and calibration. 5 Stretch Firmware Arduino code for the firmware that drives Stretch. 5 Stretch Tool Share Hardware extensions to extend the capabilities of Stretch. 6 Stretch Install Installation scripts for Stretch. 7 Stretch Web Interface Code that allows remote teleoperation through a browser."},{"location":"repos_overview/#contributing","title":"Contributing","text":"<p>We welcome code contributions from the community to improve this documentation or the software stack. To report an issue or contribute to a Stretch repo, please visit the repo's Github page and either file an issue or fork the repository of interest and raise a PR.</p>"},{"location":"repos_overview/#license","title":"License","text":"<p>This documentation is only to be used for an authentic Stretch robot produced and sold by Hello Robot Inc.</p> <p>All Hello Robot documentation and related materials are released under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-body/","title":"Overview","text":""},{"location":"stretch-body/#overview","title":"Overview","text":"<p>The stretch_body repository includes Python packages that allow a developer to interact with the hardware of the Stretch robots. These packages are:</p> Package Description hello-robot-stretch-body Python library to interface with Stretch hello-robot-stretch-body-tools Useful commandline tools for using Stretch <p>Python3 version of packages can be installed by:</p> <pre><code>pip3 install -U hello-robot-stretch-body\npip3 install -U hello-robot-stretch-body-tools\n</code></pre> <p>See docs.hello-robot.com for documentation on using Stretch. In particular, see the Stretch Body Tutorials for additional information on using these packages.</p>"},{"location":"stretch-body/#testing-and-development","title":"Testing and Development","text":"<p>See Stretch Body's README and Stretch Body Commandline Tool's README for information on testing/developing these packages.</p>"},{"location":"stretch-body/#changelog","title":"Changelog","text":"<p>See the changelog for information on what changed with each release.</p>"},{"location":"stretch-body/#license","title":"License","text":"<p>Each subdirectory contains a LICENSE.md file that applies to the directory's contents. This software is intended for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc. For further information including inquiries about dual licensing, please contact Hello Robot Inc.</p>  All materials are Copyright 2022-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-body/CHANGELOG/","title":"Changelog","text":"<p>The changes between releases of Stretch Body are documented here.</p>"},{"location":"stretch-body/CHANGELOG/#072-january-26-2023","title":"0.7.2 - January 26, 2023","text":"<ul> <li>Fix for wait_command() timeout not being respected (issue #255)</li> <li>Improvements to the System Check tool, including:<ul> <li>Check if hello stepper self recognize correctly</li> <li>Check if OV9782 camera seen for Stretch 3s</li> <li>Update pip recommendations</li> <li>Hide verbose printout</li> <li>Add battery section to hardware check</li> </ul> </li> </ul>"},{"location":"stretch-body/CHANGELOG/#070-january-25-2023","title":"0.7.0 - January 25, 2023","text":"<ul> <li>Introduces P5 firmware protocol support</li> <li>Revamped system check tool</li> <li>Revamped wrist/tool management system</li> <li>Started development on a self collision avoidance system</li> <li>Utilities and CLI to introspect UVC cameras</li> <li>Support for Dex Wrist 3</li> </ul>"},{"location":"stretch-body/CHANGELOG/#068-december-7-2023","title":"0.6.8 - December 7, 2023","text":"<ul> <li>Deprecate <code>robot.is_calibrated()</code> and introduce <code>robot.is_homed()</code> to match homing nomenclature used elsewhere (details)</li> <li>Added a <code>robot.wait_command()</code> method to easily block process execution until motion is completed (details)</li> <li>Added Dex Wrist URDF visualization to the <code>stretch_robot_urdf_visualizer.py</code> tool</li> <li>Automatic checking for updates in background while homing + user notification about out-of-date software in <code>stretch_robot_system_check.py</code> (details)</li> <li>Add mutex locking on Stretch Body + introduce <code>stretch_free_robot_process.py</code> tool (details)</li> </ul>"},{"location":"stretch-body/CHANGELOG/#062-september-11-2023","title":"0.6.2 - September 11, 2023","text":"<ul> <li>Introduces the new set of Gamepad modules to teleop stretch using the provided physical gamepad or any other UI input devices. This gamepad controller primarily uses velocity control.</li> <li>The stretch_gamepad_teleop.py tool is added and we are deprecating the old position control based stretch_xbox_teleop_controller.py</li> <li>The new controller now has a precision mode, allowing users to make fine-grain robot motions.</li> </ul>"},{"location":"stretch-body/CHANGELOG/#060-october-7-2023","title":"0.6.0 - October 7, 2023","text":"<p>This is the initial productioon release that supports Prince batch. - Introduces the P4 firmware protocol support - Adds Prince batch params and new pimu IMU support  </p>"},{"location":"stretch-body/CHANGELOG/#050-july-11-2023","title":"0.5.0 - July 11, 2023","text":"<ul> <li>Introduces the use_asyncio  mode that will enable using asynchronous IO call methods to perform robot push/pull commands and RPC transactions with the help of <code>asyncio</code> to speed up the USB device communications. This mode can be toggled back to use the regular non-async IO calls by changing the stretch params <code>use_asyncio</code>.</li> <li>By default, the asyncio mode is enabled.</li> <li>Adds the P3 protocol support for all the Arduino devices (stepper, pimu, wacc).</li> <li>Also, for the asynchronous transport layer to work, the devices firmware will need to support V1 transport protocol that is supported with firmware only above v0.5.0p3 for all hello-* arduino devices. For older firmwares, the <code>async_io</code> would be disabled automatically</li> <li>The  DXLStatusThread of robot class is now separated into two threads: DXLHeadStatusThread and DXLEndofArmStatusThread (Both threads run at 15Hz).</li> <li>Now the robot monitor, trace, sentry, and collision manager handles stepping is moved out of the NonDXLStatusThread. Instead, it is moved to a new separate thread called SystemMonitorThread is used to step these handles, which also runs at 25Hz.</li> <li>The Robot class can be <code>startup()</code> with the optional parameters to turn off some threads to save system resources.</li> <li>Added status_aux pull status RPCs feature for all the devices. <code>motor_sync_cnt</code> and <code>motor_sync_queues</code> status messages are populated with AUX pull status</li> <li>Now the RPC transactions queues is deprecated, instead <code>do_pull_transaction_vX()</code> and <code>do_push_transaction_vX()</code> methods are used from the SyncTransactionHandler.</li> <li>Asynchournous RPC transactions are handled by AsyncTransactionHandler that create analougus async methods to the one present in SyncTransactionHandler.</li> </ul>"},{"location":"stretch-body/CHANGELOG/#048-sept-14-2022","title":"0.4.8 - Sept 14, 2022","text":"<p>This is the initial production release that supports the Stretch RE2 (Mitski batch).</p> <ul> <li> <p>This includes the <code>robot_params_RE2V0.py</code> which are the initial robot settings for the RE2 version of the product.</p> </li> <li> <p>It introduces the PrismaticJoint class which consolidates the common Arm and Lift functionality.</p> </li> <li> <p>It changes the units for guarded contact motion from approximate Newtons (suffix _N) to <code>effort_pct</code> - the pecentage [-100,100] of maximum current (A) that a joint should stop at. This change requires RE1 users to migrate their code and robot parameters. See the forum post for more details.</p> </li> <li> <p>It introduces <code>mkdocs.yaml</code> to support serving the repository documenation via MKDocs.</p> </li> </ul> <p>It introduces several new features and fixes several bugs, notably:</p> <ul> <li>Adds <code>wait_until_at_setpoint()</code> to the Arm and Lift classes</li> <li>Adds use of argparse with all tools</li> <li>Moves Robot thread rates to YAML</li> <li>Cleans up the splined trajectory interface, enables velocity controlled splined trajectories for the Dynamixels</li> <li>Flags a warning for users incorrectly setting the homing offset on DXL servos</li> </ul>"},{"location":"stretch-body/CHANGELOG/#034-july-20-2022","title":"0.3.4 - July 20, 2022","text":"<p>Release to add minor features and fix minor bugs:</p> <ul> <li>Add a <code>range_pad_t</code> parameter to allow for padding of hardstops for joint homing</li> <li>Clean up tools and warnings to more consistent and legible #140</li> </ul>"},{"location":"stretch-body/CHANGELOG/#030-june-21-2022","title":"0.3.0 - June 21, 2022","text":"<p>This release moves Stretch Body to use a new parameter management format. This change will require older systems to migrate their parameters to the new format. For systems that haven't yet migrated, Stretch Body will exit with a warning that they must migrate first by running <code>RE1_migrate_params.py</code>. See the forum post for more details.</p> <p>Features:</p> <ul> <li>New param management and RE1.5 support #135</li> </ul>"},{"location":"stretch-body/CHANGELOG/#021-january-6-2022","title":"0.2.1 - January 6, 2022","text":"<p>Release to fix two bugs:</p> <ul> <li>Fix goto commands in the head jog tool #128 - Fixes goto commands in the head jog tool</li> <li>Fix port_handler location #121 - Fixes dxl buffer resetting under a serial communication failure</li> </ul>"},{"location":"stretch-body/CHANGELOG/#020-december-28-2021","title":"0.2.0 - December 28, 2021","text":"<p>This release brings support for waypoint trajectories into master. Support for waypoint trajectories was built up over the last year in the feature/waypoint_trajectories_py3 branch, however, this branch couldn't be merged because the new functionality had flaky performance due to subtle bugs. This branch also attempted to introduce support for Python3 and timestamp synchronization. Support for Python3 and other features were merged in v0.1.0. The remaining features from this branch have been broken into 7 PRs, each targeting a specific device and squashing any previous bugs through functional and performance testing. They are:</p> <ul> <li>Introduce waypoint trajectory RPCs #98</li> <li>Add individual device threading #105</li> <li>Trajectory management classes #106</li> <li>Lift and arm trajectories #110</li> <li>Dynamixel trajectories #113</li> <li>Mobile base trajectories #114</li> <li>Whole body trajectories #115</li> </ul> <p>This release also fixes several bugs. They are:</p> <ul> <li>fixed baud map bug #117</li> <li>fix contact thresh bug #116</li> <li>Fixed EndOfArm tools unittest #104</li> </ul> <p>Testing:</p> <p>Each PR in this release was tested on multiple robots, but was primarily tested on G2, on Python 2.7/Ubuntu 18.04.</p>"},{"location":"stretch-body/CHANGELOG/#0111-october-4-2021","title":"0.1.11 - October 4, 2021","text":"<p>This release gives Stretch Body the ability to support multiple firmware protocols, which at this moment is P0 and P1 firmware. P1 firmware builds on P0 to add waypoint trajectory support and a refactoring of controller functionality into classes. Additionally, this PR fixes how Dynamixel motors calculate velocity from encoder ticks.</p> <p>Features:</p> <ul> <li>Support new firmware protocol (P1) #97</li> </ul> <p>Bugfixes:</p> <ul> <li>Fix how negative dynamixel velocities are calculated #60</li> </ul>"},{"location":"stretch-body/CHANGELOG/#0110-september-23-2021","title":"0.1.10 - September 23, 2021","text":"<p>This release introduces 3 features:</p> <ol> <li>#68 and #95: Introduces two new Jupyter notebooks that can be used to interactively explore working with Stretch. See forum post for details.</li> <li>#94: Introduces Github Action files and docs. Will be enabled in the future to automatically test PRs.</li> <li>#66: Improves the statistics captured on Stretch Body's performance. Will be used to measure improvements to Stretch Body's communication with low level devices.</li> </ol> <p>Bugfixes:</p> <ol> <li>#90: Patches a bug where triggering <code>pimu.trigger_motor_sync()</code> at to high of a rate puts the robot into Runstop mode.</li> <li>#101: Fixes bugs on startup of Dynamixel devices, ensures status is populated on startup of all devices, and add bool to <code>robot.startup()</code></li> </ol> <p>Testing:</p> <p>All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1.</p>"},{"location":"stretch-body/CHANGELOG/#016-august-26-2021","title":"0.1.6 - August 26, 2021","text":"<p>This release introduces these features:</p> <ul> <li>Revised soft limits and collision avoidance</li> <li>Added velocity interfaces for arm and lift</li> </ul> <p>Bugfixes:</p> <ul> <li>Better error handling for DXL servos and their tools</li> <li>Fix bug where dxls maintain previous motion profile</li> </ul>"},{"location":"stretch-body/CHANGELOG/#014-july-20-2021","title":"0.1.4 - July 20, 2021","text":"<p>This release introduces six features and several bugfixes. The features are:</p> <ul> <li>Robot self-collision model and tutorial</li> <li>Add ability to runstop individual DXL servos</li> <li>Merged py2 and py3 tools</li> <li>Added instructions for developing/testing Stretch Body</li> <li>Collision avoidance tutorial</li> <li>Improve realsense visualizer</li> </ul> <p>Bugfixes:</p> <ul> <li>Fix issue where user soft limits overwritten by collision models</li> <li>Pin py2 deps to older version</li> </ul>"},{"location":"stretch-body/CHANGELOG/#010-may-30-2021","title":"0.1.0 - May 30, 2021","text":"<p>This release introduces eight major features and several bugfixes. The features are:</p> <ol> <li>Python param management</li> <li>Configurable baud rate/GroupRead on Dynamixels</li> <li>Pluggable end effector tools</li> <li>Pluggable end effector support in Xbox Teleop</li> <li>Python logging</li> <li>Soft motion limit</li> <li>Self collision management</li> <li>Unit testing framework - as part of each PR</li> </ol> <p>Bugfixes:</p> <ol> <li>Multiturn enable_pos bug (#12) &amp; unit tests</li> <li>Misc bugs</li> <li>Other - as part of the features</li> </ol> <p>Testing:</p> <p>All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1.</p>"},{"location":"stretch-body/body/","title":"Stretch Body","text":"<p>The stretch_body package provides a low level Python API to the Hello Robot Stretch hardware. This package comes pre-installed on Stretch robots. Tutorials for using this package can be found on the docs.</p>"},{"location":"stretch-body/body/#installing","title":"Installing","text":"<p>This package comes pre-installed on Stretch robots. To install or upgrade to a stable Stretch Body for Python3, run:</p> <pre><code>$ python3 -m pip install --upgrade hello-robot-stretch-body\n</code></pre> <p>To install or upgrade to a pre-release of Stretch Body for Python3, run:</p> <pre><code>$ python3 -m pip install --upgrade --pre hello-robot-stretch-body\n</code></pre> <p>Please report feedback on the Issue Tracker or the Forum.</p>"},{"location":"stretch-body/body/#running-tests","title":"Running tests","text":"<p>There are a number of unit, functional, and performance tests within the <code>test/</code> folder, separated into test suites by different files. Suites are separated by a device or functionality within Stretch Body that is being tested.</p> <p>In Python3, run <code>python3 -m unittest test.test_&lt;suite-name&gt;</code>.</p> <p>For example, to run the <code>stretch_body.robot.Robot</code> functional tests, run:</p> <pre><code>$ git clone https://github.com/hello-robot/stretch_body.git\n$ cd stretch_body/body\n$ python3 -m unittest test.test_robot\n</code></pre>"},{"location":"stretch-body/body/#developing","title":"Developing","text":"<p>The source code for Stretch Body resides within the <code>stretch_body/</code> folder. You can install Stretch Body as \"editable\", and directly edit the source code to test changes.</p> <p>In Python3, run <code>python3 -m pip install -e .</code></p> <p>For example, to test changes to <code>stretch_body.robot.Robot</code>, run:</p> <pre><code>$ python3 -m pip uninstall hello-robot-stretch-body # ensure previous Stretch Body installations are removed\n$ git clone https://github.com/hello-robot/stretch_body.git\n$ cd stretch_body/body\n$ python3 -m pip install -e .\n</code></pre> <p>Now, make desired edits to the stretch_body/body/stretch_body/robot.py file. Software using Stretch Body is now using the modified <code>stretch_body.robot.Robot</code> class.</p>"},{"location":"stretch-body/body/#deploying","title":"Deploying","text":"<p>Increment the version number and run the <code>deploy.sh</code> script. Verify the new release is reflected on PyPI.</p>"},{"location":"stretch-body/body/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>This software is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License v3.0 (GNU LGPLv3) as published by the Free Software Foundation.</p> <p>This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License v3.0 (GNU LGPLv3) for more details, which can be found via the following link: </p> <p>https://www.gnu.org/licenses/lgpl-3.0.en.html</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-body/body/test/","title":"Index","text":"<p>Unit tests can be run by, for example:</p> <p>python -m unittest test_dynamixel_XL430</p>"},{"location":"stretch-body/docs/robot_parameters/","title":"Robot Parameters","text":"<p>The behavior of Stretch's hardware and software is tweakable through \"robot parameters\", which are dozens of key-value pairs stored in YAML files and read into Stretch Body when the Python SDK is used. To learn about the parameter system and tweaking the values, follow the Parameter Management Tutorial. In this document, a description of every parameter and its default value is provided.</p>"},{"location":"stretch-body/docs/robot_parameters/#params","title":"params","text":"<p>Additional sources of parameters for Stretch Body to import in when organizing the robot's complete set of parameters. This parameter is an array of strings, where each string is an importable Python module. Therefore, it's important that your additional source of parameters is on the \"Python Path\" (i.e. you can import it from Python).</p> <p>The most common reason to set an additional parameter source is to supply parameters relating to a new tool.</p> Parameter Default Value params <code>[]</code>"},{"location":"stretch-body/docs/robot_parameters/#tool","title":"tool","text":"<p>You can swap the tool (or \"end-effector\") on Stretch. There's a repository of 3D printable tools that includes a USB camera tool, a phone holder tool, a dry erase marker tool, and many more. Many of these tools come with a Python class that extends Stretch Body's <code>EndOfArm</code> class to implement functionality specific to the tool. For example, the USB camera tool comes with a <code>ToolUSBCamWrist</code> class, named <code>tool_usbcam_wrist</code>, to take pictures or return a video stream directly from Stretch Body. You can also create your own custom tools and extend <code>EndOfArm</code> to expose functionality custom to your tool. More information on swapping/creating tools is available in the Tool Change Tutorial.</p> <p>After attaching the hardware to Stretch's wrist, set this parameter to your tool class's name to change which tool class is imported into Stretch Body. See also params for supplying parameters needed by your tool class.</p> Parameter Default Value robot.tool <code>'tool_stretch_gripper'</code>"},{"location":"stretch-body/docs/robot_parameters/#use_multiturn","title":"use_multiturn","text":"<p>The Dynamixel joints on the robot have a \"multiturn\" or \"Extended Position Control\" mode, which allows the Dynamixel servo to rotate many revolutions. This is in contrast to regular \"Position Control\" mode, where the servo is limited to 360\u00b0 rotation. The joints where the servos have gear reductions operate in multiturn mode to achieve the desired range. In the other joints, the servo directly controls the position of the joint, so multiple revolutions are not required. More details on the Dynamixel's control modes are available here.</p> Parameter Default Value head_pan.use_multiturn <code>0</code> * head_tilt.use_multiturn <code>0</code> stretch_gripper.use_multiturn <code>1</code> wrist_yaw.use_multiturn <code>1</code> wrist_pitch.use_multiturn <code>0</code> wrist_roll.use_multiturn <code>0</code> <p>* <code>head_pan.use_multiturn</code> is <code>0</code> for most Stretch robots, except for some early RE1s. For those robots, the parameter is set to <code>1</code> in \"stretch_configuration_params.yaml\". </p>"},{"location":"stretch-body/docs/robot_parameters/#parameters-for-command-line-tools","title":"Parameters for Command Line Tools","text":""},{"location":"stretch-body/docs/robot_parameters/#show_sw_exc","title":"show_sw_exc","text":"<p>In the <code>stretch_robot_system_check.py</code> tool, the \"Checking Software\" section is wrapped in a try/except because of the experimental nature of the code. The tool is introspecting the system and relying on resources/files to be available in specific places with specific formats. Since the tool's output needs to be easily understood, the error or exception cannot be shown (hence the try/except block). This parameter allows the tool to raise the exception and display it in the terminal. This is useful for debugging purposes.</p> Parameter Default Value system_check.show_sw_exc False"},{"location":"stretch-body/tools/","title":"Stretch Body Command Line Tools","text":"<p>This package provides Python tools that work with the Hello Robot Stretch Body package. These tools perform common tasks when working with Stretch (e.g. homing and stowing), and serve as tutorial code for working on various parts of the robot. This package comes pre-installed on Stretch robots. A tutorial for using this package can be found on the docs.</p>"},{"location":"stretch-body/tools/#installing","title":"Installing","text":"<p>This package comes pre-installed on Stretch robots. To install or upgrade to a stable Stretch Body Command Line Tools for Python3, run:</p> <pre><code>$ python3 -m pip install --upgrade hello-robot-stretch-body-tools\n</code></pre> <p>To install or upgrade to a pre-release of the Command Line Tools for Python3, run:</p> <pre><code>$ python3 -m pip install --upgrade --pre hello-robot-stretch-body-tools\n</code></pre> <p>Please report feedback on the Issue Tracker or the Forum.</p>"},{"location":"stretch-body/tools/#usage","title":"Usage","text":"<p>All of the command-line tools reside within the <code>bin/</code> folder. When this package is installed, they are accessible from anywhere as command-line tools. For example, to perform a robot system check, run:</p> <pre><code>$ stretch_robot_system_check.py\n</code></pre> <p>For more info on these tools, see the tutorial.</p>"},{"location":"stretch-body/tools/#developing","title":"Developing","text":"<p>The source code for the command-line tools resides within the <code>bin/</code> folder. You can install the tools package as \"editable\", and directly edit the source code to test changes.</p> <p>In Python3, run <code>python3 -m pip install -e .</code></p> <p>For example, to test changes to the  <code>stretch_robot_home.py</code> script, run</p> <pre><code>$ python3 -m pip uninstall hello-robot-stretch-body-tools # ensure previous Stretch Body Tools installations are removed\n$ git clone https://github.com/hello-robot/stretch_body.git\n$ cd stretch_body/tools\n$ python3 -m pip install -e .\n</code></pre> <p>Now, make desired edits to the stretch_robot_home.py file. Executing the script on the command-line will now run your modified version.</p>"},{"location":"stretch-body/tools/#deploying","title":"Deploying","text":"<p>Increment the version number and run the <code>deploy.sh</code> script. Verify the new release is reflected on PyPI.</p>"},{"location":"stretch-body/tools/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-factory/","title":"Overview","text":""},{"location":"stretch-factory/#overview","title":"Overview","text":"<p>The Stretch Factory package provides low-level Python tools for debug, testing, and calibration of the Hello Robot Stretch robots.</p> <p>These tools are provided for reference only and are intended to be used under the guidance of Hello Robot support engineers. </p> <p>This package can be installed by:</p> <pre><code>pip install  hello-robot-stretch-factory\n</code></pre> <p>The available Stretch Factory tools can be found by tab completing after typing 'REx_'. For example: <pre><code>REx_base_calibrate_imu_collect.py         REx_D435i_check.py                        REx_dynamixel_set_baud.py                 REx_stepper_calibration_run.py            REx_usb_reset.py\nREx_base_calibrate_imu_process.py         REx_discover_hello_devices.py             REx_firmware_updater.py                   REx_stepper_calibration_YAML_to_flash.py  REx_wacc_calibrate.py\nREx_base_calibrate_wheel_separation.py    REx_dynamixel_id_change.py                REx_gamepad_configure.py                  REx_stepper_ctrl_tuning.py                \nREx_calibrate_guarded_contact.py          REx_dynamixel_id_scan.py                  REx_gripper_calibrate.py                  REx_stepper_gains.py                      \nREx_calibrate_range.py                    REx_dynamixel_jog.py                      REx_hello_dynamixel_jog.py                REx_stepper_jog.py                        \nREx_cliff_sensor_calibrate.py             REx_dynamixel_reboot.py                   REx_stepper_calibration_flash_to_YAML.py  REx_stepper_mechaduino_menu.py    \n</code></pre> For useage of these tools, try for example:</p> <p><code>REx_dynamixel_id_scan.py --help</code></p>  All materials are Copyright 2022-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-factory/CHANGELOG/","title":"Changelog","text":"<p>The changes between releases of Stretch Factory is documented here.</p>"},{"location":"stretch-factory/CHANGELOG/#054-feburary-7-2024","title":"0.5.4 - Feburary 7, 2024","text":"<p>This release adds a new tool <code>REx_xrandr_display.py</code> that allows users to programmatically change and revert the display resolution.</p> <p>Example Usage: <pre><code>$ REx_xrandr_display.py --current\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nDisplay Name:       HDMI-1\nDisplay Resolution: 1920x1080x60.00\n$ REx_xrandr_display.py --set-720p\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\n$ REx_xrandr_display.py --current\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nDisplay Name:       HDMI-1\nDisplay Resolution: 1280x720x60.00\n$ REx_xrandr_display.py --revert\nFor use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n</code></pre></p>"},{"location":"stretch-factory/CHANGELOG/#053-feburary-3-2024","title":"0.5.3 - Feburary 3, 2024","text":"<p>This release adds a new tool <code>REx_camera_set_symlink.py</code> that allows the user create a USB symlink to any USB camera that is plugged into the robot. The symlink is assigned by generating an Udev rule that records the following USB attributes of the plugged in camera: <code>[idVendor,idProduct,serial]</code>.</p> <p>Example Usage: By addressing a camera port: <pre><code>$ REx_camera_set_symlink.py --port /dev/video6 --symlink hello-new-camera\n</code></pre> <pre><code>For use with S T R E T C H (R) from Hello Robot Inc.\n---------------------------------------------------------------------\n\nLinking usb port: /dev/video6 to symlink port: /dev/hello-new-camera\nSuccessfully generated udev rule at path: /etc/udev/rules.d/86-hello-new-camera.rules\nSuccessfully Identified device at port: /dev/hello-new-camera\n</code></pre></p>"},{"location":"stretch-factory/CHANGELOG/#052-feburary-2-2024","title":"0.5.2 - Feburary 2, 2024","text":"<p>There was a bug with the firmware updater tool, instead of before looking for the orginal dev/ttyACM port of the device when it is in bootloader state, it's been changed to look for the arduino zero device and use the associated dev/ttyACM port instead.</p>"},{"location":"stretch-factory/CHANGELOG/#050-january-24-2024","title":"0.5.0 - January 24, 2024","text":"<p>This release introduces the concept of \"stepper types\" to the Stretch Factory package. In P5 firmware, we introduced the ability for the uC on a stepper PCB to know which kind of stepper it is (i.e. arm, lift, right wheel, left wheel). In this release, the firmware updater checks if the stepper already knows its stepper type before erasing the flash memory through a new firmware flash. It saves the stepper_type, flashes the new firmware, then writes the stepper_type to flash memory. This release also introduces the new tool called REx_stepper_type.py tool for Hello Robot support members to be able to assist Stretch users in flashing their stepper_type bits.</p> <p>There's a couple benefits to each stepper knowing its stepper_type at the firmware level:</p> <ol> <li>The wheels on newer Stretch robots can take advantage of better runstop by actually disconnecting the H bridge from the motor. This makes it easier to backdrive the robot around by tilting the robot and pushing it like a vacuum cleaner.</li> <li>The system check tool can verify that the UDEV rules for each stepper agree with the stepper_type of the stepper. In case the UDEV rules get corrupted, this provides an additional level of redundancy.</li> </ol>"},{"location":"stretch-factory/CHANGELOG/#0413-december-13-2023","title":"0.4.13 - December 13, 2023","text":"<p>This release eliminates a failure case in the firmware updater where Hello devices are left in a soft-bricked (i.e. can be hardware reset) state from a failed firmware update attempt. The fix has been tested with 800+ firmware flashes on a variety of Stretch robots.</p>"},{"location":"stretch-factory/CHANGELOG/#0311-january-17-2023","title":"0.3.11 - January 17, 2023","text":"<p>This release adds the tool <code>REx_discover_hello_devices.py</code>. This tool will enable users to find and map all the robot-specific USB devices (i.e. Lift, Arm, Left wheel, Right wheel, Head, Wrist/End-of-arm) and assign them to the robot by updating UDEV rules and stretch configuration files.  This tool would require Stretch Body v0.4.11 and above.</p>"},{"location":"stretch-factory/CHANGELOG/#0310-january-16-2023","title":"0.3.10 - January 16, 2023","text":"<p>This release (and previous releases since 0.3.0) makes a number of small improvements to the following tools:</p> <ul> <li><code>REx_dynamixel_jog.py</code> - Jog tool can put dxl in multi-turn, position, pwm, and velocity modes</li> <li><code>REx_calibrate_range.py</code> - Add error checking for failed homing</li> <li><code>REx_calibrate_guarded_contact.py</code> - Support RE1 robots</li> <li><code>REx_base_calibrate_imu_collect.py</code> - Fix bug</li> </ul> <p>There are also organization changes to the docs and READMEs.</p>"},{"location":"stretch-factory/CHANGELOG/#030-september-1-2022","title":"0.3.0 - September 1, 2022","text":"<p>This release moves Stretch Factory to use a new naming scheme for its tools. The prefix <code>REx</code> is now used instead of <code>RE1</code>. This semantic change is in anticipation of the release of future versions of Stretch (e.g. RE2).</p> <p>In addition, two new tools are introduced:</p> <ul> <li>REx_calibrate_guarded_contact.py: Measure the efforts required to move througout the joint workspace and save contact thresholds to Configuration YAML</li> <li>REx_calibrate_range.py: Measure the range of motion of a joint and save to the Configuration YAML.</li> </ul> <p>These new tools move the the <code>effort_pct</code> contact model as supported by Stretch Body 0.3</p> <p>Additional features;</p> <ul> <li>Firmware updater tested and supports 20.04 #47</li> </ul>"},{"location":"stretch-factory/CHANGELOG/#020-june-21-2022","title":"0.2.0 - June 21, 2022","text":"<ul> <li>Support new parameter manage scheme #45</li> </ul> <p>Adds new parameter management format. This change will require older systems to migrate their parameters to the new format. For systems that haven't yet migrated, Stretch Body will exit with a warning that they must migrate first by running <code>RE1_migrate_params.py</code>. See the forum post for more details.</p>"},{"location":"stretch-factory/CHANGELOG/#010","title":"0.1.0","text":"<ul> <li>Introduce the RE1_firmware_update.py tool</li> </ul>"},{"location":"stretch-factory/CHANGELOG/#002-may-13-2020","title":"0.0.2 - May 13, 2020","text":"<p>This is the initial release of Stretch Factory. It includes tools to support debug and testing of the Stretch Hardware.</p>"},{"location":"stretch-factory/python/","title":"Overview","text":"<p>The Stretch Factory package provides low-level Python tools for debug, testing, and calibration of the Hello Robot Stretch robots.</p> <p>These tools are provided for reference only and are intended to be used by qualified Hello Robot production engineers. </p> <p>This package can be installed by:</p> <pre><code>python -m pip install  -U hello-robot-stretch-factory\n</code></pre>"},{"location":"stretch-factory/python/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation.</p> <p>The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link:</p> <p>https://www.gnu.org/licenses/gpl-3.0.html</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-factory/python/tools/","title":"Stretch Factory Tools","text":"<p>The list of tools can be found by tab completion of 'REx' at the command line:</p> <pre><code>&gt;&gt;$ REx\n\nREx_base_calibrate_imu_collect.py         REx_D435i_check.py                        REx_dynamixel_set_baud.py                 REx_stepper_calibration_run.py            REx_usb_reset.py\nREx_base_calibrate_imu_process.py         REx_discover_hello_devices.py             REx_firmware_updater.py                   REx_stepper_calibration_YAML_to_flash.py  REx_wacc_calibrate.py\nREx_base_calibrate_wheel_separation.py    REx_dynamixel_id_change.py                REx_gamepad_configure.py                  REx_stepper_ctrl_tuning.py                \nREx_calibrate_guarded_contact.py          REx_dynamixel_id_scan.py                  REx_gripper_calibrate.py                  REx_stepper_gains.py                      \nREx_calibrate_range.py                    REx_dynamixel_jog.py                      REx_hello_dynamixel_jog.py                REx_stepper_jog.py                        \nREx_cliff_sensor_calibrate.py             REx_dynamixel_reboot.py                   REx_stepper_calibration_flash_to_YAML.py  REx_stepper_mechaduino_menu.py \n</code></pre> <p>These tools are used during the factory system 'bringup' of the robot. They are organized by subsystem:</p> <ul> <li>REx_arm*</li> <li>REx_base*</li> <li>REx_dynamixel*</li> <li>REx_lift*</li> <li>REx_stepper*</li> <li>REx_wacc*</li> </ul> <p>The tools will generally interact with the lowest level interface of the hardware, make measurements, and write calibration data to the robot's YAML or devices EEPROM. For example, the following script calibrates the wrist accelerometer such that the gravity term is 9.8m/s^2.</p> <pre><code>&gt;&gt;$ REx_wacc_calibrate_gravity.py\nEnsure base is level and arm is retracted. Hit enter when ready\n\nItr 0 Val 9.32055700006\n...\nItr 99 Val 9.34092651895\nGot a average value of 9.29669019184\nScalar of 1.05485391012\nWrite parameters to stretch_re1_factory_params.yaml (y/n)? [y]\ny\nWriting yaml...\n</code></pre> <p>Caution: It is possible to break your robot by running these tools. If not used properly these tools may not respect joint torque and position limits. They may overwrite existing calibration data as well. </p>"},{"location":"stretch-factory/updates/","title":"Index","text":"<p>This directory contains recommended factory updates. </p> Update Description Serial Nos Date 001_ROS_INSTALL Scripts to upgrade to ROS stack for units shipped prior to stretch_ros being available 1003,1004 06/01/2020 002_HEAD_PAN Deprecated. Use update 005 instead. 1002 to 1008 08/01/2020 003_WRIST_SWAP Updating system YAML after installing a new wrist module 1004 9/28/2020 004_HEAD_TILT Debug head tilt 1018 9/28/2020 005_HEAD_PAN_CALIBRATION How to update robot YAML and recalibrate the head pan joint 1001 to 1022 10/8/2020 006_DXL_RUNSTOP Allow robot Dynamixel servos to stop upon runstop activation 1001 to 1022 10/8/2020 007_LIFT_FINGER_GUARD Recalibrate lift after installation of foam finger guards 1005 11/17/2020 008_SYNC_TIMESTAMPS Upgrade firmware and Stretch Body to support synchronized timestamps Prior to 1050 12/08/2020 009_STEPPER_STARTUP Fix bug related to configuration of stepper controllers at startup Prior to 1040 1/4/2020 010_WRIST_SWAP Updating system YAML after installing a new wrist module 1013 1/27/2021 011_HEAD_TILT_SWAP Updating the robot calibration and camera data after install a new head tilt module 1007 2/8/21 012_DEX_WRIST Installing and configuring the beta unit of the dexterous wrist x 2/17/21 013_WACC_INSTALL How to install and configure a replacement Wacc board x 3/7/20"},{"location":"stretch-factory/updates/001_ROS_INSTALL/","title":"001_ROS_INSTALL","text":""},{"location":"stretch-factory/updates/001_ROS_INSTALL/#background","title":"Background","text":"<p>Early Stretch units shipped without the <code>stretch_ros</code> stack installed. These scripts will install the necessary packages and bring the robot up to the current ROS compatible configuration.</p>"},{"location":"stretch-factory/updates/001_ROS_INSTALL/#installation","title":"Installation","text":"<p>To upgrade a 'pre-ROS' Stretch to use the latest ROS software stack:</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_factory\n&gt;&gt;$ cd updates/STRETCH_UPDATES_001\n&gt;&gt;$ ./stretch_install_system.sh\n&gt;&gt;$ ./stretch_updgrade_pre_ros.sh\n</code></pre> <p>Then install your URDF to the correct place</p> <pre><code>&gt;&gt;$ rosrun stretch_calibration update_with_most_recent_calibration.sh\n</code></pre> <p>Now as a sanity check that everything is working, try out the face detection demo.</p>"},{"location":"stretch-factory/updates/002_HEAD_PAN/","title":"002_HEAD_PAN","text":""},{"location":"stretch-factory/updates/002_HEAD_PAN/#note-this-update-is-deprecated-use-update-005-instead","title":"Note: This update is deprecated. Use update 005 instead.","text":""},{"location":"stretch-factory/updates/002_HEAD_PAN/#background","title":"Background","text":"<p>Early Stretch robots have a production issue where the range of motion of the head pan is unnecessarily restricted. This is due to:</p> <ul> <li>The Dynamixel servo encoder has a range of 0-4096 ticks which corresponds to a 360 degree range of motion. As configured, the servo can not move past the 4096 tick 'rollover point'. Proper installation of the head pan gear train ensures that this rollover point is outside of the normal range-of-motion of the head.</li> <li>In some early units, this rollover point is inside the normal range-of-motion. As a result, the head pan range is limited. In this case, the robot can look all the way to its left, but can not look to its right past approximately 180 degrees --whereas it should be able to look to its right 234 degrees.</li> </ul> <p>For reference, the nominal range of motion for the head is described here.</p>"},{"location":"stretch-factory/updates/002_HEAD_PAN/#impact","title":"Impact","text":"<ul> <li>The performance is degraded in autonomous actions that require a large range of motion. In particular, when mapping an environment with FUNMAP. </li> <li>The servo may not respect the hardstop of the joint. This can cause it go into an error state due to overload of the servo as it pushes into the hardstop.</li> </ul>"},{"location":"stretch-factory/updates/002_HEAD_PAN/#fix","title":"Fix","text":"<p>We will enable hardstop based homing of the head pan. This allows the Dynamixel servo to use Multiturn Mode (and avoid the encoder rollover issue).</p> <p>First, move to the latest Stretch Body package (version &gt;=0.0.17) and the lastest Stretch Body Tools package (version &gt;=0.0.13)</p> <pre><code>&gt;&gt;$ pip2 install hello-robot-stretch-body\n&gt;&gt;$ pip2 install hello-robot-stretch-body-tools\n</code></pre> <p>Now update the user YAML to enable homing of the head pan joint. Add the following to <code>~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code></p> <pre><code>head_pan: #Fix for gear offset\n  range_t:\n  - 0\n  - 3820\n  req_calibration: 1\n  use_multiturn: 1\n  zero_t: 1155\n  pwm_homing:\n    - -300\n    - 300\n</code></pre> <p>Note: This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well.</p> <p>Next, add this same bit of YAML to the factory image version of the file. This will ensure that when new user accounts are made the fix is applied. This file can be found at <code>/etc/hello-robot/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code></p>"},{"location":"stretch-factory/updates/002_HEAD_PAN/#quick-test","title":"Quick Test","text":"<p>Check that your head is back up and running correctly. Run <code>stretch_head_jog.py</code></p> <p><pre><code>&gt;&gt;$ stretch_head_jog.py \nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n------ MENU -------\nm: menu\na: increment pan 10 deg\nb: decrement pan 10 deg\nc: increment tilt 10 deg\nd: decrement tilt 10 deg\ne: ahead\nf: back\ng: tool\nh: wheels\ni: left\nj: up\np: pan go to pos ticks\nt: tilt go to pos ticks\nx: home\n1: speed slow\n2: speed default\n3: speed fast\n4: speed max\n</code></pre> Try out the homing with the 'x' command.  Verify that it looks straight ahead ('e') and looks straight back ('f') as expected when commanded from the tool's menu.</p>"},{"location":"stretch-factory/updates/002_HEAD_PAN/#urdf-calibration","title":"URDF Calibration","text":"<p>Finally, you will want to recalibrate the URDF. This is a slightly more involved process and can take around an hour. The process is described here.</p> <p>That's it, you're all set!</p>"},{"location":"stretch-factory/updates/003_WRIST_SWAP/","title":"003_WRIST_SWAP","text":""},{"location":"stretch-factory/updates/003_WRIST_SWAP/#background","title":"Background","text":"<p>After installing a new wrist module the system UDEV needs to be updated</p>"},{"location":"stretch-factory/updates/003_WRIST_SWAP/#update-udev","title":"Update UDEV","text":"<p>First, pull down the files</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_factory\n</code></pre> <p>Now copy them over</p> <pre><code>&gt;&gt;$ cd stretch_factory/updates/003_WRIST_SWAP\n&gt;&gt;$ sudo cp *.rules /etc/udev/rules.d\n&gt;&gt;$ sudo cp *.rules /etc/hello-robot/stretch-re1-1004/udev\n</code></pre> <p>Now reboot. After reboot check that the new wrist shows up on the bus</p> <pre><code>&gt;&gt;$ ls /dev/hello-dynamixel-wrist\n&gt;&gt;$ ls /dev/hello-wacc\n</code></pre>"},{"location":"stretch-factory/updates/003_WRIST_SWAP/#test-wrist","title":"Test Wrist","text":"<p>Then check that the Wacc is reporting sensor data back:</p> <pre><code>&gt;&gt;$  stretch_wacc_jog.py \nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\n-------------------\n\n------------------------------\nAx (m/s^2) 9.8684213638\nAy (m/s^2) 0.506848096848\nAz (m/s^2) 0.361166000366\nA0 381\nD0 (In) 1\nD1 (In) 1\nD2 (Out) 0\nD3 (Out) 0\nSingle Tap Count 25\nState  0\nDebug 0\nTimestamp 1601320914.65\nBoard version: Wacc.Guthrie.V1\nFirmware version: Wacc.v0.0.1p0\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\n-------------------\n</code></pre> <p>Finally, jog the wrist yaw joint:</p> <pre><code>&gt;&gt;$ stretch_wrist_yaw_home.py \nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\nMoving to first hardstop...\nContact at position: -3029\nHit first hardstop, marking to zero ticks\nRaw position: 14\nMoving to calibrated zero: (rad)\n</code></pre>"},{"location":"stretch-factory/updates/003_WRIST_SWAP/#update-wacc-calibration","title":"Update Wacc Calibration","text":"<pre><code>&gt;&gt;$ pip2 install hello-robot-stretch-factory\n&gt;&gt;$ RE1_wacc_calibrate.py\nRE1_wacc_calibrate.py \nCalibrating Wacc. Ensure arm is retracted and level to ground\nHit enter when ready\n\nItr 0 Val 9.59977857901\n...\nItr 99 Val 10.1095601333\nGot a average value of 10.1372113882\nGravity scalar of 0.967391 within bounds of 0.900000 to 1.100000\nWriting yaml...\n</code></pre> <p>Now copy the updated YAML to /etc so that it will be available to other (new) user  accounts.</p> <pre><code>&gt;&gt;$ cd ~/stretch_user/stretch-re1-1004\n&gt;&gt;$ sudo cp stretch_re1_factory_params.yaml /etc/hello-robot/stretch-re1-1004\n</code></pre> <p>You're all set!</p>"},{"location":"stretch-factory/updates/004_HEAD_TILT/","title":"004_HEAD_TILT","text":""},{"location":"stretch-factory/updates/004_HEAD_TILT/#background","title":"Background","text":"<p>Tools to debug the head tilt unit not working (assuming not a mechanical failure or unplugged cable)</p> <p>Check that both the pan (ID 11) and tilt (ID 12) are on the bus</p> <pre><code>&gt;&gt;$ RE1_dynamixel_id_scan.py /dev/hello-dynamixel-head \n[Dynamixel ID:000] ping Failed.\n[Dynamixel ID:001] ping Failed.\n[Dynamixel ID:002] ping Failed.\n[Dynamixel ID:003] ping Failed.\n[Dynamixel ID:004] ping Failed.\n[Dynamixel ID:005] ping Failed.\n[Dynamixel ID:006] ping Failed.\n[Dynamixel ID:007] ping Failed.\n[Dynamixel ID:008] ping Failed.\n[Dynamixel ID:009] ping Failed.\n[Dynamixel ID:010] ping Failed.\n[Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1060\n[Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060\n[Dynamixel ID:013] ping Failed.\n[Dynamixel ID:014] ping Failed.\n[Dynamixel ID:015] ping Failed.\n[Dynamixel ID:016] ping Failed.\n[Dynamixel ID:017] ping Failed.\n[Dynamixel ID:018] ping Failed.\n[Dynamixel ID:019] ping Failed.\n</code></pre>"},{"location":"stretch-factory/updates/004_HEAD_TILT/#things-to-try","title":"Things to try","text":"<p>Directly jog the tilt joint (ID 12) from the menu</p> <pre><code>&gt;&gt;$ RE1_dynamixel_jog.py /dev/hello-dynamixel-head 12\n</code></pre> <p>Directly jog the pan joint (ID 11) from the menu</p> <pre><code>&gt;&gt;$ RE1_dynamixel_jog.py /dev/hello-dynamixel-head 11\n</code></pre> <p>If you think a servo may have overheated, reboot the servos of the head</p> <pre><code>&gt;&gt;$ RE1_dynamixel_reboot.py /dev/hello-dynamixel-head\n[Dynamixel ID:011] Reboot Succeeded.\n[Dynamixel ID:012] Reboot Succeeded.\n</code></pre> <p>Jog the entire head. Verify the looking ahead, back, etc from the menu work as expcted</p> <pre><code>&gt;&gt;$ stretch_head_jog.py\nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n------ MENU -------\nm: menu\na: increment pan 10 deg\nb: decrement pan 10 deg\nc: increment tilt 10 deg\nd: decrement tilt 10 deg\ne: ahead\nf: back\ng: tool\nh: wheels\ni: left\nj: up\np: pan go to pos ticks\nt: tilt go to pos ticks\nx: home\n1: speed slow\n2: speed default\n3: speed fast\n4: speed max\n-------------------\n</code></pre>"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/","title":"005_HEAD_PAN_CALIBRATION","text":""},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/#background","title":"Background","text":"<p>The URDF calibration is very sensitive to the 'zero' point of the head pan actuator. We've found that recalibration of the zero point may be necessary  on occaission -- for example if the gear teeth of the joint have skipped due to very high loading.</p> <p>Starting with Stretch serial number <code>stretch-re1-1023</code> we've moved to a new method of setting the zero point. This new method allows the user to recalibrate the joint in the field.</p>"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/#update-yaml","title":"Update YAML","text":"<p>Robots with serial numbers <code>stretch-re1-1001</code> to <code>stretch-re1-1022</code> will need to update their user YAML prior to running the recalibration procedure below. Later robots do not need to update their YAML.</p> <p>Add the following to <code>~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code></p> <pre><code>head_pan:\n  range_t:\n  - 0\n  - 3827\n  use_multiturn: 1\n  zero_t: 1165\n  pwm_homing:\n  - -300\n  - 300\n</code></pre> <p>Note: This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well.</p>"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/#recalibration","title":"Recalibration","text":"<p>First, move to the latest Stretch Body package (version &gt;=0.0.17) and the latest Stretch Factory  package (version &gt;=0.0.14)</p> <pre><code>&gt;&gt;$ pip2 install hello-robot-stretch-body\n&gt;&gt;$ pip2 install hello-robot-stretch-factory\n</code></pre> <p>Now run the recalibration script. This will find the CCW hardstop and mark its position in the servos EEPROM.</p> <pre><code>&gt;&gt;$ RE1_head_calibrate_pan.py \nAbout to calibrate the head pan. Doing so will require you to recalibrated your URDF. Proceed (y/n)?\ny\nMoving to first hardstop...\nContact at position: -3\nHit first hardstop, marking to zero ticks\nRaw position: 33\nMoving to calibrated zero: (rad)\nRecalibration done. Now redo the URDF calibration (see stretch_ros documentation)\n</code></pre>"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/#urdf-recalibration","title":"URDF Recalibration","text":"<p>Finally, you will want to recalibrate the URDF. This is a slightly more involved process and can take around an hour. The process is described here.</p> <p>That's it, you're all set!</p>"},{"location":"stretch-factory/updates/006_DXL_RUNSTOP/","title":"006_DXL_RUNSTOP","text":""},{"location":"stretch-factory/updates/006_DXL_RUNSTOP/#background","title":"Background","text":"<p>For robots with serial number prior to <code>stretch-re1-1023</code> the Dynamixel servos do not respond to the runstop button. While the robot's hardware architecture prevents integrating the runstop with the Robotis servos, we have implemented a software update that simulates this behavior. </p> <p>This functionality is standard with robots starting with <code>stretch-re1-1023</code>.</p> <p>Note: This runstop behavior is only effective when there is an instance of the Robot class running. </p>"},{"location":"stretch-factory/updates/006_DXL_RUNSTOP/#update-yaml","title":"Update YAML","text":"<p>Robots with serial numbers <code>stretch-re1-1001</code> to <code>stretch-re1-1022</code> will need to update their user YAML Add the following to <code>~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code></p> <pre><code>robot_sentry:\n  dynamixel_stop_on_runstop: 1\n</code></pre> <p>Note: This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well.</p>"},{"location":"stretch-factory/updates/006_DXL_RUNSTOP/#update-stretch-body","title":"Update Stretch Body","text":"<p>First, move to the latest Stretch Body package (version &gt;=0.0.19)</p> <pre><code>&gt;&gt;$ pip2 install hello-robot-stretch-body\n</code></pre> <p>Now test it out. Try running the Xbox controller and verify that the robot head, wrist, and gripper stop their motion when the runstop is hit.</p> <pre><code>&gt;&gt;$ stretch_xbox_controller_teleop.py \n</code></pre> <p>That's it!</p>"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/","title":"007_LIFT_FINGER_GUARD","text":""},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/#background","title":"Background","text":"<p>Installing the foam finger guards on the lift will require adjustment of the robot calibration. When the lift homes in the upward direction, it will now stop short of its true hardstop. To adjust for this we will pad the lift range of motion in YAML. We will then test that the URDF calibration is still in spec. If it is out of spec, we will recalibrate the URDF.</p>"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/#update-yaml","title":"Update YAML","text":"<p>Add the following to <code>~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code>. This will override the default range setting, subtracting 6mm in each direction</p> <pre><code>lift:\n  range_m:\n  - 0.006\n  - 1.0939\n</code></pre> <p>Note: This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well.</p>"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/#test-new-range-of-motion","title":"Test New Range of Motion","text":"<p>Home the lift and then check that the lift behaves well at the hardstops. Using the menu, jog the lift to each end of range of motion.</p> <pre><code>&gt;&gt;$ stretch_lift_home.py\n&gt;&gt;$ stretch_lift_jog.py\n</code></pre>"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/#urdf-calibration","title":"URDF Calibration","text":"<p>First update to the latest version of stretch_ros</p> <pre><code>&gt;&gt;$ cd ~/catkin_ws/src/stretch_ros\n&gt;&gt;$ git pull\n</code></pre> <p>Now do the calibration:</p> <pre><code>&gt;&gt;$ stretch_robot_home.py\n&gt;&gt;$ rosrun stretch_calibration update_uncalibrated_urdf.sh\n&gt;&gt;$ roslaunch stretch_calibration collect_head_calibration_data.launch\n</code></pre> <p>The robot will collect calibration samples. This will take about 5 minutes. Then:</p> <pre><code>&gt;&gt;$ roslaunch stretch_calibration process_head_calibration_data.launch\n</code></pre> <p>This will take about an hour. Check that the reported total error at the end of calibration is low (&lt;0.03). If the fit is good, start using the calibration. </p> <pre><code>&gt;&gt;$ rosrun stretch_calibration update_with_most_recent_calibration.sh\n</code></pre> <p>And visually inspect the fit</p> <pre><code>&gt;&gt;$ &gt;&gt;$ roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre> <p>The full URDF calibration tutorial is found here </p>"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/","title":"008_SYNC_TIMESTAMPS","text":""},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#background","title":"Background","text":"<p>Prior to this update the sensor data coming from Stretch Body was only timestamped using the Linux clock. Timestamping was done at the time of the read of the USB bus and was therefore subject to OS dependent jitter and accuracy.</p> <p>Starting with this update Stretch has the ability to provide synchronized timestamps based on its microcontroller clock and a hardware sync line. Details on the timestamping function are found in this tutorial [Coming soon].</p>"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#update-firmware","title":"Update Firmware","text":"<p>Install the latest version of the firmware for the Wacc, Pimu, and Steppers. </p> <p>NOTE: For now you will want the<code>sync_timestamp</code>branch of the git repository. You'll need to pull it down.</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_firmware -b sync_timestamp\n</code></pre> <p>Then follow the tutorial for upgrading firmware (Note your Stretch may already have the Arduino IDE installed and configured). </p>"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#update-yaml","title":"Update YAML","text":"<p>Robots with serial numbers <code>stretch-re1-1001</code> to <code>stretch-re1-1022</code> will need to update their user YAML Add the following to <code>~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code></p> <pre><code>pimu_clock_manager:\n  n_slew_history: 25\n  trs: 450.0\n  use_skew_compensation: 1\n\nwacc_clock_manager:\n  n_slew_history: 25\n  trs: 687.0\n  use_skew_compensation: 1\n\nrobot_timestamp_manager:\n  sync_mode_enabled: 1\n  time_align_status: 1\n</code></pre> <p>Note: This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well. </p>"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#update-stretch-body","title":"Update Stretch Body","text":"<p>NOTE: For now pull down the  <code>sync_timestamp</code> branch of the git repository and install that.</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_body -b sync_timestamp\n&gt;&gt;$ cd ~/repos/stretch_body/body\n&gt;&gt;$ ./local_install.sh\n&gt;&gt;$ cd ../tools\n&gt;&gt;$ ./local_install.sh\n</code></pre> <p>FUTURE:</p> <p>First, move to the latest Stretch Body package (version &gt;=0.0.20)</p> <pre><code>&gt;&gt;$ pip2 install hello-robot-stretch-body\n</code></pre>"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#try-it-out","title":"Try It Out","text":"<p>Now test it out. Try running the timestamp jog tool.</p> <pre><code>&gt;&gt;$ stretch_robot_timestamps_jog.py --display\nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n------ Timestamp Manager -----\nSync mode enabled    : 1\nStatus ID            : 121\nWall time            : 1607575532.977621\nHardware sync        : 1607575532.939872\nPimu IMU             : 1607575532.930712\nLift Encdoer         : 1607575532.938915\nArm Encoder          : 1607575532.938884\nRight Wheel Encoder  : 1607575532.939187\nLeft Wheel Encoder   : 1607575532.938882\nWacc Accel           : 1607575532.934294\n------ Timestamp Manager -----\nSync mode enabled    : 1\nStatus ID            : 125\nWall time            : 1607575533.187324\nHardware sync        : 1607575533.148872\nPimu IMU             : 1607575533.140704\nLift Encdoer         : 1607575533.147775\nArm Encoder          : 1607575533.148192\nRight Wheel Encoder  : 1607575533.148234\nLeft Wheel Encoder   : 1607575533.147883\nWacc Accel           : 1607575533.142721\n...\n</code></pre> <pre><code>&gt;&gt;$ stretch_robot_timestamps_jog.py --sensor_delta\nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\nStarting sensor timestamp analysis...\nSync mode enabled: 1\nTime align status: 0\nUse skew compensation: 1\n---------------------------\nDT Pimu IMU            :-10152\nDT Left Wheel Encoder  :-717\nDT Right Wheel Encoder :-1068\nDT Lift Encoder        :-361\nDT Arm Encoder         :-337\nDT Wacc Accel          :5703\n---------------------------\nDT Pimu IMU            :-7148\nDT Left Wheel Encoder  :-708\nDT Right Wheel Encoder :-953\nDT Lift Encoder        :-470\nDT Arm Encoder         :-864\nDT Wacc Accel          :84\n</code></pre> <pre><code>&gt;&gt;$ stretch_robot_timestamps_jog.py --sensor_stats\n</code></pre> <p></p> <p>That's it!</p>"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/","title":"009_STEPPER_STARTUP","text":""},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/#background","title":"Background","text":"<p>A bug exists on earlier Stretch RE1 related to the startup conditions of the stepper controller. </p> <ol> <li>Power up machine</li> <li>Command base steppers in velocity  mode</li> <li>Command base steppers in position  mode</li> </ol> <p>The position mode command can cause sudden motion of the base as its controller is not correctly initialized. This is a firmware bug.  </p> <p>To replicate the bug:</p> <ol> <li>Place the base on a thick book or other object to get the wheels off the ground</li> <li>Power up machine from off state</li> <li>Run code below</li> </ol> <pre><code>import stretch_body.robot\nfrom time import sleep\n\nrobot = stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.base.set_translate_velocity(0)\nrobot.push_command()\n\nrobot.base.translate_by(0.1) #Causes the base to lurch forward\nrobot.push_command()\n</code></pre>"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/#affected-robots","title":"Affected Robots","text":"<p>This bug affects firmware version <code>Stepper.v0.0.1p0</code>. To check your firmware version (of the arm for example), run the following and hit enter to print the actuator status:</p> <pre><code>&gt;&gt;$ RE1_stepper_jog.py hello-motor-arm\n..\nFirmware version: Stepper.v0.0.1p0\n</code></pre>"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/#fix","title":"Fix","text":"<p>To fix the bug, the stepper firmware must be updated to version <code>Stepper.v0.0.2p0</code> or later. Note: Do not attempt to perform a firmware upgrade without contacting Hello Robot first. You will need to:</p> <ul> <li>Pull down the latest version of Stretch Factory from PyPi</li> <li>Follow the firmware updater instructions provided here.</li> </ul>"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/#verify","title":"Verify","text":"<p>To verify that the fix, try the test code again</p> <pre><code>import stretch_body.robot\nfrom time import sleep\n\nrobot = stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.base.set_translate_velocity(0)\nrobot.push_command()\n\nrobot.base.translate_by(0.1) #Causes smooth motion forward\nrobot.push_command()\n</code></pre>"},{"location":"stretch-factory/updates/010_WRIST_SWAP/","title":"010_WRIST_SWAP","text":""},{"location":"stretch-factory/updates/010_WRIST_SWAP/#background","title":"Background","text":"<p>After installing a new wrist module the system UDEV needs to be updated</p>"},{"location":"stretch-factory/updates/010_WRIST_SWAP/#update-udev","title":"Update UDEV","text":"<p>First, pull down the files</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_factory\n</code></pre> <p>Now copy them over</p> <pre><code>&gt;&gt;$ cd stretch_factory/updates/010_WRIST_SWAP\n&gt;&gt;$ sudo cp *.rules /etc/udev/rules.d\n&gt;&gt;$ sudo cp *.rules /etc/hello-robot/stretch-re1-1013/udev\n</code></pre> <p>Now reboot. After reboot check that the new wrist shows up on the bus</p> <pre><code>&gt;&gt;$ ls /dev/hello-dynamixel-wrist\n&gt;&gt;$ ls /dev/hello-wacc\n</code></pre>"},{"location":"stretch-factory/updates/010_WRIST_SWAP/#test-wrist","title":"Test Wrist","text":"<p>Then check that the Wacc is reporting sensor data back:</p> <pre><code>&gt;&gt;$  stretch_wacc_jog.py \nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\n-------------------\n\n------------------------------\nAx (m/s^2) 9.8684213638\nAy (m/s^2) 0.506848096848\nAz (m/s^2) 0.361166000366\nA0 381\nD0 (In) 1\nD1 (In) 1\nD2 (Out) 0\nD3 (Out) 0\nSingle Tap Count 25\nState  0\nDebug 0\nTimestamp 1601320914.65\nBoard version: Wacc.Guthrie.V1\nFirmware version: Wacc.v0.0.1p0\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\n-------------------\n</code></pre> <p>Finally, jog the wrist yaw joint:</p> <pre><code>&gt;&gt;$ stretch_wrist_yaw_home.py \nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\nMoving to first hardstop...\nContact at position: -3029\nHit first hardstop, marking to zero ticks\nRaw position: 14\nMoving to calibrated zero: (rad)\n</code></pre>"},{"location":"stretch-factory/updates/010_WRIST_SWAP/#update-wacc-calibration","title":"Update Wacc Calibration","text":"<pre><code>&gt;&gt;$ pip2 install hello-robot-stretch-factory\n&gt;&gt;$ RE1_wacc_calibrate.py\nRE1_wacc_calibrate.py \nCalibrating Wacc. Ensure arm is retracted and level to ground\nHit enter when ready\n\nItr 0 Val 9.59977857901\n...\nItr 99 Val 10.1095601333\nGot a average value of 10.1372113882\nGravity scalar of 0.967391 within bounds of 0.900000 to 1.100000\nWriting yaml...\n</code></pre> <p>Now copy the updated YAML to /etc so that it will be available to other (new) user  accounts.</p> <pre><code>&gt;&gt;$ cd ~/stretch_user/stretch-re1-1013\n&gt;&gt;$ sudo cp stretch_re1_factory_params.yaml /etc/hello-robot/stretch-re1-1013\n</code></pre> <p>You're all set!</p>"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/","title":"011_HEAD_TILT_SWAP","text":""},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/#background","title":"Background","text":"<p>After installing a new head tilt module the URDF calibration needs to be updated. In addition, we will want to store a local copy of the D435i calibration data.</p>"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/#test-head","title":"Test Head","text":"<p>First check that the new head hardware is working correctly. Jog the head around using the command line tool:</p> <pre><code>&gt;&gt;$  stretch_head_jog.py \nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n------ MENU -------\nm: menu\na: increment pan 10 deg\nb: decrement pan 10 deg\nc: increment tilt 10 deg\nd: decrement tilt 10 deg\ne: ahead\nf: back\ng: tool\nh: wheels\ni: left\nj: up\np: pan go to pos ticks\nt: tilt go to pos ticks\nx: home\n1: speed slow\n2: speed default\n3: speed fast\n4: speed max\n-------------------\n</code></pre> <p>Next check that the D435i Camera can generate point clouds:</p> <pre><code>&gt;&gt;$ realsense-viewer\n</code></pre>"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/#update-d435i-calibration-data","title":"Update D435i Calibration Data","text":"<p>First, pull down the files</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_factory\n</code></pre> <p>Now copy them over</p> <pre><code>&gt;&gt;$ cd ~/repos/stretch_factory/updates/011_WRIST_SWAP\n&gt;&gt;$ sudo cp * ~/stretch_user/stretch-re1-1007/calibration_D435i\n&gt;&gt;$ sudo cp * /etc/hello-robot/stretch-re1-1007/calibration_D435i\n</code></pre>"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/#update-urdf-calibration","title":"Update URDF Calibration","text":"<p>The URDF will need re-calibration given the new head hardware. The calibration procedure is described in detail here. </p> <p>You're all set!</p>"},{"location":"stretch-factory/updates/012_DEX_WRIST/","title":"012_DEX_WRIST","text":""},{"location":"stretch-factory/updates/012_DEX_WRIST/#background","title":"Background","text":"<p>This update installs and configures the Beta unit of the Stretch Dex Wrist - Beta. The procedure involves</p> <ol> <li>Install and configure the new Wacc board</li> <li>Install Stretch software packages</li> <li>Attach the Dexterous Wrist</li> <li>Update the Dynamixel servo baud rates</li> <li>Update the robot YAML</li> <li>Test the wrist with the XBox controller</li> <li>Configure for use in ROS</li> </ol> <p></p>"},{"location":"stretch-factory/updates/012_DEX_WRIST/#install-and-configure-the-new-wacc-board","title":"Install and configure the new Wacc board","text":"<p>Robots prior to the 'Joplin' batch will need an upgraded Wacc board that will be provided by Hello Robot.</p> <p>See the update 013_WACC_INSTALL </p>"},{"location":"stretch-factory/updates/012_DEX_WRIST/#install-stretch-body-software-packages","title":"Install Stretch Body Software Packages","text":"<p>You'll be installing a local beta version of relevant Stretch Body packages</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ mkdir dex_wrist\n&gt;&gt;$ cd dex_wrist\n\n&gt;&gt;$ git clone --branch feature/pluggable_end_effectors  https://github.com/hello-robot/stretch_body\n&gt;&gt;$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_tool_share\n&gt;&gt;$ git clone  https://github.com/hello-robot/stretch_factory\n&gt;&gt;\n&gt;&gt;$ cd stretch_body/body\n&gt;&gt;$ ./local_install.sh\n&gt;&gt;$ cd ../tools\n&gt;&gt;$ ./local_install.sh\n&gt;&gt;$ pip2 install urdfpy\n&gt;&gt;\n\n&gt;&gt;$ pip2 install hello-robot-stretch-tool-share\n&gt;&gt;$ cd ../../stretch_tool_share/python\n&gt;&gt;$ ./local_install.sh\n\n&gt;&gt;$ cd ~/repos/dex_wrist/stretch_factory/python\n&gt;&gt; ./local_install.sh\n</code></pre>"},{"location":"stretch-factory/updates/012_DEX_WRIST/#attach-the-dexterous-wrist","title":"Attach the Dexterous Wrist","text":"<p>First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide. </p> <p>Next, note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the  additional alignment hole that is just outside the bolt pattern (shown pointing down in the image)</p> <p></p> <p>Next, using a Philips screwdriver, attach the wrist mount bracket to the bottom of the tool plate using the provided  M2 bolts. </p> <p>NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate.</p> <p></p> <p>Next, raise the wrist module up vertically into the mounting bracket, then sliding it over horizontally so that the bearing mates onto its post. Slide in the 3D printed spacer between the pitch servo and the mounting bracket.</p> <p></p> <p></p> <p>Finally, attach the body of the pitch servo to the mounting bracket using the 3 M2.5 screws provided. (NOTE: Flat head screws provided, socket head screws shown below.)</p> <p></p>"},{"location":"stretch-factory/updates/012_DEX_WRIST/#update-the-dynamixel-servo-baud-rates","title":"Update the Dynamixel servo baud rates","text":"<p>The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600.</p> <pre><code>&gt;&gt;$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n&gt;&gt;$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n&gt;&gt;$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n</code></pre>"},{"location":"stretch-factory/updates/012_DEX_WRIST/#update-the-robot-yaml","title":"Update the robot YAML","text":"<p>The new wrist requires a number of updates to the robot YAML</p> <p>YAML doesn't allow definition of multiple fields with the same name. Depending on what is already listed in your YAML you may need to manually edit and merge fields. </p> <p>Add the following to you your  <code>~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code></p> <pre><code>factory_params: stretch_re1_factory_params.yaml\n\nparams:\n  - stretch_tool_share.stretch_dex_wrist_beta.params\n\nhead:\n  baud: 115200\n\nend_of_arm:\n  baud: 115200\n  tool: tool_stretch_dex_wrist\n  #tool: tool_stretch_gripper\n\nrobot:\n  use_collision_manager: 1\n\nhead_pan:\n  baud: 115200\n\nhead_tilt:\n  baud: 115200\n\nwrist_yaw:\n  baud: 115200\n\nstretch_gripper:\n  baud: 115200\n  range_t:\n    - 0\n    - 6667\n  zero_t: 3817\n\nlift:\n  i_feedforward: 0.75\n\nhello-motor-lift:\n  gains:\n    i_safety_feedforward: 0.75\n</code></pre> <p>Each user account on Stretch will need to update their YAML as well. It is recommended practice to stored a reference of the YAML in /etc so that it will be available to other (new) user  accounts.</p> <pre><code>&gt;&gt;$ cd ~/stretch_user/$HELLO_FLEET_ID\n&gt;&gt;$ sudo cp *.yaml /etc/hello-robot/$HELLO_FLEET_ID\n</code></pre>"},{"location":"stretch-factory/updates/012_DEX_WRIST/#configure-for-use-in-ros","title":"Configure for use in ROS","text":"<p>First pull down the new stretch_ros branch and copy in the tool description:</p> <pre><code>&gt;&gt;$ cd ~/catkin_ws/src/stretch_ros/\n&gt;&gt;$ git pull\n&gt;&gt;$ git checkout feature/pluggable_end_effector\n\n&gt;&gt;$ cd ~/repos/dex_wrist/stretch_tool_share/tool_share/stretch_dex_wrist_beta/stretch_description\n&gt;&gt;$ cp urdf/stretch_dex_wrist_beta.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf\n&gt;&gt;$ cp meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes\n</code></pre> <p>Now configure <code>stretch_description.xacro</code> to use the StretchDexWrist tool:</p> <pre><code>&gt;&gt;$ nano ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro\n</code></pre> <p>to read,</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;robot xmlns:xacro=\"http://www.ros.org/wiki/xacro\" name=\"stretch_description\"&gt;\n  &lt;xacro:include filename=\"stretch_dex_wrist_beta.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_main.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_aruco.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_d435i.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_laser_range_finder.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_respeaker.xacro\" /&gt;\n&lt;/robot&gt;\n</code></pre> <p>Update your URDF and then export the URDF for Stretch Body to use  (you may need to Ctrl-C to exit <code>rosrun</code>)</p> <pre><code>&gt;&gt;$ rosrun stretch_calibration update_urdf_after_xacro_change.sh\n&gt;&gt;$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf\n&gt;&gt;$ ./export_urdf.sh\n</code></pre>"},{"location":"stretch-factory/updates/012_DEX_WRIST/#test-the-wrist-with-the-xbox-controller","title":"Test the wrist with the XBox Controller","text":"<p>Try out the new wrist! Note that the new key mapping does not allow for control of the head. </p> <p></p> <p><pre><code>&gt;&gt;$ stretch_xbox_controller_teleop.py\n</code></pre> Ctrl-C to exit. A printable copy of the teleoperation interface is here</p>"},{"location":"stretch-factory/updates/012_DEX_WRIST/#test-the-wrist-with-rviz","title":"Test the wrist with RViz","text":"<p>Now check that the wrist appears in RVIZ and can be controlled from the keyboard interface:</p> <p><pre><code>&gt;&gt;$ roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre> You can type 'q' then Ctrl-C to exit when done. The menu interface is:</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------|\n|                                           |\n|                 i HEAD UP                 |\n|     j HEAD LEFT          l HEAD RIGHT     |\n|                , HEAD DOWN                |\n|                                           |\n|                                           |\n|  7 BASE ROTATE LEFT   9 BASE ROTATE RIGHT |\n|         home                page-up       |\n|                                           |\n|                                           |\n|                 8 LIFT UP                 |\n|                 up-arrow                  |\n|    4 BASE FORWARD         6 BASE BACK     |\n|      left-arrow           right-arrow     |\n|                2 LIFT DOWN                |\n|                down-arrow                 |\n|                                           |\n|                                           |\n|                 w ARM OUT                 |\n|   a WRIST FORWARD        d WRIST BACK     |\n|                 x ARM IN                  |\n|                                           |\n|                                           |\n|   c PITCH FORWARD        v PITCH BACK     |\n|    o ROLL FORWARD         p ROLL BACK     |\n|              5 GRIPPER CLOSE              |\n|              0 GRIPPER OPEN               |\n|                                           |\n|   step size:  b BIG, m MEDIUM, s SMALL    |\n|                  q QUIT                   |\n|                                           |\n|-------------------------------------------|\n</code></pre> <p></p>"},{"location":"stretch-factory/updates/012_DEX_WRIST/#using-the-stretch-dex-wrist","title":"Using the Stretch Dex Wrist","text":"<p>Additional care should be taken when working with the Dex Wrist as it is now easier to accidentally collide the wrist and gripper with the robot, particularly during lift descent. </p> <p>We've implemented a very coarse collision avoidance behavior in Stretch Body that is turned on by default. It is conservative and doesn't fully prevent collisions however. The collision avoidance can be turned off in the user YAML by:</p> <pre><code>collision_stretch_dex_wrist_to_base:\n  enabled: 0\n\ncollision_stretch_dex_wrist_to_self:\n  enabled: 0\n</code></pre> <p>You can jog the individual joints of the wrist using the tool:</p> <pre><code>&gt;&gt;$ stretch_dex_wrist_jog.py --pitch\n</code></pre> <p>Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints.  For example:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move arm to safe manipulation location\nrobot.stow()\nrobot.lift.move_to(0.4)\nrobot.push_command()\ntime.sleep(2.0)\n\n#Pose the Dex Wrist\nrobot.end_of_arm.move_to('wrist_yaw',0)\nrobot.end_of_arm.move_to('wrist_pitch',0)\nrobot.end_of_arm.move_to('wrist_roll',0)\nrobot.end_of_arm.move_to('stretch_gripper',50)\ntime.sleep(2.0)\n\n#Go back to stow and shutdown\nrobot.stow()\nrobot.stop()\n</code></pre> <p>For reference, the parameters for the Stretch Dex Wrist (which can be overriden in the user YAML) are found</p> <p>at <code>.local/lib/python2.7/site-packages/stretch_tool_share/stretch_dex_wrist_beta/params.py</code></p>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/","title":"012_DEX_WRIST","text":""},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#background","title":"Background","text":"<p>This update installs and configures the Beta unit of the Stretch Dex Wrist - Beta. The procedure involves</p> <ol> <li>Install and configure the new Wacc board</li> <li>Install Stretch software packages</li> <li>Attach the Dexterous Wrist</li> <li>Update the Dynamixel servo baud rates</li> <li>Update the robot YAML</li> <li>Test the wrist with the XBox controller</li> <li>Configure for use in ROS</li> </ol> <p></p>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#install-and-configure-the-new-wacc-board","title":"Install and configure the new Wacc board","text":"<p>Robots prior to the 'Joplin' batch will need an upgraded Wacc board that will be provided by Hello Robot.</p> <p>See the update 013_WACC_INSTALL </p>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#install-stretch-body-software-packages","title":"Install Stretch Body Software Packages","text":"<p>You'll be installing a local beta version of relevant Stretch Body packages</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ mkdir dex_wrist\n&gt;&gt;$ cd dex_wrist\n\n&gt;&gt;$ git clone --branch feature/pluggable_end_effectors  https://github.com/hello-robot/stretch_body\n&gt;&gt;$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_tool_share\n&gt;&gt;$ git clone  https://github.com/hello-robot/stretch_factory\n&gt;&gt;\n&gt;&gt;$ cd stretch_body/body\n&gt;&gt;$ ./local_install.sh\n&gt;&gt;$ cd ../tools\n&gt;&gt;$ ./local_install.sh\n&gt;&gt;$ pip2 install urdfpy\n&gt;&gt;\n\n&gt;&gt;$ pip2 install hello-robot-stretch-tool-share\n&gt;&gt;$ cd ../../stretch_tool_share/python\n&gt;&gt;$ ./local_install.sh\n\n&gt;&gt;$ cd ~/repos/dex_wrist/stretch_factory/python\n&gt;&gt; ./local_install.sh\n</code></pre>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#attach-the-dexterous-wrist","title":"Attach the Dexterous Wrist","text":"<p>First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide. </p> <p>Next, note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the  additional alignment hole that is just outside the bolt pattern (shown pointing down in the image)</p> <p></p> <p>Next, using a Philips screwdriver, attach the wrist mount bracket to the bottom of the tool plate using the provided  M2 bolts. </p> <p>NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate.</p> <p></p> <p>Next, raise the wrist module up vertically into the mounting bracket, then sliding it over horizontally so that the bearing mates onto its post. Slide in the 3D printed spacer between the pitch servo and the mounting bracket.</p> <p></p> <p></p> <p>Finally, attach the body of the pitch servo to the mounting bracket using the 3 M2.5 screws provided. (NOTE: Flat head screws provided, socket head screws shown below.)</p> <p></p>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#update-the-dynamixel-servo-baud-rates","title":"Update the Dynamixel servo baud rates","text":"<p>The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600.</p> <pre><code>&gt;&gt;$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n&gt;&gt;$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n&gt;&gt;$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n</code></pre>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#update-the-robot-yaml","title":"Update the robot YAML","text":"<p>The new wrist requires a number of updates to the robot YAML</p> <p>YAML doesn't allow definition of multiple fields with the same name. Depending on what is already listed in your YAML you may need to manually edit and merge fields. </p> <p>Add the following to you your  <code>~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code></p> <pre><code>factory_params: stretch_re1_factory_params.yaml\n\nparams:\n  - stretch_tool_share.stretch_dex_wrist_beta.params\n\nhead:\n  baud: 115200\n\nend_of_arm:\n  baud: 115200\n  tool: tool_stretch_dex_wrist\n  #tool: tool_stretch_gripper\n\nrobot:\n  use_collision_manager: 1\n\nhead_pan:\n  baud: 115200\n\nhead_tilt:\n  baud: 115200\n\nwrist_yaw:\n  baud: 115200\n\nstretch_gripper:\n  baud: 115200\n  range_t:\n    - 0\n    - 6667\n  zero_t: 3817\n\nlift:\n  i_feedforward: 0.75\n\nhello-motor-lift:\n  gains:\n    i_safety_feedforward: 0.75\n</code></pre> <p>Each user account on Stretch will need to update their YAML as well. It is recommended practice to stored a reference of the YAML in /etc so that it will be available to other (new) user  accounts.</p> <pre><code>&gt;&gt;$ cd ~/stretch_user/$HELLO_FLEET_ID\n&gt;&gt;$ sudo cp *.yaml /etc/hello-robot/$HELLO_FLEET_ID\n</code></pre>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#configure-for-use-in-ros","title":"Configure for use in ROS","text":"<p>First pull down the new stretch_ros branch and copy in the tool description:</p> <pre><code>&gt;&gt;$ cd ~/catkin_ws/src/stretch_ros/\n&gt;&gt;$ git pull\n&gt;&gt;$ git checkout feature/pluggable_end_effector\n\n&gt;&gt;$ cd ~/repos/dex_wrist/stretch_tool_share/tool_share/stretch_dex_wrist_beta/stretch_description\n&gt;&gt;$ cp urdf/stretch_dex_wrist_beta.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf\n&gt;&gt;$ cp meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes\n</code></pre> <p>Now configure <code>stretch_description.xacro</code> to use the StretchDexWrist tool:</p> <pre><code>&gt;&gt;$ nano ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro\n</code></pre> <p>to read,</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;robot xmlns:xacro=\"http://www.ros.org/wiki/xacro\" name=\"stretch_description\"&gt;\n  &lt;xacro:include filename=\"stretch_dex_wrist_beta.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_main.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_aruco.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_d435i.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_laser_range_finder.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_respeaker.xacro\" /&gt;\n&lt;/robot&gt;\n</code></pre> <p>Update your URDF and then export the URDF for Stretch Body to use  (you may need to Ctrl-C to exit <code>rosrun</code>)</p> <pre><code>&gt;&gt;$ rosrun stretch_calibration update_urdf_after_xacro_change.sh\n&gt;&gt;$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf\n&gt;&gt;$ ./export_urdf.sh\n</code></pre>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#test-the-wrist-with-the-xbox-controller","title":"Test the wrist with the XBox Controller","text":"<p>Try out the new wrist! Note that the new key mapping does not allow for control of the head. </p> <p></p> <p><pre><code>&gt;&gt;$ stretch_xbox_controller_teleop.py\n</code></pre> Ctrl-C to exit. A printable copy of the teleoperation interface is here</p>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#test-the-wrist-with-rviz","title":"Test the wrist with RViz","text":"<p>Now check that the wrist appears in RVIZ and can be controlled from the keyboard interface:</p> <p><pre><code>&gt;&gt;$ roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre> You can type 'q' then Ctrl-C to exit when done. The menu interface is:</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------|\n|                                           |\n|                 i HEAD UP                 |\n|     j HEAD LEFT          l HEAD RIGHT     |\n|                , HEAD DOWN                |\n|                                           |\n|                                           |\n|  7 BASE ROTATE LEFT   9 BASE ROTATE RIGHT |\n|         home                page-up       |\n|                                           |\n|                                           |\n|                 8 LIFT UP                 |\n|                 up-arrow                  |\n|    4 BASE FORWARD         6 BASE BACK     |\n|      left-arrow           right-arrow     |\n|                2 LIFT DOWN                |\n|                down-arrow                 |\n|                                           |\n|                                           |\n|                 w ARM OUT                 |\n|   a WRIST FORWARD        d WRIST BACK     |\n|                 x ARM IN                  |\n|                                           |\n|                                           |\n|   c PITCH FORWARD        v PITCH BACK     |\n|    o ROLL FORWARD         p ROLL BACK     |\n|              5 GRIPPER CLOSE              |\n|              0 GRIPPER OPEN               |\n|                                           |\n|   step size:  b BIG, m MEDIUM, s SMALL    |\n|                  q QUIT                   |\n|                                           |\n|-------------------------------------------|\n</code></pre> <p></p>"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#using-the-stretch-dex-wrist","title":"Using the Stretch Dex Wrist","text":"<p>Additional care should be taken when working with the Dex Wrist as it is now easier to accidentally collide the wrist and gripper with the robot, particularly during lift descent. </p> <p>We've implemented a very coarse collision avoidance behavior in Stretch Body that is turned on by default. It is conservative and doesn't fully prevent collisions however. The collision avoidance can be turned off in the user YAML by:</p> <pre><code>collision_stretch_dex_wrist_to_base:\n  enabled: 0\n\ncollision_stretch_dex_wrist_to_self:\n  enabled: 0\n</code></pre> <p>You can jog the individual joints of the wrist using the tool:</p> <pre><code>&gt;&gt;$ stretch_dex_wrist_jog.py --pitch\n</code></pre> <p>Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints.  For example:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move arm to safe manipulation location\nrobot.stow()\nrobot.lift.move_to(0.4)\nrobot.push_command()\ntime.sleep(2.0)\n\n#Pose the Dex Wrist\nrobot.end_of_arm.move_to('wrist_yaw',0)\nrobot.end_of_arm.move_to('wrist_pitch',0)\nrobot.end_of_arm.move_to('wrist_roll',0)\nrobot.end_of_arm.move_to('stretch_gripper',50)\ntime.sleep(2.0)\n\n#Go back to stow and shutdown\nrobot.stow()\nrobot.stop()\n</code></pre> <p>For reference, the parameters for the Stretch Dex Wrist (which can be overriden in the user YAML) are found</p> <p>at <code>.local/lib/python2.7/site-packages/stretch_tool_share/stretch_dex_wrist_beta/params.py</code></p>"},{"location":"stretch-factory/updates/013_WACC_INSTALL/","title":"013_WACC_INSTALL","text":""},{"location":"stretch-factory/updates/013_WACC_INSTALL/#background","title":"Background","text":"<p>This update installs and configures a new Wacc (Wrist + Accelerometer) board. You will need</p> <ul> <li>Replacement Wacc board</li> <li>USB-A to USB-micro cable</li> <li>1.5mm Hex wrench</li> <li>2.5mm Hex wrench</li> <li>Small flat head screw driver or similar</li> <li>Loctite 242 (blue)</li> </ul>"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#update-the-wacc-board-serial-numbers","title":"Update the Wacc board serial numbers","text":"<p>First, on the robot run:</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_factory\n&gt;&gt;$ chmod a+rw $HELLO_FLEET_PATH/$HELLO_FLEET_ID/udev/*\n</code></pre> <p>Now attach the USB cable into a USB port of the robot trunk. Run the updating tool as shown below. You will - Plug in USB when prompted - Hit enter - Unplug and plug in USB when prompted - Hit enter</p> <pre><code>&gt;&gt;$ cd stretch_factory/updates/013_WACC_INSTALL\n&gt;&gt;$ sudo dmesg -c\n&gt;&gt;$ ./add_new_wacc_pcba.py \n----------------------\nAdding WACC PCBA to robot:  stretch-re1-1039\nPlug / Reset Dynamixel device now...\nPress return when done\n\n[1035443.643968] usb 1-1.3.2.1: new high-speed USB device number 12 using xhci_hcd\n[1035443.844182] usb 1-1.3.2.1: New USB device found, idVendor=1a40, idProduct=0101\n[1035443.844199] usb 1-1.3.2.1: New USB device strings: Mfr=0, Product=1, SerialNumber=0\n[1035443.844208] usb 1-1.3.2.1: Product: USB 2.0 Hub\n[1035443.845851] hub 1-1.3.2.1:1.0: USB hub found\n[1035443.845923] hub 1-1.3.2.1:1.0: 4 ports detected\n[1035444.252052] usb 1-1.3.2.1.2: new full-speed USB device number 15 using xhci_hcd\n[1035444.479616] usb 1-1.3.2.1.2: New USB device found, idVendor=0403, idProduct=6001\n[1035444.479625] usb 1-1.3.2.1.2: New USB device strings: Mfr=1, Product=2, SerialNumber=3\n[1035444.479631] usb 1-1.3.2.1.2: Product: FT232R USB UART\n[1035444.479636] usb 1-1.3.2.1.2: Manufacturer: FTDI\n[1035444.479640] usb 1-1.3.2.1.2: SerialNumber: AQ00X8TJ\n[1035444.483900] ftdi_sio 1-1.3.2.1.2:1.0: FTDI USB Serial Device converter detected\n[1035444.484043] usb 1-1.3.2.1.2: Detected FT232RL\n[1035444.484466] usb 1-1.3.2.1.2: FTDI USB Serial Device converter now attached to ttyUSB3\n[1035445.459995] usb 1-1.3.2.1.3: new full-speed USB device number 17 using xhci_hcd\n\n---------------------------\nFound Dynamixel device with SerialNumber AQ00X8TJ\nWriting UDEV for  AQ00X8TJ\nOverwriting existing entry...\nPlug / Reset in Arduino device now...\nPress return when done\n\n---------------------------\nFound Arduino device with SerialNumber C209885C50524653312E3120FF101E39\nWriting UDEV for  hello-wacc C209885C50524653312E3120FF101E39\nOverwriting existing entry...\n---------------------------\nFound Arduino device with SerialNumber C209885C50524653312E3120FF101E39\nWriting UDEV for  hello-wacc C209885C50524653312E3120FF101E39\nOverwriting existing entry...\n</code></pre>"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#install-the-new-wacc-board","title":"Install the new Wacc board","text":"<ol> <li> <p>Power down the robot from Ubuntu and turn off the main power switch.</p> </li> <li> <p>Remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide. </p> </li> <li> <p>Manually pose the lift height and arm such that the wrist can sit on a table top in order to support the wrist weight during disassembly</p> </li> <li> <p>Using the 1.5mm wrench, remove the two M2 bolts holding the plastic cap to the end of the wrist </p> </li> </ol> <p></p> <ol> <li> <p>Using the 2.5mm wrench, remove the two M4 bolts (blue arrows) holding the wrist module to the end of arm</p> </li> <li> <p>Slide the wrist module out of the arm tube while supporting the weight of the module so that it remains parallel to the ground.  Take care that the Wacc board clears the surrounding metal structure (shown in blue)</p> </li> </ol> <p></p> <ol> <li>With the screw driver, push back and dislodge the JST power cable and USB cable from the back of the Wacc board. Remove the JST servo cable at the front of the board.</li> <li>Using the 1.5mm wrench, remove the 4 M2 bolts holding the Wacc board to the sheetmetal frame. </li> <li>Attach the replacement board onto the sheetmetal frame using the provided 4 M2 bolts</li> </ol> <p></p> <ol> <li>Reattach the USB and power cables to back of Wacc</li> <li>Carefully route the Dynamixel servo cable out of the arm as shown such that no cables are pinched when attaching the plastic cap.</li> </ol> <p></p> <ol> <li> <p>Carefully slide the wrist module back into the arm. Ensure that the cables are fully seated.</p> </li> <li> <p>Apply Loctite to the two M4 bolts. Secure the wrist module to the arm with the bolts. then attach the plastic cap with the two M2 bolts. </p> </li> </ol>"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#check-the-wacc-functionality","title":"Check the Wacc functionality","text":"<p>Power the robot back on and check that the board is on the bus</p> <pre><code>&gt;&gt;$ ls /dev/hello-dynamixel-wrist\nhello-dynamixel-wrist\n&gt;&gt;$ ls /dev/hello-wacc\nhello-wacc\n</code></pre> <p>Then check that the Wacc is reporting sensor data back:</p> <pre><code>&gt;&gt;$  stretch_wacc_jog.py \nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\n-------------------\n\n------------------------------\nAx (m/s^2) 9.8684213638\nAy (m/s^2) 0.506848096848\nAz (m/s^2) 0.361166000366\nA0 381\nD0 (In) 1\nD1 (In) 1\nD2 (Out) 0\nD3 (Out) 0\nSingle Tap Count 25\nState  0\nDebug 0\nTimestamp 1601320914.65\nBoard version: Wacc.Guthrie.V1\nFirmware version: Wacc.v0.0.1p0\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\n-------------------\n</code></pre> <p>Finally, home the wrist yaw joint to ensure that it is working.</p> <pre><code>&gt;&gt;$ stretch_wrist_yaw_home.py \nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\nMoving to first hardstop...\nContact at position: -3029\nHit first hardstop, marking to zero ticks\nRaw position: 14\nMoving to calibrated zero: (rad)\n</code></pre>"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#update-wacc-calibration","title":"Update Wacc Calibration","text":"<p><pre><code>&gt;&gt;$ RE1_wacc_calibrate.py\nRE1_wacc_calibrate.py \nCalibrating Wacc. Ensure arm is retracted and level to ground\nHit enter when ready\n\nItr 0 Val 9.59977857901\n...\nItr 99 Val 10.1095601333\nGot a average value of 10.1372113882\nGravity scalar of 0.967391 within bounds of 0.900000 to 1.100000\nWriting yaml...\n</code></pre> Note: If the RE1* tools are not present you can install them as</p> <pre><code>&gt;&gt;$ pip2 install hello-robot-stretch-factory\n</code></pre>"},{"location":"stretch-factory/updates/014_WAYPOINTS/","title":"014_WAYPOINTS","text":""},{"location":"stretch-factory/updates/014_WAYPOINTS/#background","title":"Background","text":"<p>x</p>"},{"location":"stretch-factory/updates/014_WAYPOINTS/#update-firmware","title":"Update Firmware","text":"<p>Install the latest version of the firmware for the Wacc, Pimu, and Steppers. </p> <p>NOTE: For now you will want the<code>sync_timestamp</code>branch of the git repository. You'll need to pull it down.</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_firmware -b via_trajectory\n</code></pre> <p>Then follow the tutorial for upgrading firmware (Note your Stretch may already have the Arduino IDE installed and configured). </p>"},{"location":"stretch-factory/updates/014_WAYPOINTS/#update-yaml","title":"Update YAML","text":"<p>Robots with serial numbers <code>stretch-re1-1001</code> to <code>stretch-re1-1022</code> will need to update their user YAML Add the following to <code>~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code></p> <pre><code>pimu_clock_manager:\n  n_slew_history: 25\n  trs: 450.0\n  use_skew_compensation: 1\n\nwacc_clock_manager:\n  n_slew_history: 25\n  trs: 687.0\n  use_skew_compensation: 1\n\nrobot_timestamp_manager:\n  sync_mode_enabled: 1\n  time_align_status: 1\n</code></pre> <p>Note: This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well. </p>"},{"location":"stretch-factory/updates/014_WAYPOINTS/#update-stretch-body","title":"Update Stretch Body","text":"<p>NOTE: For now pull down the  <code>sync_timestamp</code> branch of the git repository and install that.</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_body -b sync_timestamp\n&gt;&gt;$ cd ~/repos/stretch_body/body\n&gt;&gt;$ ./local_install.sh\n&gt;&gt;$ cd ../tools\n&gt;&gt;$ ./local_install.sh\n</code></pre> <p>FUTURE:</p> <p>First, move to the latest Stretch Body package (version &gt;=0.0.20)</p> <pre><code>&gt;&gt;$ pip2 install hello-robot-stretch-body\n</code></pre>"},{"location":"stretch-factory/updates/014_WAYPOINTS/#try-it-out","title":"Try It Out","text":"<p>Now test it out. Try running the timestamp jog tool.</p> <pre><code>&gt;&gt;$ stretch_robot_timestamps_jog.py --display\nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n------ Timestamp Manager -----\nSync mode enabled    : 1\nStatus ID            : 121\nWall time            : 1607575532.977621\nHardware sync        : 1607575532.939872\nPimu IMU             : 1607575532.930712\nLift Encdoer         : 1607575532.938915\nArm Encoder          : 1607575532.938884\nRight Wheel Encoder  : 1607575532.939187\nLeft Wheel Encoder   : 1607575532.938882\nWacc Accel           : 1607575532.934294\n------ Timestamp Manager -----\nSync mode enabled    : 1\nStatus ID            : 125\nWall time            : 1607575533.187324\nHardware sync        : 1607575533.148872\nPimu IMU             : 1607575533.140704\nLift Encdoer         : 1607575533.147775\nArm Encoder          : 1607575533.148192\nRight Wheel Encoder  : 1607575533.148234\nLeft Wheel Encoder   : 1607575533.147883\nWacc Accel           : 1607575533.142721\n...\n</code></pre> <pre><code>&gt;&gt;$ stretch_robot_timestamps_jog.py --sensor_delta\nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\nStarting sensor timestamp analysis...\nSync mode enabled: 1\nTime align status: 0\nUse skew compensation: 1\n---------------------------\nDT Pimu IMU            :-10152\nDT Left Wheel Encoder  :-717\nDT Right Wheel Encoder :-1068\nDT Lift Encoder        :-361\nDT Arm Encoder         :-337\nDT Wacc Accel          :5703\n---------------------------\nDT Pimu IMU            :-7148\nDT Left Wheel Encoder  :-708\nDT Right Wheel Encoder :-953\nDT Lift Encoder        :-470\nDT Arm Encoder         :-864\nDT Wacc Accel          :84\n</code></pre> <pre><code>&gt;&gt;$ stretch_robot_timestamps_jog.py --sensor_stats\n</code></pre> <p></p> <p>That's it!</p>"},{"location":"stretch-factory/updates/015_DEX_WRIST/","title":"012_DEX_WRIST","text":""},{"location":"stretch-factory/updates/015_DEX_WRIST/#background","title":"Background","text":"<p>This update installs and configures the Beta unit of the Stretch Dex Wrist - Beta. The procedure involves</p> <ol> <li>Install and configure the new Wacc board</li> <li>Install Stretch software packages</li> <li>Attach the Dexterous Wrist</li> <li>Update the Dynamixel servo baud rates</li> <li>Update the robot YAML</li> <li>Test the wrist with the XBox controller</li> <li>Configure for use in ROS</li> </ol> <p></p>"},{"location":"stretch-factory/updates/015_DEX_WRIST/#install-and-configure-the-new-wacc-board","title":"Install and configure the new Wacc board","text":"<p>Robots prior to the 'Joplin' batch will need an upgraded Wacc board that will be provided by Hello Robot.</p> <p>See the update 013_WACC_INSTALL </p>"},{"location":"stretch-factory/updates/015_DEX_WRIST/#install-stretch-body-software-packages","title":"Install Stretch Body Software Packages","text":"<p>You'll be installing a local beta version of relevant Stretch Body packages</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ mkdir dex_wrist\n&gt;&gt;$ cd dex_wrist\n\n&gt;&gt;$ git clone --branch feature/pluggable_end_effectors  https://github.com/hello-robot/stretch_body\n&gt;&gt;$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_tool_share\n&gt;&gt;$ git clone  https://github.com/hello-robot/stretch_factory\n&gt;&gt;\n&gt;&gt;$ cd stretch_body/body\n&gt;&gt;$ ./local_install.sh\n&gt;&gt;$ cd ../tools\n&gt;&gt;$ ./local_install.sh\n&gt;&gt;$ pip2 install urdfpy\n&gt;&gt;\n\n&gt;&gt;$ pip2 install hello-robot-stretch-tool-share\n&gt;&gt;$ cd ../../stretch_tool_share/python\n&gt;&gt;$ ./local_install.sh\n\n&gt;&gt;$ cd ~/repos/dex_wrist/stretch_factory/python\n&gt;&gt; ./local_install.sh\n</code></pre>"},{"location":"stretch-factory/updates/015_DEX_WRIST/#attach-the-dexterous-wrist","title":"Attach the Dexterous Wrist","text":"<p>First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide. </p> <p>Next, note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the  additional alignment hole that is just outside the bolt pattern (shown pointing down in the image)</p> <p></p> <p>Next, using a Philips screwdriver, attach the wrist mount bracket to the bottom of the tool plate using the provided  M2 bolts. </p> <p>NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate.</p> <p></p> <p>Next, raise the wrist module up vertically into the mounting bracket, then sliding it over horizontally so that the bearing mates onto its post. Slide in the 3D printed spacer between the pitch servo and the mounting bracket.</p> <p></p> <p></p> <p>Finally, attach the body of the pitch servo to the mounting bracket using the 3 M2.5 screws provided. (NOTE: Flat head screws provided, socket head screws shown below.)</p> <p></p>"},{"location":"stretch-factory/updates/015_DEX_WRIST/#update-the-dynamixel-servo-baud-rates","title":"Update the Dynamixel servo baud rates","text":"<p>The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600.</p> <pre><code>&gt;&gt;$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n&gt;&gt;$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n&gt;&gt;$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n</code></pre>"},{"location":"stretch-factory/updates/015_DEX_WRIST/#update-the-robot-yaml","title":"Update the robot YAML","text":"<p>The new wrist requires a number of updates to the robot YAML</p> <p>YAML doesn't allow definition of multiple fields with the same name. Depending on what is already listed in your YAML you may need to manually edit and merge fields. </p> <p>Add the following to you your  <code>~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml</code></p> <pre><code>factory_params: stretch_re1_factory_params.yaml\n\nparams:\n  - stretch_tool_share.stretch_dex_wrist_beta.params\n\nhead:\n  baud: 115200\n\nend_of_arm:\n  baud: 115200\n  tool: tool_stretch_dex_wrist\n  #tool: tool_stretch_gripper\n\nrobot:\n  use_collision_manager: 1\n\nhead_pan:\n  baud: 115200\n\nhead_tilt:\n  baud: 115200\n\nwrist_yaw:\n  baud: 115200\n\nstretch_gripper:\n  baud: 115200\n  range_t:\n    - 0\n    - 6667\n  zero_t: 3817\n\nlift:\n  i_feedforward: 0.75\n\nhello-motor-lift:\n  gains:\n    i_safety_feedforward: 0.75\n</code></pre> <p>Each user account on Stretch will need to update their YAML as well. It is recommended practice to stored a reference of the YAML in /etc so that it will be available to other (new) user  accounts.</p> <pre><code>&gt;&gt;$ cd ~/stretch_user/$HELLO_FLEET_ID\n&gt;&gt;$ sudo cp *.yaml /etc/hello-robot/$HELLO_FLEET_ID\n</code></pre>"},{"location":"stretch-factory/updates/015_DEX_WRIST/#configure-for-use-in-ros","title":"Configure for use in ROS","text":"<p>First pull down the new stretch_ros branch and copy in the tool description:</p> <pre><code>&gt;&gt;$ cd ~/catkin_ws/src/stretch_ros/\n&gt;&gt;$ git pull\n&gt;&gt;$ git checkout feature/pluggable_end_effector\n\n&gt;&gt;$ cd ~/repos/dex_wrist/stretch_tool_share/tool_share/stretch_dex_wrist_beta/stretch_description\n&gt;&gt;$ cp urdf/stretch_dex_wrist_beta.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf\n&gt;&gt;$ cp meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes\n</code></pre> <p>Now configure <code>stretch_description.xacro</code> to use the StretchDexWrist tool:</p> <pre><code>&gt;&gt;$ nano ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro\n</code></pre> <p>to read,</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;robot xmlns:xacro=\"http://www.ros.org/wiki/xacro\" name=\"stretch_description\"&gt;\n  &lt;xacro:include filename=\"stretch_dex_wrist_beta.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_main.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_aruco.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_d435i.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_laser_range_finder.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_respeaker.xacro\" /&gt;\n&lt;/robot&gt;\n</code></pre> <p>Update your URDF and then export the URDF for Stretch Body to use  (you may need to Ctrl-C to exit <code>rosrun</code>)</p> <pre><code>&gt;&gt;$ rosrun stretch_calibration update_urdf_after_xacro_change.sh\n&gt;&gt;$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf\n&gt;&gt;$ ./export_urdf.sh\n</code></pre>"},{"location":"stretch-factory/updates/015_DEX_WRIST/#test-the-wrist-with-the-xbox-controller","title":"Test the wrist with the XBox Controller","text":"<p>Try out the new wrist! Note that the new key mapping does not allow for control of the head. </p> <p></p> <p><pre><code>&gt;&gt;$ stretch_xbox_controller_teleop.py\n</code></pre> Ctrl-C to exit. A printable copy of the teleoperation interface is here</p>"},{"location":"stretch-factory/updates/015_DEX_WRIST/#test-the-wrist-with-rviz","title":"Test the wrist with RViz","text":"<p>Now check that the wrist appears in RVIZ and can be controlled from the keyboard interface:</p> <p><pre><code>&gt;&gt;$ roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre> You can type 'q' then Ctrl-C to exit when done. The menu interface is:</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------|\n|                                           |\n|                 i HEAD UP                 |\n|     j HEAD LEFT          l HEAD RIGHT     |\n|                , HEAD DOWN                |\n|                                           |\n|                                           |\n|  7 BASE ROTATE LEFT   9 BASE ROTATE RIGHT |\n|         home                page-up       |\n|                                           |\n|                                           |\n|                 8 LIFT UP                 |\n|                 up-arrow                  |\n|    4 BASE FORWARD         6 BASE BACK     |\n|      left-arrow           right-arrow     |\n|                2 LIFT DOWN                |\n|                down-arrow                 |\n|                                           |\n|                                           |\n|                 w ARM OUT                 |\n|   a WRIST FORWARD        d WRIST BACK     |\n|                 x ARM IN                  |\n|                                           |\n|                                           |\n|   c PITCH FORWARD        v PITCH BACK     |\n|    o ROLL FORWARD         p ROLL BACK     |\n|              5 GRIPPER CLOSE              |\n|              0 GRIPPER OPEN               |\n|                                           |\n|   step size:  b BIG, m MEDIUM, s SMALL    |\n|                  q QUIT                   |\n|                                           |\n|-------------------------------------------|\n</code></pre> <p></p>"},{"location":"stretch-factory/updates/015_DEX_WRIST/#using-the-stretch-dex-wrist","title":"Using the Stretch Dex Wrist","text":"<p>Additional care should be taken when working with the Dex Wrist as it is now easier to accidentally collide the wrist and gripper with the robot, particularly during lift descent. </p> <p>We've implemented a very coarse collision avoidance behavior in Stretch Body that is turned on by default. It is conservative and doesn't fully prevent collisions however. The collision avoidance can be turned off in the user YAML by:</p> <pre><code>collision_stretch_dex_wrist_to_base:\n  enabled: 0\n\ncollision_stretch_dex_wrist_to_self:\n  enabled: 0\n</code></pre> <p>You can jog the individual joints of the wrist using the tool:</p> <pre><code>&gt;&gt;$ stretch_dex_wrist_jog.py --pitch\n</code></pre> <p>Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints.  For example:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move arm to safe manipulation location\nrobot.stow()\nrobot.lift.move_to(0.4)\nrobot.push_command()\ntime.sleep(2.0)\n\n#Pose the Dex Wrist\nrobot.end_of_arm.move_to('wrist_yaw',0)\nrobot.end_of_arm.move_to('wrist_pitch',0)\nrobot.end_of_arm.move_to('wrist_roll',0)\nrobot.end_of_arm.move_to('stretch_gripper',50)\ntime.sleep(2.0)\n\n#Go back to stow and shutdown\nrobot.stow()\nrobot.stop()\n</code></pre> <p>For reference, the parameters for the Stretch Dex Wrist (which can be overriden in the user YAML) are found</p> <p>at <code>.local/lib/python2.7/site-packages/stretch_tool_share/stretch_dex_wrist_beta/params.py</code></p>"},{"location":"stretch-factory/updates/016_WHEEL_STEPPER/","title":"016_WHEEL_STEPPER","text":""},{"location":"stretch-factory/updates/016_WHEEL_STEPPER/#background","title":"Background","text":"<p>This update configures the robot (stretch-re1-1065) to use a new left wheel module.</p>"},{"location":"stretch-factory/updates/016_WHEEL_STEPPER/#clone-the-repo","title":"Clone the repo","text":"<p>First, configure the software:</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_factory\n&gt;&gt;$ cd stretch_factory/updates/016_WHEEL_STEPPER\n&gt;&gt;$ ./configure_new_stepper.py\n</code></pre>"},{"location":"stretch-factory/updates/016_WHEEL_STEPPER/#test-the-motor","title":"Test the Motor","text":"<p>Next power down the robot. Power the robot back on and check that the board is on the bus:</p> <pre><code>&gt;&gt;$ ls /dev/hello-motor-left-wheel\n/dev/hello-motor-left-wheel  \n</code></pre> <p>Finally, check that the base moves correctly. Use the <code>f</code> and <code>b</code> commands to jog the base forward and back.</p> <pre><code>stretch_base_jog.py \nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n--------------\nm: menu\n\n1: rate slow\n2: rate default\n3: rate fast\n4: rate max\nw: CW/CCW 90 deg\nx: forward-&gt; back 0.5m\ny: spin at 22.5deg/s\n\nf / b / l / r : small forward / back / left / right\nF / B / L / R : large forward / back / left / right\no: freewheel\np: pretty print\nq: quit\n\nInput?\n</code></pre>"},{"location":"stretch-factory/updates/017_D435I_TEST/","title":"017_D435I_TEST","text":""},{"location":"stretch-factory/updates/017_D435I_TEST/#background","title":"Background","text":"<p>This update describes a series of tests to evaluate if the Stretch D435i camera is working properly.</p>"},{"location":"stretch-factory/updates/017_D435I_TEST/#check-usb-version","title":"Check USB version","text":"<p>Reboot the robot. After reboot, check that the camera is detected as USB 3.2:</p> <pre><code>&gt;&gt;$ rs-enumerate-devices | grep Usb\n    Usb Type Descriptor           :     3.2\n</code></pre>"},{"location":"stretch-factory/updates/017_D435I_TEST/#test-data-collection","title":"Test data collection","text":"<p>Create a data collection configuration file: <pre><code>&gt;&gt;$ cd \n&gt;&gt;$ nano data_collect.cfg\n</code></pre> And add the following:</p> <pre><code>#Video streams\nDEPTH,1280,720,15,Z16,0\nINFRARED,640,480,15,Y8,1\nINFRARED,640,480,15,Y8,2\nCOLOR,1280,720,15,RGB8,0\n# IMU streams will produce data in m/sec^2 &amp; rad/sec\nACCEL,1,1,63,MOTION_XYZ32F\nGYRO,1,1,200,MOTION_XYZ32F\n</code></pre> <p>Next clear the system log <code>sudo dmesg -c</code></p> <p>And collect 1000 frames from the camera</p> <pre><code>rs-data-collect -c ./data_collect.cfg -f ./log.csv -t 60 -m 1000\n</code></pre>"},{"location":"stretch-factory/updates/018_GRIPPER_CALIBRATION/","title":"018_GRIPPER_CALIBRATION","text":""},{"location":"stretch-factory/updates/018_GRIPPER_CALIBRATION/#background","title":"Background","text":"<p>The Stretch gripper is calibrated to define the closed position, 'zero' or home position, and fully open position of the gripper.  Recalibrating may be necessary if replacing the gripper, or if the gripper is not performing correctly. </p>"},{"location":"stretch-factory/updates/018_GRIPPER_CALIBRATION/#gripper-calibration","title":"Gripper Calibration","text":"<p>First, move to the latest Stretch Factory package (version &gt;=0.0.14)</p> <p><pre><code>&gt;&gt;$ pip2 install hello-robot-stretch-factory\n</code></pre> Now run the gripper calibration script. This script will take you through the step-by-step process of calibration. 1. First, the script automatically finds the gripper closed position. <pre><code>&gt;&gt;$ REx_gripper_calibrate.py \nHit enter to find zero\nMoving to first hardstop...\nContact at position: 0\nHit first hardstop, marking to zero ticks\nHoming is now  4895\nRaw position: 12\nMoving to calibrated zero: (rad)\n</code></pre> </p> <ol> <li>Next you are asked to manually set the 'zero' or home position, entering '1' or '2' to open or close the fingers until they barely don't touch, as pictured.  <pre><code>---------------------------------------------------\nEnter 1 to open fingers. Enter 2 to close fingers. Enter 3 when the fingertips are just barely not touching.\n1\n1\n1\n1\n1\n2\n3\n('Setting zero at:', 5639)\n</code></pre> </li> <li>Next you are asked to manually set the fully open position, entering '1' or '2' to open or close the fingers until the fingers are fully open and stop moving. <pre><code>---------------------------------------------------\nEnter 1 to open fingers. Enter 2 to close fingers. Enter 3 when the fingertips are fully open, \nand no further opening motion is possible\n1\n1\n.\n.\n.\n1\n2\n3\n('Setting open at:', 8500)\n</code></pre> </li> <li>Finally, the script will move the gripper to the closed, open, and zero positions. Verify that the gripper moves to the expected poses and save the calibration. <pre><code>Hit enter to close\nHit enter to open\nHit enter to go to zero\nSave calibration [y]?y\n</code></pre></li> </ol>"},{"location":"stretch-firmware/","title":"Overview","text":""},{"location":"stretch-firmware/#overview","title":"Overview","text":"<p>The Stretch Firmware repository provides the Arduino based firmware for the Stretch robot. </p> <p>Minor version updates to Stretch Body may occasionally require the robot's firmware to also be updated.</p> <p>The repository includes the firmware for the three Stretch PCBA types: </p> <ul> <li>hello_stepper: firmware for stepper motor controller based on the Mechaduino project</li> <li>hello_wacc: firmware for wrist accelerometer board (Wacc) in the wrist </li> <li>hello_pimu:  firmware for power and imu board (Pimu) in the base</li> </ul>"},{"location":"stretch-firmware/#license","title":"License","text":"<p>For details, see the LICENSE.md file in the root directory. All materials within this repository are licensed with the GNU General Public License v3.0 (GNU GPLv3) except where other third-party licenses must apply.  </p> <p>We thank people who have contributed to this work via open-source code and open hardware. We especially thank the Mechaduino project and Tropical Labs. The motor controller firmware and hardware are derived from the excellent firmware and hardware created for the Mechaduino project by Tropical Labs.</p>  All materials are Copyright 2022-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-firmware/LICENSE/","title":"LICENSE","text":"<p>The following license (GPLv3) applies to the entire contents of this directory (the \"Contents\") except where other third-party licenses must apply. The Contents include firmware and hardware for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>We thank people who have contributed to this work via open-source code and open hardware. We especially thank the Mechaduino project and Tropical Labs. The motor controller firmware and hardware are derived from the excellent firmware and hardware created for the Mechaduino project by Tropical Labs.</p> <p>The Mechaduino firmware and hardware were originally released using the Creative Commons Attribution Share-Alike 4.0 License (CC BY-SA 4.0). With approval from Tropical Labs, we have licensed our derived firmware and hardware using the GNU General Public License v3.0 (GNU GPLv3) as described below. As stated by Creative Commons, this is permitted due to the GPLv3 being a one-way BY-SA compatible license, such that \"you may license your contributions to adaptations of BY-SA 4.0 materials under GPLv3.\".</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation.</p> <p>The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link:</p> <p>https://www.gnu.org/licenses/gpl-3.0.html</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/","title":"Arduino Core for SAMD21 CPU","text":"<p>This repository contains the source code and configuration files of the Arduino Core for Atmel's SAMD21 processor (used on the Arduino/Genuino Zero, MKR1000 and MKRZero boards).</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#installation-on-arduino-ide","title":"Installation on Arduino IDE","text":"<p>This core is available as a package in the Arduino IDE cores manager. Just open the \"Boards Manager\" and install the package called:</p> <p>\"Arduino SAMD Boards (32-bit ARM Cortex-M0+)\"</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#support","title":"Support","text":"<p>There is a dedicated section of the Arduino Forum for general discussion and project assistance:</p> <p>http://forum.arduino.cc/index.php?board=98.0</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#bugs-or-issues","title":"Bugs or Issues","text":"<p>If you find a bug you can submit an issue here on github:</p> <p>https://github.com/arduino/ArduinoCore-samd/issues</p> <p>Before posting a new issue, please check if the same problem has been already reported by someone else to avoid duplicates.</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#contributions","title":"Contributions","text":"<p>Contributions are always welcome. The preferred way to receive code cotribution is by submitting a  Pull Request on github.</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#hourly-builds","title":"Hourly builds","text":"<p>This repository is under a Continuous Integration system that every hour checks if there are updates and builds a release for testing (the so called \"Hourly builds\").</p> <p>The hourly builds are available through Boards Manager. If you want to install them:   1. Open the Preferences of the Arduino IDE.   2. Add this URL <code>http://downloads.arduino.cc/Hourly/samd/package_samd-hourly-build_index.json</code> in the Additional Boards Manager URLs field, and click OK.   3. Open the Boards Manager (menu Tools-&gt;Board-&gt;Board Manager...)   4. Install Arduino SAMD core - Hourly build   5. Select one of the boards under SAMD Hourly build XX in Tools-&gt;Board menu   6. Compile/Upload as usual</p> <p>If you already installed an hourly build and you want to update it with the latest:   1. Open the Boards Manager (menu Tools-&gt;Board-&gt;Board Manager...)   2. Remove Arduino SAMD core - Hourly build   3. Install again Arduino SAMD core - Hourly build, the Board Manager will download the latest build replacing the old one.</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#license-and-credits","title":"License and credits","text":"<p>This core has been developed by Arduino LLC in collaboration with Atmel.</p> <pre><code>  Copyright (c) 2015 Arduino LLC.  All right reserved.\n\n  This library is free software; you can redistribute it and/or\n  modify it under the terms of the GNU Lesser General Public\n  License as published by the Free Software Foundation; either\n  version 2.1 of the License, or (at your option) any later version.\n\n  This library is distributed in the hope that it will be useful,\n  but WITHOUT ANY WARRANTY; without even the implied warranty of\n  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n  See the GNU Lesser General Public License for more details.\n\n  You should have received a copy of the GNU Lesser General Public\n  License along with this library; if not, write to the Free Software\n  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n</code></pre>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/","title":"Arduino Zero Bootloader","text":""},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#1-prerequisites","title":"1- Prerequisites","text":"<p>The project build is based on Makefile system. Makefile is present at project root and try to handle multi-platform cases.</p> <p>Multi-plaform GCC is provided by ARM here: https://launchpad.net/gcc-arm-embedded/+download</p> <p>Atmel Studio contains both make and ARM GCC toolchain. You don't need to install them in this specific use case.</p> <p>For all builds and platforms you will need to have the Arduino IDE installed and the board support package for \"Arduino SAMD Boards (32-bits ARM Cortex-M0+)\". You can install the latter from the former's \"Boards Manager\" UI.</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#windows","title":"Windows","text":"<ul> <li> <p>Native command line Make binary can be obtained here: http://gnuwin32.sourceforge.net/packages/make.htm</p> </li> <li> <p>Cygwin/MSys/MSys2/Babun/etc... It is available natively in all distributions.</p> </li> <li> <p>Atmel Studio An Atmel Studio 7 Makefile-based project is present at project root, just open samd21_sam_ba.atsln file in AS7.</p> </li> </ul>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#linux","title":"Linux","text":"<p>Make is usually available by default.</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#os-x","title":"OS X","text":"<p>Make is available through XCode package.</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#2-selecting-available-sam-ba-interfaces","title":"2- Selecting available SAM-BA interfaces","text":"<p>By default both USB and UART are made available, but this parameter can be modified in sam_ba_monitor.h, line 31:</p> <p>Set the define SAM_BA_INTERFACE to * SAM_BA_UART_ONLY for only UART interface * SAM_BA_USBCDC_ONLY for only USB CDC interface * SAM_BA_BOTH_INTERFACES for enabling both the interfaces</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#3-behaviour","title":"3- Behaviour","text":"<p>This bootloader implements the double-tap on Reset button. By quickly pressing this button two times, the board will reset and stay in bootloader, waiting for communication on either USB or USART.</p> <p>The USB port in use is the USB Native port, close to the Reset button. The USART in use is the one available on pins D0/D1, labelled respectively RX/TX. Communication parameters are a baudrate at 115200, 8bits of data, no parity and 1 stop bit (8N1).</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#4-description","title":"4- Description","text":"<p>Pinmap</p> <p>The following pins are used by the program : PA25 : input/output (USB DP) PA24 : input/output (USB DM) PA11 : input (USART RX) PA10 : output (USART TX)</p> <p>The application board shall avoid driving the PA25, PA24, PB23 and PB22 signals while the boot program is running (after a POR for example).</p> <p>Clock system</p> <p>CPU runs at 48MHz from Generic Clock Generator 0 on DFLL48M.</p> <p>Generic Clock Generator 1 is using external 32kHz oscillator and is the source of DFLL48M.</p> <p>USB and USART are using Generic Clock Generator 0 also.</p> <p>Memory Mapping</p> <p>Bootloader code will be located at 0x0 and executed before any applicative code.</p> <p>Applications compiled to be executed along with the bootloader will start at 0x2000 (see linker script bootloader_samd21x18.ld).</p> <p>Before jumping to the application, the bootloader changes the VTOR register to use the interrupt vectors of the application @0x2000.&lt;- not required as application code is taking care of this.</p>"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#5-how-to-build","title":"5- How to build","text":"<p>If not specified the makefile builds for Arduino Zero:</p> <pre><code>make\n</code></pre> <p>if you want to make a custom bootloader for a derivative board you must supply all the necessary information in a <code>board_definitions_xxx.h</code> file, and add the corresponding case in <code>board_definitions.h</code>. For example for the Arduino MKR1000 we use <code>board_definitions_arduino_mkr1000.h</code> and it is build with the following command:</p> <pre><code>BOARD_ID=arduino_mkr1000 NAME=samd21_sam_ba_arduino_mkr1000 make clean all\n</code></pre>"},{"location":"stretch-firmware/arduino/hello_pimu/","title":"Stretch RE1/RE2 - Pimu Firmware","text":"<p>The Stretch Pimu Firmware runs on the Pimu PCBA. Pimu stands for Power+IMU. The firmware is responsible for </p> <ul> <li> <p>Monitoring / filtering / calibration of 9DOF IMU </p> </li> <li> <p>Battery voltage and current monitoring</p> </li> <li> <p>Buzzer</p> </li> <li> <p>Base case fan control</p> </li> <li> <p>Runstop monitoring</p> </li> <li> <p>Monitoring cliff sensors</p> </li> </ul>"},{"location":"stretch-firmware/arduino/hello_stepper/","title":"Stretch RE1/RE2 - Stepper Firmware","text":"<p>The Stretch Stepper Firmware runs on the Stepper PCBA found on the back of the Stretch stepper motors. It is a modified version of the open source Mechaduino project, which provides closed loop current control of a stepper motor.</p> <p>The firmware is organized so as to allow switching, via serial command, between 'stock Mechaduino mode' with its menu interface and the Hello Robot firmware with its RPC interface.</p> <p>The primary hardware modifications are:</p> <ul> <li>Higher current capacity by use of parallel motor drivers</li> <li>Locking JST connectors</li> <li>Addition of a 'sync' line for multi-dof synchronization</li> </ul> <p>The primary software modifications are:</p> <ul> <li> <p>RPC and serialization layer to manage controller parameters, controller commands, etc</p> </li> <li> <p>Trapezoidal trajectory generators based on the library MotionGenerator </p> </li> <li> <p>Handling of runstop and motor synchronization</p> </li> <li> <p>Implementation of a guarded move behavior</p> </li> <li> <p>Various controller variants, controller and parameter management functions</p> </li> </ul>"},{"location":"stretch-firmware/arduino/hello_wacc/","title":"Stretch RE1/RE2 - Wacc Firmware","text":"<p>The Stretch Wacc Firmware runs on the Wacc PCBA found inside the robot arm. Wacc stands for the 'Wrist + Accelerometer' board. The Wacc firmware provides</p> <ul> <li>Monitoring of the wrist accelerometer</li> <li>Monitoring the Arduino expansion header in the wrist</li> </ul> <p>The Wacc expansion header is electrically routed to potentially provide</p> <ul> <li>Analog in (x1)</li> <li>Digital in (x1)</li> <li>Digital out (x2)</li> <li>Serial SPI </li> <li>Serial I2C </li> <li>Serial UART</li> </ul> <p>See the Stretch RE1 Hardware User Guide for pin-out, electrical, and connector information.</p>"},{"location":"stretch-firmware/arduino/hello_wacc/#using-the-expansion-dio-header","title":"Using the Expansion DIO Header","text":""},{"location":"stretch-firmware/arduino/hello_wacc/#factory-interface","title":"Factory Interface","text":"<p>By default the Expansion DIO header is configured in firmware to provide two digital inputs, two digital outputs, and an analog input. This can be seen in the Wacc_status structure.</p> <pre><code>struct __attribute__ ((packed)) Wacc_Status{\n  float ax; //Accelerometer AX\n  float ay; //Accelerometer AY\n  float az; //Accelerometer AZ\n  int16_t a0; //expansion header analog in\n  uint8_t d0; //expansion header digital in\n  uint8_t d1; //expansion header digital in\n  uint8_t d2; //expansion header digital out\n  uint8_t d3; //expansion header digital out\n  uint32_t single_tap_count; //Accelerometer tap count\n  uint32_t state;\n  uint32_t timestamp; //ms, overflows every 50 days\n  uint32_t debug;\n};\n</code></pre> <p>The user can interact with these pins through the Stretch Body python interface. </p>"},{"location":"stretch-firmware/arduino/hello_wacc/#custom-interface","title":"Custom Interface","text":"<p>Advanced users may want to create a custom interface to the Expansion DIO header that utilizes SPI, I2C, or UART. For this, see the provided Stretch Firmware tutorials.</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ADXL343/","title":"Adafruit ADXL343 Accelerometer Driver","text":"<p>This driver is for the Adafruit ADXL343 Breakout (http://www.adafruit.com/products/), and is based on Adafruit's Unified Sensor Library (Adafruit_Sensor).</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ADXL343/#about-the-adxl343","title":"About the ADXL343","text":"<p>The ADXL343 is a digital accelerometer that supports both SPI and I2C mode, with adjustable data rata and 'range' (+/-2/4/8/16g).  The Adafruit_ADXL343 driver takes advantage of I2C mode to reduce the total pin count required to use the sensor.</p> <p>More information on the ADXL345 can be found in the datasheet: http://www.analog.com/static/imported-files/data_sheets/ADXL343.pdf</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ADXL343/#what-is-the-adafruit-unified-sensor-library","title":"What is the Adafruit Unified Sensor Library?","text":"<p>The Adafruit Unified Sensor Library (https://github.com/adafruit/Adafruit_Sensor) provides a common interface and data type for any supported sensor.  It defines some basic information about the sensor (sensor limits, etc.), and returns standard SI units of a specific type and scale for each supported sensor type.</p> <p>It provides a simple abstraction layer between your application and the actual sensor HW, allowing you to drop in any comparable sensor with only one or two lines of code to change in your project (essentially the constructor since the functions to read sensor data and get information about the sensor are defined in the base Adafruit_Sensor class).</p> <p>This is imporant useful for two reasons:</p> <p>1.) You can use the data right away because it's already converted to SI units that you understand and can compare, rather than meaningless values like 0..1023.</p> <p>2.) Because SI units are standardised in the sensor library, you can also do quick sanity checks working with new sensors, or drop in any comparable sensor if you need better sensitivity or if a lower cost unit becomes available, etc.</p> <p>Light sensors will always report units in lux, gyroscopes will always report units in rad/s, etc. ... freeing you up to focus on the data, rather than digging through the datasheet to understand what the sensor's raw numbers really mean.</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ADXL343/#about-this-driver","title":"About this Driver","text":"<p>Adafruit invests time and resources providing this open source code.  Please support Adafruit and open-source hardware by purchasing products from Adafruit!</p> <p>Written by Kevin (KTOWN) Townsend for Adafruit Industries.</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_FXAS21002C/","title":"Adafruit_FXAS21002C","text":"<p>Driver for the Adafruit FXAS21002C 3-Axis gyroscope breakout</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_FXOS8700/","title":"Adafruit_FXOS8700","text":"<p>Driver for the Adafruit FXOS8700 Accelerometer/Magnetometer Breakout</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/","title":"Adafruit NeoPixel Library","text":"<p>Arduino library for controlling single-wire-based LED pixels and strip such as the Adafruit 60 LED/meter Digital LED strip, the Adafruit FLORA RGB Smart Pixel, the Adafruit Breadboard-friendly RGB Smart Pixel, the Adafruit NeoPixel Stick, and the Adafruit NeoPixel Shield.</p> <p>After downloading, rename folder to 'Adafruit_NeoPixel' and install in Arduino Libraries folder. Restart Arduino IDE, then open File-&gt;Sketchbook-&gt;Library-&gt;Adafruit_NeoPixel-&gt;strandtest sketch.</p> <p>Compatibility notes: Port A is not supported on any AVR processors at this time</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#installation","title":"Installation","text":""},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#first-method","title":"First Method","text":"<ol> <li>In the Arduino IDE, navigate to Sketch &gt; Include Library &gt; Manage Libraries</li> <li>Then the Library Manager will open and you will find a list of libraries that are already installed or ready for installation.</li> <li>Then search for Neopixel strip using the search bar.</li> <li>Click on the text area and then select the specific version and install it.</li> </ol>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#second-method","title":"Second Method","text":"<ol> <li>Navigate to the Releases page.</li> <li>Download the latest release.</li> <li>Extract the zip file</li> <li>In the Arduino IDE, navigate to Sketch &gt; Include Library &gt; Add .ZIP Library</li> </ol>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#features","title":"Features","text":"<p>Controlling NeoPixels \u201cfrom scratch\u201d is quite a challenge, so we provide a library letting you focus on the fun and interesting bits.</p> <p>The library is free; you don\u2019t have to pay for anything. Adafruit invests time and resources providing this open source code, please support Adafruit and open-source hardware by purchasing products from Adafruit!</p> <p>We have included code for the following chips - sometimes these break for exciting reasons that we can't control in which case please open an issue!</p> <ul> <li>AVR ATmega and ATtiny (any 8-bit) - 8 MHz, 12 MHz and 16 MHz</li> <li>Teensy 3.x and LC</li> <li>Arduino Due</li> <li>Arduino 101</li> <li>ATSAMD21 (Arduino Zero/M0 and other SAMD21 boards) @ 48 MHz</li> <li>ATSAMD51 @ 120 MHz</li> <li>Adafruit STM32 Feather @ 120 MHz</li> <li>ESP8266 any speed</li> <li>ESP32 any speed</li> <li>Nordic nRF52 (Adafruit Feather nRF52), nRF51 (micro:bit)</li> <li>Infineon XMC1100 BootKit @ 32 MHz</li> <li>Infineon XMC1100 2Go @ 32 MHz</li> <li>Infineon XMC1300 BootKit  @ 32 MHz</li> <li>Infineon XMC4700 RelaxKit, XMC4800 RelaxKit, XMC4800 IoT Amazon FreeRTOS Kit @ 144 MHz</li> </ul> <p>Check forks for other architectures not listed here!</p> <ul> <li> </li> </ul> <p>Adafruit_NeoPixel is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#simple-to-use","title":"Simple to use","text":""},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#give-back","title":"Give back","text":""},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#supported-chipsets","title":"Supported Chipsets","text":""},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#gnu-lesser-general-public-license","title":"GNU Lesser General Public License","text":""},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#functions","title":"Functions","text":"<ul> <li>begin()</li> <li>updateLength()</li> <li>updateType()</li> <li>show()</li> <li>delay_ns()</li> <li>setPin()</li> <li>setPixelColor()</li> <li>fill()</li> <li>ColorHSV()</li> <li>getPixelColor()</li> <li>setBrightness()</li> <li>getBrightness()</li> <li>clear()</li> <li>gamma32()</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#examples","title":"Examples","text":"<p>There are many examples implemented in this library. One of the examples is below. You can find other examples here</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#simple","title":"Simple","text":"<pre><code>#include &lt;Adafruit_NeoPixel.h&gt;\n#ifdef __AVR__\n  #include &lt;avr/power.h&gt;\n#endif\n#define PIN        6\n#define NUMPIXELS 16\n\nAdafruit_NeoPixel pixels(NUMPIXELS, PIN, NEO_GRB + NEO_KHZ800);\n#define DELAYVAL 500\n\nvoid setup() {\n#if defined(__AVR_ATtiny85__) &amp;&amp; (F_CPU == 16000000)\n  clock_prescale_set(clock_div_1);\n#endif\n\n  pixels.begin();\n}\n\nvoid loop() {\n  pixels.clear();\n\n  for(int i=0; i&lt;NUMPIXELS; i++) {\n\n    pixels.setPixelColor(i, pixels.Color(0, 150, 0));\n    pixels.show();\n    delay(DELAYVAL);\n  }\n}\n</code></pre>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#contributing","title":"Contributing","text":"<p>If you want to contribute to this project:</p> <ul> <li>Report bugs and errors</li> <li>Ask for enhancements</li> <li>Create issues and pull requests</li> <li>Tell others about this library</li> <li>Contribute new protocols</li> </ul> <p>Please read CONTRIBUTING.md for details on our code of conduct, and the process for submitting pull requests to us.</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#roadmap","title":"Roadmap","text":"<p>The PRIME DIRECTIVE is to maintain backward compatibility with existing Arduino sketches -- many are hosted elsewhere and don't track changes here, some are in print and can never be changed!</p> <p>Please don't reformat code for the sake of reformatting code. The resulting large \"visual diff\" makes it impossible to untangle actual bug fixes from merely rearranged lines. (Exception for first item in wishlist below.)</p> <p>Things I'd Like To Do But There's No Official Timeline So Please Don't Count On Any Of This Ever Being Canonical:</p> <ul> <li>For the show() function (with all the delicate pixel timing stuff), break out each architecture into separate source files rather than the current unmaintainable tangle of #ifdef statements!</li> <li>Please don't use updateLength() or updateType() in new code. They should not have been implemented this way (use the C++ 'new' operator with the regular constructor instead) and are only sticking around because of the Prime Directive. setPin() is OK for now though, it's a trick we can use to 'recycle' pixel memory across multiple strips.</li> <li>In the M0 and M4 code, use the hardware systick counter for bit timing rather than hand-tweaked NOPs (a temporary kludge at the time because I wasn't reading systick correctly). (As of 1.4.2, systick is used on M4 devices and it appears to be overclock-compatible. Not for M0 yet, which is why this item is still here.)</li> <li>As currently written, brightness scaling is still a \"destructive\" operation -- pixel values are altered in RAM and the original value as set can't be accurately read back, only approximated, which has been confusing and frustrating to users. It was done this way at the time because NeoPixel timing is strict, AVR microcontrollers (all we had at the time) are limited, and assembly language is hard. All the 32-bit architectures should have no problem handling nondestructive brightness scaling -- calculating each byte immediately before it's sent out the wire, maintaining the original set value in RAM -- the work just hasn't been done. There's a fair chance even the AVR code could manage it with some intense focus. (The DotStar library achieves nondestructive brightness scaling because it doesn't have to manage data timing so carefully...every architecture, even ATtiny, just takes whatever cycles it needs for the multiply/shift operations.)</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#credits","title":"Credits","text":"<p>This library is written by Phil \"Paint Your Dragon\" Burgess for Adafruit Industries, with contributions by PJRC, Michael Miller and other members of the open source community.</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#license","title":"License","text":"<p>Adafruit_NeoPixel is free software: you can redistribute it and/or  modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. Adafruit_NeoPixel is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU Lesser General Public License along with NeoPixel.  If not, see this</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/CONTRIBUTING/","title":"Contribution Guidelines","text":"<p>This library is the culmination of the expertise of many members of the open source community who have dedicated their time and hard work. The best way to ask for help or propose a new idea is to create a new issue while creating a Pull Request with your code changes allows you to share your own innovations with the rest of the community.</p> <p>The following are some guidelines to observe when creating issues or PRs:</p> <ul> <li> <p>Be friendly; it is important that we can all enjoy a safe space as we are all working on the same project and it is okay for people to have different ideas</p> </li> <li> <p>Use code blocks; it helps us help you when we can read your code! On that note also refrain from pasting more than 30 lines of code in a post, instead create a gist if you need to share large snippets</p> </li> <li> <p>Use reasonable titles; refrain from using overly long or capitalized titles as they are usually annoying and do little to encourage others to help :smile:</p> </li> <li> <p>Be detailed; refrain from mentioning code problems without sharing your source code and always give information regarding your board and version of the library</p> </li> </ul>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel_ZeroDMA/","title":"Adafruit_NeoPixel_ZeroDMA","text":"<p>DMA-based NeoPixel library for SAMD21 and SAMD51 microcontrollers (Feather M0, M4, etc.) Doesn't require stopping interrupts, so millis() and micros() don't lose time, soft PWM (for servos, etc.) still operate normally.</p> <p>Requires LATEST Adafruit_NeoPixel and Adafruit_ZeroDMA libraries also be installed (Adafruit SAMD board support automatically includes Adafruit_ZeroDMA).</p> <p>THIS ONLY WORKS ON CERTAIN PINS. THIS IS NORMAL. Library uses SERCOM peripherals for SPI output, and the hardware only supports this on specific pins (plus, some SERCOMs are in use for Serial, I2C, etc.).</p> <p>AS OF VERSION 1.2.0: the selection of pins is more restrictive than before, to better avoid collisions with other peripherals (I2C, etc.)...with few exceptions, you can use DMA NeoPixels with impunity. And more boards are supported now.</p> <p>COMPATIBLE BOARDS AND PINS:</p> <ul> <li>Feather M0: pins 5, 6, 12 and MOSI*.</li> <li>Feather M0 Express: pins 6, 12 and MOSI*.</li> <li>Feather M4: pins 12, A2, A4 and MOSI*.</li> <li>ItsyBitsy M0: pins 5, 12 and MOSI*.</li> <li>ItsyBitsy M4: pins 2, 5, 12 and MOSI*.</li> <li>Metro M0 or Arduino Zero: pins 5, 12 and MOSI*.</li> <li>Metro M4: pins 6, 11, A3 and MOSI*.</li> <li>Metro M4 AirLift: pins 6, 11 and MOSI*.</li> <li>Grand Central: pins 11, 14, 23 and MOSI*.</li> <li>HalloWing M0: pins 4 (NEOPIX), 6 and MOSI*.</li> <li>HalloWing M4: pins 6, 8, A5 and MOSI*.</li> <li>MONSTER M4SK: pin 2.</li> <li>PyPortal, PyPortal Titano: pin 3 (SENSE connector).</li> <li>PyGamer, PyGamer Advance: pins 12 and A4.</li> <li>PyBadge, PyBadge AirLift: pins A4, MOSI*.</li> <li>Crickit M0: pins 8, 11, A8 and A11.</li> <li>Trellis M4: pin 10 (keypad NeoPixels).</li> <li>Circuit Playground M0: pin A2.</li> <li>Trinket M0: pin 4 (can't use simultaneously with I2C, SPI or Serial1).</li> <li>Gemma M0: pin D0 (can't use simultaneously with I2C, SPI or Serial1).</li> <li>QT Py: MOSI* and pin 16 (underside pad, can't use w/optional SPI flash).</li> <li>Arduino NANO 33 IoT: pins 4, 6, 7, A2, A3, MOSI*.</li> </ul> <p>* If using the MOSI pin on these boards, the corresponding SPI peripheral is not usable. A few add-ons (usually TFT display shields/wings) rely on SPI, so avoid NeoPixeling from this pin in such situations. MOSI is really only offered anymore to maintain partial compatibility with older projects that might've used the earlier library, which was less selective about such things.</p> <p>OTHER THINGS TO KNOW:</p> <p>DMA NeoPixels use a LOT of RAM: 12 bytes/pixel for RGB, 16 bytes/pixel for RGBW, about 4X as much as regular NeoPixel library (plus a little bit extra for structures &amp; stuff).</p> <p>0/1 bit timing does not precisely match NeoPixel/WS2812/SK6812 datasheet specs, but it seems to work well enough. Use at your own peril.</p> <p>Have not tested this yet with multiple instances (DMA-driven NeoPixels on multiple pins), but in theory it should work. Should also be OK mixing DMA and non-DMA NeoPixels in same sketch (just use different constructor and pins for each).</p> <p>Currently this only supports strip declaration with length &amp; pin known at compile time, so it's not a 100% drop-in replacement for all NeoPixel code right now. But probably 99%+ of all sketches are written that way, so it's perfectly usable for most. The stock NeoPixel library has the option of setting the length &amp; pin number at run-time (so these can be stored in a config file or in EEPROM)...but those functions are now considered deprecated, so we should be OK going forward.</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_Unified_Sensor/","title":"Adafruit Unified Sensor Driver","text":"<p>Many small embedded systems exist to collect data from sensors, analyse the data, and either take an appropriate action or send that sensor data to another system for processing.</p> <p>One of the many challenges of embedded systems design is the fact that parts you used today may be out of production tomorrow, or system requirements may change and you may need to choose a different sensor down the road.</p> <p>Creating new drivers is a relatively easy task, but integrating them into existing systems is both error prone and time consuming since sensors rarely use the exact same units of measurement.</p> <p>By reducing all data to a single sensors_event_t 'type' and settling on specific, standardised SI units for each sensor family the same sensor types return values that are comparable with any other similar sensor.  This enables you to switch sensor models with very little impact on the rest of the system, which can help mitigate some of the risks and problems of sensor availability and code reuse.</p> <p>The unified sensor abstraction layer is also useful for data-logging and data-transmission since you only have one well-known type to log or transmit over the air or wire.</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_Unified_Sensor/#unified-sensor-drivers","title":"Unified Sensor Drivers","text":"<p>The following drivers are based on the Adafruit Unified Sensor Driver:</p> <p>Accelerometers   - Adafruit_ADXL345   - Adafruit_LSM303DLHC   - Adafruit_MMA8451_Library</p> <p>Gyroscope   - Adafruit_L3GD20_U</p> <p>Light   - Adafruit_TSL2561   - Adafruit_TSL2591_Library</p> <p>Magnetometers   - Adafruit_LSM303DLHC   - Adafruit_HMC5883_Unified</p> <p>Barometric Pressure   - Adafruit_BMP085_Unified   - Adafruit_BMP183_Unified_Library</p> <p>Humidity &amp; Temperature   - Adafruit_DHT_Unified</p>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_Unified_Sensor/#how-does-it-work","title":"How Does it Work?","text":"<p>Any driver that supports the Adafruit unified sensor abstraction layer will implement the Adafruit_Sensor base class.  There are two main typedefs and one enum defined in Adafruit_Sensor.h that are used to 'abstract' away the sensor details and values:</p> <p>Sensor Types (sensors_type_t)</p> <p>These pre-defined sensor types are used to properly handle the two related typedefs below, and allows us determine what types of units the sensor uses, etc.</p> <pre><code>/** Sensor types */\ntypedef enum\n{\n  SENSOR_TYPE_ACCELEROMETER         = (1),\n  SENSOR_TYPE_MAGNETIC_FIELD        = (2),\n  SENSOR_TYPE_ORIENTATION           = (3),\n  SENSOR_TYPE_GYROSCOPE             = (4),\n  SENSOR_TYPE_LIGHT                 = (5),\n  SENSOR_TYPE_PRESSURE              = (6),\n  SENSOR_TYPE_PROXIMITY             = (8),\n  SENSOR_TYPE_GRAVITY               = (9),\n  SENSOR_TYPE_LINEAR_ACCELERATION   = (10),\n  SENSOR_TYPE_ROTATION_VECTOR       = (11),\n  SENSOR_TYPE_RELATIVE_HUMIDITY     = (12),\n  SENSOR_TYPE_AMBIENT_TEMPERATURE   = (13),\n  SENSOR_TYPE_VOLTAGE               = (15),\n  SENSOR_TYPE_CURRENT               = (16),\n  SENSOR_TYPE_COLOR                 = (17)\n} sensors_type_t;\n</code></pre> <p>Sensor Details (sensor_t)</p> <p>This typedef describes the specific capabilities of this sensor, and allows us to know what sensor we are using beneath the abstraction layer.</p> <pre><code>/* Sensor details (40 bytes) */\n/** struct sensor_s is used to describe basic information about a specific sensor. */\ntypedef struct\n{\n    char     name[12];\n    int32_t  version;\n    int32_t  sensor_id;\n    int32_t  type;\n    float    max_value;\n    float    min_value;\n    float    resolution;\n    int32_t  min_delay;\n} sensor_t;\n</code></pre> <p>The individual fields are intended to be used as follows:</p> <ul> <li>name: The sensor name or ID, up to a maximum of twelve characters (ex. \"MPL115A2\")</li> <li>version: The version of the sensor HW and the driver to allow us to differentiate versions of the board or driver</li> <li>sensor_id: A unique sensor identifier that is used to differentiate this specific sensor instance from any others that are present on the system or in the sensor network</li> <li>type: The sensor type, based on sensors_type_t in sensors.h</li> <li>max_value: The maximum value that this sensor can return (in the appropriate SI unit)</li> <li>min_value: The minimum value that this sensor can return (in the appropriate SI unit)</li> <li>resolution: The smallest difference between two values that this sensor can report (in the appropriate SI unit)</li> <li>min_delay: The minimum delay in microseconds between two sensor events, or '0' if there is no constant sensor rate</li> </ul> <p>Sensor Data/Events (sensors_event_t)</p> <p>This typedef is used to return sensor data from any sensor supported by the abstraction layer, using standard SI units and scales.</p> <p><pre><code>/* Sensor event (36 bytes) */\n/** struct sensor_event_s is used to provide a single sensor event in a common format. */\ntypedef struct\n{\n    int32_t version;\n    int32_t sensor_id;\n    int32_t type;\n    int32_t reserved0;\n    int32_t timestamp;\n    union\n    {\n        float           data[4];\n        sensors_vec_t   acceleration;\n        sensors_vec_t   magnetic;\n        sensors_vec_t   orientation;\n        sensors_vec_t   gyro;\n        float           temperature;\n        float           distance;\n        float           light;\n        float           pressure;\n        float           relative_humidity;\n        float           current;\n        float           voltage;\n        sensors_color_t color;\n    };\n} sensors_event_t;\n</code></pre> It includes the following fields:</p> <ul> <li>version: Contain 'sizeof(sensors_event_t)' to identify which version of the API we're using in case this changes in the future</li> <li>sensor_id: A unique sensor identifier that is used to differentiate this specific sensor instance from any others that are present on the system or in the sensor network (must match the sensor_id value in the corresponding sensor_t enum above!)</li> <li>type: the sensor type, based on sensors_type_t in sensors.h</li> <li>timestamp: time in milliseconds when the sensor value was read</li> <li>data[4]: An array of four 32-bit values that allows us to encapsulate any type of sensor data via a simple union (further described below)</li> </ul> <p>Required Functions</p> <p>In addition to the two standard types and the sensor type enum, all drivers based on Adafruit_Sensor must also implement the following two functions:</p> <p><pre><code>bool getEvent(sensors_event_t*);\n</code></pre> Calling this function will populate the supplied sensors_event_t reference with the latest available sensor data.  You should call this function as often as you want to update your data.</p> <p><pre><code>void getSensor(sensor_t*);\n</code></pre> Calling this function will provide some basic information about the sensor (the sensor name, driver version, min and max values, etc.</p> <p>Standardised SI values for sensors_event_t</p> <p>A key part of the abstraction layer is the standardisation of values on SI units of a particular scale, which is accomplished via the data[4] union in sensors_event_t above.  This 16 byte union includes fields for each main sensor type, and uses the following SI units and scales:</p> <ul> <li>acceleration: values are in meter per second per second (m/s^2)</li> <li>magnetic: values are in micro-Tesla (uT)</li> <li>orientation: values are in degrees</li> <li>gyro: values are in rad/s</li> <li>temperature: values in degrees centigrade (Celsius) </li> <li>distance: values are in centimeters</li> <li>light: values are in SI lux units</li> <li>pressure: values are in hectopascal (hPa)</li> <li>relative_humidity: values are in percent</li> <li>current: values are in milliamps (mA)</li> <li>voltage: values are in volts (V)</li> <li>color: values are in 0..1.0 RGB channel luminosity and 32-bit RGBA format</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_Unified_Sensor/#the-unified-driver-abstraction-layer-in-practice","title":"The Unified Driver Abstraction Layer in Practice","text":"<p>Using the unified sensor abstraction layer is relatively easy once a compliant driver has been created.</p> <p>Every compliant sensor can now be read using a single, well-known 'type' (sensors_event_t), and there is a standardised way of interrogating a sensor about its specific capabilities (via sensor_t).</p> <p>An example of reading the TSL2561 light sensor can be seen below:</p> <pre><code> Adafruit_TSL2561 tsl = Adafruit_TSL2561(TSL2561_ADDR_FLOAT, 12345);\n ...\n /* Get a new sensor event */ \n sensors_event_t event;\n tsl.getEvent(&amp;event);\n\n /* Display the results (light is measured in lux) */\n if (event.light)\n {\n   Serial.print(event.light); Serial.println(\" lux\");\n }\n else\n {\n   /* If event.light = 0 lux the sensor is probably saturated\n      and no reliable data could be generated! */\n   Serial.println(\"Sensor overload\");\n }\n</code></pre> <p>Similarly, we can get the basic technical capabilities of this sensor with the following code:</p> <pre><code> sensor_t sensor;\n\n sensor_t sensor;\n tsl.getSensor(&amp;sensor);\n\n /* Display the sensor details */\n Serial.println(\"------------------------------------\");\n Serial.print  (\"Sensor:       \"); Serial.println(sensor.name);\n Serial.print  (\"Driver Ver:   \"); Serial.println(sensor.version);\n Serial.print  (\"Unique ID:    \"); Serial.println(sensor.sensor_id);\n Serial.print  (\"Max Value:    \"); Serial.print(sensor.max_value); Serial.println(\" lux\");\n Serial.print  (\"Min Value:    \"); Serial.print(sensor.min_value); Serial.println(\" lux\");\n Serial.print  (\"Resolution:   \"); Serial.print(sensor.resolution); Serial.println(\" lux\");  \n Serial.println(\"------------------------------------\");\n Serial.println(\"\");\n</code></pre>"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ZeroDMA/","title":"Adafruit_ZeroDMA library","text":"<p>DMA helper/wrapped for ATSAMD21 such as Arduino Zero &amp; Feather M0</p> <p>Current version of this library no longer requires Adafruit_ASFcore as a prerequisite. However...IT BREAKS COMPATIBILITY WITH PRIOR VERSIONS. Function names, calling sequence and return types/values have changed. See examples!</p> <p>Item(s) in 'utility' directory are much pared-down derivatives of Atmel ASFcore 3 files. Please keep their original copyright and license intact when editing.</p>"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/","title":"FlashStorage library for Arduino","text":"<p>The FlashStorage library aims to provide a convenient way to store and retrieve user's data using the non-volatile flash memory of microcontrollers.</p> <p>The flash memory, due to his properties, is generally used to store the firmware code, but it can also be used to store user data.</p>"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#supported-hardware","title":"Supported hardware","text":"<p>Currently, only ATSAMD21 cpu is supported (and consequently every board based on this cpu like the Arduino Zero or Aduino MKR1000).</p>"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#limited-number-of-writes","title":"Limited number of writes","text":"<p>The flash memory has a limited amount of write cycles. Typical flash memories can perform about 10000 writes cycles to the same flash block before starting to \"wear out\" and begin to lose the ability to retain data.</p> <p>So BEWARE: IMPROPER USE OF THIS LIBRARY CAN QUICKLY AND PERMANENTLY DESTROY THE FLASH MEMORY OF YOUR MICRO, in particular you should avoid to call the <code>write()</code> function too often and make sure that in the entire life of the micro the number of calls to <code>write</code> stay well below the above limit of 10000 (it's a good rule-of-thumb to keep that number in mind even if the manufacturer of the micro guarantees a bigger number of cycles).</p> <p>The same caution must be taken if you're using the EEPROM API emulation (see below) with the <code>EEPROM.commit()</code> function.</p>"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#usage","title":"Usage","text":"<p>First of all you must declare a global <code>FlashStorage</code> object for each piece of data you intend to store in the flash memory. For example if you want to store the age of a person you must declare an <code>age_storage</code> like this:</p> <pre><code>FlashStorage(age_storage, int);\n</code></pre> <p>this instruction means \"create a <code>FlashStorage</code> to store an <code>int</code> variable and call it <code>age_storage</code>\". Now you can use <code>age_storage</code> as a place to safely store an integer:</p> <pre><code>void readAndStoreUserAge() {\n  Serial.println(\"Please enter your age:\");\n  String age = Serial.readStringUntil('\\n');\n\n  age_storage.write(age.toInt());  // &lt;-- save the age\n}\n</code></pre> <p>after a reset of the microcontroller to retrieve the stored age you can use:</p> <pre><code>int user_age = age_storage.read();\n</code></pre>"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#using-the-alternative-eeprom-like-api","title":"Using the alternative EEPROM-like API","text":"<p>If you include <code>FlashAsEEPROM.h</code> you'll get an EEPROM emulation with the internal flash memory. See EmulateEEPROM sketch for an example.</p> <p>The API is very similar to the well known Arduino EEPROM.h API but with two additional functions:</p> <ul> <li><code>EEPROM.isValid()</code> returns <code>true</code> if data in the EEPROM is valid or, in other words, if the data has been written at least once, otherwise EEPROM data is \"undefined\" and the function returns <code>false</code>.</li> <li><code>EEPROM.commit()</code> store the EEPROM data in flash. Use this with care: Every call writes the complete EEPROM data to flash. This will reduce the remainig flash-write-cycles. Don't call this method in a loop or you will kill your flash soon.</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#license","title":"License","text":"<p>This library is released under LGPL-2.1.</p>"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#faq","title":"FAQ","text":""},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#can-i-use-a-single-flashstorage-object-to-store-more-stuff","title":"Can I use a single FlashStorage object to store more stuff?","text":"<p>Yes, you can declare a <code>struct</code> with more fields and create a <code>FlashStorage</code> object to store the entire structure. See the StoreNameAndSurname sketch for an example on how to do it.</p>"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#the-content-of-the-flashstorage-is-erased-each-time-a-new-sketch-is-uploaded","title":"The content of the FlashStorage is erased each time a new sketch is uploaded?","text":"<p>Yes, every time you upload a new sketch, the previous content of the FlashStorage is erased.</p>"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#do-you-recommend-to-use-flash-instead-of-eeprom","title":"Do you recommend to use FLASH instead of EEPROM?","text":"<p>No. If your micro provides an EEPROM it's almost always better to use that because it's a kind of memory designed with the specific purpose to store user data (it has a longer lifetime, number of write cycles, etc...).</p> <p>In the absence of an EEPROM you can use this library to use a piece of the flash memory as an alternative to EEPROM but you must always keep in mind his limits.</p>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/","title":"SparkFun BNO080 IMU Library","text":"<p>SparkFun IMU BNO080 (SEN-14686)</p> <p>The BNO080/BNO085 IMU has a combination triple axis accelerometer/gyro/magnetometer packaged with an ARM Cortex M0+ running powerful algorithms. This enables the BNO080 Inertial Measurement Unit (IMU) to produce accurate rotation vector headings with an error of 5 degrees or less. It's what we've been waiting for: all the sensor data is combined into meaningful, accurate IMU information.</p> <p>This IC was designed to be implemented in Android based cellular phones to handle all the computations necessary for virtual reality goggles using only your phone. The sensor is quite powerful but with power comes a complex interface. We've written an I<sup>2</sup>C based library that provides the rotation vector (the reading most folks want from an IMU) as well as raw acceleration, gyro, and magnetometer readings. The sensor is capable of communicating over SPI and UART as well!</p> <p>In addition the BNO080 IMU provides a built-in step counter, tap detector, activity classifier (are you running, walking, or sitting still?), and a shake detector. We are duly impressed.</p> <p>Library written by Nathan Seidle (SparkFun).</p> <p>Thanks to all those who have helped improve the library:</p> <ul> <li>blendmaster for adding Linear Accel report</li> <li>per1234 for fixing our keywords file</li> <li>fm4dd for typo - PR 19</li> <li>tstellanova for heading accuracy correction - PR 40</li> <li>badVibes for gyro integrated rotation vector support - PR 41</li> <li>Filimindji for AR/VR Stabilized RotationVector and AR/VR Stabilized GameRotationVector support - PR 46</li> <li>ya-mouse for the getreadings improvements - PR 55</li> <li>Guillaume for the read-multiple-values helper functions and the interrupt example - PR56 &amp; PR59</li> <li>aedancullen for the tap detector - PR 64</li> <li>mattbradford83 for the hasReset code and example - PR 92</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/#repository-contents","title":"Repository Contents","text":"<ul> <li>/examples - Example sketches for the library (.ino). Run these from the Arduino IDE.</li> <li>/src - Source files for the library (.cpp, .h).</li> <li>keywords.txt - Keywords from this library that will be highlighted in the Arduino IDE.</li> <li>library.properties - General library properties for the Arduino package manager.</li> <li>CONTRIBUTING.md - guidance on how to contribute to this library.</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/#documentation","title":"Documentation","text":"<ul> <li>Installing an Arduino Library Guide - Basic information on how to install an Arduino library.</li> <li>Product Repository - Main repository (including hardware files)</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/#license-information","title":"License Information","text":"<p>This product is open source!</p> <p>Please review the LICENSE.md file for license information.</p> <p>If you have any questions or concerns on licensing, please contact techsupport@sparkfun.com.</p> <p>Please use, reuse, and modify these files as you see fit. Please maintain attribution to SparkFun Electronics and release any derivative under the same license.</p> <p>Distributed as-is; no warranty is given.</p> <ul> <li>Your friends at SparkFun.</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/CONTRIBUTING/","title":"CONTRIBUTING","text":""},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/CONTRIBUTING/#how-to-contribute","title":"How to Contribute","text":"<p>Thank you so much for offering to help out. We truly appreciate it.</p> <p>If you'd like to contribute, start by searching through the issues and pull requests to see whether someone else has raised a similar idea or question.</p> <p>If you decide to add a feature to this library, please create a PR following these best practices:</p> <ul> <li>Change as little as possible. Do not sumbit a PR that changes 100 lines of whitespace. Break up into multiple PRs if necessary.</li> <li>If you've added a new feature document it with a simple example sketch. This serves both as a test of your PR and as a quick way for users to quickly learn how to use your new feature.</li> <li>If you add new functions also add them to keywords.txt so that they are properly highlighted in Arduino. Read more.</li> <li>Please submit your PR using the release-candidate branch. That way, we can merge and test your PR quickly without changing the main branch</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/CONTRIBUTING/#style-guide","title":"Style guide","text":"<p>Please read and follow the Arduino API style guide. Also read and consider the Arduino style guide.</p>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/ISSUE_TEMPLATE/","title":"ISSUE TEMPLATE","text":""},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/ISSUE_TEMPLATE/#subject-of-the-issue","title":"Subject of the issue","text":"<p>Describe your issue here.</p>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/ISSUE_TEMPLATE/#your-workbench","title":"Your workbench","text":"<ul> <li>What development board or microcontroller are you using?</li> <li>What version of hardware or breakout board are you using? </li> <li>How is the breakout board to your microcontroller?</li> <li>How is everything being powered?</li> <li>Are there any additional details that may help us help you?</li> </ul>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/ISSUE_TEMPLATE/#steps-to-reproduce","title":"Steps to reproduce","text":"<p>Tell us how to reproduce this issue. Please post stripped down example code demonstrating your issue.</p>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/ISSUE_TEMPLATE/#expected-behavior","title":"Expected behavior","text":"<p>Tell us what should happen</p>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/ISSUE_TEMPLATE/#actual-behavior","title":"Actual behavior","text":"<p>Tell us what happens instead</p>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/LICENSE/","title":"SparkFun License Information","text":"<p>SparkFun uses two different licenses for our files \u2014 one for hardware and one for code.</p>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/LICENSE/#hardware","title":"Hardware","text":"<p>SparkFun hardware is released under Creative Commons Share-alike 4.0 International.</p> <p>Note: This is a human-readable summary of (and not a substitute for) the license.</p> <p>You are free to:</p> <p>Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms:</p> <p>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices:</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p>"},{"location":"stretch-firmware/arduino/libraries/SparkFun_BNO080_Cortex_Based_IMU/LICENSE/#code","title":"Code","text":"<p>SparkFun code, firmware, and software is released under the MIT License(http://opensource.org/licenses/MIT).</p> <p>The MIT License (MIT)</p> <p>Copyright (c) 2016 SparkFun Electronics</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/","title":"Data Transfer","text":"<p> NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board.</p> <p>NOTE: These tutorials may require the latest version of Stretch Body. If necessary, please update your install.</p>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#integrating-custom-data","title":"Integrating Custom Data","text":"<p>In this tutorial we explore how to plumb custom data to and from the Arduino based Stretch Wacc (Wrist + Accelerometer) board. This enables users to integrate custom sensors and actuators on to the Wrist Expansion header. </p>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#how-data-transfer-happens-in-stretch-body","title":"How Data Transfer Happens in Stretch Body","text":"<p>Data is transferred between Stretch Body Python and the Arduino based firmware using USB based serial. </p> <p>As an example, consider the transfer of a <code>Status</code> data message from the Wacc to Stretch Body. This involves:</p> <ul> <li>Stretch Body requests new Status data via <code>wacc.pull_status()</code></li> <li>This generates an RPC call to the Wacc board</li> <li>The <code>status</code> data on the Wacc board gets 'packed' and transmitted back to Stretch Body</li> <li>Stretch Body unpacks the data into the <code>status</code> dictionary of the Wacc Python class.</li> </ul> <p>Similarly, data can go the other direction (e.g., <code>Command</code> messages ).</p> <p></p> <p>Fortunately, the data transfer is managed automatically for the developer. In order to integrate your custom data you will:</p> <ul> <li>Extend the firmware  <code>Status</code> and <code>Command</code> structs to include your data</li> <li>Derive your own class from the Stretch Body <code>Wacc</code> class that packs and unpacks your custom data</li> </ul>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#calculator-example","title":"Calculator Example","text":"<p>As a simple example we will extend the Wacc to be an embedded calculator. The implementation is found in the Wacc_Calc Arduino sketch and the corresponding WaccCalc Python class.</p>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#arduino","title":"Arduino","text":""},{"location":"stretch-firmware/docs/tutorial_data_transfer/#define-data-structures","title":"Define data structures","text":"<p>First, we define the data to be sent back and forth in Common.h of the firmware.</p> <pre><code>struct __attribute__ ((packed)) Calc_Command{\n  float var1;\n  float var2;\n  uint8_t op;\n};\nstruct __attribute__ ((packed)) Calc_Status{\n  float result;\n};\n</code></pre> <p>Our calculator will perform the computation: <code>result=op(var1,var2)</code>.</p> <p>Now add these structs to the  <code>Status</code> and  <code>Command</code> structs in  Common.h:</p> <pre><code>struct __attribute__ ((packed)) Wacc_Command{\n  Calc_Command calc;\n  ...\n};\n\nstruct __attribute__ ((packed)) Wacc_Status{\n  Calc_Status calc;\n  ...\n};\n</code></pre> <p>The ordering of the data is important.  Your custom data should be at the start of the struct as the Python class will unpack this data first.</p>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#define-calculator-function","title":"Define calculator function","text":"<p>Next we add the calculator function to  Wacc.cpp:</p> <pre><code>float my_calc(uint8_t op, float var1, float var2)\n{\n  if (op==0)\n    return var1+var2;\n  if (op==1)\n    return var1*var2;\n  if (op==2)\n    return var1/var2;\n  return 0;\n}\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#integrate-calculator-into-the-control-loop","title":"Integrate calculator into the control loop","text":"<p>Finally, we integrate our calculator into the embedded control loop in Wacc.cpp. </p> <p>The function <code>stepWaccController()</code> in  Wacc.cpp is called by Timer5 at 700Hz. The calculator is fairly lightweight so its computation time should not interfere with the existing Wacc timing. Heavier computation would require careful integration with an eye to loop timing.</p> <p>Add the call to <code>stepWaccController()</code>  -- just prior to the <code>Status</code> data being copied out for transmittal back. </p> <pre><code>void stepWaccController()\n{\n...\nstat.calc.result=my_calc(cmd.calc.op,cmd.calc.var1, cmd.calc.var2);\nmemcpy((uint8_t *) (&amp;stat_out),(uint8_t *) (&amp;stat),sizeof(Wacc_Status));\n}\n</code></pre> <p>The variable <code>cmd</code>, which contains the command to the calculator, is automatically updated with fresh data via the RPC mechanism.</p>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#bump-protocol-versions","title":"Bump protocol versions","text":"<p>The packet definition of data exchanged between Python and the Arduino is tagged with a protocol version. This allows the Python Device to ensure it is exchanging compatible data. </p> <p>The mainline release of Stretch Firmware starts with protocol version <code>0</code> and increments with each new protocol release. To avoid conflicts, for this tutorial we pick an arbitrary large number (99). </p> <p>In Common.h , we bump from Protocol '0' </p> <pre><code>#define FIRMWARE_VERSION \"Wacc.v0.0.1p0\"\n</code></pre> <p>to Protocol '99'</p> <pre><code>#define FIRMWARE_VERSION \"Wacc.v0.0.1p99\"\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#python","title":"Python","text":"<p>Now we will implement WaccCalc which  derives from the Wacc Python class.</p> <pre><code>from stretch_body.wacc import *\nfrom stretch_body.transport import *\n\nclass WaccCalc(Wacc):\n    \"\"\"\n    This class demonstrates how to extend the Wacc class with custom data\n    See the corresponding tutorial for more information.\n    \"\"\"\n    def __init__(self,verbose=False):\n        Wacc.__init__(self,verbose=verbose,\n                      ext_status_cb=self.ext_unpack_status, #Set callback to unpack status\n                      ext_command_cb=self.ext_pack_command) #Set callback to pack command\n\n        self._command['calc']={'op':0,'var1':0,'var2':0} #Extend command dictionary with custom fields\n        self.status['calc'] =0.0                         #Extend status dictionary with custom fields\n        self.valid_firmware_protocol = 'pMyCalc'\n\n    def calculate(self,op,var1,var2):\n        \"\"\"\n        0: addition\n        1: multiplication\n        2: division\n        \"\"\"\n        self._command['calc']['op'] =int(op)\n        self._command['calc']['var1']=float(var1)\n        self._command['calc']['var2']=float(var2)\n        self._dirty_command=True\n\n    def pretty_print(self):\n        Wacc.pretty_print(self)\n        print 'Calc:',self.status['calc']\n\n    def ext_unpack_status(self,s):\n        \"\"\"\n        s: byte array to unpack\n        return: number of bytes unpacked\n        \"\"\"\n        sidx=0\n        self.status['calc'] = unpack_float_t(s[sidx:])\n        return 4\n\n    def ext_pack_command(self,s,sidx):\n        \"\"\"\n        s: byte array to pack in to\n        sidx: index to start packing at\n        return: new sidx\n        \"\"\"\n        pack_float_t(s, sidx, self._command['calc']['var1'])\n        sidx += 4\n        pack_float_t(s, sidx, self._command['calc']['var2'])\n        sidx += 4\n        pack_uint8_t(s, sidx, self._command['calc']['op'])\n        sidx += 1\n        return sidx\n</code></pre> <p>The class registers two callbacks for packing <code>Command</code> data and unpacking <code>Status</code> data.  They are fairly self-explanatory and can be easily extended to match your custom data.  Looking at the unpacking code: <pre><code>  def ext_unpack_status(self,s):\n        \"\"\"\n        s: byte array to unpack\n        return: number of bytes unpacked\n        \"\"\"\n        sidx=0\n        self.status['calc'] = unpack_float_t(s[sidx:])\n        return 4\n</code></pre></p> <p>, we see the use of <code>unpack_float_t</code>. This, and other functions to unpack data, are found in <code>stretch_body.transport</code>. </p> <p>It is important that the data types and order match exactly those declared in the firmware. For example in  Common.h we have:</p> <pre><code>struct __attribute__ ((packed)) Calc_Command{\n  float var1;\n  float var2;\n  uint8_t op;\n};\n</code></pre> <p>, and on the Python side we have</p> <pre><code>pack_float_t(s, sidx, self._command['calc']['var1'])\nsidx += 4\npack_float_t(s, sidx, self._command['calc']['var2'])\nsidx += 4\npack_uint8_t(s, sidx, self._command['calc']['op'])\nsidx += 1\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#test-the-calculator-ipython","title":"Test the Calculator - iPython","text":"<p>We're ready to try out our calculator. First,</p> <ul> <li>Install and setup the Arduino IDE if it isn't already as described in the Updating Firmware tutorial.</li> <li>Open the Wacc_Calc Arduino sketch in the Arduino IDE.  </li> <li>Select the <code>hello_wacc</code> board, the  <code>ttyACMx</code> port that maps to the Wacc board. Then burn the firmware as described in the the Updating Firmware tutorial.</li> </ul> <p>Now, lets try it out:</p> <pre><code>&gt;&gt;$ cd ~/repos/stretch_firmware/tutorial/python/\n&gt;&gt;$ ipython\n</code></pre> <p>And from iPython</p> <pre><code>In [1]: import wacc_calc\nIn [2]: w=wacc_calc.WaccCalc()\nIn [3]: w.startup()\nIn [4]: w.calculate(op=0,var1=100.0,var2=200.0)\nIn [5]: w.push_command()\nIn [6]: w.pull_status()\nIn [7]: print 'Result is',w.status['calc']\nResult is 300.0\n\nIn [8]: w.calculate(op=1,var1=100.0,var2=200.0)\nIn [9]: w.push_command()\nIn [10]: w.pull_status()\nIn [11]: print 'Result is',w.status['calc']\nResult is 20000.0\n\nIn [12]: w.calculate(op=2,var1=100.0,var2=200.0)\nIn [13]: w.push_command()\nIn [14]: w.pull_status()\nIn [15]: print 'Result is',w.status['calc']\nResult is 0.5\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#test-the-calculator-script","title":"Test the Calculator - Script","text":"<p>Alternatively you can use the provided tool, stretch_wacc_calc_jog.py. Here you can use the calculator through the menu.</p> <pre><code>hello-robot@stretch-re1-100x:~$ cd repos/stretch_firmware/tutorial/python/\nhello-robot@stretch-re1-100x:~/repos/stretch_firmware/tutorial/python$ ./stretch_wacc_calc_jog.py \n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\nX\n---Calculate Op(Var1,Var2) ---\nOp=0: Add\nOp=1: Mult\nOp=2: Div\nEnter Op\n1\nEnter Var1\n12\nEnter Var2\n13\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#_1","title":"Data Transfer","text":""},{"location":"stretch-firmware/docs/tutorial_data_transfer/#adding-yaml-parameters","title":"Adding YAML Parameters","text":"<p>The <code>WaccCalc</code> class can use the Stretch Body YAML files as well. For example, add to your <code>stretch_re1_user_params.yaml</code>:</p> <pre><code>wacc:\n  calc_scalar: 2.0\n</code></pre> <p>Now let's scale the commands to the Arduino according to this YAML parameter. In <code>WaccCalc</code>:</p> <pre><code>    def calculate(self,op,var1,var2):\n        \"\"\"\n        0: addition\n        1: multiplication\n        2: division\n        \"\"\"\n        self._command['calc']['op'] =int(op)\n        self._command['calc']['var1']=self.params['calc_scalar']*float(var1)\n        self._command['calc']['var2']=self.params['calc_scalar']*float(var2)\n        self._dirty_command=True\n</code></pre> <p>Run the iPython as above and you'll see the values multiplied by the YAML scalar.</p>"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#final-steps","title":"Final Steps","text":"<p>We want the Stretch Body Robot to use <code>WaccCalc</code> and not <code>Wacc</code>. To do this, add the following to your <code>stretch_re1_user_params.yaml</code>:</p> <pre><code>robot:\n  custom_wacc:\n    py_class_name: WaccCalc\n    py_module_name: wacc_calc\n</code></pre> <p>This tells Robot which module and class to create its <code>Wacc</code> instance from. </p> <p>Now, test that it works from iPython</p> <pre><code>In [1]: import stretch_body.robot\n\nIn [2]: robot=stretch_body.robot.Robot()\nStarting TransportConnection on: /dev/hello-wacc\nIn [3]: robot.startup()\n\nIn [4]: robot.wacc.calculate(op=0,var1=100.0,var2=200.0)\nIn [5]: robot.push_command()\nIn [6]: print 'Result',robot.wacc.status['calc']\nResult 300.0\n\nIn [7]: robot.stop()\n</code></pre>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/","title":"I2C Sensor","text":"<p> NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board.</p> <p>NOTE: These tutorials may require the latest version of Stretch Body. If necessary, please update your install.</p>"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#integrating-an-i2c-device","title":"Integrating an I2C Device","text":"<p>This tutorial illustrates the integration of a I2C device on to the Wrist Expansion header. It extends t. We recommend doing the Data Transfer tutorial first. </p>"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#calculator-via-i2c","title":"Calculator via I2C","text":"<p>In this tutorial we will run the calculator from the Data Transfer tutorial on an Adafruit Metro M0 Express (Arduino Zero) that is running as an I2C slave. It will take a calculator <code>Command</code> from the Wacc and return the result in <code>Status</code> message. </p> <p></p>"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#flash-firmware","title":"Flash Firmware","text":"<p>First, program the Metro with the provided sketch, zero_wacc_i2c. Be sure to:</p> <ul> <li>Select the board's port from the IDE under Tools/Port</li> <li>Select the board 'Arduino UNO' from the IDE under Tools/Board</li> </ul> <p>Next, program the Wacc with the provided sketch, hello_wacc_i2c. Be sure to:</p> <ul> <li>Select the board's port from the IDE under Tools/Port</li> <li>Select the board 'Adafruit M0 Express' from the IDE under Tools/Board</li> </ul>"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#code-walk-through","title":"Code Walk-through","text":"<p>The code is straightforward and is a natural extension of the code described in the  Data Transfer tutorial. A few sections to highlight in the  hello_wacc_i2c sketch are:</p> <p>In<code>setupWacc()</code> we add code to configure the I2C. </p> <pre><code>Wire.begin(); \n</code></pre> <p>In <code>Wacc.cpp</code> we add the I2C code:</p> <pre><code>#include &lt;Wire.h&gt;\n\nuint8_t i2c_out[9];   //I2C data out\nuint8_t i2c_in[9];    //I2C data in\nuint8_t ds_i2c_cnt=0; //Down sample counter\nfloat FS_I2C = 10;    //Rate to run transactions (Hz)\nint buf_idx=0;\n\nvoid i2cTransaction()\n{\n  //Send the commmand\n  memcpy(i2c_out, (uint8_t *) (&amp;cmd.calc), sizeof(Calc_Command));\n  Wire.beginTransmission(4); // transmit to device #4\n  for(int i=0;i&lt;sizeof(Calc_Command);i++)\n  {\n    Wire.write(i2c_out[i]);\n  }\n  Wire.endTransmission();    \n\n  //Get the result\n  Wire.requestFrom(4, sizeof(Calc_Status));\n  int buf_idx=0;\n  while (Wire.available() &amp;&amp; buf_idx&lt;sizeof(Calc_Status)) // loop through all but the last\n  {\n    uint8_t x = Wire.read();      \n    i2c_in[buf_idx++]=x;\n  }\n  memcpy((uint8_t *) (&amp;stat.calc),i2c_in,  sizeof(Calc_Status));\n}\n</code></pre> <p>Here, the 9 bytes of the <code>Calc_Command</code> are transferred out and the 4 bytes of the <code>Calc_Status</code> are received. </p> <p>Note: This simple communication protocol is not robust to handshaking errors, etc</p> <p>Note: The Wacc uses I2C to also communicate with its onboard accelerometer --  the ADXL343.  In our example we are using I2C address <code>4</code> to communicate with our Metro slave. The ADXL343 is configured to use addrex <code>0xA6</code> for a write and <code>0xA7</code> for a read.</p> <p>Finally, we call the <code>i2cTransaction()</code> function at a rate of <code>FS_I2C</code> by adding to <code>stepWaccController()</code>:</p> <pre><code> if (ds_i2c_cnt++ &gt;= (FS_CTRL / FS_I2C))\n  {\n    ds_i2c_cnt = 0;\n    i2cTransaction();\n  }\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#wire-up-the-boards","title":"Wire Up the Boards","text":"<p>Next, wire the Metro to the Expansion Header as:</p> Stretch Expansion Header Uno SCL SCL SDA SDA GND GND"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#test-the-calculator","title":"Test the Calculator","text":"<p>Now, test the setup using the provided tool, stretch_wacc_calc_jog.py. As shown below, The Metro performs the calculation of 12*13 and the result is report back to Stretch Body.</p>"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#hello-robotstretch-re1-100x-cd-reposstretch_firmwaretutorialpython-hello-robotstretch-re1-100xreposstretch_firmwaretutorialpython-stretch_wacc_calc_jogpy-menu-m-menu-r-reset-board-a-set-d2-on-b-set-d2-off-c-set-d3-on-d-set-d3-off-x-do-calculation-x-calculate-opvar1var2-op0-add-op1-mult-op2-div-enter-op-1-enter-var1-12-enter-var2-13-menu-m-menu-r-reset-board-a-set-d2-on-b-set-d2-off-c-set-d3-on-d-set-d3-off-x-do-calculation-ax-ms2-00488623343408-ay-ms2-0155020624399-az-ms2-100049753189-a0-349-d0-in-0-d1-in-1-d2-out-70-d3-out-0-single-tap-count-26-state-0-debug-0-timestamp-159158874527-board-version-waccguthriev1-firmware-version-waccv001pmyspi-calc-1560-menu-m-menu-r-reset-board-a-set-d2-on-b-set-d2-off-c-set-d3-on-d-set-d3-off-x-do-calculation-","title":"<pre><code>hello-robot@stretch-re1-100x:~$ cd repos/stretch_firmware/tutorial/python/\nhello-robot@stretch-re1-100x:~/repos/stretch_firmware/tutorial/python$ ./stretch_wacc_calc_jog.py \n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\nX\n---Calculate Op(Var1,Var2) ---\nOp=0: Add\nOp=1: Mult\nOp=2: Div\nEnter Op\n1\nEnter Var1\n12\nEnter Var2\n13\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\n\n------------------------------\nAx (m/s^2) 0.0488623343408\nAy (m/s^2) 0.155020624399\nAz (m/s^2) -10.0049753189\nA0 349\nD0 (In) 0\nD1 (In) 1\nD2 (Out) 70\nD3 (Out) 0\nSingle Tap Count 26\nState  0\nDebug 0\nTimestamp 1591588745.27\nBoard version: Wacc.Guthrie.V1\nFirmware version: Wacc.v0.0.1pMySPI\nCalc: 156.0\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\n</code></pre>","text":"All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-firmware/docs/tutorial_serial_sensor/","title":"Serial Sensor","text":"<p> NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board.</p> <p>NOTE: These tutorials may require the latest version of Stretch Body. If necessary, please update your install.</p>"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#integrating-a-serial-device","title":"Integrating a Serial Device","text":"<p>This tutorial illustrates the integration of a UART device on to the Wrist Expansion header. We recommend first reading Data Transfer tutorial to understand how data is transfered back and forth from Stretch Body to the SAMD uC. </p> <p>In this tutorial we will extend the Stretch Body Wacc class to send 10 floats down to the custom serial device. The serial device will echo the 10 floats back up to Stretch Body.</p> <p>For the purposes of the tutorial we will wire the Wacc up in a loopback configuration as a stand-in for an actual physical serial device.</p>"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#loopback-hardware-setup","title":"Loopback Hardware Setup","text":"<p>Connect the UART TX pin to the UART RX pin of the Stretch Expansion Header (or the Metro M0 if emulating the Wacc). Connector information for the Expansion Header is found in the Hardware Guide.</p> <p></p>"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#flash-firmware","title":"Flash Firmware","text":"<p>Pull down the latest version of Stretch Firmware</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_firmware\n</code></pre> <p>Next, program the (emulated) Wacc with the provided sketch, hello_wacc_serial. Be sure to:</p> <ul> <li>Select the board's port from the IDE under Tools/Port</li> <li>Select the board 'Hello Wacc' from the IDE under Tools/Board</li> </ul>"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#firmware-code-walk-through","title":"Firmware Code Walk-through","text":"<p>First, in the sketch <code>setup()</code> we configure enable the SerialExt device</p> <pre><code>  SerialExt.begin(115200);\n</code></pre> <p>Next, in Common.h we define Command data to send down from Stretch Body to the serial device. We also define Status data to report back to Stretch Body. For our example we'll send 10 floats down and 10 floats back up.</p> <pre><code>struct __attribute__ ((packed)) SerialExtCommand{\n  float data[10];\n};\nstruct __attribute__ ((packed)) SerialExtStatus{\n  float data[10];\n};\n</code></pre> <p>In <code>Wacc.cpp</code> we add the code that will communicate with the serial device. Whenever Stretch Body pushes a new command down the Wacc, this function will get called.  Here we are writing the Command data to the SerialExt port. We then send back in the Status message the data we read from SerialExt. As it is a physical loopback connection, the data back in Status will be the same as the Command message.</p> <pre><code>void serial_comms()\n{\n  char * data_down=(char*)&amp;(cmd.serial.data[0]);\n  char * data_up=(char*)(stat.serial.data);\n  for(int i=0;i&lt;40;i++)\n  {\n    SerialExt.write(data_down+i,1);\n  }\n  for(int i=0;i&lt;40;i++)\n  {\n    if (SerialExt.available())\n    {\n      data_up[i]=SerialExt.read();\n    }\n  }\n}\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#stretch-body-code-walk-through","title":"Stretch Body Code Walk-through","text":"<p>We provide an example class WaccSerialExt that extends the Wacc class of Stretch Body. This class provides two call backs that will get called on <code>pull_status</code> and <code>push_command</code> respectively. Here we see the packing and unpacking of the 10 floats found in Common.h </p> <pre><code>    def ext_unpack_status(self,s):\n        \"\"\"\n        s: byte array to unpack\n        return: number of bytes unpacked\n        \"\"\"\n        sidx=0\n        for i in range(self.n_float):\n            self.status['serial_ext'][i] = unpack_float_t(s[sidx:])\n            sidx+=4\n        return sidx\n\n    def ext_pack_command(self,s,sidx):\n        \"\"\"\n        s: byte array to pack in to\n        sidx: index to start packing at\n        return: new sidx\n        \"\"\"\n        for i in range(self.n_float):\n            pack_float_t(s, sidx, self._command['serial_ext'][i])\n            sidx += 4\n        return sidx\n</code></pre> <p>We also define a function that generates a new 'command' down to the serial device. In this case it just increments the 10 floats by one:</p> <pre><code>    def serial_data_increment(self):\n        for i in range(self.n_float):\n            self._command['serial_ext'][i] =float(self._command['serial_ext'][i]+1)\n        self._dirty_command=True\n</code></pre> <p>Finally, we do a simple test of the class with the tool stretch_wacc_serial_jog.py</p> <pre><code>from wacc_serial_ext import WaccSerialExt\nw=WaccSerialExt()\nw.startup()\ntry:\n    while True:\n        print('Hit enter to do TX/RX cycle')\n        raw_input()\n        w.serial_data_increment()\n        w.push_command()\n        print('TX to SerialExt', w._command['serial_ext'])\n        w.pull_status()\n        print('RX from SerialExt', w.status['serial_ext'])\nexcept (KeyboardInterrupt, SystemExit):\n    w.stop()\n</code></pre> <p>Run it from the command line and verify that the 10 floats are being command down to your serial device, through the loopback connection, and back, up to the WaccSerialExt class:</p> <pre><code>&gt;&gt;$ cd ~/repos/stretch_firmware/tutorial/python\n&gt;&gt;$ ./stretch_wacc_serial_jog.py \nHit enter to do TX/RX cycle\n..\n('TX to SerialExt', [2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])\n('RX from SerialExt', [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\nHit enter to do TX/RX cycle\n\n('TX to SerialExt', [3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0])\n('RX from SerialExt', [2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])\nHit enter to do TX/RX cycle\n\n..\n</code></pre> <p>NOTE: It takes one control cycle for the command values to be reported back to the status</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-firmware/docs/tutorial_spi_sensor/","title":"SPI Sensor","text":"<p> NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board.</p> <p>NOTE: These tutorials may require the latest version of Stretch Body. If necessary, please update your install.</p>"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#integrating-an-spi-device","title":"Integrating an SPI Device","text":"<p>This tutorial illustrates the integration of a SPI device on to the Wrist Expansion header. It extends the Data Transfer tutorial. We recommend doing this tutorial prior to doing this one.</p>"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#calculator-via-spi","title":"Calculator via SPI","text":"<p>In this tutorial we will run the calculator from the Data Transfer tutorial on an Adafruit Metro M0 Express (Arduino Zero) that is running as an SPI slave. It will take a calculator <code>Command</code> from the Wacc and return the result in <code>Status</code> message. </p> <p></p>"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#flash-firmware","title":"Flash Firmware","text":"<p>First, program the Metro with the provided sketch, zero_wacc_spi. Be sure to:</p> <ul> <li>Select the board's port from the IDE under Tools/Port</li> <li>Select the board 'Adafruit M0 Express' from the IDE under Tools/Board</li> </ul> <p>Next, program the Wacc with the provided sketch, hello_wacc_spi. Be sure to:</p> <ul> <li>Select the board's port from the IDE under Tools/Port</li> <li>Select the board 'Hello Wacc' from the IDE under Tools/Board</li> </ul>"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#code-walk-through","title":"Code Walk-through","text":"<p>The code is straightforward and is a natural extension of the code described in the  Data Transfer tutorial. A few sections to highlight in the  hello_wacc_spi sketch are:</p> <p>First, in the sketch <code>setup()</code> we configure the slave select pin to be an output</p> <pre><code>pinMode(HEADER_SPI_SS,OUTPUT);\n</code></pre> <p>Next in <code>setupWacc()</code> we add code to configure the SPI</p> <pre><code>SPISettings settingsA(100000, MSBFIRST, SPI_MODE1);\nSPI.begin();\nSPI.beginTransaction(settingsA);\n</code></pre> <p>In <code>Wacc.cpp</code> we add the SPI code:</p> <pre><code>#include &lt;SPI.h&gt;\n\nuint8_t spi_out[9];   //SPI data out\nuint8_t spi_in[9];    //SPI data in\nuint8_t ds_spi_cnt=0; //Down sample counter\nfloat FS_SPI = 10;    //Rate to run transactions (Hz)\n\nvoid spiTransaction()\n{\n  digitalWrite(HEADER_SPI_SS, LOW);\n  SPI.transfer('X'); //Mark start of transaction\n  memcpy(spi_out, (uint8_t *) (&amp;cmd.calc), sizeof(Calc_Command));\n  for (uint8_t idx = 0; idx &lt; 9; idx++)\n    spi_in[idx] = SPI.transfer(spi_out[idx]);\n  digitalWrite(HEADER_SPI_SS, HIGH);\n  memcpy((uint8_t *) (&amp;stat.calc),spi_in+1,  sizeof(Calc_Status));\n}\n</code></pre> <p>Here, the 9 bytes of the <code>Calc_Command</code> are transferred out and the 4 bytes of the <code>Calc_Status</code> are received. </p> <p>Note: This simple communication protocol is not robust, using an 'X' to demarcate the start of a transaction</p> <p>Finally, we call the <code>spiTransaction()</code> function at a rate of <code>FS_SPI</code> by adding to <code>stepWaccController()</code>:</p> <pre><code> if (ds_spi_cnt++ &gt;= (FS_CTRL / FS_SPI))\n  {\n    ds_spi_cnt = 0;\n    spiTransaction();\n  }\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#wire-up-the-boards","title":"Wire Up the Boards","text":"<p>Next, wire the Metro to the Expansion Header as:</p> Stretch Expansion Header Uno SS SS MISO MISO MOSI MOSI SCK SCK GND GND <p>Note: Other Arduino boards can be used. However 5V Arduinos boards will need their SPI lines level shifted to 3V3.</p>"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#test-the-calculator","title":"Test the Calculator","text":"<p>Now, test the setup using the provided tool, stretch_wacc_calc_jog.py. As shown below, The Arduino Uno performs the calculation of 12*13 and the result is report back to Stretch Body.</p> <pre><code>hello-robot@stretch-re1-100x:~$ cd repos/stretch_firmware/tutorial/python/\nhello-robot@stretch-re1-100x:~/repos/stretch_firmware/tutorial/python$ ./stretch_wacc_calc_jog.py \n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\nX\n---Calculate Op(Var1,Var2) ---\nOp=0: Add\nOp=1: Mult\nOp=2: Div\nEnter Op\n1\nEnter Var1\n12\nEnter Var2\n13\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\n\n------------------------------\nAx (m/s^2) 0.0488623343408\nAy (m/s^2) 0.155020624399\nAz (m/s^2) -10.0049753189\nA0 349\nD0 (In) 0\nD1 (In) 1\nD2 (Out) 70\nD3 (Out) 0\nSingle Tap Count 26\nState  0\nDebug 0\nTimestamp 1591588745.27\nBoard version: Wacc.Guthrie.V1\nFirmware version: Wacc.v0.0.1pMySPI\nCalc: 156.0\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\n</code></pre>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-firmware/docs/tutorial_updating_firmware/","title":"Updating Firmware","text":""},{"location":"stretch-firmware/docs/tutorial_updating_firmware/#updating-stretch-firmware","title":"Updating Stretch Firmware","text":"<p>Stretch has 6 Arduino based microcontroller boards that may require a firmware update:</p> Board USB Device Arduino Sketch Left wheel stepper /dev/hello-motor-left-wheel hello_stepper.ino Right wheel stepper /dev/hello-motor-right-wheel hello_stepper.ino Lift stepper /dev/hello-motor-lift hello_stepper.ino Arm stepper /dev/hello-motor-arm hello_stepper.ino Pimu /dev/hello-pimu hello_pimu.ino Wacc /dev/hello-wacc hello_wacc.ino <p>Firmware updates are managed through the REx_firmware_updater.py tool, which uses the Arduino Cli tool to flash the firmware.</p> <p>Before doing an upgrade first ensure the latest Stretch Body is installed as well as Stretch Factory</p> <pre><code>&gt;&gt;$ pip install  hello-robot-stretch-factory --upgrade --no-cache-dir\n&gt;&gt;$ pip install  hello-robot-stretch-body --upgrade --no-cache-dir\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_updating_firmware/#firmware-update-tool","title":"Firmware Update Tool","text":"<p>The firmware update tool will automatically recommend and install the latest version of firmware found on GitHub. It will only recommend versions that are compatible with the currently installed Stretch Body.</p> <pre><code>&gt;&gt;$ REx_firmware_updater.py --help\nusage: REx_firmware_updater.py [-h]\n                               [--current | --available | --recommended | --install | --install_version | --install_branch | --install_path INSTALL_PATH | --mgmt]\n                               [--pimu] [--wacc] [--arm] [--lift]\n                               [--left_wheel] [--right_wheel]\n\nUpload Stretch firmware to microcontrollers\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --current             Display the currently installed firmware versions\n  --available           Display the available firmware versions\n  --recommended         Display the recommended firmware\n  --install             Install the recommended firmware\n  --install_version     Install a specific firmware version\n  --install_branch      Install the HEAD of a specific branch\n  --install_path        INSTALL_PATH Install the firmware on the provided path (eg ./stretch_firmware/arduino)\n  --mgmt                Display overview on firmware management\n  --pimu                Upload Pimu firmware\n  --wacc                Upload Wacc firmware\n  --arm                 Upload Arm Stepper firmware\n  --lift                Upload Lift Stepper firmware\n  --left_wheel          Upload Left Wheel Stepper firmware\n  --right_wheel         Upload Right Wheel Stepper firmware\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_updating_firmware/#updating-to-the-latest-firmware","title":"Updating to the Latest Firmware","text":"<p>To update to the latest version of firmware:</p> <p><pre><code>&gt;&gt;$ REx_firmware_updater.py --install\nCollecting information...\n######################################## Recommended Firmware Updates ########################################\n\n\nDEVICE                    | INSTALLED                 | RECOMMENDED               | ACTION                    \n--------------------------------------------------------------------------------------------------------------\nHELLO-WACC                | Wacc.v0.0.2p1             | Wacc.v0.0.1p0             | Downgrade recommended     \nHELLO-MOTOR-LIFT          | Stepper.v0.1.0p1          | Stepper.v0.0.4p0          | Downgrade recommended     \nHELLO-PIMU                | Pimu.v0.0.2p1             | Pimu.v0.0.1p0             | Downgrade recommended     \nHELLO-MOTOR-ARM           | Stepper.v0.1.0p1          | Stepper.v0.0.4p0          | Downgrade recommended     \nHELLO-MOTOR-LEFT-WHEEL    | Stepper.v0.1.0p1          | Stepper.v0.0.4p0          | Downgrade recommended     \nHELLO-MOTOR-RIGHT-WHEEL   | Stepper.v0.1.0p1          | Stepper.v0.0.4p0          | Downgrade recommended   \n######################################### UPDATING FIRMWARE TO... ###########################################\nHELLO-WACC                | Downgrading to Wacc.v0.0.1p0             \nHELLO-MOTOR-LEFT-WHEEL    | Downgrading to Stepper.v0.0.4p0          \nHELLO-MOTOR-RIGHT-WHEEL   | Downgrading to Stepper.v0.0.4p0          \nHELLO-MOTOR-LIFT          | Downgrading to Stepper.v0.0.4p0          \nHELLO-PIMU                | Downgrading to Pimu.v0.0.1p0             \nHELLO-MOTOR-ARM           | Downgrading to Stepper.v0.0.4p0          \n------------------------------------------------\nWARNING: (1) Updating robot firmware should only be done by experienced users\nWARNING: (2) Do not have other robot processes running during update\nWARNING: (3) Leave robot powered on during update\nWARNING: (4) Ensure Lift has support clamp in place\nWARNING: (5) Lift may make a loud noise during programming. This is normal.\nProceed with update?? [y/N]: \n</code></pre> Review the recommendations and warnings before proceeding ('y'). If you prefer to only update one or more boards you can specify it on the command line. For example: <pre><code>REx_firmware_updater.py --install --arm --wacc\n</code></pre></p>"},{"location":"stretch-firmware/docs/tutorial_updating_firmware/#other-useful-commands","title":"Other Useful Commands","text":"<p>Running  <code>REx_firmware_updater.py --current</code> will report the currently install firmware:  <pre><code>&gt;&gt;$REx_firmware_updater.py --current\n######################################## Currently Installed Firmware ########################################\n------------ HELLO-WACC ------------\nInstalled Firmware: Wacc.v0.0.2p1\nInstalled Stretch Body supports protocols: p0 , p1\nInstalled protocol p1 : VALID\n------------ HELLO-MOTOR-LIFT ------------\nInstalled Firmware: Stepper.v0.1.0p1\nInstalled Stretch Body supports protocols: p0 , p1\nInstalled protocol p1 : VALID\n------------ HELLO-PIMU ------------\nInstalled Firmware: Pimu.v0.0.2p1\nInstalled Stretch Body supports protocols: p0 , p1\nInstalled protocol p1 : VALID\n------------ HELLO-MOTOR-ARM ------------\nInstalled Firmware: Stepper.v0.1.0p1\nInstalled Stretch Body supports protocols: p0 , p1\nInstalled protocol p1 : VALID\n------------ HELLO-MOTOR-LEFT-WHEEL ------------\nInstalled Firmware: Stepper.v0.1.0p1\nInstalled Stretch Body supports protocols: p0 , p1\nInstalled protocol p1 : VALID\n------------ HELLO-MOTOR-RIGHT-WHEEL ------------\nInstalled Firmware: Stepper.v0.1.0p1\nInstalled Stretch Body supports protocols: p0 , p1\nInstalled protocol p1 : VALID\n</code></pre></p> <p>Running  <code>REx_firmware_updater.py --available</code> will list the available versions on GitHub:</p> <pre><code>&gt;&gt;$ REx_firmware_updater.py --available\nCollecting information...\n####################### Currently Tagged Versions of Stretch Firmware on Master Branch #######################\n---- HELLO-WACC ----\nWacc.v0.0.1p0\n---- HELLO-MOTOR-LIFT ----\nStepper.v0.0.1p0\nStepper.v0.0.2p0\nStepper.v0.0.3p0\nStepper.v0.0.4p0\n---- HELLO-PIMU ----\nPimu.v0.0.1p0\n---- HELLO-MOTOR-ARM ----\nStepper.v0.0.1p0\nStepper.v0.0.2p0\nStepper.v0.0.3p0\nStepper.v0.0.4p0\n---- HELLO-MOTOR-LEFT-WHEEL ----\nStepper.v0.0.1p0\nStepper.v0.0.2p0\nStepper.v0.0.3p0\nStepper.v0.0.4p0\n---- HELLO-MOTOR-RIGHT-WHEEL ----\nStepper.v0.0.1p0\nStepper.v0.0.2p0\nStepper.v0.0.3p0\nStepper.v0.0.4p0\n</code></pre>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/","title":"Wacc Emulaton","text":""},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#emulating-the-wacc","title":"Emulating the Wacc","text":"<p>In this tutorial we describe how to emulate the Arduino based Stretch Wacc (Wrist + Accelerometer) board using an off-the-shelf Arduino. This enables users to develop and test custom code for the Wacc board prior to deploying it to the actual robot hardware.</p> <p>Emulation involves:</p> <ul> <li>Installing the Wacc firmware on an Arduino Zero compatible board ( Adafruit Metro M0 Express )</li> <li>Mapping the UDEV rules to the Metro</li> </ul> <p>This tutorial assumes the Metro board is plugged into one of Stretch's external USB ports. It is also possible to install Stretch Body on a developer Ubuntu machine if the robot is not available (see below).</p>"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#hardware","title":"Hardware","text":"<p>We will emulate the Wacc on the Metro board as both boards use the Atmel SAMD21G18A-AUT processor.  </p> <p>The primary difference between the Wacc and the Metro is that the Wacc integrates an ADXL343 3 axis accelerometer on its I2C bus. As this chip isn't interfaced to our emulation hardware, this data will be missing. The Wacc also lacks the On/Off switch and RGB LED of the Metro.</p> <p></p> <p>Only a subset of its pins of the Wacc (shown below) are used compared to the Metro. You can attach your custom hardware to the Metro as if it were the Stretch Expansion Header.</p> <p></p>"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#firmware","title":"Firmware","text":"<p>First, we'll install the factory Wacc firmware on the Metro. Custom Wacc firmware could be installed using the same process.</p> <p>First, download the firmware repo onto the development machine if it isn't already there:</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_firmware\n</code></pre> <p>Next, install and setup the Arduino IDE has described here.</p> <p>Now, with the Metro M0 plugged in to a USB port on Stretch:</p> <ul> <li>Open the hello_wacc sketch</li> <li>Select Tools/Board/Hello Wacc</li> <li>Select the Metro board under Tools/Port </li> <li>Upload the firmware. The red LED on the Metro should flash at 1Hz.</li> </ul> <p>Note: Take care to not accidentally burn firmware to the wrong board. You can check the mapping of boards to ports by:</p> <pre><code>&gt;&gt;$ ls -l /dev/hello*\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#setup-udev","title":"Setup UDEV","text":"<p>Next we need for the Metro to appear as a device named <code>/dev/hello-wacc</code>. For this:</p> <ul> <li>Run 'sudo dmesg -c' to clear the system log</li> <li>Hit the reset button on the Metro</li> <li>Run 'sudo dmesg | grep Serial</li> </ul> <p>You'll see the Metro serial number:</p> <pre><code>&gt;&gt;$ sudo dmesg -c | grep Serial\n[810971.303719] usb 1-3: New USB device strings: Mfr=1, Product=2, SerialNumber=3\n[810971.303725] usb 1-3: SerialNumber: 9261CC655150484735202020FF0C270C\n</code></pre> <p>Copy the serial number.  Edit the udev file:</p> <pre><code>&gt;&gt;$ sudo nano /etc/udev/rules.d/95-hello-arduino.rules\n</code></pre> <p>Find the entry for <code>hello-wacc</code> and update <code>ATTRS{serial}</code> to the new serial number.</p> <pre><code>KERNEL==\"ttyACM*\", ATTRS{idVendor}==\"2341\", ATTRS{idProduct}==\"804d\",MODE:=\"0666\", ATTRS{serial}==\"SERIAL\", SYMLINK+=\"hello-wacc\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\n</code></pre> <p>, where SERIAL is the serial number captured from dmesg.</p> <p>Now reload the udev rule</p> <pre><code>&gt;&gt;$ sudo udevadm control --reload\n</code></pre> <p>Now hit the reset button of the Metro and check that it is mapped:</p> <pre><code>&gt;&gt;$ ls -l /dev/hello-wacc \nlrwxrwxrwx 1 root root 7 Jun 10 20:04 /dev/hello-wacc -&gt; ttyACM0\n</code></pre> <p>Note: When switching back to Stretch's internal Wacc board you'll need to restore the UDEV file. This can be done by:</p> <pre><code>&gt;&gt;$ sudo cp $HELLO_FLEET_PATH/$HELLO_FLEET_ID/udev/95-hello-arduino.rules /etc/udev/rules.d/\n</code></pre>"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#test-the-emulated-wacc","title":"Test the Emulated Wacc","text":"<p>Now, check that your emulated Wacc is working. You should see the Timestamp and A0 values moving when you hit 'Enter'. Now you're ready to integrate your custom hardware.</p> <pre><code>&gt;&gt;$ stretch_wacc_jog.py \n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\n-------------------\n\n------------------------------\nAx (m/s^2) 0.0\nAy (m/s^2) 0.0\nAz (m/s^2) 0.0\nA0 375\nD0 (In) 1\nD1 (In) 1\nD2 (Out) 0\nD3 (Out) 0\nSingle Tap Count 0\nState  0\nDebug 0\nTimestamp 1591846120.85\nBoard version: Wacc.Guthrie.V1\nFirmware version: Wacc.v0.0.1p0\n</code></pre>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-firmware/docs/tutorials_overview/","title":"Overview","text":""},{"location":"stretch-firmware/docs/tutorials_overview/#integrating-custom-hardware","title":"Integrating Custom Hardware","text":"<p>These tutorials demonstrate how to use the Stretch wrist expansion header to integrate custom hardware into Stretch Body.</p> <p>NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC.  Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete.  The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board.</p> Tutorial Description Data Transfer How to plumb your custom data from the wrist Arduino through Stretch Body Wrist Board Emulation How to emulate the Stretch wrist board using an off the shelf Arduino SPI Sensor How to integrate an SPI sensor I2C Sensor How to integrate an I2C sensor Serial Sensor How to integrate a Serial UART sensor"},{"location":"stretch-hardware-guides/","title":"Overview","text":""},{"location":"stretch-hardware-guides/#overview","title":"Overview","text":"<p>The Stretch Hardware Guides repository maintains documentation on the use and specifications of the Stretch mobile manipulators hardware.</p> Stretch RE2Stretch RE1 Model Guide Description RE2 Safety Guide  Safety guide for users of the Stretch RE2 Battery Maintenance Guide Guide to care for and charge the Stretch RE2 Batteries RE2 Hardware Guide Specification and functional description of the Stretch RE2 Hardware RE2 Dex Wrist Guide Installing, configuring, and working with the Stretch RE2 Dex Wrist Model Guide Description RE1 Safety Guide  Safety guide for users of the Stretch RE1 Battery Maintenance Guide Guide to care for and charge the Stretch RE1 Batteries RE1 Hardware Guide Specification and functional description of the Stretch RE1 Hardware RE1 Dex Wrist Guide Installing, configuring, and working with the Stretch RE1 Dex Wrist"},{"location":"stretch-hardware-guides/#license","title":"License","text":"<p>For details, see the LICENSE.md file in the root directory.</p>  All materials are Copyright 2022-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-hardware-guides/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains documentation exclusively for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>The Contents are licensed under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license (the \"License\"); you may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>https://creativecommons.org/licenses/by-nd/4.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Patents pending and trademark rights cover the Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\"</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/","title":"Stretch RE1 - Battery Maintenance Guide","text":""},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#overview","title":"Overview","text":"<p>Stretch RE1 utilizes two 12V AGM SLA batteries that provide a combined 18AH of capacity. Maintaining an adequate level of charge on the battery system will enhance the battery lifetime.</p> <p>The run time for a fully charged system is dependent on the load use case. The majority of battery power is consumed by the NUC computer as Stretch uses relatively low power motors.</p> <p>A fully charged robot running a high CPU load can run approximately 2 hours before requiring a recharge.</p> <p>A fully charged robot powered on but with minimal load can run approximately 5 hours before requiring a recharge.</p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#charger","title":"Charger","text":"<p>Stretch ships with a NOCO Genius 10 charger. Earlier versions of Stretch use the NOCO G7200. These two chargers are functionally very similar.</p> <p>Please review the battery charger user manuals prior to following the guidance in this document. </p> <ul> <li>Genius 10 Manual </li> <li>G7200 Manual</li> </ul> <p>Stretch utilizes four of the available modes on these chargers.</p> Mode Function STANDBY Charger not charging the robot 12V AGM Charging  robot powered down SUPPLY 1) Power the robot during tethered use2) Repair damaged batteries. REPAIR Repair damaged batteries."},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#noco-genius-10-interface","title":"NOCO Genius 10 - Interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached2) Press and hold MODE button for 3s3) Press MODE button until SUPPLY indicator is illuminated4) Attach charger REPAIR 1) From STANDBY, charger attached2) Press and hold MODE button for 3s3) Press MODE button until REPAIR indicator is illuminated"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#noco-g7200-interface","title":"NOCO G7200 - Interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached2) Press and hold MODE button for 3s3) Press MODE button until SUPPLY indicator is illuminated4) Attach charger REPAIR 1) From STANDBY, charger attached2) Press and hold MODE button for 3s3) Press MODE button until REPAIR indicator is illuminated"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#checking-the-battery-charge","title":"Checking the Battery Charge","text":"<p>The battery charger LEDs provide an approximate indicator of battery charge when it is in 12V AGM mode. Batteries are 100% fully charge when the green LED is not fading in and out.</p> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#charging-best-practices","title":"Charging Best Practices","text":"<p>It is possible to accidentally deeply discharge the batteries by leaving the robot on for long durations without the charger attached. This is similar to leaving the lights on your car where the battery will continue to drain until fully discharged.</p> <p>We recommend following the best practices below to avoid deep discharge of the batteries and to ensure they have a long lifespan.</p> Use Case Best Practice Reason Robot is in use - tethered Leave the charger attached in SUPPLY mode while developing on the robot whenever possible.Shutdown and power off the robot when development is done. Running the robot while attempting to charge in 12V AGM mode can cause issues and is generally bad for battery health. SUPPLY mode is preferred whenever the robot needs to be powered on. Robot is in use - untethered Regularly check the battery voltage using the command line tool.Shutdown the computer and power off the robot when voltage falls below 11.5V.Attach the charger in 12V AGM mode.Charge to 100% before resuming operation. The 12V AGM charge mode expects the battery voltage to be above 10.5-11V in order to operate. Robot is not in use Shutdown the computer and turn off the robot power. Leave the charger attached and place it in 12V AGM mode. Leaving the robot power on may cause the batteries to deep dischargeThe charger will maintain a \u2018trickle charge\u2019 on the battery, keeping the charge at 100%. Robot is coming out of storage Attach charger in 12V AGM mode and charge for 2-3 hours until charger reports 100% SLA batteries naturally lose charge over time due to \u2018self-discharge\u2019."},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#when-to-plug-in-the-charger","title":"When To Plug in the Charger","text":"<p>We recommend keeping the charger attached whenever the robot is not running untethered.</p> <p>When the battery voltage drops below \u2018low voltage\u2019 threshold the robot will produce an intermittent double beep sound. This is a reminder to the user to plug in the charger.</p> <p>If desired, the intermittent beep functionality can be disabled by setting the <code>stop_at_low_voltage</code> field in the User YAML to <code>0</code>.</p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#troubleshooting","title":"Troubleshooting","text":"Issue How to Diagnose Cause Corrective Procedure Robot shows no power on activity Nothing happens when you toggle on the robot\u2019s power switch. There is no visible illumination of LEDs, motion of the laser range finder, or audible noise of the robot fans. The robot fuse may have blown. When the batteries drain the current required to maintain power goes up, which can ultimately blow the fuse. Proceed to \u201cChanging the Fuse\u201d steps below Robot powers on momentarily When you toggle on the robot\u2019s power switch some activity occurs  (illumination of LEDs, audible noise of robot fans, etc) but the computer fails to boot. The battery voltage is too low to maintain power. As the power draw increases during power-on, the voltage dips and causes the system to shut down. Connect the battery charger in 12V AGM mode and leave until fully charged. Battery won\u2019t charge in 12V AGM mode When the robot is powered down and the charger is connected in 12V AGM mode, the charger eventually switches to a different mode. The battery voltage is too low for the charger to function correctly in normal operation. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger reports 100% charge but the batteries are discharged When the robot is powered down and the charger is connected in 12V AGM mode, the charger status shows 100%.  However the robot fails to turn on properly. Damage to the batteries (usually caused by excessively low voltage) may artificially raise the open circuit voltage of the battery, causing the battery to appear fully charged, while providing low capacity. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger will not charge or stay in any mode. When placed in 12V AGM, SUPPLY, or REPAIR  mode, it continually reverts to STANDBY mode after ~ 20 minutes. Charger may be defective. Contact Hello Robot Support for a replacement."},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#recovering-from-low-battery-voltage","title":"Recovering from Low Battery Voltage","text":"<ol> <li>Turn off the robot power switch and detach the charger from the robot</li> <li>Place charger in SUPPLY Mode </li> <li>Allow robot to charge for 4-8 hours, or up to 24 hours for extreme discharge</li> <li>Switch the charger to 12V AGM mode</li> <li>Charge until at 100%</li> </ol>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#additional-information","title":"Additional Information","text":""},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#powering-down-the-robot","title":"Powering Down the Robot","text":"<p>The recommended power down procedure is</p> <ol> <li>Place a clamp on the mast below the shoulder to prevent dropping</li> <li>Shutdown the computer from the Desktop or via SSH</li> <li>When the laser range finder has stopped spinning, turn off the main power switch</li> </ol>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#replacing-the-fuse","title":"Replacing the Fuse","text":"<p>Stretch RE1 has an automotive fuse inside the base that may need to be replaced. The type of fuse depends on your build version of the RE1</p> Build Version Fuse Type Recommended Fuse Guthrie 8A 5x20mm Fast Blow Glass Bussman S505-8-R Hank and later 7.5A ATM Fast Blow Blade Bussman VP/ATM-7-1/2-RP <p>The fuse location is shown below.  For guidance on replacing the fuse, contact Hello Robot support: support@hello-robot.com.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#checking-the-battery-voltage","title":"Checking the Battery Voltage","text":"<p>Battery voltage is not always an accurate indicator of battery charge but it can be a useful proxy. </p> <p>A charged battery will typically report a voltage of 12-12.8V and will maintain that voltage across load conditions. Meanwhile, a partially charged battery may report anywhere from 10-12.8V but its voltage will drop rapidly when loaded.</p> <p>Measuring Battery Voltage from the Command Line</p> <p>The battery voltage and current draw can be checked from the command line:</p> <pre><code>$ stretch_robot_battery_check.py\n[Pass] Voltage with 12.9889035225\n[Pass] Current with 2.46239192784\n[Pass] CPU Temp with 56.0\n</code></pre> <p>Measuring Battery Voltage with a DMM</p> <p>When troubleshooting a deeply discharged battery it may be useful to directly measure the battery voltage with a digital multimeter (DMM). To do this we recommend detaching the charger cable at its inline connector and applying the DMM to the connector contacts as shown.</p> <p>NOTE: Caution should be taken as it is possible to short the battery when doing this. </p> <p></p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#repairing-damaged-batteries","title":"Repairing Damaged Batteries","text":"<p>It is possible for Stretch's batteries to become damaged due to repeated deep discharge. If the robot has continued issues maintaining a charge we recommend attempting the following procedure:</p> <p>CAUTION: The repair cycle procedure requires you to do a repair  cycle on one battery at a time, which means you need to unplug each  battery, perform the repair, and then repeat the process on the other  battery. If you fail to repeat the procedure on the other battery, there is a potential risk that high amounts of current may flow from the  repaired battery to the other one, causing damage to both the battery  and the system.</p> <ol> <li>Turn off the robot power switch</li> <li>Attach the charger to the robot and set it to 12V AGM mode. Allow the robot to fully charge</li> <li>Detach the charger from the robot</li> <li>Unplug 1 battery from the robot. For guidance on how to access the battery connectors, please contact Hello Robot support: support@hello-robot.com</li> <li>Attach the charger and place it in REPAIR mode</li> <li>Place the charger in REPAIR mode</li> <li>Wait until the repair cycle has completed and the charger returns to standby - up to 4 hours</li> <li>Repeat steps 3 to 6 for the second battery</li> </ol>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#replacing-dead-batteries","title":"Replacing Dead Batteries","text":"<p>It is possible for a mechanically skilled person to replace the Stretch batteries should it be necessary . Please contact Hello Robot Support for more information (support@hello-robot.com)</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/","title":"Stretch RE2 - Battery Maintenance Guide","text":""},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#overview","title":"Overview","text":"<p>Stretch RE2 utilizes two 12V AGM SLA batteries that provide a combined 18AH of capacity. Maintaining an adequate level of charge on the battery system will enhance the battery lifetime.</p> <p>The run time for a fully charged system is dependent on the load use case. The majority of battery power is consumed by the NUC computer as Stretch uses relatively low power motors.</p> <p>A fully charged robot running a high CPU load can run approximately 2 hours before requiring a recharge.</p> <p>A fully charged robot powered on but with minimal load can run approximately 5 hours before requiring a recharge. </p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#state-of-battery-charge","title":"State of Battery Charge","text":"<p>An accurate measure of the battery charge isn't available on Stretch (as this requires a coulomb counting system). A coarse approximation of battery charge is given by the battery voltage. </p> <p>For RE2 during normal operation with a moderate load the relationship between voltage and charge is roughly:</p> Voltage Charge LED Light Bar 11.5 - 13.4V 80%+ Bright green 11.0 - 11.5V 60%+ Yellow 10.5 - 11.0V 40%+ Orange Below 10.5V &lt;40% Red <p>As shown below, the LED lightbar color provides an indication of the battery voltage. </p> <p></p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#when-to-plug-in-the-charger","title":"When To Plug in the Charger","text":"<p>We recommend plugging in the charger whenever:</p> <ul> <li>The battery voltage is below 11V (lightbar is orange). </li> <li>The robot is not running untethered</li> <li>The robot is producing a periodic beeping sound</li> </ul>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#accidental-full-discharge","title":"Accidental Full Discharge","text":"<p>Stretch includes a few feature to help prevent accidental full discharge (for example, if the robot is left on overnight not plugged to its charger)</p> <ul> <li>Audible warning: Stretch will beep every 2 seconds if the battery voltage drops below 10.5V</li> <li>Hard power off: Stretch will completely power off if the battery voltage drops below 9.75V</li> </ul>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#charger","title":"Charger","text":"<p>Stretch ships with a NOCO Genius 10 charger. Please review the battery charger user manual prior to following the guidance in this document. </p> <ul> <li>Genius 10 Manual </li> </ul> <p>Stretch utilizes four of the available modes on these chargers.</p> Mode Function STANDBY Charger not charging the robot 12V AGM Charging  robot powered down SUPPLY 1) Power the robot during tethered use2) Repair damaged batteries. REPAIR Repair damaged batteries."},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#noco-genius-10-interface","title":"NOCO Genius 10 - Interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached2) Press and hold MODE button for 3s3) Press MODE button until SUPPLY indicator is illuminated4) Attach charger REPAIR 1) From STANDBY, charger attached2) Press and hold MODE button for 3s3) Press MODE button until REPAIR indicator is illuminated"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#checking-the-battery-charge","title":"Checking the Battery Charge","text":"<p>The battery charger LEDs provide an approximate indicator of battery charge when it is in 12V AGM mode. Batteries are 100% fully charge when the green LED is not fading in and out.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#charging-best-practices","title":"Charging Best Practices","text":"<p>It is possible to accidentally deeply discharge the batteries by leaving the robot on for long durations without the charger attached. This is similar to leaving the lights on your car where the battery will continue to drain until fully discharged.</p> <p>We recommend following the best practices below to avoid deep discharge of the batteries and to ensure they have a long lifespan.</p> Use Case Best Practice Reason Robot is in use - tethered Leave the charger attached in SUPPLY mode while developing on the robot whenever possible.Shutdown and power off the robot when development is done. Running the robot while attempting to charge in 12V AGM mode can cause issues and is generally bad for battery health. SUPPLY mode is preferred whenever the robot needs to be powered on. Robot is in use - untethered Regularly check the battery voltage using the command line tool.Shutdown the computer and power off the robot when voltage falls below 11.5V.Attach the charger in 12V AGM mode.Charge to 100% before resuming operation. The 12V AGM charge mode expects the battery voltage to be above 10.5-11V in order to operate. Robot is not in use Shutdown the computer and turn off the robot power. Leave the charger attached and place it in 12V AGM mode. Leaving the robot power on may cause the batteries to deep dischargeThe charger will maintain a \u2018trickle charge\u2019 on the battery, keeping the charge at 100%. Robot is coming out of storage Attach charger in 12V AGM mode and charge for 2-3 hours until charger reports 100% SLA batteries naturally lose charge over time due to \u2018self-discharge\u2019."},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#troubleshooting","title":"Troubleshooting","text":"Issue How to Diagnose Cause Corrective Procedure Robot shows no power on activity Nothing happens when you toggle on the robot\u2019s power switch. There is no visible illumination of LEDs, motion of the laser range finder, or audible noise of the robot fans. The robot fuse may have blown. When the batteries drain the current required to maintain power goes up, which can ultimately blow the fuse. Proceed to \u201cChanging the Fuse\u201d steps below Robot powers on momentarily When you toggle on the robot\u2019s power switch some activity occurs  (illumination of LEDs, audible noise of robot fans, etc) but the computer fails to boot. The battery voltage is too low to maintain power. As the power draw increases during power-on, the voltage dips and causes the system to shut down. Connect the battery charger in 12V AGM mode and leave until fully charged. Battery won\u2019t charge in 12V AGM mode When the robot is powered down and the charger is connected in 12V AGM mode, the charger eventually switches to a different mode. The battery voltage is too low for the charger to function correctly in normal operation. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger reports 100% charge but the batteries are discharged When the robot is powered down and the charger is connected in 12V AGM mode, the charger status shows 100%.  However the robot fails to turn on properly. Damage to the batteries (usually caused by excessively low voltage) may artificially raise the open circuit voltage of the battery, causing the battery to appear fully charged, while providing low capacity. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger will not charge or stay in any mode. When placed in 12V AGM, SUPPLY, or REPAIR  mode, it continually reverts to STANDBY mode after ~ 20 minutes. Charger may be defective. Contact Hello Robot Support for a replacement."},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#recovering-from-low-battery-voltage","title":"Recovering from Low Battery Voltage","text":"<ol> <li>Turn off the robot power switch and detach the charger from the robot</li> <li>Place charger in SUPPLY Mode </li> <li>Allow robot to charge for 4-8 hours, or up to 24 hours for extreme discharge</li> <li>Switch the charger to 12V AGM mode</li> <li>Charge until at 100%</li> </ol>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#additional-information","title":"Additional Information","text":""},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#powering-down-the-robot","title":"Powering Down the Robot","text":"<p>The recommended power down procedure is</p> <ol> <li>Place a clamp on the mast below the shoulder to prevent dropping</li> <li>Shutdown the computer from the Desktop or via SSH</li> <li>When the laser range finder has stopped spinning, turn off the main power switch</li> </ol>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#replacing-the-fuse","title":"Replacing the Fuse","text":"<p>Stretch RE2 has two automotive fuses inside the base that may need to be replaced. </p> Fuse Type Recommended Fuse 7.5A ATM Fast Blow Blade Bussman VP/ATM-7-1/2-RP <p>The fuse locations are shown below.  For guidance on replacing the fuse, contact Hello Robot support: support@hello-robot.com.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#checking-the-battery-voltage","title":"Checking the Battery Voltage","text":"<p>Battery voltage is not always an accurate indicator of battery charge but it can be a useful proxy. </p> <p>A charged battery will typically report a voltage of 12-12.8V and will maintain that voltage across load conditions. Meanwhile, a partially charged battery may report anywhere from 10-12.8V but its voltage will drop rapidly when loaded.</p> <p>Measuring Battery Voltage from the Command Line</p> <p>The battery voltage and current draw can be checked from the command line:</p> <pre><code>$ stretch_robot_battery_check.py\n[Pass] Voltage with 12.9889035225\n[Pass] Current with 2.46239192784\n[Pass] CPU Temp with 56.0\n</code></pre> <p>Measuring Battery Voltage with a DMM</p> <p>When troubleshooting a deeply discharged battery it may be useful to directly measure the battery voltage with a digital multimeter (DMM). To do this we recommend </p> <ul> <li>Detach the charger cable at its inline connector </li> <li>Apply the DMM to the connector contacts as shown</li> <li>Plug the charge cable into the charge port of the robot</li> </ul> <p>NOTE: Caution should be taken as it is possible to short the battery when doing this. </p> <p></p>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#repairing-damaged-batteries","title":"Repairing Damaged Batteries","text":"<p>It is possible for Stretch's batteries to become damaged due to repeated deep discharge. If the robot has continued issues maintaining a charge we recommend attempting the following procedure:</p> <p>CAUTION: The repair cycle procedure requires you to do a repair  cycle on one battery at a time, which means you need to unplug each  battery, perform the repair, and then repeat the process on the other  battery. If you fail to repeat the procedure on the other battery, there is a potential risk that high amounts of current may flow from the  repaired battery to the other one, causing damage to both the battery  and the system.</p> <ol> <li>Turn off the robot power switch</li> <li>Attach the charger to the robot and set it to 12V AGM mode. Allow the robot to fully charge</li> <li>Detach the charger from the robot</li> <li>Unplug 1 battery from the robot. For guidance on how to access the battery connectors, please contact Hello Robot support: support@hello-robot.com</li> <li>Attach the charger and place it in REPAIR mode</li> <li>Place the charger in REPAIR mode</li> <li>Wait until the repair cycle has completed and the charger returns to standby - up to 4 hours</li> <li>Repeat steps 3 to 6 for the second battery</li> </ol>"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#replacing-damaged-batteries","title":"Replacing Damaged Batteries","text":"<p>It is possible for a mechanically skilled person to replace the Stretch batteries should it be necessary . Please contact Hello Robot Support for more information (support@hello-robot.com)</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/","title":"Stretch RE1 - Dex Wrist User Guide","text":"<p>In this guide, we will cover the installation, configuration, and use of the Stretch Dex Wrist.</p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#overview","title":"Overview","text":"<p>The Stretch Dex Wrist is an optional add-on to the RE1. It adds pitch and roll degrees of freedom to the standard wrist yaw joint. It also includes a slightly modified version of the standard Stretch Compliant Gripper. </p> <p>NOTE: If your robot did not ship with the Stretch Dex Wrist pre-installed you will want to first proceed to the Appendix: Installation and Configuration at the end of this guide. </p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#functional-specification","title":"Functional Specification","text":""},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#working-with-the-dex-wrist","title":"Working with the Dex Wrist","text":""},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#safe-use","title":"Safe Use","text":"<p>The Dex Wrist requires added attention to safety. Its additional dexterity introduces new pinch points around the wrist pitch and roll degrees of freedom.</p> <p>NOTE: Please review the Robot Safety Guide prior to working with the Dex Wrist.</p> <p>In addition to these precautions, the Dex Wrist requires attention to pinch points between:</p> <ul> <li>The wrist pitch and wrist yaw structures during yaw motion</li> <li>The gripper and wrist pitch structures during pitch motion</li> </ul> <p>The Dex Wrist includes a pinch point safety marking as a reminder to users:</p> <p></p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#avoiding-collisions","title":"Avoiding Collisions","text":"<p>The added dexterity of the Dex Wrist introduces new opportunities for self-collision between the robot tool and the robot. These include</p> <ul> <li>Running the tool into the base during lift downward motion</li> <li>Running the tool into the ground</li> <li>Running the tool into the wrist yaw structure</li> </ul> <p>We recommend becoming familiar with the potential collision points of the Dex Wrist by commanding careful motions through the <code>stretch_xbox_controller_teleop.py</code> tool. </p> <p>With Stretch Body v0.1.0 we introduce a simple collision avoidance controller. </p> <p>The collision avoidance behavior acts to dynamically set the robot joint limits according to simple models of its kinematic state.  The avoidance behavior is defined in <code>collision_model.py</code></p> <p>For performance reasons this collision avoidance behavior is coarse and does not prevent all self-collisions and considered 'experimental'. The collision avoidance is off by default for the standard Stretch RE1. For robots with the wrist we turn it on by default. It be turned on or off by modifying the following in your user YAML:</p> <pre><code>robot:\n  use_collision_manager: 1\n</code></pre>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#xbox-teleoperation","title":"XBox Teleoperation","text":"<p>The Dex Wrist can be teleoperated using the XBox controller. When the Dex Wrist is installed the <code>stretch_xbox_controller_teleop.py</code> tool will automatically remap control of the pan-tilt head to control of the pitch-roll wrist.</p> <pre><code>$ stretch_xbox_controller_teleop.py\n</code></pre> <p>The new key mapping is shown below. A printable version is available here.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#stretch-body-interface","title":"Stretch Body Interface","text":"<p>The new WristPitch and WristRoll joints are accessed from Stretch Body in the same manner as the WristYaw joint. </p> <p>Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints.  For example:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move arm to safe manipulation location\nrobot.stow()\nrobot.lift.move_to(0.4)\nrobot.push_command()\ntime.sleep(2.0)\n\n#Pose the Dex Wrist\nrobot.end_of_arm.move_to('wrist_yaw',0)\nrobot.end_of_arm.move_to('wrist_pitch',0)\nrobot.end_of_arm.move_to('wrist_roll',0)\nrobot.end_of_arm.move_to('stretch_gripper',50)\ntime.sleep(2.0)\n\n#Go back to stow and shutdown\nrobot.stow()\nrobot.stop()\n</code></pre> <p>You can jog the individual joints of the wrist with the Stretch Body interface using the <code>stretch_dex_wrist_jog.py</code> tool that installs with the Stretch Tool Share:</p> <pre><code>$ stretch_dex_wrist_jog.py --pitch\n$ stretch_dex_wrist_jog.py --yaw\n$ stretch_dex_wrist_jog.py --roll\n</code></pre> <p>For reference, the parameters for the Stretch Dex Wrist (which can be overridden in the user YAML) can be seen in  params.py.</p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#stretch-ros-interface","title":"Stretch ROS Interface","text":"<p>The Dex Wrist can be controlled via ROS as well, as shown in the keyboard teleoperation code. To test the interface:</p> <pre><code>$ roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre> <p>You can use Ctrl-C to exit when done. The menu interface is:</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------\n\n              i HEAD UP                    \n j HEAD LEFT            l HEAD RIGHT       \n              , HEAD DOWN                  \n\n\n 7 BASE ROTATE LEFT     9 BASE ROTATE RIGHT\n home                   page-up            \n\n\n              8 LIFT UP                    \n              up-arrow                     \n 4 BASE FORWARD         6 BASE BACK        \n left-arrow             right-arrow        \n              2 LIFT DOWN                  \n              down-arrow                   \n\n\n              w ARM OUT                    \n a WRIST FORWARD        d WRIST BACK       \n              x ARM IN                     \n\n\n c PITCH FORWARD        v PITCH BACK       \n o ROLL FORWARD         p ROLL BACK        \n\n\n              5 GRIPPER CLOSE              \n              0 GRIPPER OPEN               \n\n  step size:  b BIG, m MEDIUM, s SMALL     \n\n              q QUIT                       \n\n-------------------------------------------\n</code></pre> <p></p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#appendix-installation-and-configuration","title":"Appendix: Installation and Configuration","text":"<p>Robots that did not ship with the Dex Wrist installed will require additional hardware and software installation.</p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#production-batch-variation","title":"Production Batch Variation","text":"<p>Earlier production 'batches' of Stretch will require a hardware upgrade prior to use the Dex Wrist. To check your robot's batch, run:</p> <pre><code>$ stretch_about.py\n</code></pre> <p>Refer to this table to determine what changes are required for your robot.</p> Batch Name Upgrade Wacc Board Update Baud Rate Guthrie Y Y Hank Y Y Irma Y Y Joplin N Y Kendrick or later N N"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#upgrade-wacc-board","title":"Upgrade Wacc Board","text":"<p>If your robot requires a Wacc Board upgrade please follow the instructions here with the assistance of Hello Robot support. This must be done before attaching the Dex Wrist to our robot.</p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#update-baud-rate","title":"Update Baud Rate","text":"<p>The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600 (depending on your production batch).  Use the commands below.</p> <pre><code>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n\n$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200\n---------------------\nChecking servo current baud for 57600\n----\nIdentified current baud of 57600. Changing baud to 115200\nSuccess at changing baud\n</code></pre>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#attaching-the-dex-wrist","title":"Attaching the Dex Wrist","text":"<p>Power down your Stretch before installing the Dex Wrist.</p> <p>The Dex Wrist mounts to the bottom of the Stretch Wrist Tool Plate. Installation requires</p> <ul> <li>8 M2x6mm Torx FHCS bolts (provided)</li> <li>4 M2.5x4mm Torx FHCS bolts (provided)</li> <li>2 M2.5x8mm SHCS bolts (provided)</li> <li>T6 Torx wrench (provided)</li> <li>T8 Torx wrench (provided)</li> <li>2mm Hex key (provided)</li> </ul> <p>First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide. </p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#mounting-bracket","title":"Mounting Bracket","text":"<p>Note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the  additional alignment hole that is just outside the bolt pattern (shown pointing down in the image)</p> <p></p> <p>Using the T6 Torx wrench, attach the wrist mount bracket (A) to the bottom of the tool plate using the provided  M2x6mm bolts (B). </p> <p>NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate.</p> <p></p> <p>Now route the Dynamixel cable coming from the Stretch Wrist Yaw through the hollow bore of the wrist yaw joint.</p> <p>NOTE: During this step ensure the Dynamixel cable from the wrist yaw exits out the back (towards the shoulder)</p> <p>Next, raise the wrist module up vertically into the mounting bracket</p> <p></p> <p>, then sliding it over horizontally so that the bearing mates onto its post. </p> <p></p> <p>Now rotate the wrist yaw joint so the wrist pitch servo body is accessible. Attach the pitch servo to the mounting bracket using the 4 M2.5x4mm screws (C) using the T8 Torx wrench.</p> <p></p> <p>Finally, route the Dynamixel cable into the wrist pitch servo (pink) and install the cable clip (D) using the M2.5x8mm bolts and the 2mm hex wrench.</p> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#software-configuration","title":"Software Configuration","text":"<p>Robots that did not ship with the Dex Wrist pre-installed will require their software to be updated and configured.</p> <p>NOTE: Each user account on the robot will need to run the following steps to configure the Dex Wrist.</p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#upgrade-stretch-body","title":"Upgrade Stretch Body","text":"<p>Ensure the latest version of Stretch Body and Stretch Factory are installed</p> <pre><code>$ pip2 install hello-robot-stretch-body -U --no-cache-dir\n$ pip2 install hello-robot-stretch-body-tools -U --no-cache-dir\n$ pip2 install hello-robot-stretch-factory -U --no-cache-dir\n$ pip2 install hello-robot-stretch-tool-share -U --no-cache-dir\n</code></pre>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#backup-user-yaml","title":"Backup User YAML","text":"<pre><code>$ cd $HELLO_FLEET_PATH/$HELLO_FLEET_ID\n$ cp stretch_re1_user_params.yaml stretch_re1_user_params.yaml.bak\n</code></pre>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#run-installation-script","title":"Run Installation Script","text":"<pre><code>$ cd ~/repos\n$ git clone https://github.com/hello-robot/stretch_install\n$ cd ./stretch_install\n$ git pull\n$ ./stretch_new_dex_wrist_install.sh\n</code></pre> <p>NOTE: The factory gripper calibration may not provide the full range of motion in some cases. If necessary you can dial in the gripper calibration with the tool <code>RE1_gripper_calibrate.py</code></p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/","title":"Stretch RE2 - Dex Wrist User Guide","text":"<p>In this guide, we will cover the installation, configuration, and use of the Stretch Dex Wrist.</p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#overview","title":"Overview","text":"<p>The Stretch Dex Wrist is an optional add-on to the RE2. It adds pitch and roll degrees of freedom to the standard wrist yaw joint. It also includes a slightly modified version of the standard Stretch Compliant Gripper. </p> <p>NOTE: If your robot did not ship with the Stretch Dex Wrist pre-installed you will want to first proceed to the Appendix: Installation and Configuration at the end of this guide. </p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#functional-specification","title":"Functional Specification","text":""},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#working-with-the-dex-wrist","title":"Working with the Dex Wrist","text":""},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#safe-use","title":"Safe Use","text":"<p>The Dex Wrist requires added attention to safety. Its additional dexterity introduces new pinch points around the wrist pitch and roll degrees of freedom.</p> <p>NOTE: Please review the Robot Safety Guide prior to working with the Dex Wrist.</p> <p>In addition to these precautions, the Dex Wrist requires attention to pinch points between:</p> <ul> <li>The wrist pitch and wrist yaw structures during yaw motion</li> <li>The gripper and wrist pitch structures during pitch motion</li> </ul> <p>The Dex Wrist includes a pinch point safety marking as a reminder to users:</p> <p></p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#avoiding-collisions","title":"Avoiding Collisions","text":"<p>The added dexterity of the Dex Wrist introduces new opportunities for self-collision between the robot tool and the robot. These include</p> <ul> <li>Running the tool into the base during lift downward motion</li> <li>Running the tool into the ground</li> <li>Running the tool into the wrist yaw structure</li> </ul> <p>We recommend becoming familiar with the potential collision points of the Dex Wrist by commanding careful motions through the <code>stretch_xbox_controller_teleop.py</code> tool. </p> <p>With Stretch Body v0.1.0 we introduce a simple collision avoidance controller. </p> <p>The collision avoidance behavior acts to dynamically set the robot joint limits according to simple models of its kinematic state.  The avoidance behavior is defined in <code>collision_model.py</code></p> <p>For performance reasons this collision avoidance behavior is coarse and does not prevent all self-collisions and considered 'experimental'. The collision avoidance is off by default for the standard Stretch RE1. For robots with the wrist we turn it on by default. It be turned on or off by modifying the following in your user YAML:</p> <pre><code>robot:\n  use_collision_manager: 1\n</code></pre>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#xbox-teleoperation","title":"XBox Teleoperation","text":"<p>The Dex Wrist can be teleoperated using the XBox controller. When the Dex Wrist is installed the <code>stretch_xbox_controller_teleop.py</code> tool will automatically remap control of the pan-tilt head to control of the pitch-roll wrist.</p> <pre><code>$ stretch_xbox_controller_teleop.py\n</code></pre> <p>The new key mapping is shown below. A printable version is available here.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#stretch-body-interface","title":"Stretch Body Interface","text":"<p>The new WristPitch and WristRoll joints are accessed from Stretch Body in the same manner as the WristYaw joint. </p> <p>Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints.  For example:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move arm to safe manipulation location\nrobot.stow()\nrobot.lift.move_to(0.4)\nrobot.push_command()\ntime.sleep(2.0)\n\n#Pose the Dex Wrist\nrobot.end_of_arm.move_to('wrist_yaw',0)\nrobot.end_of_arm.move_to('wrist_pitch',0)\nrobot.end_of_arm.move_to('wrist_roll',0)\nrobot.end_of_arm.move_to('stretch_gripper',50)\ntime.sleep(2.0)\n\n#Go back to stow and shutdown\nrobot.stow()\nrobot.stop()\n</code></pre> <p>You can jog the individual joints of the wrist with the Stretch Body interface using the <code>stretch_dex_wrist_jog.py</code> tool that installs with the Stretch Tool Share:</p> <pre><code>$ stretch_dex_wrist_jog.py --pitch\n$ stretch_dex_wrist_jog.py --yaw\n$ stretch_dex_wrist_jog.py --roll\n</code></pre> <p>For reference, the parameters for the Stretch Dex Wrist (which can be overridden in the user YAML) can be seen in  params.py.</p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#stretch-ros-interface","title":"Stretch ROS Interface","text":"<p>The Dex Wrist can be controlled via ROS as well, as shown in the keyboard teleoperation code. To test the interface:</p> <pre><code>$ roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre> <p>You can use Ctrl-C to exit when done. The menu interface is:</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------\n\n              i HEAD UP                    \n j HEAD LEFT            l HEAD RIGHT       \n              , HEAD DOWN                  \n\n\n 7 BASE ROTATE LEFT     9 BASE ROTATE RIGHT\n home                   page-up            \n\n\n              8 LIFT UP                    \n              up-arrow                     \n 4 BASE FORWARD         6 BASE BACK        \n left-arrow             right-arrow        \n              2 LIFT DOWN                  \n              down-arrow                   \n\n\n              w ARM OUT                    \n a WRIST FORWARD        d WRIST BACK       \n              x ARM IN                     \n\n\n c PITCH FORWARD        v PITCH BACK       \n o ROLL FORWARD         p ROLL BACK        \n\n\n              5 GRIPPER CLOSE              \n              0 GRIPPER OPEN               \n\n  step size:  b BIG, m MEDIUM, s SMALL     \n\n              q QUIT                       \n\n-------------------------------------------\n</code></pre> <p></p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#appendix-installation-and-configuration","title":"Appendix: Installation and Configuration","text":"<p>Robots that did not ship with the Dex Wrist installed will require additional hardware and software installation.</p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#attaching-the-dex-wrist","title":"Attaching the Dex Wrist","text":"<p>Power down your Stretch before installing the Dex Wrist.</p> <p>The Dex Wrist mounts to the bottom of the Stretch Wrist Tool Plate. Installation requires</p> <ul> <li>8 M2x6mm Torx FHCS bolts (provided)</li> <li>4 M2.5x4mm Torx FHCS bolts (provided)</li> <li>2 M2.5x8mm SHCS bolts (provided)</li> <li>T6 Torx wrench (provided)</li> <li>T8 Torx wrench (provided)</li> <li>2mm Hex key (provided)</li> </ul> <p>First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide. </p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#mounting-bracket","title":"Mounting Bracket","text":"<p>Note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the  additional alignment hole that is just outside the bolt pattern (shown pointing down in the image)</p> <p></p> <p>Using the T6 Torx wrench, attach the wrist mount bracket (A) to the bottom of the tool plate using the provided  M2x6mm bolts (B). </p> <p>NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate.</p> <p></p> <p>Now route the Dynamixel cable coming from the Stretch Wrist Yaw through the hollow bore of the wrist yaw joint.</p> <p>NOTE: During this step ensure the Dynamixel cable from the wrist yaw exits out the back (towards the shoulder)</p> <p>Next, raise the wrist module up vertically into the mounting bracket</p> <p></p> <p>, then sliding it over horizontally so that the bearing mates onto its post. </p> <p></p> <p>Now rotate the wrist yaw joint so the wrist pitch servo body is accessible. Attach the pitch servo to the mounting bracket using the 4 M2.5x4mm screws (C) using the T8 Torx wrench.</p> <p></p> <p>Finally, route the Dynamixel cable into the wrist pitch servo (pink) and install the cable clip (D) using the M2.5x8mm bolts and the 2mm hex wrench.</p> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#software-configuration","title":"Software Configuration","text":"<p>Robots that did not ship with the Dex Wrist pre-installed will require their software to be updated and configured.</p> <p>NOTE: Each user account on the robot will need to run the following steps to configure the Dex Wrist.</p>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#upgrade-stretch-body","title":"Upgrade Stretch Body","text":"<p>Ensure the latest version of Stretch Body and Stretch Factory are installed</p> <pre><code>$ pip3 install hello-robot-stretch-body -U \n$ pip3 install hello-robot-stretch-body-tools -U\n$ pip3 install hello-robot-stretch-factory -U \n$ pip3 install hello-robot-stretch-tool-share -U\n</code></pre>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#backup-user-yaml","title":"Backup User YAML","text":"<pre><code>$ cd $HELLO_FLEET_PATH/$HELLO_FLEET_ID\n$ cp stretch_user_params.yaml stretch_user_params.yaml.bak\n</code></pre>"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#run-installation-script","title":"Run Installation Script","text":"<pre><code>$ cd ~/repos\n$ git clone https://github.com/hello-robot/stretch_install\n$ cd ./stretch_install\n$ git pull\n$ ./stretch_new_dex_wrist_install.sh\n</code></pre> <p>NOTE: The factory gripper calibration may not provide the full range of motion in some cases. If necessary you can dial in the gripper calibration with the tool <code>RE1_gripper_calibrate.py</code></p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/","title":"Stretch RE1 - Hardware Guide","text":"<p>This manual provides the engineering data and user guidance for working with the Hello Robot Stretch RE1 hardware.  </p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#disclaimer","title":"Disclaimer","text":"<p>The Hello Robot Stretch is intended for use in the research of mobile manipulation applications by users experienced in the use and programming of research robots. This product is not intended for general use in the home by consumers, and lacks the required certifications for such use. Please see the section on   Regulatory Compliance for further details.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#functional-specification","title":"Functional Specification","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#body-plan","title":"Body Plan","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#hardware-architecture","title":"Hardware Architecture","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#robot-subsystems","title":"Robot Subsystems","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#base","title":"Base","text":"<p>The base is a two wheel differential drive with a passive Mecanum wheel for a caster.  It includes four cliff sensors to allow detection of stairs, thresholds, etc.</p> <p></p> Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm <p>The base has 6 M4 threaded inserts available for mounting user accessories such as a tray. The mounting pattern is shown below.</p> <p>The inserts are recessed 1mm from the top of the base shell.</p> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#base-imu","title":"Base IMU","text":"<p>The base has a 9 DOF IMU using the 9 DOF FXOS8700 + FXAS21002 chipset. The IMU orientation is as shown below:</p> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#trunk","title":"Trunk","text":"<p>Development and charge ports are at the back of the base in the trunk. The trunk cover slides into place vertically and is non-latching.</p> <p>The trunk height has been designed to accommodate one or more USB based Intel Neural Compute Sticks.</p> <p>Two mounting holes are provided inside the trunk. These allow the user to strain relief tethered cables (eg, HDMI and keyboard) during development. It is recommended to strain relief such cables to prevent accidental damage during base motion.</p> <p></p> Item Notes A Vent Intake vent for computer fan B 6 Port USB Hub USB 3.0 , powered 5V/3A C Ethernet Connected to computer NIC D On/Off Robot power on / off. Switch is illuminated when on. E Charge Rated for upplied 12V/7A charger F HDMI Connected to computer HDMI G Mounting points M4 threaded holes <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#head","title":"Head","text":"<p>The head provides the audio interface to the robot, a pan tilt depth camera, a runstop, as well as a developer interface to allow the addition of additional user hardware.</p> <p></p> Item Notes A Pan tilt depth camera Intel RealSense D435i Two Dynamixel XL430-W250-T servos B Speakers C Mounting holes 2x M4 threaded, spacing 25mm D Developer Interface USB2.0-A with 5V@500mA fused  JST XHP-2,  12V@3A fused Pin 1: 12V Pin 2: GND E Microphone array With programmable 12 RGB LED ring  F Runstop G Audio volume control"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#pan-tilt","title":"Pan Tilt","text":"<p>The head pan-tilt unit utilizes two Dynamixel XL430-W250-T servos. It incorporates a small fan in order to ensure proper cooling of the servo and camera during dynamic repeated motions of the tilt DOF.</p> <p>The nominal \u2018zero\u2019 position is of the head is shown below, along with the corresponding range of motion.</p> <p></p> DOF Range (deg) Min(deg) Max (deg) Pan 346 -234  112 Tilt 115 -25 90"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#respeaker-microphone-array","title":"ReSpeaker Microphone Array","text":"<p>The ReSPeaker has 12 RGB LEDs that can be controlled programatically. By default they display sound intensity and direction of the microphone array. The ReSpeaker has 4 mems microphones mounted on a 64.61mm circle at 45 degree spacing. The drawing below shows the position and orientation of the microphone array relative to the head pan axis.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#runstop","title":"Runstop","text":"<p>The runstop allows the user to pause the motion of the four primary DOF (base, lift, and arm) by tapping the illuminated button on the head. When the runstop is enabled, these DOF are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume. </p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#lift","title":"Lift","text":"<p>The lift degree of freedom provides vertical translation of the arm. It is driven by a closed loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive. The \u2018shoulder\u2019 includes two mounting holes and a small delivery tray.</p> <p>NOTE: When using the shoulder mounting screws do not use M4 bolts longer than 7mm in length. Otherwise damage may occur. </p> <p></p> <p></p> Item Notes A Delivery tray B Mounting holes Threaded M4. Spacing 34.5 mm. Length not to exceed 7mm C Aruco Tag Size 40x40 mm"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#arm","title":"Arm","text":"<p>The arm comprises 5 telescoping carbon fiber links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion.</p> <p>The arm exhibits a small amount of play (lash) in the X, Y, Z, and theta directions which is a normal characteristic of its design. Despite this it can achieve good repeatability, in part because its gravity loading is fairly constant.</p> <p>The retracted arm and wrist combined are designed to fit within the footprint of the base. The arm is designed to have:</p> <ul> <li>Reach: 0.52m</li> </ul>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist","title":"Wrist","text":"<p>The wrist includes:</p> <ul> <li>Yaw DOF to allow for stowing of the tool</li> <li>2 Aruco tags for calibration and visual localization of the tool</li> <li>Expansion port with</li> <li>Arduino expansion header</li> <li>USB-A connector</li> <li>Tool plate with dual sided mounting</li> <li>Dynamixel X-Series TTL bus</li> </ul>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-control-interface","title":"Wrist Control Interface","text":"<p>The wrist yaw degree-of-freedom uses a Dynamixel XL430 servo. Additional Dynamixel servos can be daisy chained off of this servo, allowing for one more additional degree-of-freedoms to be easily  integrated onto the robot (such as the provided Stretch Gripper). </p> <p>Stretch comes with a XL430 compatible control cable preinstalled into this servo. If a different cable needs to be installed the servo cap can be removed as shown.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-tool-plate","title":"Wrist Tool Plate","text":"<p>The tool plate allows for mounting on the top or the bottom using the M2 bolt pattern. The mounting pattern is compatible with Robotis Dynamixel frames as well:</p> <ul> <li>FR12-H101K</li> <li>FR12-S102K</li> <li>FR12-S101K</li> </ul> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-yaw-range-of-motion","title":"Wrist Yaw Range of Motion","text":"<p>The wrist yaw DOF is calibrated so that the index hole faces forward at the 'zero' position. From this pose the wrist has a ROM of +256/-76 degrees as shown.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-accelerometer","title":"Wrist Accelerometer","text":"<p>The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The  sensor is mounted inside the distal link of the arm as shown below.</p> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-expansion-usb","title":"Wrist Expansion USB","text":"<p>The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-expansion-header","title":"Wrist Expansion Header","text":"<p>The wrist includes an expansion header that provides access to pins of the wrist Arduino board.  The header connector can be accessed by removing the cap at the end of the arm.</p> <p></p> <p>The header is wired to a Atmel SAMD21G18A-AUT (datasheet) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow:</p> <ul> <li>General purpose digital I/O</li> <li>Analog input</li> </ul> <p>In addition, the firmware can be configured for other pin functions, including:</p> <ul> <li>Serial SPI</li> <li>Serial I2C</li> <li>Serial UART</li> </ul> <p>The Stretch Firmware Manual covers this modification.</p> <p>**The header pins utilize 3V3 TTL logic. They do not have interface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded **</p> <p>The pin mapping is:</p> Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS  Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input  Analog in (A0) <p>The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN).  It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305).</p> <p>Pin 1 &amp; 10 are indicated below.</p> <p></p> <p>The expansion DIO schematic shown below.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-mounts","title":"Wrist Mounts","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#gripper","title":"Gripper","text":"<p>The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position.  As shown, it includes mounting features on  one side to allow for attachment of simple rigid tools such as hooks and pullers. </p> <p></p> Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 <p>The attachment features are spaced at 9mm.</p> <p>The weight of the Stretch Compliant Gripper is 240g.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#gripper-removal","title":"Gripper Removal","text":"<p>Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse.</p> <ol> <li>Unplug the Dynamixel cable from the back of the gripper. </li> <li>Remove the 4 screws holding the gripper to the bracket.</li> <li>Remove the gripper from the mounting bracket</li> <li>Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate.</li> </ol> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#robot-care","title":"Robot Care","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#battery-maintenance","title":"Battery Maintenance","text":"<p>Please review the Battery Maintenance Guide for proper care and charging of the Stretch batteries.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#belt-tension","title":"Belt Tension","text":"<p>A neoprene timing belt drives the arm up and down the lift. It may detension over long periods of time if it experiences sustained loading. In this case, slack will become visually apparent in the belt as the lift moves.</p> <p>The belt is very straightforward to re-tension. Please contact support@hello-robot.com for tensioning instructions.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#keeping-the-robot-clean","title":"Keeping the Robot Clean","text":"<p>The robot surfaces can be wiped down with an alcohol wipe or a moist rag from time to time in order to remove and debris or oils that accumulate on the shells or mast. </p> <p>The drive wheels can accumulate dust over time and begin to lose traction. They should be periodically wiped down as well.</p> <p>When possible, the Trunk cover for the base should be kept on in order to keep dust and debris out of the Trunk connectors.</p> <p>If the D435i camera requires cleaning use appropriate lens cleaning fluid and a microfiber cloth.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#keeping-the-robot-calibrated","title":"Keeping the Robot Calibrated","text":"<p>The robot comes pre-calibrated with a robot-specific URDF. This calibration allows the D435i depth sensor to accurately estimate where the robot wrist, and body, is in the depth image.</p> <p>The robot may become slightly uncalibrated over time for a variety of reasons:</p> <ul> <li>Normal wear and tear and loosening of joints of the robot</li> <li>The head structure is accidentally load and the structure becomes very slightly bent</li> <li>The wrist and should structure become accidentally highly loaded and become slightly bent</li> </ul> <p>The calibration accuracy can be checked using the provided ROS tools. If necessary, the user can recalibrate the robot. See the Stretch URDF Calibration Guide for more information.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#transporting-the-robot","title":"Transporting the Robot","text":"<p>Stretch was designed to be easily transported in the back of a car, up a stair case, or around a building.</p> <p>For short trips, the robot can be simply rolled around by grabbing its mast. It may be picked up by its mast and carried up stairs as well. </p> <p>For safety, please use two people to lift the robot.</p> <p>For longer trips it is recommended to transport the robot in its original cardboard box with foam packaging. The metal protective cage that surrounds the head is only necessary if the robot might be shipped and the box will not remain upright.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#system-check","title":"System Check","text":"<p>It is useful to periodically run stretch_robot_system_check.py. This  will check that the robot's hardware devices are  present and within normal operating conditions. </p> <pre><code>$ stretch_robot_system_check.py\n\n---- Checking Devices ----\n[Pass] : hello-wacc\n[Pass] : hello-motor-left-wheel\n[Pass] : hello-motor-arm\n[Pass] : hello-dynamixel-wrist\n[Pass] : hello-motor-right-wheel\n[Pass] : hello-motor-lift\n[Pass] : hello-pimu\n[Pass] : hello-respeaker\n[Pass] : hello-lrf\n[Pass] : hello-dynamixel-head\n\n---- Checking Pimu ----\n[Pass] Voltage = 12.8763639927\n[Pass] Current = 3.25908634593\n[Pass] Temperature = 36.3404559783\n[Pass] Cliff-0 = -4.72064208984\n[Pass] Cliff-1 = -8.56213378906\n[Pass] Cliff-2 = 1.08505249023\n[Pass] Cliff-3 = 5.68453979492\n[Pass] IMU AZ = -9.80407142639\n\n\n---- Checking EndOfArm ----\n[Dynamixel ID:013] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: wrist_yaw\n[Pass] Calibrated: wrist_yaw\n\n[Dynamixel ID:014] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: stretch_gripper\n[Pass] Calibrated: stretch_gripper\n\n\n---- Checking Head ----\n[Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: head_tilt\n\n[Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: head_pan\n\n\n---- Checking Wacc ----\n[Pass] AX = 9.4840593338\n\n\n---- Checking hello-motor-left-wheel ----\n[Pass] Position = 43.9992256165\n\n\n---- Checking hello-motor-right-wheel ----\n[Pass] Position = 15.1164712906\n\n\n---- Checking hello-motor-arm ----\n[Pass] Position = 59.7719421387\n[Pass] Position Calibrated = True\n\n\n---- Checking hello-motor-lift ----\n[Pass] Position = 83.7744064331\n[Pass] Position Calibrated = True\n\n\n---- Checking for Intel D435i ----\nBus 002 Device 016: ID 8086:0b3a Intel Corp. \n[Pass] : Device found \n</code></pre>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#regulatory-compliance","title":"Regulatory Compliance","text":"<p>The Stretch Research Edition 1 (Stretch RE1) is not certified for use as a consumer device in the U.S.</p> <p>Unless stated otherwise, the Stretch RE1 is not subjected to compliance testing nor certified to meet any requirements, such as requirements for EMI, EMC, or ESD.</p> <p>Per FCC 47 CFR, Part 15, Subpart B, section 15.103(c), we claim the Stretch Research Edition 1 as an exempted device, since it is a digital device used exclusively as industrial, commercial, or medical test equipment, where test equipment is equipment intended primarily for purposes of performing scientific investigations.</p> <p>OET BULLETIN NO. 62, titled \"UNDERSTANDING THE FCC REGULATIONS FOR COMPUTERS AND OTHER DIGITAL DEVICES\" from December 1993 provides further clarification of the Section 15.103(c) exemption: \u201cTest equipment includes devices used for maintenance, research, evaluation, simulation and other analytical or scientific applications in areas such as industrial plants, public utilities, hospitals, universities, laboratories, automotive service centers and electronic repair shops.\u201d</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/","title":"Stretch RE2 - Hardware Guide","text":"<p>This manual provides the engineering data and user guidance for working with the Hello Robot Stretch RE2 hardware.  </p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#disclaimer","title":"Disclaimer","text":"<p>The Hello Robot Stretch is intended for use in the research of mobile manipulation applications by users experienced in the use and programming of research robots. This product is not intended for general use in the home by consumers, and lacks the required certifications for such use. Please see the section on Regulatory Compliance for further details.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#functional-specification","title":"Functional Specification","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#body-plan","title":"Body Plan","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#hardware-architecture","title":"Hardware Architecture","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#robot-subsystems","title":"Robot Subsystems","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#base","title":"Base","text":"<p>The base is a two wheel differential drive with a passive Mecanum wheel for a caster.  It includes four cliff sensors to allow detection of stairs, thresholds, etc.</p> <p></p> Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm <p>The base has 6 M4 threaded inserts available for mounting user accessories such as a tray. The mounting pattern is shown below.</p> <p>The inserts are recessed 1mm from the top of the base shell.</p> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#base-imu","title":"Base IMU","text":"<p>The base has a 9 DOF IMU using the 9 DOF FXOS8700 + FXAS21002 chipset. The IMU orientation is as shown below:</p> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#trunk","title":"Trunk","text":"<p>Development and charge ports are at the back of the base in the trunk. </p> <p></p> Item Notes A Vent Intake vent for computer fan B 6 Port USB Hub USB 3.0 , powered 5V/3A C Ethernet Connected to computer NIC D On/Off Robot power on / off. Switch is illuminated when on. E Charge Rated for upplied 12V/7A charger F HDMI Connected to computer HDMI G LED Light Bar Indicates battery voltage H 12V access plug Allows customer cable access to 12V Aux on Pimu PCBA"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#12v-access-plug","title":"12V Access Plug","text":"<p>The 12V access plug allows users to supply 12V power to an external device. This 12V power is directly connected to the battery and is not a regulated voltage supply. As the battery voltage decreases, the 12V access plug voltage also decreases. It is recommended that the user purchase a regulated voltage adapter to connect to this access plug and then connect the external device to the regulated voltage adapter, instead of connecting it directly to the 12V access plug. The maximum continuous current of the 12V access plug is 6A max.</p> <p>[!WARNING] If the cable on the 12V access plug is wired incorrectly, the main power board of the robot will get damaged</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#head","title":"Head","text":"<p>The head provides the audio interface to the robot, a pan tilt depth camera, a runstop, as well as a developer interface to allow the addition of additional user hardware.</p> <p></p> Item Notes A Pan tilt depth camera Intel RealSense D435i Two Dynamixel XL430-W250-T servos B Speakers D Developer Interface Volume control, USB2.0-A with 5V@500mA fused  JST XHP-2,  12V@3A fused Pin 1: 12V Pin 2: GND E Microphone array With programmable 12 RGB LED ring  F Runstop"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#pan-tilt","title":"Pan Tilt","text":"<p>The head pan-tilt unit utilizes two Dynamixel XL430-W250-T servos. It incorporates a small fan in order to ensure proper cooling of the servo and camera during dynamic repeated motions of the tilt DOF.</p> <p>The nominal \u2018zero\u2019 position is of the head is shown below, along with the corresponding range of motion.</p> <p></p> DOF Range (deg) Min(deg) Max (deg) Pan 346 -234  112 Tilt 115 -25 90"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#respeaker-microphone-array","title":"ReSpeaker Microphone Array","text":"<p>The ReSPeaker has 12 RGB LEDs that can be controlled programatically. By default they display sound intensity and direction of the microphone array. The ReSpeaker has 4 mems microphones mounted on a 64.61mm circle at 45 degree spacing. The drawing below shows the position and orientation of the microphone array relative to the head pan axis.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#mounting-points","title":"Mounting Points","text":"<p>The top of the head includes 3x M4 threaded mounting points as shown below.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#runstop","title":"Runstop","text":"<p>The runstop allows the user to pause the motion of the four primary DOF (base, lift, and arm) by tapping the illuminated button on the head. When the runstop is enabled, these DOF are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume. </p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#lift","title":"Lift","text":"<p>The lift degree of freedom provides vertical translation of the arm. It is driven by a closed loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive. The \u2018shoulder\u2019 includes four mounting holes and a small delivery tray.</p> <p>NOTE: When using the shoulder mounting screws do not use M4 bolts longer than 7mm in length. Otherwise damage may occur. </p> <p></p> <p></p> Item Notes A Delivery tray B Mounting holes Threaded M4. Length not to exceed 7mm. C Aruco Tag Size 40x40 mm D Developer ports USB2.0-A with 5V@500mA fused ;  JST XHP-2,  12V@3A fused Pin 1: 12V Pin 2: GND"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#arm","title":"Arm","text":"<p>The arm comprises 5 telescoping carbon fiber links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion.</p> <p>The arm exhibits a small amount of play (lash) in the X, Y, Z, and theta directions which is a normal characteristic of its design. Despite this it can achieve good repeatability, in part because its gravity loading is fairly constant.</p> <p>The retracted arm and wrist combined are designed to fit within the footprint of the base. The arm is designed to have:</p> <ul> <li>Reach: 0.52m</li> </ul>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist","title":"Wrist","text":"<p>The wrist includes:</p> <ul> <li>Yaw DOF to allow for stowing of the tool</li> <li>2 Aruco tags for calibration and visual localization of the tool</li> <li>Expansion port with</li> <li>Arduino expansion header</li> <li>USB-A connector</li> <li>Tool plate with dual sided mounting</li> <li>Dynamixel X-Series TTL bus</li> </ul>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-control-interface","title":"Wrist Control Interface","text":"<p>The wrist yaw degree-of-freedom uses a Dynamixel XL430 servo. Additional Dynamixel servos can be daisy chained off of this servo, allowing for one more additional degree-of-freedoms to be easily  integrated onto the robot (such as the provided Stretch Gripper). </p> <p>Stretch comes with a XL430 compatible control cable preinstalled into this servo. If a different cable needs to be installed the servo cap can be removed as shown.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-tool-plate","title":"Wrist Tool Plate","text":"<p>The tool plate allows for mounting on the top or the bottom using the M2 bolt pattern. The mounting pattern is compatible with Robotis Dynamixel frames as well:</p> <ul> <li>FR12-H101K</li> <li>FR12-S102K</li> <li>FR12-S101K</li> </ul> <p>The tool plate includes a 'Zero indicator'. This mark indicates the forward position of the tool. It will point in the direction of the arm extension when the wrist yaw joint is at its zero position.</p> <p>In addition, the tool plate includes two index holes. These can be used to index a tool (e.g., Stretch Gripper) during installation. Compatible pins on the tool ensure that it is installed at the correct orientation.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-yaw-range-of-motion","title":"Wrist Yaw Range of Motion","text":"<p>The wrist yaw DOF is calibrated so that the index hole faces forward at the 'zero' position. From this pose the wrist has a ROM of +256/-76 degrees as shown.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-accelerometer","title":"Wrist Accelerometer","text":"<p>The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The  sensor is mounted inside the distal link of the arm as shown below.</p> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-expansion-usb","title":"Wrist Expansion USB","text":"<p>The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-expansion-header","title":"Wrist Expansion Header","text":"<p>The wrist includes an expansion header that provides access to pins of the wrist Arduino board.  The header connector can be accessed by removing the cap at the end of the arm.</p> <p></p> <p>The header is wired to a Atmel SAMD21G18A-AUT (datasheet) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow:</p> <ul> <li>General purpose digital I/O</li> <li>Analog input</li> </ul> <p>In addition, the firmware can be configured for other pin functions, including:</p> <ul> <li>Serial SPI</li> <li>Serial I2C</li> <li>Serial UART</li> </ul> <p>The Stretch Firmware Manual covers this modification.</p> <p>**The header pins utilize 3V3 TTL logic. They have limited nterface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded **</p> <p>The pin mapping is:</p> Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS  Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input  Analog in (A0) <p>The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN).  It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305).</p> <p>Pin 1 &amp; 10 are indicated below.</p> <p></p> <p>The expansion DIO schematic shown below.</p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-mounts","title":"Wrist Mounts","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#gripper","title":"Gripper","text":"<p>The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position.  As shown, it includes mounting features on  one side to allow for attachment of simple rigid tools such as hooks and pullers. </p> <p></p> Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 <p>The attachment features are spaced at 9mm.</p> <p>The weight of the Stretch Compliant Gripper is 240g.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#gripper-removal","title":"Gripper Removal","text":"<p>Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse.</p> <ol> <li>Unplug the Dynamixel cable from the back of the gripper. </li> <li>Remove the 4 screws holding the gripper to the bracket.</li> <li>Remove the gripper from the mounting bracket</li> <li>Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate.</li> </ol> <p></p> <p></p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#robot-care","title":"Robot Care","text":""},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#battery-maintenance","title":"Battery Maintenance","text":"<p>Please review the Battery Maintenance Guide for proper care and charging of the Stretch batteries.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#belt-tension","title":"Belt Tension","text":"<p>A neoprene timing belt drives the arm up and down the lift. It may detension over long periods of time if it experiences sustained loading. In this case, slack will become visually apparent in the belt as the lift moves.</p> <p>The belt is very straightforward to re-tension. Please contact support@hello-robot.com for tensioning instructions.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#keeping-the-robot-clean","title":"Keeping the Robot Clean","text":"<p>The robot surfaces can be wiped down with an alcohol wipe or a moist rag from time to time in order to remove and debris or oils that accumulate on the shells or mast. </p> <p>The drive wheels can accumulate dust over time and begin to lose traction. They should be periodically wiped down as well.</p> <p>When possible, the Trunk cover for the base should be kept on in order to keep dust and debris out of the Trunk connectors.</p> <p>If the D435i camera requires cleaning use appropriate lens cleaning fluid and a microfiber cloth.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#keeping-the-robot-calibrated","title":"Keeping the Robot Calibrated","text":"<p>The robot comes pre-calibrated with a robot-specific URDF. This calibration allows the D435i depth sensor to accurately estimate where the robot wrist, and body, is in the depth image.</p> <p>The robot may become slightly uncalibrated over time for a variety of reasons:</p> <ul> <li>Normal wear and tear and loosening of joints of the robot</li> <li>The head structure is accidentally load and the structure becomes very slightly bent</li> <li>The wrist and should structure become accidentally highly loaded and become slightly bent</li> </ul> <p>The calibration accuracy can be checked using the provided ROS tools. If necessary, the user can recalibrate the robot. See the Stretch URDF Calibration Guide for more information.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#transporting-the-robot","title":"Transporting the Robot","text":"<p>Stretch was designed to be easily transported in the back of a car, up a stair case, or around a building.</p> <p>For short trips, the robot can be simply rolled around by grabbing its mast. It may be picked up by its mast and carried up stairs as well. </p> <p>For safety, please use two people to lift the robot.</p> <p>For longer trips it is recommended to transport the robot in its original cardboard box with foam packaging. The metal protective cage that surrounds the head is only necessary if the robot might be shipped and the box will not remain upright.</p>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#system-check","title":"System Check","text":"<p>It is useful to periodically run stretch_robot_system_check.py. This  will check that the robot's hardware devices are  present and within normal operating conditions. </p> <pre><code>$ stretch_robot_system_check.py\n\n---- Checking Devices ----\n[Pass] : hello-wacc\n[Pass] : hello-motor-left-wheel\n[Pass] : hello-motor-arm\n[Pass] : hello-dynamixel-wrist\n[Pass] : hello-motor-right-wheel\n[Pass] : hello-motor-lift\n[Pass] : hello-pimu\n[Pass] : hello-respeaker\n[Pass] : hello-lrf\n[Pass] : hello-dynamixel-head\n\n---- Checking Pimu ----\n[Pass] Voltage = 12.8763639927\n[Pass] Current = 3.25908634593\n[Pass] Temperature = 36.3404559783\n[Pass] Cliff-0 = -4.72064208984\n[Pass] Cliff-1 = -8.56213378906\n[Pass] Cliff-2 = 1.08505249023\n[Pass] Cliff-3 = 5.68453979492\n[Pass] IMU AZ = -9.80407142639\n\n\n---- Checking EndOfArm ----\n[Dynamixel ID:013] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: wrist_yaw\n[Pass] Calibrated: wrist_yaw\n\n[Dynamixel ID:014] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: stretch_gripper\n[Pass] Calibrated: stretch_gripper\n\n\n---- Checking Head ----\n[Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: head_tilt\n\n[Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1060\n[Pass] Ping of: head_pan\n\n\n---- Checking Wacc ----\n[Pass] AX = 9.4840593338\n\n\n---- Checking hello-motor-left-wheel ----\n[Pass] Position = 43.9992256165\n\n\n---- Checking hello-motor-right-wheel ----\n[Pass] Position = 15.1164712906\n\n\n---- Checking hello-motor-arm ----\n[Pass] Position = 59.7719421387\n[Pass] Position Calibrated = True\n\n\n---- Checking hello-motor-lift ----\n[Pass] Position = 83.7744064331\n[Pass] Position Calibrated = True\n\n\n---- Checking for Intel D435i ----\nBus 002 Device 016: ID 8086:0b3a Intel Corp. \n[Pass] : Device found \n</code></pre>"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#regulatory-compliance","title":"Regulatory Compliance","text":"<p>The Stretch Research Edition 1 (Stretch re2) is not certified for use as a consumer device in the U.S.</p> <p>Unless stated otherwise, the Stretch re2 is not subjected to compliance testing nor certified to meet any requirements, such as requirements for EMI, EMC, or ESD.</p> <p>Per FCC 47 CFR, Part 15, Subpart B, section 15.103(c), we claim the Stretch Research Edition 1 as an exempted device, since it is a digital device used exclusively as industrial, commercial, or medical test equipment, where test equipment is equipment intended primarily for purposes of performing scientific investigations.</p> <p>OET BULLETIN NO. 62, titled \"UNDERSTANDING THE FCC REGULATIONS FOR COMPUTERS AND OTHER DIGITAL DEVICES\" from December 1993 provides further clarification of the Section 15.103(c) exemption: \u201cTest equipment includes devices used for maintenance, research, evaluation, simulation and other analytical or scientific applications in areas such as industrial plants, public utilities, hospitals, universities, laboratories, automotive service centers and electronic repair shops.\u201d</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-install/","title":"Overview","text":"<p>The stretch_install repository provides scripts required to install the Stretch software.</p>"},{"location":"stretch-install/#guides","title":"Guides","text":"Guide Purpose Updating Software Updating the various components of Stretch's software + troubleshooting Adding a New User Creating a new Ubuntu user and setting it up with Stretch packages and robot configuration Creating a New ROS Workspace Creating and compiling a new ROS workspace, either ROS1's catkin or ROS2's ament Performing a Robot Install Installing a new operating system and setting it up with the default Ubuntu user and full software stack Configuring the DexWrist Changing a user's robot configuration to enable usage of the Dex Wrist"},{"location":"stretch-install/#contributing","title":"Contributing","text":"<p>Thank you for considering contributing to this repo! Please take a look at the contribution guide for more details.</p>"},{"location":"stretch-install/#license","title":"License","text":"<p>All Hello Robot installation materials are released under the GNU General Public License v3.0 (GNU GPLv3). Details can be found in the LICENSE file.</p>"},{"location":"stretch-install/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains installation software for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>This software is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation.</p> <p>This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link: </p> <p>https://www.gnu.org/licenses/gpl-3.0.html</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-install/docs/add_new_user/","title":"Adding a New User","text":""},{"location":"stretch-install/docs/add_new_user/#why","title":"Why","text":"<p>When multiple people are working with the robot, it can be helpful to create separate Ubuntu user accounts. Here's a few reasons you might add a new user:</p> <ul> <li>Your code and data is protected by password in your user account</li> <li>Your code is dependent on a specific version of Stretch's software and you don't want other users to accidentally upgrade it (NOTE: some of Stretch's software is installed per user, e.g. Stretch Body and Stretch ROS, whereas other parts of the software is installed per OS, e.g. ROS1 or ROS2)</li> <li>Logs from your experiments will not be mixed with logs from other users</li> </ul> <p>Finally, note that changes made in your user account can break another user account's set up. For example, if you configure a different gripper or end effector onto the robot with your account, other accounts will have outdated configurations for what gripper is attached to the robot.</p>"},{"location":"stretch-install/docs/add_new_user/#how","title":"How","text":"<p>Login to the \"hello-robot\" user. The \"hello-robot\" user account has administrator privileges. Go to Users system settings and unlock the administrator actions.</p> <p></p> <p>Click \"Add User...\" and complete the subsequent form.</p> <p></p> <p>Logout and the log back in as the new user. Open a terminal and execute the following to pull down the Stretch Install repository.</p> <pre><code>cd ~\ngit clone https://github.com/hello-robot/stretch_install\ncd stretch_install\ngit pull\n</code></pre> <p>Execute the following to set up the new user account with Stretch packages and the robot's configuration data.</p> <pre><code>./stretch_new_user_install.sh\n</code></pre> <p>Finally, reboot your robot and execute the following to confirm the new user account was set up successfully.</p> <pre><code>stretch_robot_system_check.py\n</code></pre> <p>Your new user account is now set up successfully!</p>"},{"location":"stretch-install/docs/configure_BIOS/","title":"Configuring the BIOS","text":"<p>This documentation describes how to configure the BIOS of an Intel NUC for compatibility with the stretch installation procedure.</p>"},{"location":"stretch-install/docs/configure_BIOS/#accessing-the-nuc-bios-settings","title":"Accessing the NUC BIOS Settings","text":"<p>First plug in the NUC to a 19V DC power supply. Next power on the NUC using the power button on the fron of the NUC.</p> <p>When powered on, the NUC should display a welcome screen similar to the picture below:</p> <p></p> <p>When this label becomes visible press 'F2' to enter into the BIOS configuration menu.</p> <p>The BIOS Settings page should look like the picture below:</p> <p></p> <p>Select the 'Advanced' drop down menu near the top right of the screen, and then slect the option 'Boot'</p> <p></p> <p>From the 'Boot' settings page select the 'Secure Boot' tab.</p> <p></p> <p>Turn off 'Secure Boot' by toggling the checkbox labeled 'Secure Boot' to unchecked.</p> <p></p> <p>Next Select the 'Power' tab</p> <p></p> <p>From the power settings screen select the 'Power On' option from the 'After Power Failure' drop down selection.</p> <p></p> <p>Next Select the Security tab</p> <p></p> <p>Turn on UEFI third party drivers compatibility by toggling the checkbox labeled 'Allow UEFI Third Party Driver loaded' to checked.</p> <p></p> <p>Now use the F10 key to save BIOS configuration changes and exit.</p>"},{"location":"stretch-install/docs/contributing/","title":"Contributing to Stretch Install","text":"<p>Thank you for considering contributing to this repository. Stretch Install houses bash scripts and tutorials that enable users to setup/configure their robots. This guide explains the layout of this repo and how best to make and test changes.</p>"},{"location":"stretch-install/docs/contributing/#repo-layout","title":"Repo Layout","text":"<ul> <li><code>README.md</code> &amp; <code>LICENSE.md</code> - includes info about the repo and a table of tutorials available</li> <li><code>stretch_new_*_install.sh</code> - high level scripts meant to be run by the user</li> <li><code>factory/</code> - subscripts and assets not meant to be run by the user</li> <li><code>18.04/</code> - subscripts and assets specific to performing a Ubuntu 18.04 software install<ul> <li><code>stretch_initial_setup.sh</code> - a bunch of checks and initial setup that are run before performing a robot install</li> <li><code>stretch_install_*.sh</code> - helper scripts that install a specific set of packages</li> <li><code>stretch_create_*_workspace.sh</code> - creates a ROS/ROS2 workspace</li> <li><code>stretch_ros*.repos</code> - the ROS packages that are included and compiled in the ROS workspace by the <code>stretch_create_*_workspace.sh</code> script</li> <li><code>hello_robot_*.desktop</code> - autostarts programs to run when the robot boots up</li> </ul> </li> <li><code>&lt;&gt;.04/</code> - Ubuntu &lt;&gt;.04 software install related subscripts/assets. Similar in layout to 18.04/</li> <li><code>docs/</code> - contains tutorials for using the scripts in this repo</li> </ul> <p>Once you're ready to make changes to this repo, you can fork it on Github.</p>"},{"location":"stretch-install/docs/contributing/#contributing-to-the-tutorials","title":"Contributing to the tutorials","text":"<p>The tutorials in the <code>docs/</code> folder are markdown files that get rendered into our https://docs.hello-robot.com site. If you edit them and file a pull request towards this repo, the changes to the tutorials will get reflected on the docs site. In order to live preview changes to these tutorials, first install mkdocs using:</p> <pre><code>python3 -m pip install mkdocs mkdocs-material mkdocstrings==0.17.0 pytkdocs[numpy-style] jinja2=3.0.3\n</code></pre> <p>Then, run the dev server using:</p> <pre><code>python3 -m mkdocs serve\n</code></pre> <p>Now you can make additions or changes to the source markdown files and see the changes reflected live in the browser.</p>"},{"location":"stretch-install/docs/contributing/#contributing-to-the-installation-scripts","title":"Contributing to the installation scripts","text":"<p>If you are looking to change scripts/assets of an existing software installation (e.g. Ubuntu 18.04/20.04), look within the <code>factory/&lt;&gt;.04/</code> directory and make changes to the appriopriate files. If you're looking to add support for a new Ubuntu distro (e.g. Ubuntu 19.04), create <code>factory/19.04</code> with assets from a previous installation and tweak the scripts until they works correctly for the Ubuntu distro you are targeting. Then, edit the high level scripts (e.g. <code>stretch_new_*_install.sh</code>) to call your distro's specific assets correctly. Ensuring that the tutorials in the <code>docs/</code> work for your new distro is a good way to ensure that your <code>factory/&lt;&gt;.04/</code> directory works correctly. Since bash scripts change behavior based on the underlying system, it can be helpful to use containers to create reproducible behaviors while you're developing support for the new distro. Multipass works well on Ubuntu systems. You can create a new container emulating any Ubuntu distro using the command:</p> <pre><code>multipass launch -c 6 -d 30G -m 7G -n &lt;container-name&gt; 19.04\n</code></pre> <p>Swap <code>19.04</code> in the above command with the distro you're targeting. The above command creates a containers with 30GB disk space, 7GB swap, 6 cores, and the name <code>&lt;container-name&gt;</code>. We've found that at least 30GB disk space is needed for the Ubuntu 18.04/20.04 installations.</p> <p>Then, you can access the shell of your new container using:</p> <pre><code>multipass shell &lt;container-name&gt;\n</code></pre> <p>Other helpful multipass subcommand include <code>transfer</code>, which allows you to transfer files to the container, and <code>delete</code>, which allows you to delete the container. See the multipass docs for more details.</p>"},{"location":"stretch-install/docs/contributing/#filing-a-pull-request","title":"Filing a Pull Request","text":"<p>Once your changes are committed to your fork, you can open a pull request towards the Stretch Install master branch. A member of Hello Robot's software team will review your new PR and get it merged and available for all Stretch users.</p>"},{"location":"stretch-install/docs/install_ubuntu_18.04/","title":"Ubuntu 18.04 Installation","text":"<p>This guide describes how to perform an OS installation of Ubuntu 18.04 LTS onto Stretch.</p>"},{"location":"stretch-install/docs/install_ubuntu_18.04/#ubuntu-image","title":"Ubuntu Image","text":"<p>Download the 18.04.1 amd64 Ubuntu desktop image by clicking this link:</p> <p>http://old-releases.ubuntu.com/releases/18.04.1/ubuntu-18.04.1-desktop-amd64.iso</p> <p>Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive.</p>"},{"location":"stretch-install/docs/install_ubuntu_18.04/#installation","title":"Installation","text":"<p>Insert a bootable drive into the USB hub in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu.</p> <p></p> <p>From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\".</p> <p></p> <p>From here, the monitor should show the grub bootloader and display a menu similar to what is shown below:</p> <p></p> <p>From this menu select 'Install Ubuntu'. The Ubuntu 18.04 installer will be launched.</p> <p>At the first screen you will be prompted to select a language for the system. Select 'English' as shown below</p> <p></p> <p>Next you will be prompted to select a keyboard layout. Select 'English(US)'.</p> <p></p> <p>The next page will show a menu to select a wifi network if you are not already connected.</p> <p></p> <p>It is suggested to use a wired connection if possible for a faster install; your connection status should be visible in the top right of the display.</p> <p></p> <p></p> <p>On the next page titled, 'Updates and other software', select 'Minimal Installation' under 'What apps would you like to install to start with?'</p> <p>Also, check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and, uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'.</p> <p></p>"},{"location":"stretch-install/docs/install_ubuntu_18.04/#erase-reinstall-vs-install-alongside","title":"Erase &amp; Reinstall vs Install Alongside","text":"<p>On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 18.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase &amp; reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB.</p> <p>Here's what the Erase &amp; Reinstall option will look like, and an screenshot of the Install Alongside option is shown below.</p> <p></p> <p></p> <p>There will be a prompt to confirm you wish to create the appropriate partitions for the ubuntu install.</p> <p>If you've chosen the erase &amp; reinstall option, ensure there is nothing on the hard drive you wish to save before selecting continue</p> <p></p> <p>Next, select your timezone.</p> <p></p> <p>Finally, enter the identifying information as written below replacing '1000' with the appropriate serial number for the robot. The robot's serial number can be found on the left wall of the robot's trunk.</p> <ul> <li>name: Hello Robot Inc.</li> <li>computer name: stretch-re1-1000</li> <li>username: hello-robot</li> <li>password: xxxx</li> </ul> <p>Also select the 'Log in automatically' option. When finished the 'Who are you' page should look like the picture below.</p> <p></p> <p>Ubuntu will now be installed.</p> <p></p> <p>After the installation is completed, you will be prompted to remove the installation medium and restart.</p> <p></p> <p>Remove the installation medium and turn off the robot.</p> <p>Ubuntu 18.04 is now installed successfully.</p>"},{"location":"stretch-install/docs/install_ubuntu_20.04/","title":"Ubuntu 20.04 Installation","text":"<p>This guide describes how to perform an OS installation of Ubuntu 20.04 LTS onto Stretch.</p>"},{"location":"stretch-install/docs/install_ubuntu_20.04/#ubuntu-image","title":"Ubuntu Image","text":"<p>Download the 20.04.4 amd64 Ubuntu desktop image.</p> <p>Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive.</p>"},{"location":"stretch-install/docs/install_ubuntu_20.04/#installation","title":"Installation","text":"<p>Insert a bootable drive into the USB hub in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu.</p> <p></p> <p>From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\".</p> <p></p> <p>From here, the monitor should show the grub bootloader and display a menu similar to what is shown below:</p> <p></p> <p>From this menu select 'Ubuntu'. A disk errors checker will start and then the Ubuntu 20.04 installer will be launched.</p> <p></p> <p>At the first screen you will be prompted to select a language for the system. Select 'English' as shown below and click on the \"Install Ubuntu\".</p> <p></p> <p>Next you will be prompted to select a keyboard layout. Select 'English(US)'.</p> <p></p> <p>The next page will show a menu to select a Wifi network if you are not already connected.</p> <p></p> <p>It is suggested to use a wired connection if possible for a faster install; your connection status should be visible in the top right of the display.</p> <p></p> <p></p> <p>On the next page titled, 'Updates and other software', select 'Minimal Installation' under 'What apps would you like to install to start with?'</p> <p>Also, check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and, uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'.</p> <p></p>"},{"location":"stretch-install/docs/install_ubuntu_20.04/#erase-reinstall-vs-install-alongside","title":"Erase &amp; Reinstall vs Install Alongside","text":"<p>On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 20.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase &amp; reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB.</p> <p>Here's what the Erase &amp; Reinstall option will look like, and an screenshot of the Install Alongside option is shown below.</p> <p></p> <p></p> <p>There will be a prompt to confirm you wish to create the appropriate partitions for the ubuntu install.</p> <p>If you've chosen the erase &amp; reinstall option, ensure there is nothing on the hard drive you wish to save before selecting continue</p> <p></p> <p>Next, select your timezone.</p> <p></p> <p>Finally, enter the identifying information as written below replacing '1000' with the appropriate serial number for the robot. The robot's serial number can be found on the left wall of the robot's trunk.</p> <ul> <li>name: Hello Robot Inc.</li> <li>computer name: stretch-re1-1000</li> <li>username: hello-robot</li> <li>password: xxxx</li> </ul> <p>Also select the 'Log in automatically' option. When finished the 'Who are you' page should look like the picture below.</p> <p></p> <p>Ubuntu will now be installed.</p> <p></p> <p>After the installation is completed, you will be prompted to remove the installation medium and restart.</p> <p> </p> <p>Remove the installation medium and press ENTER to restart.</p> <p>Ubuntu 20.04 is now installed successfully.</p>"},{"location":"stretch-install/docs/install_ubuntu_22.04/","title":"Ubuntu 22.04 Installation","text":"<p>This guide describes how to perform an OS installation of Ubuntu 22.04 LTS onto Stretch.</p>"},{"location":"stretch-install/docs/install_ubuntu_22.04/#ubuntu-image","title":"Ubuntu Image","text":"<p>Download the 22.04.2 amd64 Ubuntu desktop image.</p> <p>Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive.</p>"},{"location":"stretch-install/docs/install_ubuntu_22.04/#installation","title":"Installation","text":"<ol> <li> <p>Insert a bootable drive into the USB hub in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu.</p> <ul> <li></li> </ul> </li> <li> <p>From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\". This will take you to the grub menu.</p> <ul> <li></li> </ul> </li> <li> <p>From the grub menu, select 'Ubuntu' or look for a similar option that mentions \"Install Ubuntu\".</p> <ul> <li></li> </ul> </li> <li> <p>A disk errors checker will start and then the Ubuntu 22.04 installer will be launched.</p> <ul> <li></li> </ul> </li> <li> <p>The first screen of the installer will prompt you to select a language for the system and choose between trying or installing Ubuntu. Select 'English' and \"Install Ubuntu\".</p> <ul> <li></li> </ul> </li> <li> <p>Next you will be prompted to select a keyboard layout. Select 'English(US)'.</p> <ul> <li></li> </ul> </li> <li> <p>The next page will show a menu to select a Wifi network if you are not already connected. For a faster and more reliable install, we suggest using a wired connection if one is available to you.</p> <ul> <li></li> <li>Your connection status will be visible in the top right of the display.     | Wifi      | Ethernet |     | ----------- | ----------- |     |  |  |</li> </ul> </li> <li> <p>The next page configures what gets installed. Select 'Minimal Installation' under 'What apps would you like to install to start with?' Check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'.</p> <ul> <li></li> </ul> </li> </ol>"},{"location":"stretch-install/docs/install_ubuntu_22.04/#erase-reinstall-vs-install-alongside","title":"Erase &amp; Reinstall vs Install Alongside","text":"<ol> <li> <p>On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 22.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase &amp; reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB.     | Erase &amp; Reinstall      | Install Alongside |     | ----------- | ----------- |     |  |  |</p> </li> <li> <p>There will be a prompt to confirm you wish to create the appropriate partitions for the Ubuntu install. Clicking 'Continue' will begin making changes to your robot's hard drive, so ensure there is nothing on the hard drive you wish to save before selecting continue.</p> <ul> <li></li> </ul> </li> <li> <p>Next, select your timezone.</p> <ul> <li></li> </ul> </li> <li> <p>Finally, enter the identifying information as written below, replacing 'stretch-rey-xxxx' with the appropriate serial number for the robot. The robot's serial number can be found on a sticker on the left wall of the robot's trunk. Also select the 'Log in automatically' option.</p> <ul> <li>name: Hello Robot Inc.</li> <li>computer name: stretch-rey-xxxx</li> <li>username: hello-robot</li> <li>password: choose your own</li> <li></li> </ul> </li> <li> <p>Ubuntu will now be installed.</p> <ul> <li></li> </ul> </li> <li> <p>After the installation is completed, you will be prompted to restart. Select 'Restart Now'.</p> <ul> <li></li> </ul> </li> <li> <p>Remove the installation medium and press ENTER to restart.</p> <ul> <li></li> </ul> </li> </ol> <p>Ubuntu 22.04 is now installed successfully.</p>"},{"location":"stretch-install/docs/robot_install/","title":"Performing a Robot Installation","text":""},{"location":"stretch-install/docs/robot_install/#why","title":"Why","text":"<p>A fresh robot-level install should only be done under the guidance of Hello Robot Support. This guide will lead you through a robot-level install, which can be used to:</p> <ul> <li>Erase the previous OS and set up Stretch with an entirely fresh software stack</li> <li>Erase a corrupted OS and set up Stretch with an entirely fresh software stack</li> <li>Upgrade Stretch by installing a newer software stack alongside the previous OS</li> </ul> <p>Each OS installs on a separate partition on the hard drive. You can create as many robot-level installs (i.e. new partitions) as will fit in your robot's 500GB hard drive.</p> <p>Currently, there are 3 available versions of the software stack, listed from oldest to newest:</p> <ol> <li>(Deprecated) Ubuntu 18.04 LTS shipped on robots until summer 2022, and included software for ROS Melodic and Python2</li> <li>(Stable) Ubuntu 20.04 LTS comes with ROS Noetic and Python3.8. This is what currently ships on robots.</li> <li>(Experimental) Ubuntu 22.04 LTS comes with ROS2 Humble and Python3.10. In the future, this version will ship on robots. Warning: Both the installation scripts and the software installed are under active development. Please proceed with caution.</li> </ol>"},{"location":"stretch-install/docs/robot_install/#how","title":"How","text":"<p>There are a few steps to performing a new robot install:</p> <ol> <li>Plug in charger &amp; Attach clip-clamp</li> <li>Backup robot configuration data</li> <li>Setup the BIOS (only necessary for NUCs not previously configured by Hello Robot)</li> <li>Install Ubuntu</li> <li>Run the new robot installation script</li> </ol> <p>It typically takes ~2 hours to go through these steps and complete a new robot install.</p>"},{"location":"stretch-install/docs/robot_install/#plug-in-charger-attach-clip-clamp","title":"Plug in charger &amp; Attach clip-clamp","text":"<p>Since a new robot install can take a few hours, it's important the robot remain on the charger throughout the install. Switch the charger into SUPPLY mode and plug the charger into the robot.</p> <p></p> <p>Next, attach the clip-clamp below the shoulder as shown. This allows the arm to rest on the clamp during the firmware portion of the install.</p>"},{"location":"stretch-install/docs/robot_install/#back-up-robot-configuration-data","title":"Back up robot configuration data","text":"<p>It is a good idea to backup all valuable data beforehand. If your new robot install will replace a previous one, data from the previous robot install will be deleted. Even if your new robot install will live alongside the previous one(s), data from the previous robot install(s) can be lost.</p> <p>In particular, your new robot install will require the old install's robot calibration data. The steps to copy this material from an existing install is:</p> <ol> <li>Boot into the robot's original Ubuntu partition and plug in a USB key.</li> <li>The robot calibration data lives inside of a directory called <code>stretch-re&lt;y&gt;-&lt;xxxx&gt;</code>, where <code>&lt;y&gt;</code> is your robot model number (1 or 2), and <code>&lt;xxxx&gt;</code> is your robot's serial number. There's a few versions of this directory and you will need to decide which version to backup. Each Ubuntu user has a version of this directory located at <code>/home/$USER/stretch_user/stretch-re&lt;y&gt;-&lt;xxxx&gt;</code>. These user versions are updated when the user runs a URDF calibration, swaps out an end effector, updates Stretch parameters, and more. There's also a system version located at <code>/etc/hello-robot/stretch-re&lt;y&gt;-&lt;xxxx&gt;</code>, which is likely the oldest version since it was created at Hello Robot HQ. If you're not sure which version to backup, use the version at <code>/etc/hello-robot/stretch-re&lt;y&gt;-&lt;xxxx&gt;</code> for the next step.</li> <li>Copy the <code>stretch-re&lt;y&gt;-&lt;xxxx&gt;</code> directory, where <code>&lt;xxxx&gt;</code> is your robot's serial number, to a USB key.<ul> <li>For example, if you're copying the system version, you can run a command similar to <code>cp -r /etc/hello-robot/stretch-re&lt;y&gt;-&lt;xxxx&gt; /media/$USER/&lt;USBKEY&gt;</code> from the command line, where <code>&lt;USBKEY&gt;</code> and <code>&lt;xxxx&gt;</code> is replaced with the mounted USB key's name and the robot's serial number, respectively.</li> <li>Or, you can open the file explorer to copy the directory.</li> </ul> </li> </ol> <p>If your previous partition is corrupted or inaccessible, contact Hello Robot support and they will be able to supply an older version of the <code>stretch-re&lt;y&gt;-&lt;xxxx&gt;</code> directory.</p>"},{"location":"stretch-install/docs/robot_install/#setup-the-bios","title":"Setup the BIOS","text":"<p>This step can be skipped if your robot had an existing software install on it. Otherwise, follow the guide to set up the BIOS.</p>"},{"location":"stretch-install/docs/robot_install/#install-ubuntu","title":"Install Ubuntu","text":"<p>Choose between the following guides based on which version of Ubuntu you're installing (see above for info on what software ships with each OS). Within these guides, you'll have the choice of whether to replace the previous OS partition or to install alongside it. If you choose to install alongside it, you'll also be able to choose the size of each partition on the hard drive.</p> <ul> <li>Ubuntu 18.04 Installation guide</li> <li>Ubuntu 20.04 Installation guide</li> <li>Ubuntu 22.04 Installation guide</li> </ul> <p>After the Ubuntu install, the default <code>hello-robot</code> user account will be set up.</p>"},{"location":"stretch-install/docs/robot_install/#run-the-new-robot-installation-script","title":"Run the new robot installation script","text":"<p>Login to the <code>hello-robot</code> user account on your new Ubuntu partition, open a terminal, and run:</p> <pre><code>sudo apt update\nsudo apt install git zip\n</code></pre> <p>Note: The system may not be able to run 'apt' immediately after a reboot as the OS may be running automatic updates in the background. Typically, waiting 10-20 minutes will allow you to use 'apt' again.</p> <p>Next, place the robot's calibration data in the home folder using the following steps:</p> <ol> <li>Plug in the USB key that contains the backed up calibration data.</li> <li>Copy the <code>stretch-re&lt;y&gt;-&lt;xxxx&gt;</code> directory, where <code>&lt;xxxx&gt;</code> is your robot's serial number, from the USB key into the home folder (i.e. <code>/home/$USER/</code>).<ul> <li>For example, you can run a command similar to <code>cp -r /media/$USER/&lt;USBKEY&gt;/stretch-re&lt;y&gt;-&lt;xxxx&gt; /home/$USER/</code> from the command line, where <code>&lt;USBKEY&gt;</code> and <code>&lt;xxxx&gt;</code> are replaced with your USB key's name and your robot's serial number, respectively.</li> <li>Or, you can open the file explorer to copy the directory.</li> </ul> </li> </ol> <p>Next, pull down the Stretch Install repository and being the installation process:</p> <pre><code>cd ~/\ngit clone https://github.com/hello-robot/stretch_install\ncd stretch_install\ngit pull\n./stretch_new_robot_install.sh\n</code></pre> <p>Once the script has started, it will ask you for your robot's serial number, Y/N confirmation, and the password. Then, the script will typically take 20-30 minutes to complete on a wired connection. Once it finishes, it should print out something similar to:</p> <pre><code>#############################################\nDONE! INSTALLATION COMPLETED SUCCESSFULLY.\n[...]\n#############################################\n</code></pre> <p>If it has not printed out 'DONE', then the robot install did not complete successfully. Take a look at the troubleshooting section below for solutions to common issues, or contact Hello Robot support via email or the forum.</p>"},{"location":"stretch-install/docs/robot_install/#post-install-steps","title":"Post install steps","text":"<p>Next, we'll complete the post install steps. First, in order for the many changes to take effect, the robot will need a full reboot. The steps are:</p> <ol> <li>Ensure there's a clamp under the lift</li> <li>Shutdown the Ubuntu OS through the GUI or use <code>sudo shutdown -h now</code> in the terminal</li> <li>Turn the power switch in the robot's trunk to the off position (orange power LED becomes unlit)</li> <li>Ensure a keyboard/monitor is plugged into the robot. When the robot powers up, you can use the keyboard to decide which OS to boot into.</li> <li>Turn the power switch in the robot's trunk to the on position (orange power LED becomes lit)</li> <li>Boot into the new Ubuntu partition and log in if necessary</li> </ol> <p>Next, we'll ensure the robot's firmware is upgraded to the latest available. Newer firmware unlocks new features (e.g. waypoint trajectory following) and fixes bugs. See the firmware releases for details.</p> <pre><code>REx_firmware_updater.py --install\n</code></pre> <p>Next, if your robot has the Stretch Dex Wrist, we'll configure the software to recognize it. Skip this step if you are not using the Dex Wrist.</p> <pre><code>cd ~/stretch_install\n./stretch_new_dex_wrist_install.sh\nREx_calibrate_guarded_contact.py --arm\n</code></pre> <p>Next, we'll run Stretch's homing procedure, where every joint's zero is found. Robots with relative encoders (vs absolute encoders) need a homing procedure when they power on. For Stretch, it's a 30-second procedure that must occur everytime the robot wakes up.</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>Finally, we'll run the system check to confirm the robot is ready to use. If you see any failures or errors, contact Hello Robot support via email or the forum.</p> <pre><code>stretch_robot_system_check.py\n</code></pre> <p>Your robot is now configured with a new robot install! In order to set up new users on this robot install, see the Adding a New User guide.</p>"},{"location":"stretch-install/docs/robot_install/#troubleshooting","title":"Troubleshooting","text":"<p>This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum.</p>"},{"location":"stretch-install/docs/robot_install/#expecting-var-hello_fleet_id-to-be-undefined-error","title":"'Expecting var HELLO_FLEET_ID to be undefined' error","text":"<p>If you are seeing the following error:</p> <pre><code>[...]\nChecking ~/.bashrc doesn't already define HELLO_FLEET_ID...\nExpecting var HELLO_FLEET_ID to be undefined. Check end of ~/.bashrc file, delete all lines in 'STRETCH BASHRC SETUP' section, and open a new terminal. Exiting.\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>You are performing a new robot install on a robot that has already gone through the robot install process. If this is intentional, you will need to manually delete lines that a previous robot install appended to the <code>~/.bashrc</code> dotfile. Open the <code>~/.bashrc</code> file in an editor and look near the end for a section that looks like:</p> <pre><code>######################\n# STRETCH BASHRC SETUP\n######################\nexport HELLO_FLEET_PATH=/home/ubuntu/stretch_user\nexport HELLO_FLEET_ID=stretch-re1-2000\nexport PATH=${PATH}:~/.local/bin\nexport LRS_LOG_LEVEL=None #Debug\nsource /opt/ros/noetic/setup.bash\nsource /home/ubuntu/catkin_ws/devel/setup.bash\n[...]\n</code></pre> <p>Delete this section from the <code>~/.bashrc</code>. Note that it's common for other programs (e.g. Conda, Ruby) to append to your <code>~/.bashrc</code> as well, and deleting those lines accidentally can impede their functionality. Take care to only delete lines related to 'STRETCH BASHRC SETUP' section. Next, open a new terminal. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) automatically runs the commands in the <code>~/.bashrc</code> dotfile when opened, so the new terminal won't be set up with the lines that were just deleted. Now you can run a new robot install and this error should gone.</p>"},{"location":"stretch-install/docs/robot_install/#expecting-stretch-re1-xxxx-to-be-present-in-the-home-folder-error","title":"'Expecting stretch-re1-xxxx to be present in the home folder' error","text":"<p>If you are seeing the following error:</p> <pre><code>[...]\nChecking robot calibration data in home folder...\nExpecting robot calibration stretch-rey-xxxx to be present in the the home folder. Exiting.\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>The install scripts exited before performing the robot install because it was unable to find the robot's calibration data folder, 'stretch-rey-xxxx'. Please ensure you have backed up your robot's calibration data to a USB key and copied the 'stretch-rey-xxxx' folder to the home folder of your new partition. See the Run the new robot installation script section for more details. Then, run the install scripts again and the error should be gone.</p>"},{"location":"stretch-install/docs/robot_install/#repo-not-up-to-date-error","title":"'Repo not up-to-date' error","text":"<p>If you are seeing the following error:</p> <pre><code>[...]\nChecking install repo is up-to-date...\nRepo not up-to-date. Please perform a 'git pull'. Exiting.\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>The version of Stretch Install being used is out of date. In a terminal, go to the Stretch Install folder (should be in the home folder: <code>cd ~/stretch_install</code>), and perform a <code>git pull</code> to pull down the latest version. If the git pull fails, ensure Stretch Install has a clean working tree using <code>git status</code>. If you see any red files, save them if important, delete Stretch Install, and reclone it.</p>"},{"location":"stretch-install/docs/robot_install/#failed-to-fetch-error","title":"'Failed to fetch' error","text":"<p>If you are seeing the following error:</p> <pre><code>Install &lt;some package&gt;\nE: Failed to fetch &lt;url to some .deb file&gt;  Connection failed [IP: &lt;some IP address&gt;]\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>Ubuntu's system package manager, Apt, has failed to contact the server that hosts some package that the install scripts need to download. Typically, these issues are transient and waiting some time before rerunning the install script will solve the issue.</p>"},{"location":"stretch-install/docs/robot_install/#dpkg-returned-an-error-code-error","title":"'dpkg returned an error code' error","text":"<p>If you are seeing the following error:</p> <pre><code>Install &lt;some package&gt;\nE: Sub-process /usr/bin/dpkg returned an error code (1)\n\n#############################################\nFAILURE. INSTALLATION DID NOT COMPLETE.\n[...]\n</code></pre> <p>Ubuntu's system package manager, Apt, has failed to complete some step of the install process for a package that the install scripts need to install. Typically, these issues are transient and waiting some time before rerunning the install script will solve the issue. If you continue to see this error, contact Hello Robot support via email or the forum.</p>"},{"location":"stretch-install/docs/robot_install/#firmware-protocol-mismatch-error","title":"'Firmware protocol mismatch' error","text":"<p>If you are seeing the following error: <pre><code>----------------\nFirmware protocol mismatch on hello-&lt;X&gt;.\nProtocol on board is p&lt;X&gt;.\nValid protocol is: p&lt;X&gt;.\nDisabling device.\nPlease upgrade the firmware and/or version of Stretch Body.\n----------------\n</code></pre></p> <p>Your version of Stretch Body does not align with the firmware installed with your robot. It's recommended that Stretch Body is first upgraded to the latest version available (but if you're intentionally running an older version, you can skip this step and the firmware updater will downgrade your firmware appriopriately). To upgrade Stretch Body, follow the instructions here.</p> <p>Next, run the firmware updater tool to automatically update the firmware to the required version for your software.</p> <pre><code>REx_firmware_updater.py --install\n</code></pre> <p>The firmware mismatch errors should now be gone.</p>"},{"location":"stretch-install/docs/robot_install/#homing-error","title":"Homing Error","text":"<p>If using <code>stretch_robot_home.py</code> does not result in the robot being homed, try running the command again. If this does not work, try shutting down the robot, turning off the robot with the power switch, waiting for a few seconds, and then powering it on again. Then, try <code>stretch_robot_home.py</code> again.</p>"},{"location":"stretch-install/docs/robot_install/#ros-launch-file-fails","title":"ROS Launch File Fails","text":"<p>ROS launch files have nondeterministic behavior. Sometimes they need to be run more than once for the nodes to start in a successful order that works. For example, a common symptom of a failed launch is the visualization of the robot's body appearing white and flat in RViz.</p>"},{"location":"stretch-install/docs/ros_workspace/","title":"Creating a new ROS Workspace","text":""},{"location":"stretch-install/docs/ros_workspace/#why","title":"Why","text":"<p>ROS1 and ROS2 organize software by \"workspaces\", where ROS packages are developed, compiled, and made available to run from the command line. By default, a ROS1 workspace called <code>catkin_ws</code> is available in the home directory. If your robot is running Ubuntu 20.04, an additional ROS2 workspace called <code>ament_ws</code> is also available in the home directory. This guide will show you how to replace existing or create new ROS1/2 workspaces for developing ROS software.</p>"},{"location":"stretch-install/docs/ros_workspace/#how","title":"How","text":"<p>Open a terminal and execute the following to pull down the Stretch Install repository.</p> <pre><code>cd ~\ngit clone https://github.com/hello-robot/stretch_install\ncd stretch_install\ngit pull\n</code></pre>"},{"location":"stretch-install/docs/ros_workspace/#ubuntu-1804","title":"Ubuntu 18.04","text":"<p>Run the following command to create a ROS1 Melodic workspace (replacing <code>&lt;optional-path-to-ws&gt;</code> for the <code>-w</code> flag with a filepath to the workspace. Not providing the flag defaults to <code>~/catkin_ws</code>).</p> <pre><code>./factory/18.04/stretch_create_catkin_workspace.sh -w &lt;optional-path-to-ws&gt;\n</code></pre>"},{"location":"stretch-install/docs/ros_workspace/#ubuntu-2004","title":"Ubuntu 20.04","text":"<p>Run the following command to create a ROS1 Noetic workspace (replacing <code>&lt;optional-path-to-ws&gt;</code> for the <code>-w</code> flag with a filepath to the workspace. Not providing the flag defaults to <code>~/catkin_ws</code>).</p> <pre><code>./factory/20.04/stretch_create_catkin_workspace.sh -w &lt;optional-path-to-ws&gt;\n</code></pre>"},{"location":"stretch-install/docs/ros_workspace/#ubuntu-2204","title":"Ubuntu 22.04","text":"<p>Run the following command to create a ROS2 Humble workspace (replacing <code>&lt;optional-path-to-ws&gt;</code> for the <code>-w</code> flag with a filepath to the workspace. Not providing the flag defaults to <code>~/ament_ws</code>).</p> <pre><code>./factory/22.04/stretch_create_ament_workspace.sh -w &lt;optional-path-to-ws&gt;\n</code></pre>"},{"location":"stretch-install/docs/ros_workspace/#wrap-up","title":"Wrap up","text":"<p>Close your current terminal and open a new one. The new terminal will have automatically activated the ROS workspace(s).</p> <p>Your new ROS workspace is now set up successfully!</p>"},{"location":"stretch-install/docs/ros_workspace/#troubleshooting","title":"Troubleshooting","text":"<p>This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum.</p>"},{"location":"stretch-install/docs/ros_workspace/#conflicting-ros-version-sourced-error","title":"'Conflicting ROS version sourced' error","text":"<p>If you are seeing the following error:</p> <pre><code>###########################################\nCREATING &lt;ROS VERSION&gt; WORKSPACE at &lt;WS DIR&gt;\n###########################################\n[...]\nEnsuring correct version of ROS is sourced...\nCannot create workspace while a conflicting ROS version is sourced. Exiting.\n</code></pre> <p>The ROS workspace is not created because the check that a conflicting ROS version isn't already sourced has failed. For example, if you're creating an ROS2 Ament workspace, but ROS1 Noetic was previously sourced in the same environment, the check will error out since the new ROS2 workspace would fail to find its dependencies correctly in this environment. Sourcing a version of ROS typically happens using the following command: <code>source /opt/ros/&lt;ros version&gt;/setup.bash</code>. If you ran this command to source a conflicting version previously, simply open a new terminal and the new environment won't have the conflicting ROS version sourced. If you didn't run this command and you're still getting the error, it's likely because the command exists in the <code>~/.bashrc</code> dotfile. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) runs the commands in the <code>~/.bashrc</code> dotfile. Look at the bottom of this dotfile for this command, comment it out temporarily, and open a new terminal. This new shell environment should have no trouble creating the ROS workspace.</p>"},{"location":"stretch-install/docs/ros_workspace/#ros_distro-was-set-before-warning","title":"'ROS_DISTRO was set before' warning","text":"<p>If you are seeing the following warning:</p> <pre><code>ROS_DISTRO was set to '&lt;ROS VERSION&gt;' before. Please make sure that the environment does not mix paths from different distributions.\n</code></pre> <p>Multiple versions of ROS are being sourced in the same environment. This is known to cause issues with the <code>rosdep</code> tool, and might cause issues elsewhere as well. If you haven't explicitly sourced conflicting versions by using the <code>source /opt/ros/&lt;ros version&gt;/setup.bash</code> (a variant on this command could look like <code>source ~/&lt;ws dir&gt;/develop/setup.bash</code>) command twice, then it's likely that one or two versions of ROS are implicitly being sourced in the <code>~/.bashrc</code> dofile. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) runs the commands in the <code>~/.bashrc</code> dotfile. Look at the bottom of this dotfile for the <code>source</code> command and ensure conflicting versions aren't being sourced.</p>"},{"location":"stretch-install/docs/updating_software/","title":"Updating Stretch Software","text":"<p>Stretch's software is improved with new features and bug fixes with each update. In this guide, we cover when and how to update the various software components on your Stretch.</p>"},{"location":"stretch-install/docs/updating_software/#when-to-update","title":"When to Update","text":"<p>We develop our software publicly on Github, allowing anyone to follow/propose the development of a code feature or bug fix. While we wholeheartedly welcome collaboration on Github, it is not necessary to be active on Github to follow our software releases. We announce every major release of software on our forum. These are stable releases with code that has been extensively tested on many Stretch robots. To be notified of new releases, create an account on the forum and click the bell icon in the top left of the announcements section. The forum is also available to report issues and ask questions about any of our software packages.</p>"},{"location":"stretch-install/docs/updating_software/#how-to-update","title":"How to Update","text":"<p>Each Stretch is shipped with firmware, a Python SDK, and ROS packages developed specifically for Stretch. There are separate processes for updating each of these components.</p>"},{"location":"stretch-install/docs/updating_software/#stretch-ros","title":"Stretch ROS","text":"<p>Stretch ROS is the Robot Operating System (ROS) interface to the robot. Many robotics developers find ROS useful to bootstrap their robotics software developments. You may update it using the following commands:</p> <pre><code>roscd stretch_core\ngit pull\ncd ../../..\nrosdep install -iy --from-paths src\n</code></pre>"},{"location":"stretch-install/docs/updating_software/#stretch-python-modules","title":"Stretch Python Modules","text":"<p>There are a few Python modules that allow users to work with the robot in pure Python2/3. These modules are:</p> <ul> <li>Stretch Body is the Python SDK to the robot. It abstracts away the low level details of communication with the embedded devices and provides an intuitive API to working with the robot.</li> <li>Stretch Body Tools is a set of command line tools that builds on Stretch Body. They allow you to quickly home, stow, command joints, read sensors, and more on the robot.</li> <li>Stretch Tool Share is a repository of accessories (e.g. expo marker gripper, docking station) for Stretch, created by the community and Hello Robot.</li> <li>Stretch Factory is a library/set of command line tools that enable calibrations, firmware upgrading, and other factory related tasks.</li> </ul> <p>On Ubuntu 18.04, update them using:</p> <pre><code>python -m pip install -U hello-robot-stretch-body hello-robot-stretch-body-tools hello-robot-stretch-tool-share hello-robot-stretch-factory\n</code></pre> <p>On Ubuntu 20.04, update them using:</p> <pre><code>python3 -m pip install -U hello-robot-stretch-body hello-robot-stretch-body-tools hello-robot-stretch-tool-share hello-robot-stretch-factory\n</code></pre>"},{"location":"stretch-install/docs/updating_software/#stretch-firmware","title":"Stretch Firmware","text":"<p>The firmware and the Python SDK (called Stretch Body) communicate on an established protocol. Therefore, it is important to maintain a protocol match between the different firmware and Stretch Body versions. Fortunately, there is a tool that handles this automatically. In the command line, run the following command:</p> <pre><code>REx_firmware_updater.py --recommended\n</code></pre> <p>This script will automatically determine what version is currently running on the robot and provide a recommendation for a next step. Follow the next steps provided by the firmware updater script.</p>"},{"location":"stretch-install/docs/updating_software/#ubuntu","title":"Ubuntu","text":"<p>The operating system upon which Stretch is built is called Ubuntu. This operating system provides the underlying packages that power Stretch's software packages. Furthermore, users of Stretch depend on this operating system and the underlying packages to develop software on Stretch. Therefore, it is important to keep the OS and these underlying packages up to date. In the command line, run the following command:</p> <pre><code>sudo apt update\nsudo apt upgrade\nsudo apt autoremove\n</code></pre> <p>Apt is the package manager that handles updates for all Ubuntu packages.</p>"},{"location":"stretch-install/docs/updating_software/#verify-updated-successfully","title":"Verify updated successfully","text":"<p>Finally, reboot your robot and execute the following to confirm that all software updated successfully.</p> <pre><code>stretch_robot_system_check.py\n</code></pre> <p>If you run into any errors, see the troubleshooting guide below or contact Hello Robot Support.</p>"},{"location":"stretch-install/docs/updating_software/#troubleshooting","title":"Troubleshooting","text":""},{"location":"stretch-install/docs/updating_software/#param-migration-error","title":"Param Migration Error","text":"<p>If you see the following error:</p> <pre><code>Please run tool REx_migrate_params.py before continuing. For more details, see https://forum.hello-robot.com/t/425\n</code></pre> <p>This error appears because the organization of Stretch's parameters has changed since Stretch Body v0.3 and requires a migration of these parameters to the new organization system. Executing the following command will automaticaly migrate your parameters over:</p> <pre><code>REx_migrate_params.py\n</code></pre> <p>To learn more about Stretch's parameter system, see this tutorial.</p>"},{"location":"stretch-install/docs/updating_software/#firmware-mismatch-error","title":"Firmware Mismatch Error","text":"<p>If you see the following error:</p> <pre><code>----------------\nFirmware protocol mismatch on /dev/XXXX.\nProtocol on board is pX.\nValid protocol is: pX.\nDisabling device.\nPlease upgrade the firmware and/or version of Stretch Body.\n----------------\n</code></pre> <p>This error appears because the low level Python SDK and the firmware cannot communicate to each other. There is a protocol mismatch preventing communication between the two. Simply run the following script and follow its recommendations to upgrade/downgrade the firmware as necessary to match the protocol level of Stretch Body.</p> <pre><code>$ REx_firmware_updater.py --status\n</code></pre>"},{"location":"stretch-install/factory/22.04/iron2humble_migration/","title":"Iron -&gt; Humble Migration","text":"<p>The Stretch Install repo has switched the version of ROS2 installed with the Ubuntu 22.04 software stack. On Sept 7th, 2023, we switched it to ROS2 Iron. On Sept 14, 2023, we switched it back to ROS2 Humble. We expect some users to have upgraded in that week, so this guide serves to help migrate from Iron -&gt; Humble.</p>"},{"location":"stretch-install/factory/22.04/iron2humble_migration/#steps","title":"Steps","text":"<ol> <li> <p>Verify this guide applies to you     <pre><code>lsb_release -d\necho $ROS_DISTRO\n</code></pre>     You should see \"Description: Ubuntu 22.04.* LTS\" and \"iron\". If not, you do not need this guide.</p> </li> <li> <p>Clone Stretch Install into your home folder:     <pre><code>cd ~/\ngit clone https://github.com/hello-robot/stretch_install.git\ncd ~/stretch_install\ngit pull\n</code></pre></p> </li> <li> <p>Run the system install script     <pre><code>./factory/22.04/stretch_install_system.sh\n</code></pre>     The script can fails silently. If the last line printed out isn't \"Install librealsense2 packages\", something went wrong. Contact Hello Robot Support.</p> </li> <li> <p>Update the .bashrc script     <pre><code>gedit ~/.bashrc\n</code></pre>     A text editor will open. Scroll to the bottom of the file, and look for a section called \"STRETCH BASHRC SETUP\". Under this section, you'll see a line that says <code>source /opt/ros/iron/setup.bash</code>. Change this line to <code>source /opt/ros/humble/setup.bash</code>. Next, look for a line that says <code>source $HOME/ament_ws/install/setup.bash</code> or similar. Delete this line. Now save the file and close the editor.</p> </li> <li> <p>Refresh your terminal by closing your current terminal and opening a new one</p> </li> <li> <p>Run the ament workspace creation script     <pre><code>cd ~/stretch_install\n./factory/22.04/stretch_create_ament_workspace.sh\n</code></pre>     The script can fails silently. If the last line printed out isn't \"Setup calibrated robot URDF...\", something went wrong. Contact Hello Robot Support.</p> </li> <li> <p>Refresh your terminal by closing your current terminal and opening a new one</p> </li> </ol> <p>Success! Your robot is now set up with ROS2 Humble correctly.</p>"},{"location":"stretch-ros/","title":"Melodic Deprecated","text":"<p>This branch of stretch_ros is DEPRECATED. Please use the Robot Install upgrade guide to upgrade to Ubuntu 20.04/ROS Noetic or Ubuntu 22.04/ROS2 Humble. The Melodic README is saved below.</p>"},{"location":"stretch-ros/#overview","title":"Overview","text":"<p>The stretch_ros repository holds ROS related code for the Stretch RE1 mobile manipulator from Hello Robot Inc.  For an overview of the capabilities in this repository, we recommend you look at the following forum post.</p> <p>Please be aware that the code in this repository is currently under heavy development. </p> Resource Description hello_helpers Miscellaneous helper code used across the stretch_ros repository stretch_calibration Creates and updates calibrated URDFs for the Stretch RE1 stretch_core Enables basic use of the Stretch RE1 from ROS stretch_deep_perception Demonstrations that use open deep learning models to perceive the world stretch_demos Demonstrations of simple autonomous manipulation stretch_description Generate and export URDFs stretch_funmap Demonstrations of Fast Unified Navigation, Manipulation And Planning (FUNMAP) stretch_gazebo Support for simulation of Stretch in the Gazebo simulator stretch_moveit_config Config files to use Stretch with the MoveIt Motion Planning Framework stretch_navigation Support for the ROS navigation stack, including move_base, gmapping, and AMCL"},{"location":"stretch-ros/#code-status-development-plans","title":"Code Status &amp; Development Plans","text":"<p>We intend for the following high-level summary to provide guidance about the current state of the code and planned development activities.</p> Directory Testing Status Notes hello_helpers GOOD stretch_calibration GOOD stretch_core GOOD stretch_deep_perception GOOD stretch_demos FAIR stretch_description GOOD stretch_funmap FAIR stretch_gazebo FAIR differs from stretch_core in its underlying controllers stretch_moveit_config FAIR does not support mobile base planning in ROS 1 stretch_navigation GOOD"},{"location":"stretch-ros/#licenses","title":"Licenses","text":"<p>This software is intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc.</p> <p>For license details for this repository, see the LICENSE files found in the directories. A summary of the licenses follows: </p> Directory License hello_helpers Apache 2.0 stretch_calibration GPLv3 stretch_core Apache 2.0 stretch_deep_perception Apache 2.0 stretch_demos Apache 2.0 stretch_description BSD 3-Clause Clear License stretch_funmap LGPLv3 stretch_gazebo Apache 2.0 stretch_moveit_config Apache 2.0 stretch_navigation Apache 2.0"},{"location":"stretch-ros/hello_helpers/","title":"hello_helpers","text":""},{"location":"stretch-ros/hello_helpers/#overview","title":"Overview","text":"<p>hello_helpers mostly consists of the hello_helpers Python module. This module provides various Python files used across stretch_ros that have not attained sufficient status to stand on their own.</p>"},{"location":"stretch-ros/hello_helpers/#capabilities","title":"Capabilities","text":"<p>fit_plane.py : Fits planes to 3D data.</p> <p>hello_misc.py : Various functions, including a helpful Python object with which to create ROS nodes. </p> <p>hello_ros_viz.py : Various helper functions for vizualizations using RViz.</p>"},{"location":"stretch-ros/hello_helpers/#typical-usage","title":"Typical Usage","text":"<p><pre><code>import hello_helpers.fit_plane as fp\n</code></pre> <pre><code>import hello_helpers.hello_misc as hm\n</code></pre> <pre><code>import hello_helpers.hello_ros_viz as hr\n</code></pre></p>"},{"location":"stretch-ros/hello_helpers/#license","title":"License","text":"<p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros/hello_helpers/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros/stretch_calibration/","title":"stretch_calibration","text":""},{"location":"stretch-ros/stretch_calibration/#overview","title":"Overview","text":"<p>stretch_calibration provides tools for calibrating and managing the URDF for the Stretch RE1 robot from Hello Robot Inc. The code's primary role is to generate a geometric model (i.e., a URDF) of the robot's body that corresponds well with views of the body from the robot's 3D camera (i.e., a Intel RealSense D435i). The code achieves this objective by adjusting the geometry of the model to predict where the 3D camera will see markers (i.e., ArUco markers) on the robot's body. </p> <p>Hello Robot Inc. uses this code to calibrate each robot prior to shipping. Users may wish to recalibrate their robots to compensate for changes over time or take advantage of improvements to the calibration code. </p> <p>In addition, after changing a tool, this code can be used to generate a new calibrated URDF that incorporates the tool without performing a new calibration optimization. </p>"},{"location":"stretch-ros/stretch_calibration/#checking-the-current-calibration-with-new-observations","title":"Checking the Current Calibration with New Observations","text":"<ol> <li>Make sure the basic joint limit calibration has been performed.</li> </ol> <p><code>stretch_robot_home.py</code></p> <ol> <li>Make sure the uncalibrated URDF is up to date.</li> </ol> <p><code>rosrun stretch_calibration update_uncalibrated_urdf.sh</code></p> <ol> <li>Collect new observations</li> </ol> <p><code>roslaunch stretch_calibration collect_check_head_calibration_data.launch</code></p> <ol> <li>Test how well the current calibrated model fits the new observations</li> </ol> <p><code>rosrun stretch_calibration check_head_calibration.sh</code></p> <ul> <li> <p>The total_error printed on the command line should be less than 0.05. If it is not, an error will be printed on the command line. </p> </li> <li> <p>In RViz the white markers represent the locations for the ArUco markers predicted by the calibrated URDF. The colored markers represent the observed locations of the ArUco markers on the robot's body. For a good fit, the white markers will be close to the colored markers. </p> </li> </ul> <p></p>"},{"location":"stretch-ros/stretch_calibration/#visually-inspecting-the-current-calibration","title":"Visually Inspecting the Current Calibration","text":"<p>The following command will allow you to visually inspect a calibration with Rviz. You can use RViz to see how well the robot's 3D body model matches point clouds from the 3D camera. While visualizing the 3D model and point clouds in RViz, you can use keyboard commands in the terminal to move the head around, the lift up and down, and the arm in and out. The keyboard commands will be printed in the terminal.</p> <p>A good calibration should result in a close correspondence between the robot's 3D body model and the point cloud throughout the ranges of motion for the head, lift, and arm. You may notice higher error when the head is looking upward due to challenges associated with head tilt backlash. You might also notice higher error when the arm is fully extended, since small angular errors can result in larger positional errors at the robot's wrist.</p> <ol> <li>Test the current head calibration</li> </ol> <p><code>roslaunch stretch_calibration simple_test_head_calibration.launch</code></p>"},{"location":"stretch-ros/stretch_calibration/#examples-of-good-and-bad-visual-fit","title":"Examples of Good and Bad Visual Fit","text":"<p>In the images below, examples of good and bad fit between the point cloud and the geometric model are presented side by side. To make the distinction clear, the images have green and red circles indicating where the fit is either good or bad.</p> <p></p> <p></p>"},{"location":"stretch-ros/stretch_calibration/#calibrate-the-stretch-re1","title":"Calibrate the Stretch RE1","text":"<ol> <li>Make sure the basic joint limit calibration has been performed.</li> </ol> <p><code>stretch_robot_home.py</code></p> <ol> <li>Make sure the uncalibrated URDF is up to date.</li> </ol> <p><code>rosrun stretch_calibration update_uncalibrated_urdf.sh</code></p> <ol> <li>Collect head calibration data</li> <li> <p>Put the robot on a flat surface. Give it room to move its arm and good lighting. Then, have the robot collect data using the command below. While the robot is collecting data, do not block its view of its markers. </p> <p><code>roslaunch stretch_calibration collect_head_calibration_data.launch</code></p> </li> <li> <p>Process head calibration data</p> </li> <li> <p>Specify how much data to use and the quality of the fit</p> <ul> <li> <p>YAML file with parameters: stretch_ros/stretch_calibration/config/head_calibration_options.yaml</p> </li> <li> <p>More data and higher quality fitting result in optimizations that take longer</p> </li> <li> <p>When quickly testing things out</p> </li> <li>~3 minutes without visualization</li> <li><code>data_to_use: use_very_little_data</code></li> <li> <p><code>fit_quality: fastest_lowest_quality</code></p> </li> <li> <p>When calibrating the robot </p> </li> <li>~1 hour without visualization</li> <li><code>data_to_use: use_all_data</code></li> <li><code>fit_quality: slow_high_quality</code></li> </ul> </li> <li> <p>Perform the optimization to fit the model to the collected data</p> <ul> <li>Without visualization (faster)  </li> </ul> <p><code>roslaunch stretch_calibration process_head_calibration_data.launch</code></p> <ul> <li>With visualization (slower)</li> </ul> <p><code>roslaunch stretch_calibration process_head_calibration_data_with_visualization.launch</code></p> </li> <li> <p>Inspect the fit of the most recent head calibration</p> </li> </ol> <p><code>rosrun stretch_calibration visualize_most_recent_head_calibration.sh</code></p> <ol> <li>Start using the newest head calibration</li> </ol> <p><code>rosrun stretch_calibration update_with_most_recent_calibration.sh</code></p> <ol> <li>Test the current head calibration</li> </ol> <p><code>roslaunch stretch_calibration simple_test_head_calibration.launch</code></p> <p>Use RViz to visually inspect the calibrated model. The robot's 3D body model should look similar to the structure of your robot. You may refer to the section above to see examples of good and bad fit.</p>"},{"location":"stretch-ros/stretch_calibration/#generate-a-new-urdf-after-changing-the-tool","title":"Generate a New URDF After Changing the Tool","text":"<p>If you change the Stretch RE1's tool attached to the wrist and want to generate a new URDF for it, you can do so with xacro files in the /stretch_ros/stretch_description/urdf/ directory. Specifically, you can edit stretch_description.xacro to include a xacro other than the default stretch_gripper.xacro. </p> <p>After changing the tool xacro you will need to generate a new URDF and also update this new URDF with the previously optimized calibration parameters. To do so, follow the directions below: </p> <ol> <li>In a terminal start roscore. This will enable the main script to proceed and terminate without pressing Ctrl-C.</li> </ol> <p><code>roscore</code></p> <ol> <li>Next, in a different terminal terminal run</li> </ol> <p><code>rosrun stretch_calibration update_urdf_after_xacro_change.sh</code></p> <p>This will update the uncalibrated URDF with the current xacro files and then create a calibrated URDF using the most recent calibration parameters.</p>"},{"location":"stretch-ros/stretch_calibration/#revert-to-a-previous-calibration","title":"Revert to a Previous Calibration","text":"<p>When a new calibration is performed, it is timestamped and added to the calibration directory under \"stretch_user/\". If you'd like to revert to a previous calibration, you may run the following command. It will move the most recent calibration files to a reversion directory and update the calibration in the stretch_description package from the remaining most recent calibration files.</p> <ol> <li>Revert to the previous head calibration</li> </ol> <p><code>rosrun stretch_calibration revert_to_previous_calibration.sh</code></p>"},{"location":"stretch-ros/stretch_calibration/#license","title":"License","text":"<p>stretch_calibration is licensed with the GPLv3. Please see the LICENSE file for details.</p>"},{"location":"stretch-ros/stretch_calibration/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation.</p> <p>The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link:</p> <p>https://www.gnu.org/licenses/gpl-3.0.html</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros/stretch_core/","title":"stretch_core","text":""},{"location":"stretch-ros/stretch_core/#overview","title":"Overview","text":"<p>stretch_core provides the core ROS interfaces to the Stretch RE1 mobile manipulator from Hello Robot Inc. It includes the following nodes: </p> <p>stretch_driver : node that communicates with the low-level Python library (stretch_body) to interface with the Stretch RE1</p> <p>detect_aruco_markers : node that detects and estimates the pose of ArUco markers, including the markers on the robot's body</p> <p>d435i_* : various nodes to help use the Stretch RE1's 3D camera</p> <p>keyboard_teleop : node that provides a keyboard interface to control the robot's joints</p>"},{"location":"stretch-ros/stretch_core/#license","title":"License","text":"<p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros/stretch_core/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros/stretch_deep_perception/","title":"stretch_deep_perception","text":""},{"location":"stretch-ros/stretch_deep_perception/#overview","title":"Overview","text":"<p>stretch_deep_perception provides demonstration code that uses open deep learning models to perceive the world. </p> <p>This code depends on the stretch_deep_perception_models repository, which should be installed under ~/stretch_user/ on your Stretch RE1 robot.</p> <p>Link to the stretch_deep_perception_models repository: https://github.com/hello-robot/stretch_deep_perception_models</p>"},{"location":"stretch-ros/stretch_deep_perception/#getting-started-demos","title":"Getting Started Demos","text":"<p>There are four demonstrations for you to try.</p>"},{"location":"stretch-ros/stretch_deep_perception/#face-estimation-demo","title":"Face Estimation Demo","text":"<p>First, try running the face detection demonstration via the following command:</p> <pre><code>roslaunch stretch_deep_perception stretch_detect_faces.launch \n</code></pre> <p>RViz should show you the robot, the point cloud from the camera, and information about detected faces. If it detects a face, it should show a 3D planar model of the face and 3D facial landmarks. These deep learning models come from OpenCV and the Open Model Zoo (https://github.com/opencv/open_model_zoo).</p> <p>You can use the keyboard_teleop commands within the terminal that you ran roslaunch in order to move the robot's head around to see your face.</p> <pre><code>             i (tilt up)\n\nj (pan left)               l (pan right)\n\n             , (tilt down)\n</code></pre> <p>Pan left and pan right are in terms of the robot's left and the robot's right.</p> <p>Now shut down everything that was launched by pressing q and Ctrl-C in the terminal.</p>"},{"location":"stretch-ros/stretch_deep_perception/#object-detection-demo","title":"Object Detection Demo","text":"<p>Second, try running the object detection demo, which uses the tiny YOLO v3 object detection network (https://pjreddie.com/darknet/yolo/). RViz will display planar detection regions. Detection class labels will be printed to the terminal. </p> <pre><code>roslaunch stretch_deep_perception stretch_detect_objects.launch\n</code></pre> <p>Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal.</p>"},{"location":"stretch-ros/stretch_deep_perception/#body-landmark-detection-demo","title":"Body Landmark Detection Demo","text":"<p>Third, try running the body landmark point detection demo. The deep learning model comes from the Open Model Zoo (https://github.com/opencv/open_model_zoo). RViz will display colored 3D points on body landmarks. The network also provides information to connect these landmarks, but this demo code does not currently use it.</p> <pre><code>roslaunch stretch_deep_perception stretch_detect_body_landmarks.launch \n</code></pre> <p>Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal.</p>"},{"location":"stretch-ros/stretch_deep_perception/#nearest-mouth-detection-demo","title":"Nearest Mouth Detection Demo","text":"<p>Finally, try running the nearest mouth detection demo. RViz will display a 3D frame of reference estimated for the nearest mouth detected by the robot. Sometimes the point cloud will make it difficult to see. Disabling the point cloud view in RViz will make it more visible.</p> <p>We have used this frame of reference to deliver food near a person's mouth. This has the potential to be useful for assistive feeding. However, use of this detector in this way could be risky. Please be very careful and aware that you are using it at your own risk.</p> <p>A less risky use of this detection is for object delivery. stretch_demos has a demonstration that delivers an object based on this frame of reference by holding out the object some distance from the mouth location and below the mouth location with respect to the world frame. This works well and is inspired by similar methods used with the robot EL-E at Georgia Tech [1]. </p> <pre><code>roslaunch stretch_deep_perception stretch_detect_nearest_mouth.launch \n</code></pre>"},{"location":"stretch-ros/stretch_deep_perception/#references","title":"References","text":"<p>[1] Hand It Over or Set It Down: A User Study of Object Delivery with an Assistive Mobile Manipulator, Young Sang Choi, Tiffany L. Chen, Advait Jain, Cressel Anderson, Jonathan D. Glass, and Charles C. Kemp, IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2009. http://pwp.gatech.edu/hrl/wp-content/uploads/sites/231/2016/05/roman2009_delivery.pdf</p>"},{"location":"stretch-ros/stretch_deep_perception/#license","title":"License","text":"<p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros/stretch_deep_perception/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros/stretch_demos/","title":"stretch_demos","text":""},{"location":"stretch-ros/stretch_demos/#overview","title":"Overview","text":"<p>stretch_demos provides simple demonstrations for the Stretch RE1 mobile manipulator from Hello Robot Inc. For an overview of the demos, we recommend you look at the following forum post: https://forum.hello-robot.com/t/autonomy-video-details</p>"},{"location":"stretch-ros/stretch_demos/#getting-started-demos","title":"Getting Started Demos","text":"<p>Please be aware that these demonstrations typically do not perform careful collision avoidance. Instead, they expect to operate in freespace and detect contact through motor current if an obstacle gets in the way. Please be careful when trying out these demonstrations.</p>"},{"location":"stretch-ros/stretch_demos/#handover-object-demo","title":"Handover Object Demo","text":"<p>First, place the robot near you so that it can freely move back and forth and reach near your body. Then, launch the handover object demo using the following command: </p> <pre><code>roslaunch stretch_demos handover_object.launch\n</code></pre> <p>For this demonstration, the robot will pan its head back and forth looking for a face. It will remember the 3D location of the mouth of the nearest face that it has detected. If you press \"y\" or \"Y\" on the keyboard in the terminal, the robot will move the grasp region of its gripper toward a handover location below and away from the mouth. </p> <p>The robot will restrict itself to Cartesian motion to do this. Specifically, it will move its mobile base backward and forward, its lift up and down, and its arm in and out. If you press \"y\" or \"Y\" again, it will retract its arm and then move to the most recent mouth location it has detected. </p> <p>At any time, you can also use the keyboard teleoperation commands in the terminal window. With this, you can adjust the gripper, including pointing it straight out and making it grasp an object to be handed over.</p>"},{"location":"stretch-ros/stretch_demos/#grasp-object-demo","title":"Grasp Object Demo","text":"<p>For this demonstration, the robot will look for the nearest elevated surface, look for an object on it, and then attempt to grasp the largest object using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. </p> <p>Once the robot is in position, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. </p> <p>Now that the robot is ready, launch the demo with the following command:</p> <pre><code>roslaunch stretch_demos grasp_object.launch\n</code></pre> <p>Then, press the key with \" (quotes) on it while in the terminal to initiate a grasp attempt.</p> <p>While attempting the grasp the demo will save several images under the ./stretch_user/debug/ directory within various grasping related directories. You can view these images to see some of what the robot did to make its decisions.</p>"},{"location":"stretch-ros/stretch_demos/#clean-surface-demo","title":"Clean Surface Demo","text":"<p>For this demonstration, the robot will look for the nearest elevated surface, look for clear space on it, and then attempt to wipe the clear space using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. </p> <p>You should also place a soft cloth in the robot's gripper.</p> <p>Once the robot is in position with a cloth in its gripper, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. </p> <p>Now that the robot is ready, launch the demo with the following command:</p> <pre><code>roslaunch stretch_demos clean_surface.launch\n</code></pre> <p>Then, press the key with the / and ? on it while in the terminal to initiate a surface cleaning attempt.</p>"},{"location":"stretch-ros/stretch_demos/#license","title":"License","text":"<p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros/stretch_demos/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros/stretch_description/","title":"stretch_description","text":""},{"location":"stretch-ros/stretch_description/#overview","title":"Overview","text":"<p>stretch_description provides materials for a URDF kinematic model of the Stretch RE1 mobile manipulator from Hello Robot Inc.</p>"},{"location":"stretch-ros/stretch_description/#details","title":"Details","text":"<p>The meshes directory contains STL mesh files representing the exterior geometry of various parts of the robot. </p> <p>The urdf directory contains xacro files representing various parts of the robot that are used to generate the robot's URDF. </p> <p>stretch_ros expects a URDF with the name stretch.urdf to reside within the urdf directory. The file stretch.urdf serves as the URDF for the robot and must be generated. Typically, it is a calibrated urdf file for the particular Stretch RE1 robot being used. To generate this file, please read the documentation within stretch_ros/stretch_calibration. </p> <p>The xacro_to_urdf.sh will usually only be indirectly run as part of various scripts and launch files within stretch_ros/stretch_calibration. </p> <p>Sometimes a stretch_uncalibrated.urdf file will reside with the urdf directory. This file is typically generated directly from the xacro files without any alterations. </p>"},{"location":"stretch-ros/stretch_description/#exporting-a-urdf","title":"Exporting a URDF","text":"<p>Sometimes a URDF is useful outside of ROS, such as for simulations and analysis. Running the export_urdf.sh script in the urdf directory will export a full URDF model of the robot based on stretch.urdf. </p> <p>The exported URDF will be found within an exported_urdf directory. It is also copied to a directory for your specific robot found under ~/stretch_user. The exported URDF includes meshes and controller calibration YAML files. The exported URDF can be visualized using stretch_urdf_show.py, which is part of the stretch_body Python code. </p>"},{"location":"stretch-ros/stretch_description/#changing-the-tool","title":"Changing the Tool","text":"<p>If you wish to remove the default gripper and add a different tool, you will typically edit /stretch_description/urdf/stretch_description.xacro. Specifically, you will replace the following line in order to include the xacro for the new tool and then follow directions within stretch_ros/stretch_calibration to generate a new calibrated urdf file (stretch.urdf) that includes the new tool.</p> <p><code>&lt;xacro:include filename=\"stretch_gripper.xacro\" /&gt;</code></p> <p>As an example we provide the xacro <code>stretch_dry_erase_marker.xacro</code> and its dependent mesh files with stretch_ros. </p> <p>Some of the tools found in the Stretch Body Tool Share include URDF data. To integrate these tools into the URDF for your Stretch</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_tool_share\n&gt;&gt;$ cd stretch_tool_share/&lt;tool name&gt;\n&gt;&gt;$ cp stretch_description/urdf/* ~/catkin_ws/src/stretch_ros/stretch_description/urdf/\n&gt;&gt;$ cp stretch_description/meshes/* ~/catkin_ws/src/stretch_ros/stretch_description/meshes/\n</code></pre> <p>Next add the xacro for the particular tool to <code>/stretch_description/urdf/stretch_description.xacro</code>. Then you can generate and preview the uncalibrated URDF:</p> <pre><code>&gt;&gt;$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf\n&gt;&gt;$ cp stretch.urdf stretch.urdf.bak\n&gt;&gt;$ rosrun stretch_calibration update_urdf_after_xacro_change.sh\n</code></pre> <p>Now visualize the new tool</p> <pre><code>&gt;&gt;$ roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre>"},{"location":"stretch-ros/stretch_description/#license-and-patents","title":"License and Patents","text":"<p>Patents are pending that cover aspects of the Stretch RE1 mobile manipulator.</p> <p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros/stretch_description/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"). The Contents consist of software and data used with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>The Clear BSD License</p> <p>Copyright (c) 2021 Hello Robot Inc. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met:</p> <pre><code> * Redistributions of source code must retain the above copyright notice,\n this list of conditions and the following disclaimer.\n\n * Redistributions in binary form must reproduce the above copyright\n notice, this list of conditions and the following disclaimer in the\n documentation and/or other materials provided with the distribution.\n\n * Neither the name of the copyright holder nor the names of its\n contributors may be used to endorse or promote products derived from this\n software without specific prior written permission.\n</code></pre> <p>NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"stretch-ros/stretch_description/urdf/export_urdf_license_template/","title":"Export urdf license template","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\") except where otherwise noted. The Contents consist of software and data used with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike-4.0-International (CC BY-NC-SA 4.0) license (the \"License\"); you may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>https://creativecommons.org/licenses/by-nc-sa/4.0/</p> <p>Unless required by applicable law or agreed to in writing, the Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Patents pending and trademark rights cover the Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\"</p> <p>The Contents may incorporate some parts of the \"RealSense Camera description package for Intel 3D D400 cameras\" released with an Apache 2.0 license and Copyright 2017 Intel Corporation. The details of the Apache 2.0 license can be found via the following link:  </p> <p>https://www.apache.org/licenses/LICENSE-2.0</p> <p>Specifically, the Contents may include the d435.dae mesh file and content generated by the _d435.urdf.xacro found within the GitHub repository available for download via the following link as of May 4, 2020:</p> <p>https://github.com/IntelRealSense/realsense-ros/tree/development/realsense2_description</p> <p>These specific materials are subject to the requirements of their original Apache 2.0 license.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros/stretch_funmap/","title":"stretch_funmap","text":""},{"location":"stretch-ros/stretch_funmap/#overview","title":"Overview","text":"<p>stretch_funmap is an implementation of Fast Unified Navigation, Manipulation And Planning (FUNMAP). FUNMAP provides navigation, manipulation, and planning capabilities for the Stretch RE1 mobile manipulator. stretch_funmap includes examples of efficient ways to take advantage of the Stretch RE1's unique properties. </p> <p>Previous commercially-available mobile manipulators have consisted of a serial manipulator (i.e., links connected by rotary joints) placed on a mobile base [1]. Widely used software (e.g., the Robot Operating System (ROS)) [1] typically expects a velocity-controlled mobile base that can be held still while the arm manipulates [3, 4]. </p> <p>In contrast, the Stretch RE1's mobile base is integral to manipulation and typically moves throughout a task. It can also perform high-fidelity position control with its mobile base. FUNMAP uses approximate geometric models and computer-vision algorithms to efficiently find plans that take advantage of its prismatic joints (e.g., telescoping arm) and Cartesian structure. In contrast to typical approaches that treat navigation (e.g., ROS Navigation Stack ) and manipulation (e.g., MoveIt! [5, 6]) separately, FUNMAP does both. </p>"},{"location":"stretch-ros/stretch_funmap/#getting-started-demo","title":"Getting Started Demo","text":"<p>First, make sure that your Stretch RE1 has clearance to rotate in place and will rotate without straining any cables connected to the trunk. Ideally, you should have the robot untethered.</p> <p>Next, run the following launch file:</p> <pre><code>roslaunch stretch_funmap mapping.launch\n</code></pre> <p>Now, you will take a head scan, which will involve the head panning around, the base rotating, and the head panning around again to overcome the blindspot due to the mast.</p> <pre><code>While in the terminal in which you ran roslaunch, press the space bar to initiate the head scan.\n</code></pre> <p>At this point, you should see a 3D map resulting from the head scan in RViz. You can rotate it around and look at it. It has been created by merging many 3D scans.</p> <p>If you have the robot untethered, you can now specify a navigation goal for the robot. If the robot finds a navigation plan to the goal, it will attempt to navigate to it. While navigating, it will look down with its 3D camera in an attempt to stop if it detects an obstacle.</p> <p><pre><code>In RViz, press the \"2D Nav Goal\" button on the top bar with a magenta arrow icon.\nSpecify a nearby navigation goal pose on the floor of the map by clicking and drawing a magenta arrow.\n</code></pre> For this to work, the navigation goal must be in a place that the robot can reach and that the robot has scanned well. For example, the robot will only navigate across floor regions that it has in its map.</p> <p>If the robot finds a path, you should see green lines connecting white spheres in RViz that display its plan as it attempts to navigate to the goal.</p> <p>Once the robot has reached the goal, you can take another head scan.</p> <pre><code>While in the terminal, press the space bar to initiate another head scan.\n</code></pre> <p>The robot should take the head scan and merge it with the previous scans. If all goes well, the merged 3D map will be visible in RViz.</p> <p>You can also have the robot automatically drive to a place that it thinks is a good place for it to take a head scan in order to map the environment.</p> <pre><code>While in the terminal, press the key with \\ and | on it. \nThis should work even if caps lock is enabled or the shift key is pressed. \n</code></pre> <p>If the robot reached it's goal, then you can now take another head scan. </p> <pre><code>While in the terminal, press the space bar to initiate another head scan.\n</code></pre> <p>By repeating this process, you can create a 3D map of the environment. FUNMAP uses images to represent the environment with each pixel value representing the highest observed 3D point at a planar location. By default, all of the merged maps are saved to the following directory:</p> <pre><code>./stretch_user/debug/merged_maps/\n</code></pre> <p>You can see the image representations by looking at files with the following naming pattern: </p> <pre><code>./stretch_user/debug/merged_maps/merged_map_DATETIME_mhi_visualization.png\n</code></pre> <p>You can also click on a reaching goal for the Stretch RE1 by clicking on \"Publish Point\" in Rviz and then selecting a 3D point on the map. FUNMAP will attempt to generate a navigation and manipulation plan to reach close to the selected 3D location with Stretch's gripper.</p> <pre><code>In RViz, select a reach goal by clicking on \"Publish Point\" on the top bar with a red map location icon). \nThen, click on a 3D location on the map to specify a reaching target for the robot's gripper.\n</code></pre> <p>That concludes the demonstration. Have fun with FUNMAP!</p>"},{"location":"stretch-ros/stretch_funmap/#more-funmap","title":"More FUNMAP","text":"<p>FUNMAP represents human environments with Max Height Images (MHIs). An MHI is an image for which each pixel typically represents the height of the robot\u2019s environment. Given a volume of interest (VOI) with its z-axis aligned with gravity, an MHI, I, maps locations to heights. Specifically, I(x,y)=z, where (x,y) represents a discretized planar location within the VOI and z represents the discretized height of the maximum occupied voxel within the VOI at that planar location (see Figure 1 above). </p> <p>The placement of the Stretch RE1 3D camera at a human head height enables it to capture MHIs that represent the horizontal surfaces with which humans frequently interact, such as table tops, countertops, and chairs. The orientation of its 3D camera enables the robot to quickly scan an environment to create a room-scale MHI by panning its head at a constant tilt angle. </p> <p>In contrast to other environment representations, such as point clouds and 3D mesh models, MHIs support fast, efficient operations through optimized image processing and are compatible with deep learning methods for images. Related representations have primarily been used for navigation, including for legged robots on rough terrain, but have not emphasized elevated surfaces in human environments nor incorporated manipulation [7, 8]. Object grasping systems for bin picking have used related representations, but have only considered small areas with objects and not incorporated navigation [9, 10]. </p> <p>During development, we have used FUNMAP to create MHIs for which each pixel represents a 6mm x 6mm region of the environment\u2019s floor and has a height resolution of less than 6mm per unit. This allows a single 2000 x 2000 pixel, 8-bit image to represent a 12m x 12m environment from 10cm below the estimated floor plane to 1.2m above the floor plane, which captures the great majority of open horizontal surfaces in human environments with which people interact. MHIs also have the potential to represent enclosed surfaces (e.g., surfaces in shelves and cabinets) and vertical surfaces (e.g., doors) by defining new VOIs with different orientations and heights, which is a capability that merits future exploration.</p> <p>For navigation and planning, FUNMAP uses fast distance transforms and morphological operators to efficiently create cost functions for robust optimization-based planning. For example, a high value of a distance transform at a floor location implies that the mobile base will be farther from obstacles. Similarly, a high value of a distance transform for a location on an elevated surface implies that the robot\u2019s end effector will be farther from obstacles. </p> <p> </p> <p>Figure 3: Left: The cyan lines represent achievable driving goals on the floor from which the robot can reach the target (red circle) on the table (dark blue). Right: When the target (red circle) is farther back near obstacles on the table (dark blue), the robot can reach the target from fewer locations (cyan lines). </p> <p> </p> <p>Figure 4: Left: Example of a piecewise linear navigation plan (green line segments and white spheres) being executed. Right: Example of a successfully executed plan to reach a target location on a table (light blue) while avoiding a wall with shelves (dark blue and purple).</p> <p>The Stretch RE1\u2019s slender links and Cartesian kinematics support rapid optimization of plans due to the simplified geometry of the robot\u2019s motions. FUNMAP uses piecewise linear paths on the cost image for fast navigation and manipulation planning. </p> <p>FUNMAP enables the Stretch RE1 to reach a 3D target position. FUNMAP models the linear motion of the telescoping arm as it extends to reach a target as a line in a manipulation cost image (see Figure 2). FUNMAP uses a navigation cost image and Djikstra\u2019s algorithm with a priority queue to efficiently estimate the cost of navigating to all floor locations (see Figure 3 left). FUNMAP then combines these results to find a minimum cost plan that increases the distance to obstacles and the manipulable workspace of the robot (see Figure 3 right). </p>"},{"location":"stretch-ros/stretch_funmap/#references","title":"References","text":"<p>[1] Bostelman, Roger, Tsai Hong, and Jeremy Marvel. \"Survey of research for performance measurement of mobile manipulators.\" Journal of Research of the National Institute of Standards and Technology 121, no. 3 (2016): 342-366.</p> <p>[2] Quigley, Morgan, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote, Jeremy Leibs, Rob Wheeler, and Andrew Y. Ng. \"ROS: an open-source Robot Operating System.\" In ICRA workshop on open source software, vol. 3, no. 3.2, p. 5. 2009.</p> <p>[3] Sachin Chitta, Eitan Marder-Eppstein, Wim Meeussen, Vijay Pradeep, Adolfo Rodr\u00edguez Tsouroukdissian, et al.. ros_control: A generic and simple control framework for ROS. The Journal of Open Source Software, 2017, 2 (20), pp.456 - 456.</p> <p>[4] Guimar\u00e3es, Rodrigo Longhi, Andr\u00e9 Schneider de Oliveira, Jo\u00e3o Alberto Fabro, Thiago Becker, and Vin\u00edcius Amilgar Brenner. \"ROS navigation: Concepts and tutorial.\" In Robot Operating System (ROS), pp. 121-160. Springer, Cham, 2016.</p> <p>[5] Chitta, Sachin, Ioan Sucan, and Steve Cousins. \"Moveit! [ros topics].\" IEEE Robotics &amp; Automation Magazine 19, no. 1 (2012): 18-19.</p> <p>[6] Chitta, Sachin. \"MoveIt!: an introduction.\" In Robot Operating System (ROS), pp. 3-27. Springer, Cham, 2016.</p> <p>[7] Fankhauser, P\u00e9ter, and Marco Hutter. \"A universal grid map library: Implementation and use case for rough terrain navigation.\" In Robot Operating System (ROS), pp. 99-120. Springer, Cham, 2016.</p> <p>[8] Lu, David V., Dave Hershberger, and William D. Smart. \"Layered costmaps for context-sensitive navigation.\" In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 709-715. IEEE, 2014.</p> <p>[9] Zeng, Andy, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. \"Tossingbot: Learning to throw arbitrary objects with residual physics.\" arXiv preprint arXiv:1903.11239 (2019).</p> <p>[10] Zeng, Andy, Shuran Song, Kuan-Ting Yu, Elliott Donlon, Francois R. Hogan, Maria Bauza, Daolin Ma et al. \"Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching.\" In 2018 IEEE international conference on robotics and automation (ICRA), pp. 1-8. IEEE, 2018.</p>"},{"location":"stretch-ros/stretch_funmap/#license","title":"License","text":"<p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros/stretch_funmap/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>This software is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License v3.0 (GNU LGPLv3) as published by the Free Software Foundation.</p> <p>This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License v3.0 (GNU LGPLv3) for more details, which can be found via the following link: </p> <p>https://www.gnu.org/licenses/lgpl-3.0.en.html</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros/stretch_gazebo/","title":"stretch_gazebo","text":""},{"location":"stretch-ros/stretch_gazebo/#overview","title":"Overview","text":"<p>stretch_gazebo is an implementation of simulating a Stretch robot with Gazebo simulator.</p>"},{"location":"stretch-ros/stretch_gazebo/#details","title":"Details","text":"<p>The urdf directory contains a xacro file that extends the capabilities of the original xacro files living in stretch_description package to include Gazebo functionality.</p> <p>The config directory contains rviz files and ros_control controller configuration files for various parts of the robot including:</p> <ul> <li>Base: diff_drive_controller/DiffDriveController</li> <li>Arm: position_controllers/JointTrajectoryController</li> <li>Gripper: position_controllers/JointTrajectoryController</li> <li>Head: position_controllers/JointTrajectoryController</li> <li>Joints: joint_state_controller/JointStateController</li> </ul> <p>The launch directory includes two files:</p> <ul> <li>gazebo.launch: Opens up an empty Gazebo world and spawns the robot loading all the controllers, including all the sensors except Cliff sensors and respeaker.</li> <li>teleop_keyboard.launch: Allows keyboard teleop in the terminal and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel, which the robot is taking velocity commands from.</li> <li>teleop_joy.launch: Spawns a joy and teleop_twist_joy instance and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel, which the robot is taking velocity commands from. Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless it is being pressed. For an Logitech F310 joystick this button is A.</li> </ul> <p>The script directory contains a single python file that publishes ground truth odometry of the robot from Gazebo.</p>"},{"location":"stretch-ros/stretch_gazebo/#setup","title":"Setup","text":"<p>These set up instructions will not be required on newly shipped robots. Follow these instructions if stretch_gazebo is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros and realsense_gazebo_plugin packages to your catkin workspace. Then install dependencies and build the packages, with the following set of commands:</p> <pre><code>mkdir -p ~/catkin_ws/src\ncd ~/catkin_ws/src\ngit clone https://github.com/hello-robot/stretch_ros\ngit clone https://github.com/pal-robotics/realsense_gazebo_plugin\ncd ~/catkin_ws\nrosdep install --from-paths src --ignore-src -r -y\ncatkin_make\n</code></pre> <p>In order to use the built packages, make the packages discoverable by sourcing the ROS workspace: <code>source ~/catkin_ws/devel/setup.bash</code>. It is popular to add the sourcing command to your <code>~/.bashrc</code> file, so that the ROS packages are discoverable in every new terminal that is opened.</p>"},{"location":"stretch-ros/stretch_gazebo/#running-demo","title":"Running Demo","text":"<pre><code># Terminal 1:\nroslaunch stretch_gazebo gazebo.launch rviz:=true\n# Terminal 2:\nroslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=keyboard # or use teleop_type:=joystick if you have a controller\n</code></pre> <p>This will launch an Rviz instance that visualizes the sensors and an empty world in Gazebo with Stretch and load all the controllers. Although, the base will be able to move with the joystick commands, the joystick won't give joint commands to arm, head or gripper. To move these joints see the next section about Running Gazebo with MoveIt! and Stretch.</p> <p></p>"},{"location":"stretch-ros/stretch_gazebo/#running-gazebo-with-moveit-and-stretch","title":"Running Gazebo with MoveIt! and Stretch","text":"<pre><code># Terminal 1:\nroslaunch stretch_gazebo gazebo.launch\n# Terminal 2:\nroslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=keyboard # or use teleop_type:=joystick if you have a controller\n# Terminal 3\nroslaunch stretch_moveit_config demo_gazebo.launch\n</code></pre> <p>This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to this moveit tutorial. A few notes to be kept in mind:</p> <ul> <li>Planning group can be changed via Planning Group drop down in Planning tab of Motion Planning Rviz plugin.</li> <li>Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning Rviz plugin.</li> <li>stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin.</li> <li>When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning Rviz plugin.</li> </ul> <p></p>"},{"location":"stretch-ros/stretch_gazebo/#differences-in-gazebo-vs-stretch","title":"Differences in Gazebo vs Stretch","text":"<p>The simulated Stretch RE1 differs from the robot in the following ways.</p>"},{"location":"stretch-ros/stretch_gazebo/#gazebo-sensors-vs-stretch-sensors","title":"Gazebo Sensors vs Stretch Sensors","text":"Sensor Gazebo Stretch Notes LIDAR :heavy_check_mark: :heavy_check_mark: Base IMU :heavy_check_mark: :heavy_check_mark: Wrist Accelerometer :heavy_check_mark: :heavy_check_mark: Modeled as an IMU Realsense D435i :heavy_check_mark: :heavy_check_mark: Respeaker (Mic Array) :x: :heavy_check_mark: Cliff Sensors :x: :heavy_check_mark: <p>Notes: Although there is no microphone in Gazebo, Respeaker can be represented with a ROS node that accesses compputer's microphone. Cliff sensors are not modeled but they can also be represented as 1D LIDAR sensors. See LIDAR definition in stretch_gazebo.urdf.xacro file.</p>"},{"location":"stretch-ros/stretch_gazebo/#moveit-controllers-vs-stretch_core","title":"MoveIt Controllers vs stretch_core","text":"<p>Actuators are defined as ros_control transmission objects in Gazebo using PositionJointInterfaces. MoveIt is configured to use three different action servers to control the body parts of stretch in Gazebo through the srdf file in stretch_moveit_config package. See the section above about MoveIt for details. Please note that this behavior is different than stretch_core as it works with a single Python interface to control all the joints.</p>"},{"location":"stretch-ros/stretch_gazebo/#uncalibrated-xacro-vs-calibrated-urdf","title":"Uncalibrated XACRO vs Calibrated URDF","text":"<p>We provide stretch_calibration to generate a calibrated URDF that is unique to each robot. The calibrated URDF is generated from the nominal description of Stretch RE1, the xacro files that live in stretch_description. The simulated Stretch RE1 is generated from the gazebo xacro description in the urdf directory and is not calibrated.</p>"},{"location":"stretch-ros/stretch_gazebo/#license","title":"License","text":"<p>For license information, please see the LICENSE files.</p>"},{"location":"stretch-ros/stretch_gazebo/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros/stretch_navigation/","title":"stretch_navigation","text":""},{"location":"stretch-ros/stretch_navigation/#overview","title":"Overview","text":"<p>stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive the stretch RE1 around a mapped space. Running this code will require the robot to be untethered.</p>"},{"location":"stretch-ros/stretch_navigation/#setup","title":"Setup","text":"<p>These set up instructions are already performed on Stretch RE1 robots. Follow these instructions if stretch_navigation is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros to your catkin workspace. Then install dependencies and build the packages, with the following set of commands:</p> <pre><code>mkdir -p ~/catkin_ws/src\ncd ~/catkin_ws/src\ngit clone https://github.com/hello-robot/stretch_ros\ncd ~/catkin_ws\nrosdep install --from-paths src --ignore-src -r -y\ncatkin_make\n</code></pre>"},{"location":"stretch-ros/stretch_navigation/#quickstart","title":"Quickstart","text":"<p>The first step is to map the space that the robot will navigate in. The <code>mapping.launch</code> will enable you to do this. First run:</p> <pre><code>roslaunch stretch_navigation mapping.launch\n</code></pre> <p>Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to <code>stretch_user/</code>.</p> <pre><code>mkdir -p ~/stretch_user/maps\nrosrun map_server map_saver -f ${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;\n</code></pre> <p>The <code>&lt;map_name&gt;</code> does not include an extension. Map_saver will save two files as <code>&lt;map_name&gt;.pgm</code> and <code>&lt;map_name&gt;.yaml</code>.</p> <p>Next, with <code>&lt;map_name&gt;.yaml</code>, we can navigate the robot around the mapped space. Run:</p> <pre><code>roslaunch stretch_navigation navigation.launch map_yaml:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p>Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location in the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place.</p> <p>It is also possible to send 2D Pose Estimates and Nav Goals programatically. In your own launch file, you may include <code>navigation.launch</code> to bring up the navigation stack. Then, you can send <code>move_base_msgs::MoveBaseGoal</code> messages in order to navigate the robot programatically.</p>"},{"location":"stretch-ros/stretch_navigation/#running-in-simulation","title":"Running in Simulation","text":"<p>To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the <code>mapping_gazebo.launch</code> and <code>navigation_gazebo.launch</code> launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch.</p> <pre><code>roslaunch stretch_navigation mapping_gazebo.launch gazebo_world:=worlds/willowgarage.world\n</code></pre>"},{"location":"stretch-ros/stretch_navigation/#teleop-using-a-joystick-controller","title":"Teleop using a Joystick Controller","text":"<p>The mapping launch files, <code>mapping.launch</code> and <code>mapping_gazebo.launch</code> expose the ROS argument, \"teleop_type\". By default, this ROS arg is set to \"keyboard\", which launches keyboard teleop in the terminal. If the xbox controller that ships with Stretch RE1 is plugged into your computer, the following command will launch mapping with joystick teleop:</p> <pre><code>roslaunch stretch_navigation mapping.launch teleop_type:=joystick\n</code></pre>"},{"location":"stretch-ros/stretch_navigation/#using-ros-remote-master","title":"Using ROS Remote Master","text":"<p>If you have set up ROS Remote Master for untethered operation, you can use Rviz and teleop locally with the following commands:</p> <pre><code># On Robot\nroslaunch stretch_navigation mapping.launch rviz:=false teleop_type:=none\n\n# On your machine, Terminal 1:\nrviz -d `rospack find stretch_navigation`/rviz/mapping.launch\n# On your machine, Terminal 2:\nroslaunch stretch_core teleop_twist.launch teleop_type:=keyboard # or use teleop_type:=joystick if you have a controller\n</code></pre>"},{"location":"stretch-ros/stretch_navigation/#license","title":"License","text":"<p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros/stretch_navigation/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros2/","title":"Overview","text":""},{"location":"stretch-ros2/#ros-2-galactic-development-branch","title":"ROS 2 Galactic Development Branch","text":"<p>This is a development branch that we are using to port stretch_ros to ROS 2 Galactic, Python 3, and Ubuntu 20.04. We plan to begin shipping this version preinstalled on Stretch RE1 robots in the future. It is not in a usable state. It is also unstable, since we are actively conducting development in this branch. Since we have performed limited testing, you may encounter unexpected behaviors. Also, installation requires Ubuntu 20.04 on a second partition of your robot's hard drive.</p> <p>We are beginning to use this port internally at Hello Robot to test it, improve it, and add new capabilities.</p>"},{"location":"stretch-ros2/#available-support-in-ros-2","title":"Available support in ROS 2","text":"<ul> <li>Stretch Core (Partial)<ul> <li>Stretch driver</li> <li>RPLidar driver</li> <li>D435i driver</li> <li>Aruco Detection</li> <li>Keyboard teleop (known bugs)</li> </ul> </li> <li>Hello Helpers</li> <li>Stretch Calibration</li> <li>Stretch Description</li> <li>Stretch MoveIt2<ul> <li>Joint Goals not involving base</li> <li>Pose Goals not involving base</li> <li>MoveGroup C++ API</li> <li>Python API (work in progress)</li> </ul> </li> <li>Stretch Nav2<ul> <li>Mapping with slam_toolbox</li> <li>Navigation with nav2</li> <li>Simple Commander Python API</li> </ul> </li> <li>Stretch Deep Perception<ul> <li>Object Detection with YOLOv5 using PyTorch</li> <li>Head Pose Estimation with OpenVINO</li> </ul> </li> </ul>"},{"location":"stretch-ros2/#known-issues","title":"Known Issues","text":"<p>No support for:  - Stretch Core      - ReSpeaker driver     - Dex Wrist  - Stretch Dashboard  - Stretch Deep Perception  - Stretch Demos  - Stretch FUNMAP  - Stretch Gazebo  - Stretch OctoMap  - Stretch RTABMap  - The deep perception demos won't work with a default installation, since they require OpenCV compiled with OpenVINO.  - There is no support for the Respeaker Microphone Array.  - There is no support for the Dexterous Wrist.</p>"},{"location":"stretch-ros2/#directories","title":"Directories","text":"<p>The stretch_ros2 repository holds ROS 2 related code for the Stretch mobile manipulator from Hello Robot Inc.</p> Resource Description hello_helpers Miscellaneous helper code used across the stretch_ros repository stretch_calibration Creates and updates calibrated URDFs for the Stretch mobile manipulator stretch_core ROS 2 drivers for Stretch stretch_deep_perception Demonstrations that use open deep learning models to perceive the world stretch_demos Demonstrations of simple autonomous manipulation stretch_description Generate and export URDFs stretch_funmap Demonstrations of Fast Unified Navigation, Manipulation And Planning (FUNMAP) stretch_gazebo Support for simulation of Stretch in the Gazebo simulator stretch_moveit_config Config files to use Stretch with the MoveIt2 Motion Planning Framework stretch_navigation Support for the ROS navigation stack Nav2, including slam_toolbox, AMCL and Simple Commander stretch_octomap Support for mapping using OctoMap: efficient probabilistic 3D Mapping based on Octrees stretch_rtabmap Support for mapping using Real-Time Appearance-Based Mapping (RTAB-Map)"},{"location":"stretch-ros2/#licenses","title":"Licenses","text":"<p>This software is intended for use with the Stretch mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc.</p> <p>For license details for this repository, see the LICENSE files found in the directories. A summary of the licenses follows: </p> Directory License hello_helpers Apache 2.0 stretch_calibration GPLv3 stretch_core Apache 2.0 stretch_deep_perception Apache 2.0 stretch_demos Apache 2.0 stretch_description BSD 3-Clause Clear License stretch_funmap LGPLv3 stretch_gazebo Apache 2.0 stretch_moveit_config Apache 2.0 stretch_navigation Apache 2.0 stretch_octomap Apache 2.0 stretch_rtabmap Apache 2.0"},{"location":"stretch-ros2/hello_helpers/","title":"hello_helpers","text":""},{"location":"stretch-ros2/hello_helpers/#overview","title":"Overview","text":"<p>hello_helpers mostly consists of the hello_helpers Python module. This module provides various Python files used across stretch_ros2 that have not attained sufficient status to stand on their own.</p>"},{"location":"stretch-ros2/hello_helpers/#ported-to-ros-2","title":"Ported to ROS 2","text":"<ul> <li><code>fit_plane.py</code>: Fits planes to 3D data</li> <li><code>gripper_conversion.py</code>: Used for converting measurements for the gripper</li> <li><code>hello_misc.py</code>: Various functions, including a helpful Python object with which to create ROS nodes (Partially)</li> <li><code>hello_ros_viz.py</code>: Various helper functions for visualizations using RViz</li> </ul>"},{"location":"stretch-ros2/hello_helpers/#typical-usage","title":"Typical Usage","text":"<p><pre><code>import hello_helpers.fit_plane as fp\n</code></pre> <pre><code>import hello_helpers.hello_misc as hm\n</code></pre> <pre><code>import hello_helpers.hello_ros_viz as hr\n</code></pre></p>"},{"location":"stretch-ros2/hello_helpers/#license","title":"License","text":"<p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros2/hello_helpers/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros2/stretch_calibration/","title":"stretch_calibration","text":""},{"location":"stretch-ros2/stretch_calibration/#overview","title":"Overview","text":"<p>stretch_calibration provides tools for calibrating and managing the URDF for the Stretch robot. The code's primary role is to generate a geometric model (i.e., a URDF) of the robot's body that corresponds well with views of the body from the robot's 3D camera (i.e., a Intel RealSense D435i). The code achieves this objective by adjusting the geometry of the model to predict where the 3D camera will see markers (i.e., ArUco markers) on the robot's body. </p> <p>Hello Robot Inc. uses this code to calibrate each robot prior to shipping. Users may wish to recalibrate their robots to compensate for changes over time or take advantage of improvements to the calibration code. </p> <p>In addition, after changing a tool, this code can be used to generate a new calibrated URDF that incorporates the tool without performing a new calibration optimization. </p>"},{"location":"stretch-ros2/stretch_calibration/#checking-the-current-calibration-with-new-observations","title":"Checking the Current Calibration with New Observations","text":"<ol> <li>Make sure the basic joint limit calibration has been performed.</li> </ol> <p><code>stretch_robot_home.py</code></p> <ol> <li>Make sure the uncalibrated URDF is up to date.</li> </ol> <p><code>ros2 run stretch_calibration update_uncalibrated_urdf</code></p> <ol> <li>Collect new observations</li> </ol> <p><code>ros2 launch stretch_calibration collect_check_head_calibration_data.launch.py</code></p> <ol> <li>Test how well the current calibrated model fits the new observations</li> </ol> <p><code>ros2 run stretch_calibration check_head_calibration</code></p> <ul> <li> <p>The total_error printed on the command line should be less than 0.05. If it is not, an error will be printed on the command line. </p> </li> <li> <p>In RViz the white markers represent the locations for the ArUco markers predicted by the calibrated URDF. The colored markers represent the observed locations of the ArUco markers on the robot's body. For a good fit, the white markers will be close to the colored markers. </p> </li> </ul> <p></p>"},{"location":"stretch-ros2/stretch_calibration/#visually-inspecting-the-current-calibration","title":"Visually Inspecting the Current Calibration","text":"<p>The following command will allow you to visually inspect a calibration with Rviz. You can use RViz to see how well the robot's 3D body model matches point clouds from the 3D camera. While visualizing the 3D model and point clouds in RViz, you can use keyboard commands in the terminal to move the head around, the lift up and down, and the arm in and out. The keyboard commands will be printed in the terminal.</p> <p>A good calibration should result in a close correspondence between the robot's 3D body model and the point cloud throughout the ranges of motion for the head, lift, and arm. You may notice higher error when the head is looking upward due to challenges associated with head tilt backlash. You might also notice higher error when the arm is fully extended, since small angular errors can result in larger positional errors at the robot's wrist.</p> <ol> <li>Test the current head calibration</li> </ol> <p><code>ros2 launch stretch_calibration simple_test_head_calibration.launch.py</code></p>"},{"location":"stretch-ros2/stretch_calibration/#examples-of-good-and-bad-visual-fit","title":"Examples of Good and Bad Visual Fit","text":"<p>In the images below, examples of good and bad fit between the point cloud and the geometric model are presented side by side. To make the distinction clear, the images have green and red circles indicating where the fit is either good or bad.</p> <p></p> <p></p>"},{"location":"stretch-ros2/stretch_calibration/#calibrate-stretch","title":"Calibrate Stretch","text":"<ol> <li>Make sure the basic joint limit calibration has been performed.</li> </ol> <p><code>stretch_robot_home.py</code></p> <ol> <li>Make sure the uncalibrated URDF is up to date.</li> </ol> <p><code>ros2 run stretch_calibration update_uncalibrated_urdf</code></p> <ol> <li>Collect head calibration data</li> <li> <p>Put the robot on a flat surface. Give it room to move its arm and good lighting. Then, have the robot collect data using the command below. While the robot is collecting data, do not block its view of its markers. </p> <p><code>ros2 launch stretch_calibration collect_head_calibration_data.launch.py</code></p> </li> <li> <p>Process head calibration data</p> </li> <li> <p>Specify how much data to use and the quality of the fit</p> <ul> <li> <p>YAML file with parameters: stretch_ros/stretch_calibration/config/head_calibration_options.yaml</p> </li> <li> <p>More data and higher quality fitting result in optimizations that take longer</p> </li> <li> <p>When quickly testing things out</p> </li> <li>~3 minutes without visualization</li> <li><code>data_to_use: use_very_little_data</code></li> <li> <p><code>fit_quality: fastest_lowest_quality</code></p> </li> <li> <p>When calibrating the robot </p> </li> <li>~1 hour without visualization</li> <li><code>data_to_use: use_all_data</code></li> <li><code>fit_quality: slow_high_quality</code></li> </ul> </li> <li> <p>Perform the optimization to fit the model to the collected data</p> <ul> <li>Without visualization (faster)  </li> </ul> <p><code>ros2 launch stretch_calibration process_head_calibration_data.launch.py</code></p> <ul> <li>With visualization (slower)</li> </ul> <p><code>ros2 launch stretch_calibration process_head_calibration_data_with_visualization.launch.py</code></p> </li> <li> <p>Inspect the fit of the most recent head calibration</p> </li> </ol> <p><code>ros2 run stretch_calibration visualize_most_recent_head_calibration</code></p> <ol> <li>Start using the newest head calibration</li> </ol> <p><code>ros2 run stretch_calibration update_with_most_recent_calibration</code></p> <ol> <li>Build the workspace and source environment variables</li> </ol> <p><code>cd ~/ament_ws</code></p> <p><code>colcon build</code></p> <p><code>source install/setup.bash</code></p> <ol> <li>Test the current head calibration</li> </ol> <p><code>ros2 launch stretch_calibration simple_test_head_calibration.launch.py</code></p> <p>Use RViz to visually inspect the calibrated model. The robot's 3D body model should look similar to the structure of your robot. You may refer to the section above to see examples of good and bad fit.</p>"},{"location":"stretch-ros2/stretch_calibration/#generate-a-new-urdf-after-changing-the-tool","title":"Generate a New URDF After Changing the Tool","text":"<p>If you change Stretch's tool attached to the wrist and want to generate a new URDF for it, you can do so with xacro files in the /stretch_ros/stretch_description/urdf/ directory. Specifically, you can edit stretch_description.xacro to include a xacro other than the default stretch_gripper.xacro. </p> <p>After changing the tool xacro you will need to generate a new URDF and also update this new URDF with the previously optimized calibration parameters. To do so, follow the directions below: </p> <ol> <li>In a terminal run</li> </ol> <p><code>ros2 run stretch_calibration update_urdf_after_xacro_change</code></p> <p>This will update the uncalibrated URDF with the current xacro files and then create a calibrated URDF using the most recent calibration parameters.</p>"},{"location":"stretch-ros2/stretch_calibration/#revert-to-a-previous-calibration","title":"Revert to a Previous Calibration","text":"<p>When a new calibration is performed, it is timestamped and added to the calibration directory under \"stretch_user/\". If you'd like to revert to a previous calibration, you may run the following command. It will move the most recent calibration files to a reversion directory and update the calibration in the stretch_description package from the remaining most recent calibration files.</p> <ol> <li>Revert to the previous head calibration</li> </ol> <p><code>ros2 run stretch_calibration revert_to_previous_calibration</code></p> <ol> <li>Build the workspace and source environment variables</li> </ol> <p><code>cd ~/ament_ws</code></p> <p><code>colcon build</code></p> <p><code>source install/setup.bash</code></p>"},{"location":"stretch-ros2/stretch_calibration/#license","title":"License","text":"<p>stretch_calibration is licensed with the GPLv3. Please see the LICENSE file for details.</p>"},{"location":"stretch-ros2/stretch_calibration/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation.</p> <p>The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link:</p> <p>https://www.gnu.org/licenses/gpl-3.0.html</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros2/stretch_core/","title":"stretch_core","text":""},{"location":"stretch-ros2/stretch_core/#overview","title":"Overview","text":"<p>stretch_core provides the core ROS interfaces to the Stretch mobile manipulator. It includes the following:</p> <p>Nodes:  - <code>d435i_*.py</code> : various nodes to help use Stretch's 3D camera  - <code>detect_aruco_markers.py</code>: node that detects and estimates the pose of ArUco markers, including the markers on the robot's body  - <code>joint_trajectory_server.py</code>: action server to execute trajectories generated using planners like MoveIt2  - <code>keyboard_*.py</code> : nodes that provides a keyboard interface to control the robot's joints  - <code>stretch_driver.py</code>: node that communicates with the low-level Python library (stretch_body) to interface with the Stretch RE1  - <code>trajectory_components.py</code>: node defining classes for each joint to generate trajectory waypoints</p> <p>Launch files:  - <code>d435i_*.launch.py</code>: launches the d435i driver node with defined resolution  - <code>keyboard_teleop.launch.py</code>: launches the node that allows teleoperating robot joints with keyboard  - <code>rplidar.launch.py</code>: launches the RPLidar driver node  - <code>stretch_aruco.launch.py</code>: launches the aruco detection node  - <code>stretch_driver.launch.py</code>: launches the stretch driver node with defined configurations</p> <p>Config:  - <code>controller_calibration_head.yaml</code>: stores the backlash errors in the head servos  - <code>laser_filter_params.yaml</code>: configures the filters that are used for laser scan filtering  - <code>stretch_marker_dict.yaml</code>: stores information about ArUco markers known and assigned for use with Stretch</p>"},{"location":"stretch-ros2/stretch_core/#api","title":"API","text":""},{"location":"stretch-ros2/stretch_core/#nodes","title":"Nodes","text":""},{"location":"stretch-ros2/stretch_core/#stretch_driver","title":"stretch_driver","text":""},{"location":"stretch-ros2/stretch_core/#parameters","title":"Parameters","text":""},{"location":"stretch-ros2/stretch_core/#broadcast_odom_tf","title":"broadcast_odom_tf","text":"<p>If set to true, stretch_driver will publish an odom to base_link TF.</p>"},{"location":"stretch-ros2/stretch_core/#fail_out_of_range_goal","title":"fail_out_of_range_goal","text":"<p>If set to true, motion action servers will fail on out-of-range commands.</p>"},{"location":"stretch-ros2/stretch_core/#mode","title":"mode","text":"<p>Can be set to <code>position</code>, <code>navigation</code>, and <code>trajectory</code> modes.</p>"},{"location":"stretch-ros2/stretch_core/#calibrated_controller_yaml_file","title":"calibrated_controller_yaml_file","text":"<p>Path to the calibrated controller args file</p>"},{"location":"stretch-ros2/stretch_core/#published-topics","title":"Published Topics","text":""},{"location":"stretch-ros2/stretch_core/#mode-std_msgsstring","title":"/mode (std_msgs/String)","text":"<p>This topic publishes which mode the driver is in at 15hz. stretch_driver has a few modes that change how the robot is controlled. The modes are:</p> <ul> <li>\"position\": The default and simplest mode. In this mode, you can control every joint on the robot using position commands. For example, the telescoping arm (whose range is from zero fully retracted to ~52 centimeters fully extended) would move to 25cm out when you send it a 0.25m position cmd . Two kinds of position commands are available for the mobile base, translation and rotation, with joint names \"translate_mobile_base\" and \"rotate_mobile_base\", and these commands have no limits since the wheels can spin continuously.</li> <li>Position commands are tracked in the firmware by a trapezoidal motion profile, and specifing the optional velocity and acceleration in JointTrajectoryPoint changes the shape of the trapezoid. </li> <li>Position commands can be contact sensitive, which is helpful for manipulating objects in the world. For example, I could open a cabinet by reaching out with the telescoping arm and detecting contact with the door. In order to specify contact thresholds for a position command, the optional effort in JointTrajectoryPoint is misused to mean a threshold for the effort the robot will apply while executing the position command.</li> <li>Position commands can be preemptable, so you can issue a new position command before the previous one has finished and the robot will smoothly move to execute the latest command. This feature is helpful for scripts that use visual servo-ing or allow a user to teleop the robot.</li> <li>The driver can be switched into \"position\" mode at any time using the switch_to_position_mode service.</li> <li>\"navigation\": In this mode, every joint behaves identically to \"position\" mode except for the mobile base. The mobile base responds to velocity commands at a topic instead of position commands via the \"translate_mobile_base\" and \"rotate_mobile_base\" joints in the action server. You would publish geometry_msgs/Twist messages to the /stretch/cmd_vel topic. Since Twist messages are generalized to robots that can move with velocity in any direction, only the <code>Twist.linear.x</code> (translational velocity) and <code>Twist.angular.z</code> (rotational velocity) fields apply for differential drive mobile bases.</li> <li>Velocity control of the base is a common way to move mobile robots around. For example, the Navigation Stack is a piece of software that uses this mode to move the robot to different points in a map.</li> <li>This mode has a few safety features to prevent Stretch from \"running away\" at the last commanded velocity if the node that was sending commands were to crash for whatever reason. The first is a 0.5 second timeout within the stretch_driver node, which means that if the driver doesn't receive a new Twist command within 0.5s, the base will be commanded to stop smoothly. The second is a 1 second timeout within the firmware of the wheels, which means that even if the ROS layer were to crash, the robot will still be commanded to stop abruptly within 1 second at the lowest layer. This low-level feature was added relatively recently to the firmware, so be sure to update your firmware to the latest version.</li> <li>The driver can be switched into \"navigation\" mode at any time using the switch_to_navigation_mode service.</li> <li>\"trajectory\": In this mode, every joint follows a trajectory that is modeled as a spline. The key benefit of this mode is control over the timing at which the robot achieves positions (a.k.a waypoints), enabling smooth and coordinated motion through a preplanned trajectory. More details here.</li> <li>\"homing\": This is the mode reported by the driver when a homing sequence has been triggered (e.g. through the home_the_robot service). While this mode is active, no other commands will be accepted by the driver. After the robot has completed its 30 second homing sequence, it will return to the mode it was in before.</li> <li>\"stowing\": This is the mode reported by the driver when a stowing sequence has been triggered (e.g. through the stow_the_robot service). While this mode is active, no other commands will be accepted by the driver. After the robot has completed its stowing sequence, it will return to the mode it was in before.</li> <li>\"runstopped\": This is the mode reported by the driver when the robot is in runstop, either through the user pressing the glowing white button in Stretch's head or through the runstop service. While this mode is active, no other commands will be accepted by the driver. After the robot has been taken out of runstop, it will return to the mode it was in before, or \"position\" mode if the driver was launched while the robot was runstopped.   </li> </ul>"},{"location":"stretch-ros2/stretch_core/#battery-sensor_msgsbatterystate","title":"/battery (sensor_msgs/BatteryState)","text":"<p>This topic publishes Stretch's battery and charge status. Charging status, the <code>power_supply_status</code> field, is estimated by looking at changes in voltage readings over time, where plugging-in causes the voltage to jump up (i.e. status becomes 'charging') and pulling the plug out is detected by a voltage dip (i.e. status becomes 'discharging'). Estimation of charging status is most reliable when the charger is in SUPPLY mode (see docs here for how to change charging modes). Charging status is unknown at boot of this node. Consequently, the <code>current</code> field is positive at boot of this node, regardless of whether the robot is charging/discharging. After a charging state change, there is a ~10 second timeout where state changes won't be detected. Additionally, outlier voltage readings can slip past the filters and incorrectly indicate a charging state change (albeit rarely). Finally, voltage readings are affected by power draw (e.g. the onboard computer starts a computationally taxing program), which can lead to incorrect indications of charging state change. Stretch RE2s have a hardware switch in the charging port that can detect when a plug has been plugged in, regardless of whether the plug is providing any power. Therefore, this node combines the previous voltage-based estimate with readings from this hardware switch to make better charging state estimates on RE2s (effectively eliminating the false positive case where a computational load draws more power).</p> <p>Since a battery is always present on a Stretch system, we instead misuse the <code>present</code> field to indicate whether a plug is plugged in to the charging port (regardless of whether it's providing power) on RE2 robots. This field is always false on RE1s. The unmeasured fields (e.g. charge in Ah) return a NaN, or 'not a number'.</p>"},{"location":"stretch-ros2/stretch_core/#is_homed-std_msgsbool","title":"/is_homed (std_msgs/Bool)","text":"<p>This topic publishes whether Stretch's encoders has been homed. If the robot isn't homed, joint states will be incorrect and motion commands won't be accepted by the driver. The home_the_robot service can be used to home Stretch.</p>"},{"location":"stretch-ros2/stretch_core/#is_runstopped-std_msgsbool","title":"/is_runstopped (std_msgs/Bool)","text":"<p>This topic publishes whether Stretch is runstopped. When the robot is runstopped (typically by pressing glowing white button in Stretch's head), motion for all joints is stopped and new motion commands aren't accepted. This is a safety feature built into the firmware for the four primary actuators. It's also possible to runstop the robot programmatically using the runstop service</p>"},{"location":"stretch-ros2/stretch_core/#published-services","title":"Published Services","text":""},{"location":"stretch-ros2/stretch_core/#home_the_robot-std_srvstrigger","title":"/home_the_robot (std_srvs/Trigger)","text":"<p>This service will start Stretch's homing procedure, where every joint's zero is found. Robots with relative encoders (vs absolute encoders) need a homing procedure when they power on. For Stretch, it's a 30-second procedure that must occur everytime the robot wakes up before you may send motion commands to or read correct joint positions from Stretch's prismatic and multiturn revolute joints. When this service is triggered, the mode topic will reflect that the robot is in \"homing\" mode, and after the homing procedure is complete, will switch back to whatever mode the robot was in before this service was triggered. While stretch_driver is in \"homing\" mode, no commands to the cmd_vel topic or the follow joint trajectory action service will be accepted.</p> <p>Other ways to home the robot include using the <code>stretch_robot_home.py</code> CLI tool from a terminal, or calling <code>robot.home()</code> from Stretch's Python API.</p> <p>Runstopping the robot while this service is running will yield undefined behavior and likely leave the driver in a bad state.</p>"},{"location":"stretch-ros2/stretch_core/#stow_the_robot-std_srvstrigger","title":"/stow_the_robot (std_srvs/Trigger)","text":"<p>This service will start Stretch's stowing procedure, where the arm is stowed into the footprint of the mobile base. This service is more convenient than sending a follow joint trajectory command since it knows what gripper is installed at the end of arm and stows these additional joints automatically. When this service is triggered, the mode topic will reflect that the robot is in \"stowing\" mode, and after the homing procedure is complete, will switch back to whatever mode the robot was in before this service was triggered. While stretch_driver is in \"stowing\" mode, no commands to the cmd_vel topic or the follow joint trajectory action service will be accepted.</p> <p>Other ways to stow the robot include using the <code>stretch_robot_stow.py</code> CLI tool from a terminal, or calling <code>robot.stow()</code> from Stretch's Python API.</p> <p>Runstopping the robot while this service is running will yield undefined behavior and likely leave the driver in a bad state.</p>"},{"location":"stretch-ros2/stretch_core/#runstop-std_srvssetbool","title":"/runstop (std_srvs/SetBool)","text":"<p>This service can put Stretch into runstop or take Stretch out of runstop. It's common to put the robot into/out of runstop by pressing the glowing white button in Stretch's head (at which point the robot will beep and the button will be blinking to indicate that it's runstopped), and holding the button down for two seconds to take it out of runstop (the button will return to non-blinking). This service acts as a programmatic way to achieve the same effect. When this service is triggered, the mode topic will reflect that the robot is in \"runstopped\" mode, and after the robot is taken out of runstop, the driver will switch back to whatever mode the robot was in before this service was triggered. While stretch_driver is in \"runstopped\" mode, no commands to the cmd_vel topic or the follow joint trajectory action service will be accepted.</p>"},{"location":"stretch-ros2/stretch_core/#testing","title":"Testing","text":"<p>Colcon is used to run the integration tests in the /test folder. The command to run the entire suite of tests is:</p> <pre><code>$ cd ~/ament_ws\n$ colcon test --packages-select stretch_core\n</code></pre> <p>Here are description of each test suite:</p> <ul> <li>test_services.py: tests the ROS2 services within the stretch_driver node</li> <li>test_action_trajectory_mode.py: tests the ROS2 FollowJointTrajectory action server's trajectory mode</li> <li>test_flake8.py and test_pep257.py: linters that identify unrecommended code style</li> </ul>"},{"location":"stretch-ros2/stretch_core/#license","title":"License","text":"<p>For license information, please see the LICENSE files.</p>"},{"location":"stretch-ros2/stretch_core/API/","title":"API","text":""},{"location":"stretch-ros2/stretch_core/API/#api-documentation-stretch-core","title":"API Documentation Stretch Core","text":""},{"location":"stretch-ros2/stretch_core/API/#stretch_driver","title":"stretch_driver","text":"<p>This is the API documentation for the stretch_driver.py file in the stretch_core package of the stretch_ros2 repository. This driver provides an interface to communicate with the Stretch Robot from Hello Robot Inc.</p>"},{"location":"stretch-ros2/stretch_core/API/#parameters","title":"Parameters","text":"<p><code>broadcast_odom_tf</code>: description: Whether to broadcast the odom TF default_value='False' choices=['True', 'False']</p> <p><code>mode</code>: description: The mode in which the Stretch ROS driver commands the robot default='position' choices=['position', 'navigation', 'trajectory']</p> <p><code>fail_out_of_range_goal</code>: description: Whether the motion action servers fail on out-of-range commands default_value='True' choices=['True', 'False']</p> <p><code>calibrated_controller_yaml_file</code>: description: Path to the calibrated controller args file default: stretch_core/config/controller_calibration_head.yaml</p>"},{"location":"stretch-ros2/stretch_core/API/#published-topics","title":"Published Topics","text":"<p><code>/odom</code>: The odometry from the wheel encoders through a message of type nav_msgs.msg.Odometry <code>/battery</code>: Publishes the battery state information through a message of type sensor_msgs.msg.BatteryState <code>/is_homed</code>: Publishes whether the robot is homed through a message of type std_msgs.msg.Bool <code>/mode</code>: Publishes the command mode the Stretch Driver is in through a message of type std_msgs.msg.String <code>/imu_mobile_base</code>: Publishes IMU data from the Pimu through a message of type sensor_msgs.msg.Imu <code>/magnetometer_mobile_base</code>: Publishes magnetometer data fromt he Pimu through a message of type sensor_msgs.msg.MagneticField <code>/imu_wrist</code>: Publishes IMU data from the Wacc through a message of type sensor_msgs.msg.Imu <code>/joint_states</code>: Publishes joint positions through a message of type sensor_msgs.msg.JointState</p>"},{"location":"stretch-ros2/stretch_core/API/#subscribed-topics","title":"Subscribed Topics","text":"<p><code>/cmd_vel</code>: Translational and rotational velocities through a message of type geometry_msgs.msg.Twist</p>"},{"location":"stretch-ros2/stretch_core/API/#exposed-services","title":"Exposed Services","text":"<p><code>/switch_to_position_mode</code>: change mode to position mode <code>/switch_to_trajectory_mode</code>: change mode to trajectory mode <code>/switch_to_navigation_mode</code>: change mode to navigation mode <code>/stop_the_robot</code>: stop the robot <code>/home_the_robot</code>: home the robot <code>/runstop</code>: switches the robot to standby mode where it will ignore new commands</p>"},{"location":"stretch-ros2/stretch_core/API/#exposed-action-servers","title":"Exposed Action Servers","text":"<p><code>/stretch_controller/follow_joint_trajectory</code>: action server to control the robot through joint trajectories</p>"},{"location":"stretch-ros2/stretch_core/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros2/stretch_deep_perception/","title":"stretch_deep_perception","text":""},{"location":"stretch-ros2/stretch_deep_perception/#overview","title":"Overview","text":"<p>stretch_deep_perception provides demonstration code that uses open deep learning models to perceive the world. </p> <p>This code depends on the stretch_deep_perception_models repository, which should be installed under ~/stretch_user/ on your Stretch robot.</p> <p>Link to the stretch_deep_perception_models repository: https://github.com/hello-robot/stretch_deep_perception_models</p>"},{"location":"stretch-ros2/stretch_deep_perception/#getting-started-demos","title":"Getting Started Demos","text":"<p>There are two demonstrations for you to try.</p>"},{"location":"stretch-ros2/stretch_deep_perception/#face-estimation-demo","title":"Face Estimation Demo","text":"<p>First, try running the face detection demonstration via the following command:</p> <pre><code>ros2 launch stretch_deep_perception stretch_detect_faces.launch.py \n</code></pre> <p>RViz should show you the robot, the point cloud from the camera, and information about detected faces. If it detects a face, it should show a 3D planar model of the face and 3D facial landmarks. These deep learning models come from OpenCV and the Open Model Zoo (https://github.com/opencv/open_model_zoo).</p> <p>You can use the keyboard_teleop commands within the terminal that you ran the launch in order to move the robot's head around to see your face.</p> <pre><code>             i (tilt up)\n\nj (pan left)               l (pan right)\n\n             , (tilt down)\n</code></pre> <p>Pan left and pan right are in terms of the robot's left and the robot's right.</p> <p>Now shut down everything that was launched by pressing q and Ctrl-C in the terminal.</p>"},{"location":"stretch-ros2/stretch_deep_perception/#object-detection-demo","title":"Object Detection Demo","text":"<p>Second, try running the object detection demo, which uses the tiny YOLO v5 object detection network (https://pytorch.org/hub/ultralytics_yolov5/). RViz will display planar detection regions. Detection class labels will be printed to the terminal. </p> <pre><code>ros2 launch stretch_deep_perception stretch_detect_objects.launch.py\n</code></pre>"},{"location":"stretch-ros2/stretch_deep_perception/#references","title":"References","text":"<p>[1] Hand It Over or Set It Down: A User Study of Object Delivery with an Assistive Mobile Manipulator, Young Sang Choi, Tiffany L. Chen, Advait Jain, Cressel Anderson, Jonathan D. Glass, and Charles C. Kemp, IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2009. http://pwp.gatech.edu/hrl/wp-content/uploads/sites/231/2016/05/roman2009_delivery.pdf</p>"},{"location":"stretch-ros2/stretch_deep_perception/#license","title":"License","text":"<p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros2/stretch_deep_perception/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros2/stretch_description/","title":"stretch_description","text":""},{"location":"stretch-ros2/stretch_description/#overview","title":"Overview","text":"<p>stretch_description provides materials for a URDF kinematic model of the Stretch mobile manipulator.</p>"},{"location":"stretch-ros2/stretch_description/#quick-view","title":"Quick View","text":"<pre><code>ros2 launch stretch_description display.launch.py\n</code></pre>"},{"location":"stretch-ros2/stretch_description/#details","title":"Details","text":"<p>The meshes directory contains STL mesh files representing the exterior geometry of various parts of the robot. </p> <p>The urdf directory contains xacro files representing various parts of the robot that are used to generate the robot's URDF. </p> <p>stretch_ros2 expects a URDF with the name stretch.urdf to reside within the urdf directory. The file stretch.urdf serves as the URDF for the robot and must be generated. Typically, it is a calibrated urdf file for the particular Stretch robot being used. To generate this file, please read the documentation within stretch_ros2/stretch_calibration. </p> <p>The xacro_to_urdf.py will usually only be indirectly run as part of various scripts and launch files within stretch_ros2/stretch_calibration. </p> <p>Sometimes a stretch_uncalibrated.urdf file will reside with the urdf directory. This file is typically generated directly from the xacro files without any alterations. </p>"},{"location":"stretch-ros2/stretch_description/#exporting-a-urdf","title":"Exporting a URDF","text":"<p>Sometimes a URDF is useful outside of ROS, such as for simulations and analysis. Running the export_urdf.py script in the urdf directory will export a full URDF model of the robot based on stretch.urdf. </p> <p>The exported URDF will be found within an exported_urdf directory. It is also copied to a directory for your specific robot found under ~/stretch_user. The exported URDF includes meshes and controller calibration YAML files. The exported URDF can be visualized using stretch_urdf_show.py, which is part of the stretch_body Python code. </p>"},{"location":"stretch-ros2/stretch_description/#changing-the-tool","title":"Changing the Tool","text":"<p>If you wish to remove the default gripper and add a different tool, you will typically edit /stretch_description/urdf/stretch_description.xacro. Specifically, you will replace the following line in order to include the xacro for the new tool and then follow directions within stretch_ros2/stretch_calibration to generate a new calibrated urdf file (stretch.urdf) that includes the new tool.</p> <p><code>&lt;xacro:include filename=\"stretch_gripper.xacro\" /&gt;</code></p> <p>As an example we provide the xacro <code>stretch_dry_erase_marker.xacro</code> and its dependent mesh files with stretch_ros. </p> <p>Some of the tools found in the Stretch Body Tool Share include URDF data. To integrate these tools into the URDF for your Stretch</p> <pre><code>&gt;&gt;$ cd ~/repos\n&gt;&gt;$ git clone https://github.com/hello-robot/stretch_tool_share\n&gt;&gt;$ cd stretch_tool_share/&lt;tool name&gt;\n&gt;&gt;$ cp stretch_description/urdf/* ~/catkin_ws/src/stretch_ros/stretch_description/urdf/\n&gt;&gt;$ cp stretch_description/meshes/* ~/catkin_ws/src/stretch_ros/stretch_description/meshes/\n</code></pre> <p>Next add the xacro for the particular tool to <code>/stretch_description/urdf/stretch_description.xacro</code>. Then you can generate and preview the uncalibrated URDF:</p> <pre><code>&gt;&gt;$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf\n&gt;&gt;$ cp stretch.urdf stretch.urdf.bak\n&gt;&gt;$ ros2 run stretch_calibration update_urdf_after_xacro_change.py\n</code></pre> <p>Now visualize the new tool</p> <pre><code>&gt;&gt;$ ros2 launch stretch_calibration simple_test_head_calibration.launch\n</code></pre>"},{"location":"stretch-ros2/stretch_description/#license-and-patents","title":"License and Patents","text":"<p>Patents are pending that cover aspects of the Stretch RE1 mobile manipulator.</p> <p>For license information, please see the LICENSE files. </p>"},{"location":"stretch-ros2/stretch_description/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"). The Contents consist of software and data used with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>The Clear BSD License</p> <p>Copyright (c) 2021 Hello Robot Inc. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met:</p> <pre><code> * Redistributions of source code must retain the above copyright notice,\n this list of conditions and the following disclaimer.\n\n * Redistributions in binary form must reproduce the above copyright\n notice, this list of conditions and the following disclaimer in the\n documentation and/or other materials provided with the distribution.\n\n * Neither the name of the copyright holder nor the names of its\n contributors may be used to endorse or promote products derived from this\n software without specific prior written permission.\n</code></pre> <p>NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"stretch-ros2/stretch_description/urdf/export_urdf_license_template/","title":"Export urdf license template","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\") except where otherwise noted. The Contents consist of software and data used with the Stretch RE1 and Stretch 2 mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2023 Hello Robot Inc.</p> <p>The Contents are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike-4.0-International (CC BY-NC-SA 4.0) license (the \"License\"); you may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>https://creativecommons.org/licenses/by-nc-sa/4.0/</p> <p>Unless required by applicable law or agreed to in writing, the Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Patents pending and trademark rights cover the Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\"</p> <p>The Contents may incorporate some parts of the \"RealSense Camera description package for Intel 3D D400 cameras\" released with an Apache 2.0 license and Copyright 2017 Intel Corporation. The details of the Apache 2.0 license can be found via the following link:  </p> <p>https://www.apache.org/licenses/LICENSE-2.0</p> <p>Specifically, the Contents may include the d435.dae mesh file and content generated by the _d435.urdf.xacro found within the GitHub repository available for download via the following link as of Mar 28, 2023:</p> <p>https://github.com/IntelRealSense/realsense-ros/tree/ros2-development/realsense2_description</p> <p>These specific materials are subject to the requirements of their original Apache 2.0 license.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros2/stretch_moveit2/","title":"Index","text":""},{"location":"stretch-ros2/stretch_moveit2/#overview","title":"Overview","text":"<p>stretch_moveit2 configures MoveIt 2 for Stretch. The MoveIt Motion Planning Framework makes whole body planning, manipulation, 3D perception, control/navigation, and more available on Stretch.</p>"},{"location":"stretch-ros2/stretch_moveit2/#quickstart","title":"Quickstart","text":"<p>Before proceeding, it's a good idea to home the robot. <pre><code>stretch_robot_home.py\n</code></pre></p> <p>To launch RViz with MoveIt 2, run the following command. (Press Ctrl+C in the terminal to terminate): <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre></p> <p>Alternatively, if you want to integrate MoveIt2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go: <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre></p>"},{"location":"stretch-ros2/stretch_moveit2/#tutorials","title":"Tutorials","text":"<p>For instructions on working with MoveIt 2 on Stretch, go through the tutorials here.</p> <p>MoveIt 2 Python API support is currently under development.</p>"},{"location":"stretch-ros2/stretch_moveit2/API/","title":"API","text":""},{"location":"stretch-ros2/stretch_moveit2/API/#api-documentation-stretch-moveit-2","title":"API Documentation Stretch MoveIt 2","text":""},{"location":"stretch-ros2/stretch_moveit2/API/#movegroup_moveit2","title":"movegroup_moveit2","text":"<p>This is the API documentation for the movegroup_moveit2 node in the stretch_moveit2 package of the stretch_ros2 repository. MoveIt 2 provides a way to plan for and control control the robot joints both in task and joint space.</p>"},{"location":"stretch-ros2/stretch_moveit2/API/#parameters","title":"Parameters","text":"<p><code>robot_description</code>: description: the robot URDF model passed as a path or as the entire file contents default: 'stretch_description/urdf/stretch.urdf'</p> <p><code>semantic_config</code>: description: The robot semantic configuration with robot SRDF default: 'stretch_moveit2/config/stretch_description.srdf'</p> <p><code>allow_trajectory_execution</code>: description: Whether to allow trajectory execution on actual robot default: 'True'</p> <p><code>fake_execution</code>: description: Whether to allow fake execution default: 'False'</p> <p><code>max_safe_path_cost</code>: description: default: '1'</p> <p><code>jiggle_fraction</code>: description: default: '0.05'</p> <p><code>publish_monitored_planning_scene</code>: description: Whether to publish the planning scene monitor default: 'True'</p> <p><code>capabilities</code>: description: The non-default MoveGroup capabilities to enable default: ''</p> <p><code>disable_capabilities</code>: description: The default MoveGroup capabilities to disable default: ''</p> <p><code>pipeline</code>: description: Specify the planning pipeline default: 'ompl'</p> <p><code>debug</code>: description: Whether to launch in debug mode, by defualt this is False default='False' choices=['True', 'False']</p> <p>Parameters specific to Stretch <code>db</code>: description: Whether to start a database. By default, we do not start a database (it can be large) default='False' choices=['True', 'False']</p> <p><code>db_path</code>: description: Allow user to specify database location default='stretch_moveit2/default_warehouse_mongo_db'</p> <p><code>use_stretch_driver</code>: description: Whether to launch stretch_driver separately default='False' choices=['True', 'False']</p> <p><code>use_rviz</code>: description: Whether to launch RViz for visualization default='True' choices=['True', 'False']</p>"},{"location":"stretch-ros2/stretch_moveit2/API/#published-topics","title":"Published Topics","text":""},{"location":"stretch-ros2/stretch_moveit2/API/#subscribed-topics","title":"Subscribed Topics","text":""},{"location":"stretch-ros2/stretch_moveit2/API/#exposed-services","title":"Exposed Services","text":""},{"location":"stretch-ros2/stretch_moveit2/API/#exposed-action-servers","title":"Exposed Action Servers","text":""},{"location":"stretch-ros2/stretch_moveit2/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-ros2/stretch_moveit2/demo/","title":"Demo","text":""},{"location":"stretch-ros2/stretch_moveit2/demo/#overview","title":"Overview","text":"<p>MoveIt 2 is a whole-body motion planning framework for mobile manipulators that allows planning pose and joint goals in environments with and without obstacles. Stretch being a mobile manipulator is uniquely well-suited to utilize the planning capabilities of MoveIt 2 in different scenarios.</p>"},{"location":"stretch-ros2/stretch_moveit2/demo/#motivation","title":"Motivation","text":"<p>Stretch has a kinematically simple 3 DoF arm (+2 with DexWrist) that is suitable for pick and place tasks of varied objects. Its mobile base provides it with 2 additional degrees of freedom that afford it more manipulability and also the ability to move around freely in its environment. To fully utilize these capabilities, we need a planner that can plan for both the arm and the mobile base at the same time. With MoveIt 2 and ROS 2, it is now possible to achieve this, empowering users to plan more complicated robot trajectories in difficult and uncertain environments.</p>"},{"location":"stretch-ros2/stretch_moveit2/demo/#demo-with-stretch-robot","title":"Demo with Stretch Robot","text":""},{"location":"stretch-ros2/stretch_moveit2/demo/#installing-ros-2-on-ubuntu-2004","title":"Installing ROS 2 on Ubuntu 20.04","text":"<p>By default, Stretch RE1 ships with Ubuntu 18.04, ROS Melodic, and Python2 packages. The following steps will install a second operating system with Ubuntu 20.04, ROS Noetic, ROS 2 Galactic (where MoveIt 2 is supported), and Python3 packages. If you already have an Ubuntu 20.04 partition on the robot set up with the correct ROS workspaces, skip to the next section.</p> <p>If your robot is an RE1 and you have not upgraded the software stack recently, follow the steps in this link to perform a fresh robot installation. We recommend backing up important data before you begin.</p>"},{"location":"stretch-ros2/stretch_moveit2/demo/#installing-moveit-2-and-its-dependencies","title":"Installing MoveIt 2 and Its Dependencies","text":"<p>If you performed the above steps successfully, you don't need to follow the steps in this section as the install scripts take care of satisfying all dependencies you need to run MoveIt 2 on your robot. However, if you already had a stable ROS 2 installation running but were missing the most up to date workspaces, follow the steps in this link to update your ROS workspaces. We recommend backing up important data before you begin as this step replaces your existing workspace with a new one.</p>"},{"location":"stretch-ros2/stretch_moveit2/demo/#planning-with-moveit-2-using-rviz","title":"Planning with MoveIt 2 Using RViz","text":"<p>Before we proceed, it's always a good idea to home the robot first by running the following script so that we have the correct joint positions being published on the /joint_states topic. This is necessary for planning trajectories on Stretch with MoveIt.</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>The easiest way to run MoveIt 2 on your robot is through RViz. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To launch RViz with MoveIt 2, run the following command. (Press Ctrl+C in the terminal to terminate)</p> <pre><code>ros2 launch stretch_moveit_moveit2 movegroup_moveit2.launch.py\n</code></pre> <p>Follow instructions in this tutorial to plan and execute trajectories using the interactive markers in RViz.</p> <p>Use the interactive markers to drag joints to desired positions or go to the manipulation tab in the Motion Planning pane to fine-tune joint values using the sliders. Next, click the 'Plan' button to plan the trajectory. If the plan is valid, you should be able to execute the trajectory by clicking the 'Execute' button. Below we see Stretch raising its arm without any obstacle in the way.</p> <p></p> <p>To plan with obstacles, you can insert objects like a box, cyclinder or sphere, in the planning scene to plan trajectories around the object. This can be done by adding an object using the Scene Objects tab in the Motion Planning pane. Below we see Stretch raising its arm with a flat cuboid obstacle in the way. The mobile base allows Stretch to move forward and then back again while raising the arm to avoid the obstacle.</p> <p></p>"},{"location":"stretch-ros2/stretch_moveit2/demo/#planning-with-moveit-2-using-the-movegroup-c-api","title":"Planning with MoveIt 2 Using the MoveGroup C++ API","text":"<p>If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. Execute the launch file again and go through the comments in the node to understand what's going on. (Press Ctrl+C in the terminal to terminate)</p> <pre><code>ros2 launch stretch_moveit_config movegroup_moveit2.launch.py\n</code></pre> <p>Follow instructions in this tutorial to plan and execute trajectories using the MoveGroup C++ API.</p> <p></p> <p>Read the comments in the code for a breakdown of the node.</p>"},{"location":"stretch-ros2/stretch_nav2/","title":"Index","text":""},{"location":"stretch-ros2/stretch_nav2/#overview","title":"Overview","text":"<p>The stretch_nav2 package provides the standard ROS 2 navigation stack (Nav2) with its launch files. This package utilizes slam_toolbox and Nav2 to drive Stretch around a mapped space. Running this code will require the robot to be untethered. We recommend stowing the arm while running navigation on the robot.</p>"},{"location":"stretch-ros2/stretch_nav2/#quickstart","title":"Quickstart","text":"<p>The first step is to map the space that the robot will navigate in. The <code>offline_mapping.launch.py</code> will enable you to do this. First, run:</p> <pre><code>ros2 launch stretch_nav2 offline_mapping.launch.py\n</code></pre> <p>Rviz will show the robot and the map that is being constructed. With the terminal open, use the joystick (see instructions below for using a keyboard) to teleoperate the robot around. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, open a new terminal and run the following commands to save the map to the <code>stretch_user/</code> directory.</p> <pre><code>mkdir ${HELLO_FLEET_PATH}/maps\nros2 run nav2_map_server map_saver_cli -f ${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;\n</code></pre> <p>NOTE: The <code>&lt;map_name&gt;</code> does not include an extension. The map_saver node will save two files as <code>&lt;map_name&gt;.pgm</code> and <code>&lt;map_name&gt;.yaml</code>.</p> <p>Tip: For a quick sanity check, you can inspect the saved map using a pre-installed tool called Eye of Gnome (eog) by running the following command:</p> <pre><code>eog ${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.pgm\n</code></pre> <p>Next, with <code>&lt;map_name&gt;.yaml</code>, we can navigate the robot around the mapped space. Run:</p> <pre><code>ros2 launch stretch_nav2 navigation.launch.py map:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p>A new RViz window should pop up with a <code>Startup</code> button in a menu at the bottom left of the window. Press the <code>Startup</code> button to kick-start all navigation related lifecycle nodes. Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location on the map does not match the robot's location in the real space. To correct this, from the top bar of Rviz, use <code>2D Pose Estimate</code> to lay an arrow down roughly where the robot is located in the real space. This gives an initial estimate of the robot's location to AMCL, the localization package. AMCL will better localize the robot once we pass the robot a <code>2D Nav Goal</code>.</p> <p>In the top bar of Rviz, use <code>2D Nav Goal</code> to lay down an arrow where you'd like the robot to navigate. In the terminal, you'll see Nav2 go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior - spinning around 180 degrees in place or backing up.</p> <p>Tip: If navigation fails or the robot becomes unresponsive to subsequent goals through RViz, you can still teleoperate the robot using the Xbox controller.</p>"},{"location":"stretch-ros2/stretch_nav2/#teleop-using-a-joystick-controller","title":"Teleop using a Joystick Controller","text":"<p>The launch files expose the launch argument \"teleop_type\". By default, this argument is set to \"joystick\", which launches joystick teleop in the terminal with the xbox controller that ships with Stretch RE1. The xbox controller utilizes a dead man's switch safety feature to avoid unintended movement of the robot. This is the switch located on the front left side of the controller marked \"LB\". Keep this switch pressed and translate or rotate the base using the joystick located on the right side of the xbox controller.</p> <p>If the xbox controller is not available, the following commands will launch mapping and navigation, respectively, with keyboard teleop:</p> <p><pre><code>ros2 launch stretch_nav2 offline_mapping.launch.py teleop_type:=keyboard\n</code></pre> or <pre><code>ros2 launch stretch_nav2 navigation.launch.py teleop_type:=keyboard map:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre></p>"},{"location":"stretch-ros2/stretch_nav2/#license","title":"License","text":"<p>For license information, please see the LICENSE files.</p>"},{"location":"stretch-ros2/stretch_nav2/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-tool-share/","title":"Overview","text":""},{"location":"stretch-tool-share/#overview","title":"Overview","text":"<p>We designed Stretch's hardware to be easily extended. You can make your own tool and attach it to the wrist to creatively expand what the Stretch RE1 can do. Your tool can also use Dynamixel X-series servos from Robotis via the provided TTL bus. </p> <p>In this repository, we provide examples of tools that we've created. We've released them with a permissive Apache 2.0 license, so you're free to use them as you wish. We hope they'll inspire you to create your own tools.</p> <p>We also include URDF and mesh files for many of the tools in their <code>stretch_description</code> folder. See the Stretch ROS documentation for guidance on integrating these tools into your robot model.</p> <p>We'd love it if you shared your creations with the community. We recommend you create a GitHub repository similar to this one for your own tools and then post an announcement to the forum to let people know about it. </p> Tool NVIDIA Jetson Orin AGX Backpack Stretch Dex Wrist Stretch Teleop Kit Stretch Docking Station Wrist USB Board Camera Stretch 2 Model Swivel Tablet Mount Phone Holder V1 ReactorX Wrist V1 Dry Erase Holder V1 Swiffer Mount V1 Tray Cup Holder V1 Puller V1 Stretch RE1 Arm Stretch RE1 Head Easy-Access Card Holder Arm-Mounted Tray Button Pusher"},{"location":"stretch-tool-share/#licenses","title":"Licenses","text":"<p>The contents in this repository that represent parts of a Stretch robot, such as its head, arm, wrist, and default gripper, are covered by the CC BY-NC-SA 4.0 license. Please note that the Stretch robot and its default gripper are also covered by patents. Please see the ROBOT_LICENSE file for details.</p> <p>Other contents in this repository created by Hello Robot Inc. that specifically pertain to the tools that attach to the Stretch mobile manipulators as accessories are covered by the Apache 2.0 license. Please see the TOOL_LICENSE file for details.</p> <p>The contents of this repository are intended for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-tool-share/ROBOT_LICENSE/","title":"ROBOT LICENSE","text":"<p>The following license applies to the entire contents of this directory that represent parts of the Stretch robots (the \"Robot Contents\"), such as the robot's head, arm, wrist, and default gripper. The Robot Contents consists of software and data for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>The Robot Contents are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike-4.0-International (CC BY-NC-SA 4.0) license (the \"License\"); you may not use the Robot Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>https://creativecommons.org/licenses/by-nc-sa/4.0/</p> <p>Unless required by applicable law or agreed to in writing, the Robot Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Patents pending and trademark rights cover the Robot Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\"</p> <p>For further information about the Robot Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-tool-share/TOOL_LICENSE/","title":"TOOL LICENSE","text":"<p>The following license applies to the contents (the \"Tool Contents\") in this repository specific to tools created by Hello Robot Inc. that attach to the Stretch robots. This license explicitly excludes any contents covered by the license found in the ROBOT_LICENSE.md file. The Tool Contents does not include any part of the Stretch robot, such as its arm, wrist, or default gripper.</p> <p>The Tool Contents consists of software and data related to tools intended for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>The Tool Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Tool Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Tool Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Tool Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-tool-share/python/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch mobile manipulators, which are robots produced and sold by Hello Robot Inc.</p> <p>Copyright 2020-2024 Hello Robot Inc.</p> <p>The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation.</p> <p>The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link:</p> <p>https://www.gnu.org/licenses/gpl-3.0.html</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/","title":"Arm Mounted Tray","text":""},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/#arm-mounted-tray","title":"Arm-Mounted Tray","text":"<p>Created by: Hello Robot Inc</p>"},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/#overview","title":"Overview","text":"<p>This tray conveniently clips onto Stretch's first arm link to provide a surface for carrying objects, including two hooks in the front and a cup holder to assist a variety of tasks. </p>"},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/#parts-list","title":"Parts List","text":"Item Qty Vendor Tray 1 PLA 3D Printer Arm Clips 2 PLA 3D Printer M4x6mm Socket Head Screw 4 McMaster-Carr M4 Nut 4 McMaster-Carr"},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D assembly</p> <p></p> <ol> <li>Install one M4 screw through each tray hole, and secure each with an M4 nut set in the tray clips. Ensure the hooks on the tray clips are facing the flat side of the tray, away from the cupholder.</li> <li>Press the tray onto the first arm link until it snaps into place. </li> </ol>"},{"location":"stretch-tool-share/tool_share/button_pusher_V2/","title":"Index","text":""},{"location":"stretch-tool-share/tool_share/button_pusher_V2/#button-pusher","title":"Button Pusher","text":"<p>Create by: Hello Robot Inc. and Henry Evans</p> <p>A simple part to help Stretch push buttons or anything else you can imagine. The concave shape is designed to avoid blocking the gripper teleop camera, allowing for high-percision and easy use.  </p> <p></p> <p>We found adding a layer of Dycem non-slip material helped with a more secure grip on the tool. Foam or cardboard blocks under each gripper pad may help stabilize as well. </p>"},{"location":"stretch-tool-share/tool_share/button_pusher_V2/#parts-list","title":"Parts List","text":"Item Qty Vendor button_pusher_V2.stl 1 PLA 3D printer Dycem 1 Various"},{"location":"stretch-tool-share/tool_share/card_holder_V1.5/","title":"Card Holder","text":""},{"location":"stretch-tool-share/tool_share/card_holder_V1.5/#easy-access-playing-card-holder","title":"Easy-Access Playing Card Holder","text":"<p>Created by: Hello Robot Inc</p> <p></p> <p>A spinning card holder that Stretch can manipulate to position cards in an optimal position for grasping or to signal which card an opponent should take. </p> <p></p>"},{"location":"stretch-tool-share/tool_share/card_holder_V1.5/#parts-list","title":"Parts List","text":"Item Qty Vendor Gear Wheel 1 PLA 3D printer A Frame 1 PLA 3D printer Base 1 PLA 3D printer Pull Tab 1 PLA 3D printer Foam Block* 1 Recycled Material <p>*We used extra packing foam, but styrofoam or anything similar should work!</p>"},{"location":"stretch-tool-share/tool_share/card_holder_V1.5/#assembly-instructions","title":"Assembly instructions","text":"<ol> <li>Cut an approximately 6 inch diameter circle out of the foam. Slice around the perimiter about 2 inches into the foam to create a slot to hold cards.  </li> <li> <p>Gently press the spikes on the front of the gear wheel into the center of the foam circle. Push the axel on the A frame through the hole in the back of the wheel gear and into the foam until the back of the wheel gear meets the front of the A frame. </p> </li> <li> <p>With the foam and gear wheel secured onto the A frame, push the frame into the openings on the base.</p> </li> <li> <p>Slide the pull tab into the slot on the A Frame. The teeth on the pull tab should interlock with the teeth on the wheel gear so pulling and pushing the tab spins the wheel.</p> </li> </ol>"},{"location":"stretch-tool-share/tool_share/dry_erase_holder_V1/","title":"Dry Erase Holder","text":""},{"location":"stretch-tool-share/tool_share/dry_erase_holder_V1/#dry-erase-holder","title":"Dry Erase Holder","text":"<p>Created by: Hello Robot Inc</p> <p>This tool allows Stretch to hold a dry erase marker.  It is spring loaded, allowing for compliant interaction between the marker and a white board. </p> <p>The tool can be integrated into your robot URDF by integrating its stretch_description as described in the Stretch ROS documentation.</p> <p></p>"},{"location":"stretch-tool-share/tool_share/dry_erase_holder_V1/#parts-list","title":"Parts List","text":"Item Qty Vendor Expo Dry Erase 1 Amazon M5x50mm Hex Head Bolt 1 McMaster-Carr M5 Nut 2 McMaster-Carr wrist_end_cap_5mm 1 PLA 3D Printer dry_erase_bushing_block 1 PLA 3D Printer Size 30 Rubber Band 2 McMaster-Carr 3/4\" Shaft Collar 1 McMaster-Carr"},{"location":"stretch-tool-share/tool_share/dry_erase_holder_V1/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D assembly</p> <p></p> <ol> <li>Install the bolt into the dry_erase_bushing block and secure from below with an M5 nut.</li> <li>Attach the dry erase bushing block to the tool plate, securing from below with the wrist_end_cap_5mm and an M5 nut. Orient the block so the marker points forward. </li> <li>Attach the shaft collar to your dry erase marker, approximately 8mm from the back of the marker.</li> <li>Slide the marker into the bushing block. Loop a rubber band around the back of the marker and over to one of the pegs on the side of the bushing block. Repeat with the other peg.</li> <li>The marker should now easily spring back when pushed against. You're ready to write!</li> </ol>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/","title":"Index","text":""},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/#nvidia-jetson-orin-agx-backpack","title":"Nvidia Jetson Orin AGX Backpack","text":"<p>Created by: Hello Robot Inc</p> <p>This tool allows an Nvidia Jetson Orin AGX to be mounted and wired to the Stretch base. The design uses a laser cut piece of white Delrin ordered from Ponoko.com, and a number of off the shelf items listed below.</p> <p></p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/#parts-list","title":"Parts List","text":"Item Qty Vendor DC Power Pigtails Cable, DC 5.5mm x 2.5mm Male Plug Jack to Bare Wire Open End Power Supply Replacement 3Ft 1 Amazon 12V DC to 19V DC Mini Size Ultra-slim High Efficiency 90W Power Adapter DD90M-19V 1 BIX JST VH 3.96 mm Pitch 2 Pin 1 Amazon CAT6A Slim Cable UTP Booted 1.5 FT 1 Amazon White Delrin Laser Cut Base using Jetson_Orin_AGX_base_plate_flat_pattern.DXF 1 Ponoko Male-Female Threaded Hex Standoff 4 McMaster-Carr Cable Tie Mount 5 McMaster-Carr Cable Zip Ties 4 Inch 8 Amazon Phillips Rounded Head Thread-Forming Screws 5 McMaster-Carr Black-Oxide Alloy Steel Button Head Torx Screws 4 McMaster-Carr Torx Flat Head Thread-Cutting Screws for Metal 4 McMaster-Carr"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/#assembly-instructions","title":"Assembly instructions","text":"<p>NVIDIA Jetson Orin AGX Mount Assembly Instructions PDF  1. Cut the DC power cable 20\" in length. Pigtail the cable and expose 3/16\" of copper wire . Crimp with JST-VH crimper tool and attach the JST-VH 2 pin connector.</p> <ol> <li> <p>Remove base shell from Stretch base.</p> </li> <li> <p>Connect the 2 pin connector to 12volt Aux on PIMU.</p> </li> <li> <p>Using a digital multimeter place the positive (red) probe inside the barrel jack connector of the new cable, and touch the negative (black) probe on the outside of the barrel jack connector.</p> </li> <li> <p>Turn on Stretch and measure the voltage coming out of new cable that is connected to the 12volt Aux connector using a digital multimeter. Ensure that the voltage reading is positive 12V or higher       &gt; [!WARNING]       &gt; Ensuring that the voltage coming out of the new cable is positive, is crucial in protecting Stretch's main power board from damaging.</p> </li> <li> <p>Route cable through Aux hole in the base shell and re-install base shell.</p> </li> <li> <p>Mount the Jetson Orin AGX to white delrin custom plate using 4x Torx Flat Head Thread-Cutting Screws for Metal.</p> </li> <li> <p>Add 4x Male-Female Threaded Hex Standoff to Stretch Base accessory mounts.</p> </li> <li> <p>Mount the white delrin custom plate to standoffs using 4x Steel Button Head Torx Screws.</p> </li> <li> <p>Plug in ethernet cable to Jetson Orin AGX.</p> </li> <li> <p>Plug your custom length DC power cable into the BIX power adaptor. Plug the Bix power adapter to Jetson Orin AGX.     &gt; [!IMPORTANT]     &gt; The BIX Power Adapter is a crucial component that we highly recommend purchasing for this setup. It adds additional safety features to the power path system. Not adding this part to the setup can lead to the Jetson power system failing.</p> </li> <li> <p>Attach 5x Cable tie mounts using 5x Phillips Rounded Head Thread forming screws.</p> </li> <li> <p>Zip-tie cables and cut excess.</p> </li> </ol>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/#software","title":"Software","text":"<p>Adding this requires limiting the range of motion of the lift joint so that it does not collide with the part. To do this, modify the lower range in your <code>stretch_user_params.yaml</code> to:</p> <pre><code>lift:\n  range_m: [0.25, 1.xxx]\n</code></pre> <p>Note: <code>1.xx</code> is the calibrated upper bound found from <code>stretch_params.py | grep lift | grep range_m</code></p> <p>Note: This conservatively limits the range of motion such that the standard gripper will not collide when stowed.</p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/","title":"Index","text":""},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/#nvidia-jetson-orin-agx-setup","title":"Nvidia Jetson Orin AGX Setup","text":"<p>Created by: Hello Robot Inc</p> <p>What you will need to start with this is the Jetson Orin AGX Dev kit, as we know there are several ways to make the same thing, but the easiest way is to download the SDK Manager that Nvidia has, you can download it from here, also what you will need is a computer with Ubuntu 20.04 because with Ubuntu 22.04 the SDK Manager will not work, you can work with the 18.04 version if you want but we would recommend 20.04 so that you can have the latest version of drivers according to this information:</p> <p></p> <p>Also if you want to follow the steps for the SDK Manager from the NVIDIA Documentation you can read it from here. And we will recommend you to put your Jetson in Force Recovery mode, if you don\u2019t know how to do this for the Jetson AGX Orin you can see this page or follow the next steps:</p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/#force-recovery-mode","title":"Force Recovery Mode","text":"<p>Note  If you want to be sure that the buttons you are pressing are the correct ones make sure to see the Hardware Layout page.</p> <p>To enter in the Force Recovery mode first make sure that the buttons are facing you, then you need to check the state of your Jetson, if it's power on follow the next steps: 1. Press and hold down the Force Recovery Button (2 arrows/middle one). 2. Press and hold down the reset button (1 arrow/right one). 3. Release both buttons.</p> <p>If your Jetson is power off then there are other steps that you will need to do : 1. Press and hold down the Force Recovery Button (2 arrows/middle one). 2. Plug in a DC power supply's USB Type-C plug into the USB Type-C port ( 4 ) above the DC jack, or plug a DC plug of a power supply into DC jack ( 5 ). 3. If the white LED is not lit, press the Power button (Power symbol/left one). 4. Release both buttons.</p> <p>Perfect, now with this your Jetson has entered in the Force Recovery Mode, you can go and watch the next tutorials!</p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/#jetson-tutorials","title":"Jetson Tutorials","text":"Tutorial Description 1 Jetson Initial Setup Initial Setup for Jetson using SDK Manager 2 Jetson Connection Via Headless Configuration Ways to connect your Jetson using only CLI."},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_connection_headless_configuration/","title":"Jetson Connection Via Headless Configuration","text":"<p>Note     If you didn't configure your Jetson with the SDK Manager because of the Ubuntu versions then maybe this could be of great help for you.</p> <p>As always there are several ways to enter your Jetson with only a terminal for the first one you can read it in the documentation that NVIDIA has or read it below:</p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_connection_headless_configuration/#connecting-via-screen-command","title":"Connecting via screen command","text":"<p>Note     To check the hardware layout of the Jetson AGX Orin click here. 1. First you will need to connect in the USB micro-B port ( 9 ) the micro USB cable into one of Stretch's USB ports. If you want to connect an ethernet cable is completly optional. 2. Then connect the included power supply into the USB Type-C\u2122 port above the DC jack ( 4 ). 3. Your developer kit should automatically power on, and the white LED near the power button will light. If not, press the Power button ( 1 ). 4. Wait up to 1 minute. 5. On your computer, use a serial terminal application to connect via host serial port to the developer kit</p> <p>Now that you have done this steps it's time to connect to our Jetson, you can use a Windows, Mac or Linux computer for this, the next steps are going to be in Linux:</p> <p>First you will need to locate the tty device in your terminal, to do this you can write down</p> <pre><code>dmesg | grep --color 'tty'\n</code></pre> <p>If you didn't have your Jetson connected with the micro usb you can connect it and try again and see what's different it should appear something like this: <pre><code>[ xxxx.xxxxxx] cdc_acm 1-3.3.4:1.1: ttyACM3: USB ACM device\n</code></pre></p> <p>In case you want to check it better you can input the command <pre><code>ls -l /dev/ttyACM3   # In our case was ACM3\n</code></pre></p> <p>And it should output the general information of the ttyACM3 device <pre><code>crw-rw---- 1 root dialout 166, 3 Aug 24 10:42 /dev/ttyACM3\n</code></pre></p> <p>Now for the connection part, if you don't have installed the screen program in Linux you can install it writing down this in the terminal: <pre><code>sudo apt-get install -y screen\n</code></pre></p> <p>Now with the device name discover in the previous step you can use it to input the screen command: <pre><code>sudo screen /dev/ttyACM3 115200\n</code></pre> And now in your terminal you will have to login in your ubuntu account that you have on the jetson.</p> <p> </p> <p>Note     If you didn't configure your Jetson with the SDK Manager you can complete the CUI-based \"Jetson Initial configuration\" (oem-config) using your PC's keyboard. Read the NVIDIA documentation.</p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_connection_headless_configuration/#connecting-via-ssh","title":"Connecting via SSH","text":"<p>For this type of connection first we need to know what's our board IP Adress. This is for the ones that have a serial console access, use the ifconfig command to get the required information in the Jetson terminal:  <pre><code>hello-robot@ubuntu:~$ ifconfig\n\n...\n\neth0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500\n        inet 10.1.10.205  netmask 255.255.255.0  broadcast 10.1.10.205\n\nwlan0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 10.1.10.205  netmask 255.255.255.0  broadcast 10.1.10.205\n</code></pre> This will show you all the connections that your Jetson has and you can have 2 output options with the IP address, the first one is if you have your Jetson connected to the internet via ethernet cable, with this the IP address will show up in the ethernet section, the second one is if you don't have an ethernet cable connected and instead you are connected via normal internet connection then the IP address will show up in the wlan0 section. With this now you can conncect your Jetson via SSH, if you have your Jetson connected to a monitor and you want to try it on Stretch follow the assembly instructions and then from your Stretch terminal just input the next command: <pre><code>ssh jetson-username@ip-address\n</code></pre> And now you will need to write the password that you configure and that's it, see below the output after you write down your password: <pre><code>jetson-username@ip-address password:\n\nWelcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.10.120-tegra aarch64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\nThis system has been minimized by removing packages and content that are\nnot required on a system that users do not log into.\n\nTo restore this content, you can run the 'unminimize' command.\n\nExpanded Security Maintenance for Applications is not enabled.\n\n9 updates can be applied immediately.\n9 of these updates are standard security updates.\nTo see these additional updates run: apt list --upgradable\n\n35 additional security updates can be applied with ESM Apps.\nLearn more about enabling ESM Apps service at https://ubuntu.com/esm\n\nLast login: Thu Aug 24 10:21:30 2023\n</code></pre> Now you can create your own projects using the Jetson of your preference! You go and watch the Github Repository from NVIDIA called Jetson Inference there are some tutorials for inference and real time vision or even a docker file with ROS to try your own projects.</p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_inital_setup/","title":"Jetson Initial Setup Tutorial","text":"<p>Warning     This is only if you are flashing the Jetson for the first time, because every data that you have in here will be erased completely, this is only for the first time.</p> <p>Note     The connection between the Jetson and the robot computer is by micro usb.</p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_inital_setup/#step-1","title":"Step 1","text":"<p>To begin with this tutorial you must have installed the NVIDIA SDK Manager and also have your Jetson in Force Recovery Mode, if you haven't done that you can go and see the README file and then come back.</p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_inital_setup/#step-2","title":"Step 2","text":"<p>Once the SDK Manager it\u2019s installed and already running you\u2019ll need to select the appropriate login tab.</p> <p> </p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_inital_setup/#step-3","title":"Step 3","text":"<p>Once you log in it will show you the step by step tutorial that you need to follow, we recommend that you have the latest version of jetpack installed and it depends if you want to disable the Host Machine button or the Additional SDKS button, then click the continue button.</p> <p> </p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_inital_setup/#step-4","title":"Step 4","text":"<p>Don\u2019t worry if it doesn\u2019t detect your board you can check that later in step 2-3 from the SDK Manager, in those steps you will select, download and install the components that you want to have for your Jetson, this will take around 20-40 minutes to download depending on your internet connection and around 40-45 minute to install all the components.</p> <p> </p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_inital_setup/#step-5","title":"Step 5","text":"<p>During the process of the installation the SDK Manager will show you up a dialog when it's ready to flash your Jetson, you\u2019ll need to select the appropriate board, in our case the Jetson AGX Orin, but if you have another one like the AGX Xavier like the image from below it\u2019s also fine!</p> <p> </p> <p>You can see that there are 2 options, the automatic setup or the manual setup, we would recommend to use the manual setup and configure by your own the OEM Configuration and the Storage Device, if you have your board connected but the SDK Manager doesn\u2019t detect it, at least for the Jetson Orin AGX, click the refresh button but if this doesn\u2019t work you can watch the troubleshooting section below.</p> <p>Once you have done all of this and everything is installed then you are finished, you can start to configure the Jetson, with a monitor is easier! Now you can check how to run docker with ROS2 in your Jetson and start your own projects for deep learning. </p> <p>If you want tu shutdown your Jetson you could input the command <code>sudo shutdown -h now</code> in your Jetson terminal, also you can do it via GUI but sometimes the Jetson's fan may still be working, so to avoid this it's better to do it the CLI way.</p>"},{"location":"stretch-tool-share/tool_share/jetson_orin_agx_mount/tutorials/jetson_inital_setup/#troubleshooting","title":"Troubleshooting","text":"<p>Note     To check the hardware layout of the Jetson AGX Orin click here.</p> <p>If the SDK Manager isn\u2019t detecting your board, at least with the AGX Orin, you\u2019ll need to do the following steps: 1. Be sure that your Jetson is in Force Recovery mode 2. To be sure open a terminal (Ctrl + Alt + T) and write down lsusb 3. To determine whether the Dev Kit is in recovery mode once you write the lsusb command you\u2019ll need to see Bus  Device : ID 0955:  Nvidia Corp. Which  and  is any 3 digit number and  it\u2019s a four digit number that represents the type of your Jetson module, for more information read the NVIDIA documentation all the way down. 4. If your  doesn\u2019t match the correct module name with the Jetson AGX Orin, for example instead of 7023 is 7045, you\u2019ll need to unplug the micro USB and press the Force Recovery Button ( 2 ) + Reset Button( 3 ), with this you\u2019ll be able to enter the Force Recovery mode, and just in case keep pressing the Force Recovery button and then plug in the micro USB again. 5. Now check with the command lsusb and it should appear the correct , in the case of our Jetson AGX Orin it was 7023. 6. Now continue with the installation in your SDK Manager and you are good to go."},{"location":"stretch-tool-share/tool_share/phone_holder_V1/","title":"Phone Holder","text":""},{"location":"stretch-tool-share/tool_share/phone_holder_V1/#phone-holder","title":"Phone Holder","text":"<p>Create by: Hello Robot Inc</p> <p>This tool allows you to attach a smart phone to your Stretch. It uses an off the shelf phone holder with and a photography standard 1/4-20 attachment post. </p> <p> </p>"},{"location":"stretch-tool-share/tool_share/phone_holder_V1/#parts-list","title":"Parts List","text":"Item Qty Vendor 1/4-20 x 3\" BHCS 1 McMaster-Carr 1/4-20 Nut 2 McMaster-Carr wrist_mount_spacer.STL 2 PLA 3D printer Ulanzi ST-01 Phone Tripod Mount 1 Amazon"},{"location":"stretch-tool-share/tool_share/phone_holder_V1/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D assembly</p> <p></p> <ol> <li>Install the two wrist_mount spacers onto wrist</li> <li>Install the long bolt and lock into place using the first nut. Friction will hold this in a fixed orientation.</li> <li>Install the second nut and then the phone mount, screwing onto the bolt</li> <li>Lock the phone mount into place, using the second nut as a jam nut</li> </ol> <p>If you want to adjust the phone orientation relative to the wrist yaw, loosen up the jam nut, adjust, then tighten.</p>"},{"location":"stretch-tool-share/tool_share/puller_v1/","title":"Puller","text":""},{"location":"stretch-tool-share/tool_share/puller_v1/#puller","title":"Puller","text":"<p>Create by: Hello Robot Inc</p> <p>This is a a simple 'puller' attachment for the Stretch Compliant Gripper. We've used it to pull open many common drawers, cabinet doors, and even a mini-fridge door. You can also use it to push things closed, such as drawers. You can think of it as a circular hook used to pull things or a finger used to push things.</p> <p>It attaches to the 6-32 stud on the side of the gripper. By turning the gripper sideways during manipulation, the hook can drop over the drawer handle, allowing the arm to retract and pull the door open. </p> <p> </p>"},{"location":"stretch-tool-share/tool_share/puller_v1/#parts-list","title":"Parts List","text":"Item Qty Vendor 6-32 x 0.5\" BHCS 1 McMaster-Carr 6-32 x 1\" aluminum threaded standoff 1 McMaster-Carr Puller_V1.STL 1 PLA 3D printer"},{"location":"stretch-tool-share/tool_share/puller_v1/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D assembly</p> <p></p> <ol> <li>Screw the standoff on to the gripper's threaded post. Secure tightly and add a drop of light duty Loctite if desired.</li> <li>Attach the plastic pull to the standoff using the BHCS. </li> </ol>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/","title":"ReactorX Wrist","text":""},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#reactor-x-wrist","title":"Reactor X Wrist","text":"<p>Created by: Hello Robot Inc</p> <p>Here we describe how to modify an Interbotx ReactorX 150 Dynamixel arm to add a Pitch-Roll wrist plus parallel jaw gripper to Stretch, giving you a Yaw-Pitch-Roll wrist!</p> <p>The ReactorX 150 comes standard with Dynamixel XM430-W350-T servos for the shoulder joints and XL430-W250-T servos for its wrist and gripper. We're going to disassemble the arm and then rebuild it so that the Pitch and Roll DOF use the stronger XM430-W350-T servos.</p> <p>View the 3D assembly</p> <p></p> <p></p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#parts-list","title":"Parts List","text":"Item Qty Vendor ReactorX 150 Robot Arm 1 Trossen Robotics Robotis FR12-S101K Frame 1 Robotis M2x4mm SHCS 10 McMaster Carr"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#wrist-build","title":"Wrist Build","text":"<p>Start with a ReactorX 150</p> <p></p> <p>Detach the the wrist unit from the arm. This includes the final pitch / roll /gripper DOFs with the XL430 servos.</p> <p></p> <p>Remove the XM430 servos from the shoulder of the arm. The XM430s have the metal horns as shown.</p> <p></p> <p>No replace the pitch and roll servos with the XM430s. Pay attention to how the cables route and which ports they plug into so you can recable the wrist correctly. </p> <p>To attach the two side plates to the pitch servo (above, right), you'll want to use the shorter M2x4mm screws. The ones that came off of the XL430s are too long and will cause the servo to bind up if used. </p> <p></p> <p>Recable the servos and attach the pitch and roll joints to the gripper. You're done!</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#other-modifications","title":"Other Modifications","text":"<p>The FR12-S101K frame doesn't allow a X-Series TTL connector to pass through by default. You will need to drill out one of the 8mm holes on a drill press to 10mm. This will allow the cable routing to go through the center of Wrist Yaw rotation (recommended)</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#dynamixel-configuration","title":"Dynamixel Configuration","text":"<p>The Dynamixel servos that come with the Reactor arm need to be reprogrammed with the correct IDs and baudrate. Out of the box, the Reactor servos use a baudrate of 1Mbps while Stretch requires 115200Kbps. To reconfigure the servos:</p> <ol> <li>Download and install the Dynamixel Wizard 2.0 from Robotis</li> <li>Attach your 3 DOF gripper-wrist to a PC using a U2D2 adapter from Robotis</li> <li>Launch the Wizard and click Scan. It should identify the three servos.</li> <li>Select each servo from the left hand drop down menu and set its baudrate and ID. Hit save.</li> </ol> <p>By convention we use Dynamixel IDs of:</p> <ul> <li>Wrist PITCH: 14</li> <li>Wrist ROLL: 15</li> <li>Gripper: 16</li> </ul> <p></p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#assembly-instructions","title":"Assembly instructions","text":"<p>Now attach your wrist to the Stretch tool plate using the Robotis Frame and their provided hardware, as shown above.</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#code","title":"Code","text":"<p>You're ready to start using your wrist. Once it is plugged into the robot you will want to:</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#install-stretch-tool-share","title":"Install Stretch Tool Share","text":"<pre><code>&gt;&gt;$ pip install -U hello-robot-stretch-tool-share\n</code></pre>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#backup-user-yaml","title":"Backup User YAML","text":"<pre><code>$ cd $HELLO_FLEET_PATH/$HELLO_FLEET_ID\n$ cp stretch_user_params.yaml stretch_user_params.yaml.bak\n</code></pre>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#update-user-yaml","title":"Update User YAML","text":"<pre><code>   robot:\n     tool: tool_reactor_wrist\n     params: ['stretch_tool_share.reactorx_wrist_v1.params']\n   lift:\n     i_feedforward: 0.75\n   hello-motor-lift:\n     gains:\n       i_safety_feedforward: 0.75\n</code></pre>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#set-the-baud-rate","title":"Set the Baud Rate","text":"<p>The default YAML for the Reactor wrist is set to 115200. Your robot may be running at 57600. </p> <p>You can check the current baud settings by:</p> <pre><code>&gt;&gt;$ stretch_params.py | grep baud\nstretch_body.robot_params.nominal_params   param.end_of_arm.baud                                                  115200                        \nstretch_body.robot_params.nominal_params   param.head.baud                                                        115200                        \nstretch_body.robot_params.nominal_params   param.head_pan.baud                                                    115200                        \nstretch_body.robot_params.nominal_params   param.head_tilt.baud                                                   115200                        \nstretch_body.robot_params.nominal_params   param.stretch_gripper.baud                                             115200                        \nstretch_body.robot_params.nominal_params   param.tool_none.baud                                                   115200                        \nstretch_body.robot_params.nominal_params   param.tool_stretch_gripper.baud                                        115200                        \nstretch_body.robot_params.nominal_params   param.wrist_yaw.baud                                                   115200  \n</code></pre> <p>You can change the baud for a joint (eg ID 15)</p> <pre><code>&gt;&gt;$ REx_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 15 115200\n</code></pre> <p>Ensure that all baud rates are at 115200 for the end-of-arm (IDs 13, 14, 15, 16)</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#jog-the-wrist","title":"Jog the Wrist","text":"<p>Try out the jog tool</p> <pre><code>&gt;&gt;$ reactor_wrist_jog.py\n</code></pre>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#important-notes","title":"Important Notes","text":"<p>With the release of the Stretch Dex Wrist, the Reactor Wrist build is not being actively supported by Hello Robot. Parameters describing the range of motion of the wrist pitch, roll, and gripper were most recently updated by Kavya Puthuveetil from RCHI Lab @ CMU for an independent research project using the Reactor X Wrist in September 2022. As a result, these values may not match the convention used for other end of arm tools in Stretch Tool Share created by Hello Robot. We provide visualizations of the updated range of motion and zero position of each joint here for your reference.</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#wrist-pitch","title":"Wrist Pitch","text":"<p> Pitch Angle = 0 \u2192 Pitch Angle = \u03c0/2</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#wrist-roll","title":"Wrist Roll","text":"<p> Roll Angle = 0 \u2192 Roll Angle = \u03c0</p> <p> Roll Angle = 0 \u2192 Roll Angle = -\u03c0</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#gripper","title":"Gripper","text":"<p> Gripper Angle = \u03c0/2 \u2192 Gripper Angle = 0</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#example-all-joints-zeroed","title":"Example: All Joints Zeroed","text":"<p> Pitch, Roll, Gripper Angle = 0</p>"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#modifying-reactor-x-wrist-parameters","title":"Modifying Reactor X Wrist Parameters","text":"<p>To customize the range of motion or zero position of any of the gripper joints for your own application, you can modify the 'range_t' and 'zero_t' parameters, respectively, defined in <code>stretch_tool_share/python/stretch_tool_share/reactorx_wrist_v1/params.py</code>.</p> <p>Note that your changes will not be applied unless you override the system version of the Python package <code>hello-robot-stretch-tool-share</code> with a local install. To do this, execute the file <code>stretch_tool_share/python/local_install.sh</code> by navigating to the <code>stretch_tool_share/python</code> directory and running:</p> <pre><code>./ local_install.sh\n</code></pre>"},{"location":"stretch-tool-share/tool_share/stretch_2_STEP/","title":"Index","text":""},{"location":"stretch-tool-share/tool_share/stretch_2_STEP/#stretch-2-model","title":"Stretch 2 Model","text":"<p>Created by: Hello Robot Inc</p> <p>This is a STEP model of the Stretch 2 robot (Nina batch). It includes a poseable SolidWorks SLDASM of the mated STEP files for both the standard Stretch 2 and the DexWrist Stretch 2 . It can assist the design of your own tools for your Stretch. </p> <p></p> <p></p> Files CAD"},{"location":"stretch-tool-share/tool_share/stretch_RE1_arm/","title":"Stretch RE1 Arm","text":""},{"location":"stretch-tool-share/tool_share/stretch_RE1_arm/#stretch-re1-arm-model","title":"Stretch RE1 Arm Model","text":"<p>Created by: Hello Robot Inc</p> <p>This is an STL model of the Stretch RE1 arm (retracted) including the end-of-arm interface. It can assist the design of your own tools for your Stretch. </p> <p></p> Files Stretch_RE1_Arm.STL"},{"location":"stretch-tool-share/tool_share/stretch_RE1_head/","title":"Stretch RE1 Head","text":""},{"location":"stretch-tool-share/tool_share/stretch_RE1_head/#stretch-re1-head-model","title":"Stretch RE1 Head Model","text":"<p>Created by: Hello Robot Inc</p> <p>These are STL models of the Stretch RE1 head shells. They can assist the design of your own tools to attach to Stretch's head. These shells can be printed in PLA using an FDM printer. The heat set inserts use McMaster Carr part number 94180A321.</p> <p> </p> Files Stretch_RE1_Head_Left.STL Stretch_RE1_Head_Right.STL Stretch_RE1_Head_Camera.STL"},{"location":"stretch-tool-share/tool_share/stretch_dex_wrist/","title":"Stretch Dex Wrist","text":""},{"location":"stretch-tool-share/tool_share/stretch_dex_wrist/#stretch-dex-wrist","title":"Stretch Dex Wrist","text":"<p>Created by: Hello Robot Inc</p> <p>The Stretch Dex Wrist is commercially available from Hello Robot. The following hardware guides are available:</p> <ul> <li>RE1 DexWrist Hardware Guide</li> <li>RE2 DexWrist Hardware Guide</li> </ul> <p>Additional resources available here include:</p> <ul> <li>Gazebo Support</li> <li>URDF</li> </ul>"},{"location":"stretch-tool-share/tool_share/stretch_dex_wrist/gazebo_support/","title":"Index","text":""},{"location":"stretch-tool-share/tool_share/stretch_dex_wrist/gazebo_support/#stretch-dex-wrist-gazebo-support","title":"Stretch Dex Wrist - Gazebo Support","text":"<p>Created by: Hello Robot Inc</p> <p>The Stretch RE1 robot with the Dex Wrist can also be simulated with Gazebo simulator. The information on Stretch robot's Gazebo implementation can be found here stretch_gazebo.</p> <p>To add the Dex Wrist to Stretch with Gazebo support: <pre><code>cd ~/catkin_ws/src/stretch_ros/\ngit pull\n\ncd ~/repos\ngit clone https://github.com/hello-robot/stretch_tool_share\n\ncd stretch_tool_share/tool_share/stretch_dex_wrist\ncp stretch_description/urdf/stretch_dex_wrist.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf\ncp stretch_description/urdf/stretch_description.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf\ncp stretch_description/meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes\n\ncp gazebo_support/stretch_gazebo.urdf.xacro ~/catkin_ws/src/stretch_ros/stretch_gazebo/urdf\ncp gazebo_support/dex_wrist.yaml ~/catkin_ws/src/stretch_ros/stretch_gazebo/config\ncp gazebo_support/gazebo.launch ~/catkin_ws/src/stretch_ros/stretch_gazebo/launch\n</code></pre> During the Gazebo simulation with Dex Wrist, the wrist's Roll and Pitch can be controlled using the spawned \"stretch_dex_wrist_controller\" of type position_controllers/JointTrajectoryController </p> <p>Note: Still there is no planned support to run MoveIt with Stretch Dex Wrist in Gazebo.</p>"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/","title":"Stretch Docking Station","text":""},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#stretch-docking-station","title":"Stretch Docking Station","text":"<p>Created by: Hello Robot Inc</p>"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#overview","title":"Overview","text":"<p>The Stretch Docking Station provides automated charging of Stretch. A 110mm Aruco tag on the docking station allows the robot to accomplish visually guided docking. </p>"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#hardware","title":"Hardware","text":"<p>Coming soon.</p>"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#assembly","title":"Assembly","text":"<p>Coming soon.</p>"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#software","title":"Software","text":"<p>Coming soon.</p>"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#ros-support","title":"ROS Support","text":""},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#urdf","title":"URDF","text":"<p>The Stretch Docking Station,  can be included in Gazebo and RViz. The URDF data is available on the Stretch Tool Share.</p> <pre><code>x\n</code></pre>"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#gazebo","title":"Gazebo","text":"<p>Coming soon.</p>"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/","title":"Stretch Teleop Kit","text":""},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#stretch-teleop-kit","title":"Stretch Teleop Kit","text":"<p>Created by: Hello Robot Inc</p>"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#overview","title":"Overview","text":"<p>The Stretch Teleop Kit allows for improved remote teleoperation of Stretch. It adds two fish-eye USB cameras to Stretch. One is added to the robot's gripper for a better view while manipulating. The other is added to the robot's head and points downward, providing a better view while navigating. </p>"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#hardware","title":"Hardware","text":"<p>The Stretch Teleop Kit uses two Spinel UC20MPE_F185 USB cameras that provide a 185 degree FOV and 2MP resolution. These board cameras are mounted in 3D printed shells and attached to existing mount points of the Stretch. </p>"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#assembly","title":"Assembly","text":"<p>Hello Robot has provided the STL files, BOM, and assembly instructions necessary to build your own Stretch Teleop Kit. Alternatively, the kit is available for sale by Hello Robot. </p> <p>*Note if building your own kit:: The Spinel cameras come with a custom USB to JST ZH cable. We recommend using a custom length cable however in order to improve the cable routing of your system. Please contact Hello Robot for details. </p> 3D Printed Parts 3DP-808_Teleop_Camera_Mount_Front.STL 3DP-809_Head_Teleop_Mount_Back.STL 3DP-810_Head_Teleop_Mount.STL 3DP-811_Gripper_Teleop_Mount_Back.STL 3DP-812_Gripper_Teleop_Mount_Bottom.STL"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#software","title":"Software","text":""},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#teleoperation-interface","title":"Teleoperation Interface","text":"<p>The Stretch Teleop Kit includes the open-source teleoperation interface (beta) that utilizes these cameras as shown below. </p> <p></p>"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#ros-support","title":"ROS Support","text":""},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#urdf","title":"URDF","text":"<p>The Stretch RE1 URDF can be augmented with these two cameras as well. The URDF information is found here.</p> <p>To add the Teleop Kit to your URDF:</p> <pre><code>cd ~/catkin_ws/src/stretch_ros/\ngit pull\n\ncd ~/repos\ngit clone https://github.com/hello-robot/stretch_tool_share\n\ncd ~/repos/stretch_tool_share/tool_share/stretch_teleop_kit/stretch_description\ncp urdf/stretch_teleop_kit.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf\ncp urdf/stretch_description.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf\ncp meshes/*teleop*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes\n</code></pre>"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#gazebo","title":"Gazebo","text":"<p>The Stretch RE1 robot with the Teleop Kit can also be simulated with Gazebo simulator. The information on Stretch robot's Gazebo implementation can be found here stretch_gazebo.</p> <p>To add Teleop kit to the Stretch Gazebo implementation:</p> <pre><code>cd ~/catkin_ws/src/stretch_ros/\ngit pull\n\ncd ~/repos\ngit clone https://github.com/hello-robot/stretch_tool_share\n\ncd ~/repos/stretch_tool_share/tool_share/stretch_teleop_kit/stretch_description\ncp urdf/stretch_teleop_kit.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf\ncp urdf/stretch_gazebo.urdf.xacro ~/catkin_ws/src/stretch_ros/stretch_gazebo/urdf/stretch_gazebo.urdf.xacro\ncp meshes/*teleop*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes\n</code></pre> <p>During Gazebo simulation the two Teleop camera's video streams would be published to the topics \"teleop/gripper_camera\" and \"teleop/head_camera\".</p> <p></p>"},{"location":"stretch-tool-share/tool_share/swiffer_mount_V1/","title":"Swiffer Mount","text":""},{"location":"stretch-tool-share/tool_share/swiffer_mount_V1/#swiffer-mount","title":"Swiffer Mount","text":"<p>Created by: Hello Robot Inc</p> <p>This tool allows a Swiffer duster to your Stretch. The designs uses a clamp to hold the Swiffer handle in place, allowing the height and orientation of the Swiffer to be adjusted by loosening the clamp.</p> <p></p>"},{"location":"stretch-tool-share/tool_share/swiffer_mount_V1/#parts-list","title":"Parts List","text":"Item Qty Vendor M5x50mm BHCS 3 McMaster-Carr M5 nut 4 McMaster-Carr swiffer_mount.STL 1 PLA 3D printer swiffer_mount_clamp.STL 1 PLA 3D printer wrist_end_cap_5mm.STL 1 PLA 3D printer Swiffer Sweeper Cleaner Dry and Wet Mop Kit 1 Amazon"},{"location":"stretch-tool-share/tool_share/swiffer_mount_V1/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D Assembly</p> <p></p> <p></p> <ol> <li>Attach the M5 bolt through the swiffer_mount_primary part and lock into place by tightening the M5 nut</li> <li>Attach  swiffer_mount_primary to the tool plate using the wrist_end_cap_5mm and a second M5 nut. Friction holds the assembly in place against the tool plate.</li> <li>Attach the swiffer_mount_clamp part using two M5 bolts and nuts, clamping the shaft of the Swiffer handle in place. Adjust the length and orientation of the Swiffer and clamp in place.</li> <li>Clean away!</li> </ol>"},{"location":"stretch-tool-share/tool_share/swivel_tablet_mount/","title":"Index","text":""},{"location":"stretch-tool-share/tool_share/swivel_tablet_mount/#swivel-tablet-mount","title":"Swivel Tablet Mount","text":"<p>Created by: Hello Robot Inc</p> <p>This tool allows a tablet to be attached to the Stretch wrist. The design uses an off-the-shelf swivel phone holder, two 3D printed adapter plates, and a die-cut double stick tape..</p> <p></p> <p></p>"},{"location":"stretch-tool-share/tool_share/swivel_tablet_mount/#parts-list","title":"Parts List","text":"Item Qty Vendor M3x10mm SHCS 2 McMaster-Carr Double stick mounting tape 2 3/8\" 1 Amazon swivel_adapter_top.STL 1 PLA 3D printer swivel_adapter_bottom.STL 1 PLA 3D printer tablet_tape_adapter.STL 1 PLA 3D printer Magnetic car phone holder 1 Amazon"},{"location":"stretch-tool-share/tool_share/swivel_tablet_mount/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D Assembly</p> <p></p> <ol> <li>Remove the rubber cover from the phone holder. You'll see metal plate holding the magnets. Remove the metal plate with a screw driver.</li> <li>Attach the 'tablet_tape_adapter' to the phone holder, reusing the screw you just remove. Note: If the adapter does not sit flat you may want to clip the plastic 'L' tabs from the holder using side cutters.</li> <li>Unscrew the large nut on the phone holder and disengage the ball from its socket.</li> <li>Remove the red sticker from the phone holder, exposing the double stick tape. Stick the phone holder firmly onto the 'swivel adapter'</li> <li>Attach the swivel adapter to the top of the Stretch wrist using the M3 bolts</li> <li>Attach the tablet_tape_adapter to your tablet in the desired position using the 'double stick mounting tape'</li> <li>Finally, insert the ball back into the socket of the phone holder. Attach firmly using the large nut.</li> </ol>"},{"location":"stretch-tool-share/tool_share/tray_cup_holder_V1/","title":"Tray Cup Holder","text":""},{"location":"stretch-tool-share/tool_share/tray_cup_holder_V1/#tray-cup-holder","title":"Tray Cup Holder","text":"<p>Created by: Hello Robot Inc</p> <p>This is a tray and cup holder attachment for your Stretch.  The tray stows nicely within the robot footprint during navigation and can un-stow upon delivery to a person.</p> <p>The 3D printed attachment clips easily onto the bottom of an off the shelf tray. </p> <p> </p> <p></p>"},{"location":"stretch-tool-share/tool_share/tray_cup_holder_V1/#parts-list","title":"Parts List","text":"Item Qty Vendor M5 Nut 2 McMaster-Carr M5x50mm Hex Head Bolt 1 McMaster-Carr wrist_end_cap_5mm.STL 1 PLA 3D printer tray_clip.STL 1 PLA 3D printer AUTUT Universal Car Cup Holder 1 Amazon"},{"location":"stretch-tool-share/tool_share/tray_cup_holder_V1/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D assembly</p> <p></p> <ol> <li>Install the M5 bolt into the tray clip and secure it rigidly to the part using the M5 nut.</li> <li>Clip the tray onto the tray clip.</li> <li>Drop the tray clip assembly on to the tool plate from above. Secure it to the tool plate using the wrist_end_cap_5mm and M5 nut. </li> </ol>"},{"location":"stretch-tool-share/tool_share/wrist_USB_board_camera/","title":"Wrist USB Camera","text":""},{"location":"stretch-tool-share/tool_share/wrist_USB_board_camera/#wrist-usb-board-camera","title":"Wrist USB Board Camera","text":"<p>Create by: Hello Robot Inc</p> <p>This design allows you to attach a USB board camera to the wrist yaw joint of Stretch. The wrist mounted camera can pan left-right, making it perfect for simple remote inspection tasks.</p> <p>The camera can be integrated into your robot URDF by integrating its stretch_description as described in the Stretch ROS documentation.</p> <p> </p>"},{"location":"stretch-tool-share/tool_share/wrist_USB_board_camera/#parts-list","title":"Parts List","text":"Item Qty Vendor M2x8mm SHCS 8 McMaster-Carr M2x6mm Thread Forming Screw 5 McMaster-Carr Board_Camera_Ball_Shell.STL 1 PLA 3D printer Board_Camera_Ball_Cover.STL 1 PLA 3D printer ELP 2MP USB Board Camera (Suggested) 1 Amazon / Spinel"},{"location":"stretch-tool-share/tool_share/wrist_USB_board_camera/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D assembly</p> <p></p> <ol> <li>Attach the Board_Camera_Ball_Shell to the wrist tool plate using the 8 M2 bolts</li> <li>Attach the camera to the Board_Camera_Ball_Shell using 4 self-threading screws</li> <li>Attach the USB cable and route it down and through the wrist yaw passage</li> <li>Attach the Board_Camera_Ball_Cover using 1 self-threading screw</li> <li>Plug the USB cable into the USB-A port on the wrist</li> </ol> <p>NOTE: The USB cable that comes with the camera uses a JST-PH connector. You may want to make your own cable with a custom length using a right-angle USB-A cable. This may require a JST-PH crimp tool.</p> <p>NOTE: A variety of USB board cameras are available from ELP and Spinel, among others. The shells may need to be modified to accommodate variations in the mechanical packaging of these cameras.</p>"},{"location":"stretch-tool-share/tool_share_SE3/eoa_wrist_dw3_tool_tablet_12in/","title":"Index","text":"<p>To install:</p> <p>Add the following to your user yaml:</p> <p>robot:   tool: eoa_wrist_dw3_tool_tablet_12in</p> <p>params:   - stretch_tool_share_se3.eoa_wrist_dw3_tool_tablet_12in.params</p> <p>Copy mesh and xacro data to ~/repos/stretch_urdf/stretch_urdf/ <p>Make a new top level xacro under stretch_urdf, eg:</p> <p>stretch_description_SE3_eoa_wrist_dw3_tool_tablet_12in.xacro</p> <p> <p> </p> <p>Then run ~/repos/stretch_urdf/urdf_generate.py to create a new URDF that matches your tool Then copy the URDF and meshes to </p> <p>pkg = str(importlib_resources.files(\"stretch_urdf\"))  # .local/lib/python3.10/site-packages/stretch_urdf)</p> <p>Then copy the URDF and meshes for ROS2...</p> <p>Note: Need to copy mesh and urdf files to the PKG install place  1298  cd ~/repos/stretch_tool_share/tool_share_SE3/eoa_wrist_dw3_tool_tablet_12in  1300  cp meshes/*.STL ~/repos/stretch_urdf/stretch_urdf/SE3/meshes/  1301  cp xacro/stretch_tool_tablet_12in.xacro ~/repos/stretch_urdf/stretch_urdf/SE3/xacro/</p> <p>cp stretch_urdf/SE3/.urdf ~/.local/lib/python3.10/site-packages/stretch_urdf/SE3/  1319  ./stretch_urdf/tools/stretch_urdf_viz.py   1320  cp stretch_urdf/SE3/meshes/tablet* ~/.local/lib/python3.10/site-packages/stretch_urdf/SE3/meshes/  1321  ./stretch_urdf/tools/stretch_urdf_viz.py   1322  ls  1323  cd stretch_urdf/  1324  pip3 install -e .  1329  pip3 list | grep stretch</p> <p>Verify that new tool shows up here  1330  ./stretch_urdf/tools/stretch_urdf_viz.py </p>"},{"location":"stretch-tutorials/","title":"Overview","text":""},{"location":"stretch-tutorials/#overview","title":"Overview","text":"<p> The Stretch Tutorials repository provides tutorials on programming Stretch robots. The tutorials are organized into the following tracks.</p> Tutorial Track Description Getting to Know Stretch Everything a new user of Stretch needs to get started Stretch Body Python SDK Learn how to program Stretch using its low level Python interface ROS Learn how to program Stretch using its ROS interface ROS 2 (Beta) Learn how to program Stretch using its ROS2 interface Stretch Tool Share Learn how to update the end of arm tool hardware  All materials are Copyright 2022-2024 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/tutorial_template/","title":"Motivation","text":"<p>The aim of this document is to be the starting point for generating new ROS tutorials and to maintain consistency in structure and tone across the documentation. It starts off by formalizing the key objectives and then goes on to templatize the structure and tone that we should adopt going forward.</p>"},{"location":"stretch-tutorials/tutorial_template/#objectives","title":"Objectives","text":"<ol> <li>Be inclusive of all levels of understanding - start from the basics and link relevant external content to be concise yet informative</li> <li>Reinforce that getting started with Stretch is a breeze - use easy to understand vocab and a friendly tone to encourage readers</li> <li>Encourage users to read other tutorials - wherever possible, link other tutorials from the documentation to convey completeness and complexity</li> <li>Have a clear flow - start with the theory, show with GIFs what to expect, and then breakdown the code</li> </ol> <p>What follows can be considered the template.</p>"},{"location":"stretch-tutorials/tutorial_template/#major-topic","title":"Major Topic","text":"<p>In this tutorial, we will work with Stretch to explore the main theme of tutorial using primary module and also learn how to achieve secondory theme. If you want to know more about any previously covered topic on Stretch and how to get it up and running, we recommend visiting the previous tutorials on link to topic and link to topic.</p> <p>Motivation for the problem the topic solves. The great thing about Stretch is that it comes preloaded with software that makes it a breeze to achieve theme of the tutorial.</p> <p>By the end of this tutorial, we will have a clear idea about how first minor topic works and how to use it to achieve second minor topic with Stretch. Let's jump in!</p>"},{"location":"stretch-tutorials/tutorial_template/#first-minor-topic-title","title":"First Minor Topic Title","text":"<p>PyTorch is an open source end-to-end machine learning framework that makes many pretrained production quality neural networks available for general use. In this tutorial we will use the YOLOv5s model trained on the COCO dataset.</p> <p>YOLOv5 is a popular object detection model that divides a supplied image into a grid and detects objects in each cell of the grid recursively. The YOLOv5s model that we have deployed on Stretch has been pretrained on the COCO dataset which allows Stretch to detect a wide range of day to day objects. However, that\u2019s not all, in this demo we want to go a step further and use this extremely versatile object detection model to extract useful information about the scene.</p>"},{"location":"stretch-tutorials/tutorial_template/#second-minor-topic-title","title":"Second Minor Topic Title","text":"<p>Now, let\u2019s use what we have learned so far to upgrade the collision avoidance demo in a way that Stretch is able to scan an entire room autonomously without bumping into things or people. To account for dynamic obstacles getting too close to the robot, we will define a keepout distance of 0.4 m - detections below this value stop the robot. To keep Stretch from getting too close to static obstacles, we will define another variable called turning distance of 0.75 m - frontal detections below this value make Stretch turn to the left until it sees a clear path ahead.</p> <p>Building up on this, let's implement a simple logic for obstacle avoidance. The logic can be broken down into three steps: 1. If the minimum value from the frontal scans is greater than 0.75 m, then continue to move forward 2. If the minimum value from the frontal scans is less than 0.75 m, then turn to the right until this is no longer true 3. If the minimum value from the overall scans is less than 0.4 m, then stop the robot</p>"},{"location":"stretch-tutorials/tutorial_template/#third-minor-topic-title","title":"Third Minor Topic Title","text":"<p>If a tutorial covers more than two minor topics, it might be a good idea to break it down into multiple tutorials</p>"},{"location":"stretch-tutorials/tutorial_template/#warning","title":"Warning","text":"<p>Running this tutorial on Stretch might result in harm to robot, humans or the surrounding environment. Please ensure these conditions. We recommend taking these actions to ensure safe operation.</p>"},{"location":"stretch-tutorials/tutorial_template/#see-it-in-action","title":"See It In Action","text":"<p>Go ahead and execute the following commands to run the demo and visualize the result in RViz: Terminal 1: <pre><code>Enter first command here\n</code></pre></p> <p>Terminal 2: <pre><code>Enter second command here\n</code></pre></p> <p>Enter GIF to show robot behavior</p> <p> </p> <p>Enter GIF to show robot and sensor visualization in RViz</p> <p> </p>"},{"location":"stretch-tutorials/tutorial_template/#code-breakdown","title":"Code Breakdown","text":"<p>Now, let's jump into the code to see how things work under the hood. Follow along link to code to have a look at the entire script.</p> <p>We make use of two separate Python classes for this demo. The FrameListener class is derived from the Node class and is the place where we compute the TF transformations. For an explantion of this class, you can refer to the TF listener tutorial. <pre><code>class FrameListener(Node):\n</code></pre></p> <p>The AlignToAruco class is where we command Stretch to the pose goal. This class is derived from the FrameListener class so that they can both share the node instance. <pre><code>class AlignToAruco(FrameListener):\n</code></pre></p> <p>The constructor initializes the Joint trajectory action client. It also initializes the attribute called offset that determines the end distance between the marker and the robot. <pre><code>    def __init__(self, node, offset=0.75):\n        self.trans_base = TransformStamped()\n        self.trans_camera = TransformStamped()\n        self.joint_state = JointState()\n        self.offset = offset\n        self.node = node\n\n        self.trajectory_client = ActionClient(self.node, FollowJointTrajectory, '/stretch_controller/follow_joint_trajectory')\n        server_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\n        if not server_reached:\n            self.node.get_logger().error('Unable to connect to arm action server. Timeout exceeded.')\n            sys.exit()\n</code></pre></p> <p>The apply_to_image() method passes the stream of RGB images from the realsense camera to the YOLOv5s model and returns detections in the form of a dictionary consisting of class_id, label, confidence and bouding box coordinates. The last part is exactly what we need for further computations. <pre><code>    def apply_to_image(self, rgb_image, draw_output=False):\n        results = self.model(rgb_image)\n\n        ...\n\n        if draw_output:\n            output_image = rgb_image.copy()\n            for detection_dict in results:\n                self.draw_detection(output_image, detection_dict)\n\n        return results, output_image\n</code></pre></p> <p>Motivate users to play with the code or continue exploring more topics. Now go ahead and experiment with a few more pretrained models using PyTorch or OpenVINO on Stretch. If you are feeling extra motivated, try creating your own neural networks and training them. Stretch is ready to deploy them!</p>"},{"location":"stretch-tutorials/getting_started/","title":"Overview","text":""},{"location":"stretch-tutorials/getting_started/#tutorial-track-getting-to-know-stretch","title":"Tutorial Track: Getting to Know Stretch","text":"<p>New to Stretch? Welcome!</p> <p>Please take the time to get to know your robot by going through these tutorials in order.</p>"},{"location":"stretch-tutorials/getting_started/#what-version-of-robot-do-i-have","title":"What Version of Robot Do I Have?","text":"<p>Stretch RE1 and Stretch 2 are very similar. One quick way to tell the difference is to look at the robot's hostname:</p> <p><pre><code>hostname\n</code></pre> <pre><code>--- OUTPUT ---\nstretch-re2-2001\n</code></pre></p> <p>Another way is to look for the distinctive  pink stripe on the base of Stretch 2:</p> <p></p>"},{"location":"stretch-tutorials/getting_started/#basics","title":"Basics","text":"Tutorial Description 1 Safety Guide Guide to safe operation of the Stretch 2 Quick Start RE1 Unboxing Stretch RE1 and getting started 2 Quick Start Stretch 2 Unboxing Stretch 2 and getting started 3 Best Practices Best practices to keep Stretch charged and healthy 4 Troubleshooting Solutions to common issues"},{"location":"stretch-tutorials/getting_started/#advanced","title":"Advanced","text":"Tutorial Description 1 Untethered Operation Setting up your network to work with Stretch untethered 2 Updating Software How to periodically update the Stretch software  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/getting_started/best_practices/","title":"Stretch Best Practices","text":""},{"location":"stretch-tutorials/getting_started/best_practices/#keeping-the-robot-charged","title":"Keeping the Robot Charged","text":"<p>Keeping Stretch charged is important to the long-term health of its batteries  - and to the success of any project you develop on Stretch. Please read carefully the following guides and adopt their best-practices for battery health.</p> Stretch RE1Stretch 2 <p>Stretch RE1 Battery Maintenance Guide</p> <p>Stretch 2 Battery Maintenance Guide</p>"},{"location":"stretch-tutorials/getting_started/best_practices/#keeping-the-robot-healthy","title":"Keeping the Robot Healthy","text":"<p>While Stretch can tolerate a fair amount of misuse, the following best practices can help it achieve a long and healthy life.</p> Video Description Stretch Best Practices - Powered Off Video How to work with Stretch when its power is off Stretch Best Practices - Powered On Video How to work with Stretch when its power is on  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/getting_started/command_line_tools/","title":"Command Line Tools","text":"<p>The Stretch robot comes with a set of command line tools that are helpful for introspection during general use or while troubleshooting issues. This page provides an overview of these tools. If you like, visit the stretch_body repository to have a look under the hood.</p> <p>You can execute these commands from anywhere in the terminal. We recommend you execute these commands as we follow each one of them. You can also find the description for the utility each tool provides by passing the optional '-h' flag along with the tool name in the terminal. For example, from anywhere in the terminal execute: <pre><code>stretch_about.py -h\n</code></pre></p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#system-information","title":"System Information","text":""},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_aboutpy","title":"stretch_about.py","text":"<p>This tool displays the model and serial number information as an image.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_about_textpy","title":"stretch_about_text.py","text":"<p>This tool displays the model and serial number information as text.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_versionsh","title":"stretch_version.sh","text":"<p>This script prints the version information for various software packages on the robot.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_paramspy","title":"stretch_params.py","text":"<p>This tool prints the Stretch parameters to the console.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_monitorpy","title":"stretch_robot_monitor.py","text":"<p>This tool runs the Robot Monitor and prints to the console.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_urdf_visualizerpy","title":"stretch_robot_urdf_visualizer.py","text":"<p>This tool allows you to visualize robot URDF.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#introspection","title":"Introspection","text":""},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_system_checkpy","title":"stretch_robot_system_check.py","text":"<p>This tool checks that all robot hardware is present and is reporting sane values.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_battery_checkpy","title":"stretch_robot_battery_check.py","text":"<p>This is a tool to print the battery state to the console.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_hardware_echopy","title":"stretch_hardware_echo.py","text":"<p>This tool echoes the robot and computer hardware details to the console.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_dynamixel_rebootpy","title":"stretch_robot_dynamixel_reboot.py","text":"<p>This tool reboots all Dynamixel servos on the robot.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_pimu_scopepy","title":"stretch_pimu_scope.py","text":"<p>This tool allows you to visualize Pimu (Power+IMU) board data with an oscilloscope. Pass the '-h' flag along with the command to see optional arguments.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_wacc_scopepy","title":"stretch_wacc_scope.py","text":"<p>This is a tool to visualize Wacc (Wrist+Accel) board data with an oscilloscope. Pass the '-h' flag along with the command to see optional arguments.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_realsense_visualizerpy","title":"stretch_realsense_visualizer.py","text":"<p>This is a tool to test the Realsense D435i Camera. Pass the '-h' flag along with the command to see optional arguments.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_respeaker_testpy","title":"stretch_respeaker_test.py","text":"<p>This tool allows you to record and playback audio via Respeaker.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_audio_testpy","title":"stretch_audio_test.py","text":"<p>This tool allows you to test the audio system.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#homing-joints","title":"Homing Joints","text":""},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_homepy","title":"stretch_robot_home.py","text":"<p>This tool calibrates the robot by finding zeros for all robot joints.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_gripper_homepy","title":"stretch_gripper_home.py","text":"<p>This tool calibrates the gripper position by closing until the motion stops.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_wrist_yaw_homepy","title":"stretch_wrist_yaw_home.py","text":"<p>This tool calibrates the wrist_yaw position by moving to both hardstops.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_arm_homepy","title":"stretch_arm_home.py","text":"<p>This tool calibrates arm position by moving to hardstop.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_lift_homepy","title":"stretch_lift_home.py","text":"<p>This tool calibrates the lift position by moving to the upper hardstop.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#jogging-joints","title":"Jogging Joints","text":""},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_jogpy","title":"stretch_robot_jog.py","text":"<p>This tool prints all robot data to the console.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_gripper_jogpy","title":"stretch_gripper_jog.py","text":"<p>This tool allows you to jog the griper from the keyboard.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_wrist_yaw_jogpy","title":"stretch_wrist_yaw_jog.py","text":"<p>This tool allows you to jog the wrist_yaw joint from the keyboard.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_arm_jogpy","title":"stretch_arm_jog.py","text":"<p>This tool allows you to jog the arm motion from the keyboard.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_lift_jogpy","title":"stretch_lift_jog.py","text":"<p>This tool allows you to jog the lift motion from the keyboard.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_base_jogpy","title":"stretch_base_jog.py","text":"<p>This tool allows you to jog the base motion from the keyboard.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_head_jogpy","title":"stretch_head_jog.py","text":"<p>This tool allows you to jog the head from the keyboard.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#jogging-modules","title":"Jogging Modules","text":""},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_wacc_jogpy","title":"stretch_wacc_jog.py","text":"<p>This tool allows you to command and query the Wacc (Wrist Accelerometer) board from the keyboard.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_pimu_jogpy","title":"stretch_pimu_jog.py","text":"<p>This tool allows you to command and query the Pimu (Power+IMU) board from the keyboard.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_rp_lidar_jogpy","title":"stretch_rp_lidar_jog.py","text":"<p>This is a tool to control the RP-Lidar. Pass the '-h' flag along with the command to see optional arguments.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_trajectory_jogpy","title":"stretch_trajectory_jog.py","text":"<p>This tool allows you to test out splined trajectories on the various joint from a GUI or text menu. Pass the '-h' flag along with the command to see optional arguments.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#teleoperation","title":"Teleoperation","text":""},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_stowpy","title":"stretch_robot_stow.py","text":"<p>This tool moves the robot to stow position.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_keyboard_teleoppy","title":"stretch_robot_keyboard_teleop.py","text":"<p>This tool allows you to control the robot base, lift, arm, head, and tool from the keyboard.</p>"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_xbox_controller_teleoppy","title":"stretch_xbox_controller_teleop.py","text":"<p>This tool allows you to jog the robot from an Xbox Controller.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/","title":"Stretch RE1: Quick Start Guide","text":"<p>Congratulations on your Stretch RE1! This guide will get you started with your new robot. </p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#safety","title":"Safety","text":"<p>Stretch has the potential to cause harm if not properly used. All users should review the Stretch Safety Guide before operating the robot.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#unboxing","title":"Unboxing","text":"<p>Please watch the Stretch Unboxing Video.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#robot-tour","title":"Robot Tour","text":"<p>A few items you'll want to know about before getting started.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#power","title":"Power","text":"<p>The entire robot powers up and down with the On/Off switch. When powering down, we recommend selecting 'Power Off' from the Ubuntu Desktop before hitting the Off switch</p> <p></p> <p>The provided battery charger can be plugged and unplugged at any time during operation. Stretch uses the following charger modes:</p> Mode Function STANDBY Charger not charging the robot 12V AGM Charging while robot is powered down SUPPLY 1) Power the robot during tethered use2) Repair damaged batteries. REPAIR Repair damaged batteries. <p>Please review the Battery Maintenance Guide for proper care and charging of Stretch batteries. </p> <p></p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#runstop","title":"Runstop","text":"<p>The illuminated button on the head is its Runstop. Just tap it, you'll hear a beep and it will start flashing. This will pause the motion of the primary robot joints during operation. This can be useful if the robot makes an unsafe motion, or if you just want to free up the robot motors while you roll it around.</p> <p>To allow motion once again, hold the button down for two seconds. After the beep, the motion can resume.</p> <p></p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#safe-handling","title":"Safe Handling","text":"<p>Like any robot, it is possible to break Stretch if you're not careful. Use common sense when applying forces to its joints, transporting it, etc. </p> <p>The Stretch Unpowered Best Practices Video provides a quick overview of how to work with the robot.</p> <p>Things that won't hurt the robot:</p> <ul> <li>Manually push and pull the arm (when the motor isn't holding a position).</li> <li>Manually raise and lower the lift (when the motor isn't holding a position).</li> <li>Manually tilt and roll the base around (when the motors aren't holding a position).</li> <li>Pick up and carry Stretch (while holding it by the mast, two people for safety).</li> </ul> <p>Things to be mindful of:</p> <ul> <li>Manually moving the head and wrist. They will move but they want to go at their own speed.</li> <li>The arm will slowly descend when the robot is powered off. If the arm is retracted it may rest the tool on the base. If desired to hold the arm up when un-powered, the provided 'clip-clamp' can be clipped onto the mast below the shoulder to support it. </li> </ul> <p>Things that can hurt the robot: </p> <ul> <li>Driving the wrist and gripper into the base. When the arm and wrist are stowed it is possible to collide the two.</li> <li>Getting the gripper stuck on something and then driving the arm, lift, or base. </li> <li>Laying the robot down with its weight on its camera.</li> <li>Trying to ride on the robot, getting it wet, etc. (eg, common sense)</li> </ul>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#hello-world-demo","title":"Hello World Demo","text":"<p>Stretch comes ready to run out of the box. The Xbox Teleoperation demo will let you quickly test out the robot's capabilities by teleoperating it with an Xbox Controller. </p> <p></p> <p>Note</p> <p>You will find the USB Dongle already plugged into the USB port of the base trunk.</p> <p>To start the demo after unboxing:</p> <ul> <li>Remove the 'trunk' cover and power on the robot</li> <li>Wait for about 45 seconds. You will hear the Ubuntu startup sound, followed by two beeps (indicating the demo is running). </li> <li>Hit the Connect button on the controller. The upper two LEDs of the ring will illuminate.</li> <li>Hit the Home Robot button. Stretch will go through its homing calibration routine. </li> </ul> <p>Warning</p> <p>Make sure the space around the robot is clear before running the Home function</p> <p>You're ready to go! A few things to try:</p> <ul> <li>Hit the Stow Robot button. The robot will assume the stow pose.</li> <li>Practice driving the robot around. </li> <li>Pull the Fast Base trigger while driving. When stowed, it will make Stretch drive faster</li> <li>Manually stop the arm or lift from moving to make it stop upon contact.</li> <li>Try picking up your cellphone from the floor </li> <li>Try grasping a cup from a countertop</li> <li>Try delivering an object to a person</li> </ul> <p>If you're done, hold down the Shutdown PC button for 2 seconds. This will cause the PC to turn off. You can then power down the robot. Or proceed to the next step...</p> <p>Now that you're familiar with the robot, take a minute to watch the Stretch Powered Best Practices Video.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#get-plugged-in","title":"Get Plugged In","text":"<p>Let's get plugged in.</p> <ul> <li>Remove the 'trunk' cover and power on the robot if it's not already on.</li> <li>Plug in a mouse, keyboard and HDMI monitor to the robot trunk</li> <li>Plug in the battery charger</li> <li>Place the charger in SUPPLY mode</li> </ul> <p>Log in to the robot computer. The default user credentials came in the box with the robot. </p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#start-coding","title":"Start Coding","text":"<p>Python is the easiest way to begin writing code for the robot. This section will give you a quick look at Stretch Body, which is the low-level Python interface to the robot. Detailed information on the Stretch Body Interface can be found here.</p> <p>Stretch is configured to run the Xbox Controller demo in the background at startup. To run your own code you'll need to kill this process so that it doesn't contend with your code.</p> <pre><code>pkill -f stretch_xbox*\n</code></pre> <p>While you're at it, disable this autoboot feature. You can always turn it back on later. </p> <p>Search for 'Startup' from Ubuntu Activities. Uncheck the box for 'hello_robot_xbox_teleop' </p> <p></p> <p>Now open up a Terminal. From the command line, first, verify that that all of the hardware is present and happy</p> <pre><code>stretch_robot_system_check.py\n</code></pre> <p>You may see a few joints reported in red because they haven't yet been calibrated. If so, home the robot</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>Once the robot has homed, let's write some quick test code:</p> <pre><code>ipython\n</code></pre> <p>Now let's move the robot around using the Robot API. Try typing in these interactive commands at the iPython prompt:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.stow()\n\nrobot.arm.move_to(0.25)\nrobot.push_command()\n\nrobot.arm.move_to(0.0)\nrobot.push_command()\n\nrobot.lift.move_to(0.4)\nrobot.push_command()\n\nrobot.pretty_print()\n\nrobot.lift.pretty_print()\n\nrobot.head.pose('tool')\nrobot.head.pose('ahead')\n\nrobot.end_of_arm.move_to('wrist_yaw',0)\n\nrobot.end_of_arm.move_to('stretch_gripper',50)\nrobot.end_of_arm.move_to('stretch_gripper',-50)\n\nrobot.stow()\nrobot.stop()\n</code></pre> <p>Note</p> <p>The iPython interpreter also allows you to execute blocks of code in a single go instead of running commands line by line. To end the interpreter session, type exit() and press enter.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#change-credentials","title":"Change Credentials","text":"<p>Finally, we recommend that you change the login credentials for the default user, hello-robot. </p> <pre><code>sudo passwd hello-robot\n</code></pre> <p>If you'd like to set up a new user account, check out the Stretch Installation Guide. In a lab setting, it's useful for lab members to have their own user accounts to run experiments.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#power-down","title":"Power Down","text":"<p>The recommended power-down procedure is</p> <ol> <li>Place a clamp on the mast below the shoulder to prevent dropping</li> <li>Shutdown the computer from the Desktop</li> <li>When the laser range finder has stopped spinning, turn off the main power switch</li> <li>Attach the charger</li> <li>Place the charger in 12V AGM mode</li> </ol>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#join-the-community-forum","title":"Join the Community Forum","text":"<p>Join the Hello Robot Community. We'd welcome hearing your feedback as you get to know your robot.</p> <p>Hello Robot support monitors the forum closely and will quickly get back to you on any questions or issues you post.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#further-exploration","title":"Further Exploration","text":"<p>Encounter any issues while getting started? Please let us know at support@hello-robot.com. Also, take a minute to review the Stretch Troubleshooting Guide</p> <p>We recommend next exploring the ROS-based demos that ship with Stretch.  These are found in the stretch_ros repository.</p> <p>That's it. Happy coding!</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/","title":"Stretch 2: Quick Start Guide","text":"<p>Congratulations on your Stretch 2! This guide will get you started with your new robot. </p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#safety","title":"Safety","text":"<p>Stretch has the potential to cause harm if not properly used. All users should review the Stretch Safety Guide before operating the robot.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#unboxing","title":"Unboxing","text":"<p>Please watch the Stretch Unboxing Video. Please note that the unboxing instructions for a Stretch 2 are the same as for a Stretch RE1.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#robot-tour","title":"Robot Tour","text":"<p>A few items you'll want to know about before getting started.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#power","title":"Power","text":"<p>The entire robot powers up and down with the On/Off switch. When powering down, we recommend selecting 'Power Off' from the Ubuntu Desktop prior to hitting the Off switch</p> <p></p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#runstop","title":"Runstop","text":"<p>The illuminated button on the head is its Runstop. Just tap it, you'll hear a beep and it will start flashing. This will pause the motion of the primary robot joints during operation. This can be useful if the robot makes an unsafe motion, or if you just want to free up the robot motors while you roll it around.</p> <p>To allow motion once again, hold the button down for two seconds. After the beep, the motion can resume.</p> <p></p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#led-lightbar","title":"LED Lightbar","text":"<p>The LED lightbar in the base provides a simple way to quickly ascertain the robot's state. At all times its color indicates the battery voltage. </p> <p></p> <p>More information on the voltage display is available in the Battery Maintenance Guide</p> <p>The lightbar will also flash as follows:</p> Mode Flashing Normal Operation None Runstopped Rapid flash at 1 Hz Charger plugged in Slow strobe at 0.5 Hz"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#hello-world-demo","title":"Hello World Demo","text":"<p>Stretch comes ready to run out of the box. The Xbox Teleoperation demo will let you quickly test out the robot's capabilities by teleoperating it with an Xbox Controller. </p> <p></p> <p>Note: You will find the USB Dongle already plugged into the USB port of the base trunk.</p> <p>To start the demo after unboxing and turning the power on:</p> <ul> <li>Wait for about 45 seconds. You will hear the Ubuntu startup sound, followed by two beeps (indicating the demo is running). </li> <li>Hit the Connect button on the controller. The upper two LEDs of the ring will illuminate.</li> <li>Hit the Home Robot button. Stretch will go through its homing calibration routine. </li> <li>Note: make sure the space around the robot is clear before running the Home function</li> </ul> <p>You're ready to go! A few things to try:</p> <ul> <li>Hit the Stow Robot button. The robot will assume the stow pose.</li> <li>Practice driving the robot around. </li> <li>Pull the Fast Base trigger while driving. When stowed, it will make Stretch drive faster</li> <li>Manually stop the arm or lift from moving to make it stop upon contact.</li> <li>Try picking up your cellphone from the floor </li> <li>Try grasping a cup from a countertop</li> <li>Try delivering an object to a person</li> </ul> <p>If you're done, let's power down. First, attach the clip-clamp just below the shoulder as shown. </p> <p>Hold down the Shutdown PC button on the Xbox controller for 2 seconds. This will cause the PC to turn off. You can then power down the robot with the On/Off switch. </p> <p>Now that you're familiar with the robot, take a minute to watch the Stretch Powered Best Practices Video. Please note that the best practice instructions for a Stretch 2 are the same as for a Stretch RE1.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#safe-handling","title":"Safe Handling","text":"<p>Like any robot, it is possible to break Stretch if you're not careful. Use common sense when applying forces to its joints, transporting it, etc. </p> <p>The Stretch Unpowered Best Practices Video provides a quick overview of how to work with the robot. Please note that the best practice instructions for a Stretch 2 are the same as for a Stretch RE1.</p> <p>Things that won't hurt the robot:</p> <ul> <li>Manually push and pull the arm (when the motor isn't holding a position).</li> <li>Manually raise and lower the lift (when the motor isn't holding a position).</li> <li>Manually tilt and roll the base around (when the motors aren't holding a position).</li> <li>Pick up and carry Stretch (while holding it by the mast, two people for safety).</li> </ul> <p>Things to be mindful of:</p> <ul> <li>Manually moving the head and wrist. They will move but they want to go at their own speed.</li> <li>The lift will slowly descend when the robot is powered off. If the arm is retracted it may come to rest the tool on the base. If desired to hold the arm up when un-powered, the provided 'clip-clamp' can be clipped onto the mast below the shoulder to support it. </li> </ul> <p>Things that can hurt the robot: </p> <ul> <li>Driving the wrist and gripper into the base. When the arm and wrist are stowed it is possible to collide the two.</li> <li>Getting the gripper stuck on something and then driving the arm, lift, or base. </li> <li>Laying the robot down with its weight on its camera.</li> <li>Trying to ride on the robot, getting it wet, etc. (eg, common sense)</li> </ul>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#charging-the-battery","title":"Charging the Battery","text":"<p>The provided battery charger can be plugged and unplugged at any time during operation. Stretch uses the following charger modes:</p> Mode Function STANDBY Charger not charging the robot 12V AGM Charging while robot is powered down SUPPLY 1) Power the robot during tethered use2) Repair damaged batteries. REPAIR Repair damaged batteries. <p>Please review the Battery Maintenance Guide for proper care and charging of Stretch batteries. </p> <p></p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#get-plugged-in","title":"Get Plugged In","text":"<p>Let's get plugged in.</p> <ul> <li>Power up the robot</li> <li>Plug in a mouse, keyboard and HDMI monitor to the robot trunk</li> <li>Plug in the battery charger</li> <li>Place the charger in SUPPLY mode</li> </ul> <p>Log in to the robot computer. The default user credentials came in the box with the robot. </p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#start-coding","title":"Start Coding","text":"<p>Python is the easiest way to begin writing code for the robot. This section will give you a quick look at Stretch Body, which is the low-level Python interface to the robot. Detailed information on the Stretch Body Interface can be found here.</p> <p>Stretch is configured to run the Xbox Controller demo in the background at startup. To run your own code you'll need to kill this process so that it doesn't contend with your code.</p> <pre><code>pkill -f stretch_xbox*\n</code></pre> <p>While you're at it, disable this autoboot feature. You can always turn it back on later. </p> <p>Search for 'Startup' from Ubuntu Activities. Uncheck the box for 'hello_robot_xbox_teleop' </p> <p></p> <p>Now open up a Terminal. From the command line, first, verify that all of the hardware is present and happy</p> <pre><code>stretch_robot_system_check.py\n</code></pre> <p>You may see a few joints reported in red because they haven't yet been calibrated. If so, home the robot</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>Once the robot has homed, let's write some quick test code:</p> <pre><code>ipython\n</code></pre> <p>Now let's move the robot around using the Stretch Body Robot API. Try typing in these interactive commands in the iPython prompt:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.stow()\n\nrobot.arm.move_to(0.25)\nrobot.push_command()\n\nrobot.arm.move_to(0.0)\nrobot.push_command()\n\nrobot.lift.move_to(0.4)\nrobot.push_command()\n\nrobot.pretty_print()\n\nrobot.lift.pretty_print()\n\nrobot.head.pose('tool')\nrobot.head.pose('ahead')\n\nrobot.end_of_arm.move_to('wrist_yaw',0)\n\nrobot.end_of_arm.move_to('stretch_gripper',50)\nrobot.end_of_arm.move_to('stretch_gripper',-50)\n\nrobot.stow()\nrobot.stop()\n</code></pre> <p>Note</p> <p>The iPython interpreter also allows you to execute blocks of code in a single go instead of running commands line by line. To end the interpreter session, type exit() and press enter.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#change-credentials","title":"Change Credentials","text":"<p>Finally, we recommend that you change the login credentials for the default user, hello-robot. </p> <pre><code>sudo passwd hello-robot\n</code></pre> <p>If you'd like to set up a new user account, check out the Stretch Installation Guide. In a lab setting, it's useful for lab members to have their own user accounts to run experiments.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#power-down","title":"Power Down","text":"<p>The recommended power-down procedure is</p> <ol> <li>Place a clamp on the mast below the shoulder to prevent a slow drop (if this is a concern)</li> <li>Shutdown the computer from the Desktop</li> <li>When the laser range finder has stopped spinning, turn off the main power switch</li> <li>Attach the charger</li> <li>Place the charger in 12V AGM mode</li> </ol>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#join-the-community-forum","title":"Join the Community Forum","text":"<p>Join the Hello Robot Community. We'd welcome hearing your feedback as you get to know your robot.</p> <p>Hello Robot support monitors the forum closely and will quickly get back to you on any questions or issues you post.</p>"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#further-exploration","title":"Further Exploration","text":"<p>Encounter any issues while getting started? Please let us know at support@hello-robot.com. Also, take a minute to review the Stretch Troubleshooting Guide</p> <p>We recommend next exploring the ROS-based demos that ship with Stretch.  These are found in the stretch_ros repository.</p> <p>That's it. Happy coding!</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/getting_started/safety_guide/","title":"Robot Safety Guide","text":"<p>The Stretch robots are potentially dangerous machines with safety hazards. If improperly used they can cause injury or death. </p> <ul> <li>All users must carefully read the following safety information before using the robot.</li> <li>Anyone near the robot who has not read this safety information must be closely supervised at all times and made aware that the robot could be dangerous.</li> <li>Only use the robot after inspecting the surrounding environment for potential hazards.</li> </ul>"},{"location":"stretch-tutorials/getting_started/safety_guide/#intended-use","title":"Intended Use","text":"<p>The Stretch robots are intended for use by researchers to conduct research in controlled indoor environments. This product is not intended for other uses and lacks the required certifications for other uses, such as use in a home environment by consumers.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#safety-hazards","title":"Safety Hazards","text":"<p>As described later in this document, we have designed Stretch to be safer than previous commercially-available human-scale mobile manipulators, so that researchers can explore the future of mobile manipulation. For example, we have made it smaller and lighter weight with backdrivable torque-sensing joints that can stop when they detect contact.</p> <p>Nonetheless, Stretch is a research robot that can be dangerous. Researchers must use Stretch carefully to avoid damage, injury, or death. Here, we list several safety hazards that researchers must consider before and while using Stretch. </p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-put-people-and-animals-at-risk","title":"Stretch Can Put People And Animals At Risk","text":"<p>As described in more detail later, Stretch can put people and animals at risk. People and animals near the robot must be closely supervised at all times. At all times, an experienced researcher must carefully monitor the robot and be prepared to stop it. Any people near the robot must be made aware that the robot could be dangerous. Before any use of the robot near people or animals, researchers must carefully assess and minimize risks.  </p> <p>Researchers who use the robot near children, animals, vulnerable adults, or other people do so at their own risk. Researchers must take appropriate precautions and obtain the required approvals from their organizations.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-topple-onto-a-person","title":"Stretch Can Topple Onto A Person","text":"<p>The robot may drive off stairs, push or pull itself over with its telescoping arm, fall over while attempting to traverse a threshold or encounter obstacles that cause it to fall on or otherwise collide with people, causing injury. </p> <p>Operate the robot only on flat surfaces away from stairs or other obstacles that may cause it to topple, and do not allow the robot to push or pull itself over.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-should-not-be-lifted-by-a-single-person","title":"Stretch Should Not Be Lifted By A Single Person","text":"<p>Stretch with the standard gripper weighs about 23 kg (50.5 lb), so two or more people should lift and carry the robot. A single person can move the robot around by enabling the runstop button, tilting it over, and rolling it on flat ground. </p> <p>At least two people should lift and carry the robot when needed.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-cause-lacerations","title":"Stretch Can Cause Lacerations","text":"<p>The robot's wrist and tool have sharp edges that can cause lacerations or punctures to the skin or the eyes. </p> <p>Operate the robot away from eyes and other sensitive body parts.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-trap-crush-or-pinch-body-parts","title":"Stretch Can Trap, Crush, Or Pinch Body Parts","text":"<p>The robot has moving joints that can trap, crush or pinch hands, fingers, or other body parts. The robot could also injure a person or animal by driving over a body part. </p> <p>Keep body parts away from the trap, crush, and pinch points during robot motion, including underneath the wheels.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-entrap-loose-clothing-or-hair","title":"Stretch Can Entrap Loose Clothing Or Hair","text":"<p>The robot's shoulder and telescoping arm have rollers that can pull in and entrap loose clothing or hair. </p> <p>Keep loose clothing and long hair away from the robot's shoulder and telescoping arm when either is in motion.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-has-flammable-components","title":"Stretch Has Flammable Components","text":"<p>The robot has polyurethane covers that are flammable and must be kept away from potential ignition sources, such as open flames and hot surfaces. The robot\u2019s head, shoulder, and mobile base have polyurethane covers. </p> <p>Keep the robot away from potential ignition sources and always have a working fire extinguisher nearby.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-is-an-electrical-device","title":"Stretch Is An Electrical Device","text":"<p>Stretch has batteries, electronics, wires, and other electrical components throughout its body. It also provides uncovered connectors that provide power. While the robot has fuses to reduce electrical risks, users must be careful. </p> <p>Keep the robot dry and away from liquids, avoid electrical shocks, ensure power cables and wires are in good condition, be careful with the robot\u2019s connectors, and generally exercise caution while working with this electrical device.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-perform-dangerous-activities","title":"Stretch Can Perform Dangerous Activities","text":"<p>Stretch is a versatile robot capable of performing many actions, including actions that would be dangerous to people. For example, if a dangerous object is held by or affixed to the robot, such as a knife, a heavy object, or breakable glass, the robot can become very dangerous. Likewise, the robot is capable of physically altering the environment in ways that would be dangerous, such as turning a knob that releases gas from a gas stove. </p> <p>Users must be cautious while using the robot to ensure it interacts safely with people and the surrounding environment.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-is-an-open-platform-that-can-be-made-more-dangerous","title":"Stretch Is An Open Platform That Can Be Made More Dangerous","text":"<p>Stretch is an open platform with user-modifiable and user-extensible hardware and software. User changes to the hardware or software can entail serious risks. For example, when shipped, the robot has conservative settings that restrict its speed and the forces it applies to reduce the risks associated with the robot. By modifying the robot, users could enable the robot to move at unsafe speeds and apply unsafe forces. As another example, improper electrical connections could result in a fire. </p> <p>Researchers who choose to modify or extend the robot\u2019s hardware or software do so at their own risk and should be careful to understand the implications of their modifications or extensions. Changes to the robot could result in dangerous situations that cause injury or death.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#additional-risks","title":"Additional Risks","text":"<p>The most important aspects of safety with Stretch are to use good judgment and common sense. Additional important considerations follow:</p> <ul> <li>If the robot appears to be damaged, stop the robot immediately.</li> <li>Always be ready to stop the robot.</li> <li>Do not operate the robot unless an experienced user is present and attentive.</li> <li>Be aware that the robot can move in unexpected ways.</li> <li>Do not put fingers or other objects into the channel that runs along the length of the mast. A belt moves within this channel.</li> <li>Keep an eye on cords, rugs, and any other floor hazards as the robot drives.</li> <li>Keep the robot at least 3 meters from ledges, curbs, stairs, and any other toppling hazard.</li> <li>Do not operate the robot outdoors.</li> <li>Do not attempt to ride the robot.</li> <li>Do not have the robot hold sharp objects.</li> <li>Do not attempt to service the robot without supervision by Hello Robot.</li> </ul>"},{"location":"stretch-tutorials/getting_started/safety_guide/#other-problems-will-likely-occur","title":"Other Problems Will Likely Occur","text":"<p>\u201cAnticipate potential problems and hazards. Always imagine what might happen if the robot malfunctions or behaves in a way different from the desired action. Be vigilant.\u201d - PR2 User Manual by Willow Garage from October 5, 2012</p> <p>Stretch is a complex device that includes many mechanical, electrical, and computational systems that have been designed to work together. Be prepared for something to go wrong. For example, a motor control board might fail, software might not operate as anticipated, an unexpected process might still be running on the robot, or the batteries for the Xbox-style controller or the robot itself might run out. </p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#safety-features","title":"Safety Features","text":"<p>We have considered safety from the outset in the design of Stretch. </p> <ul> <li>Runstop: The illuminated runstop button on Stretch\u2019s head can be used to pause the operation of the four primary joints (base, lift, and arm) of the robot when it is in motion. </li> <li>Lightweight design: The overall mass of Stretch with the standard gripper is 23Kg (50.5lb), and the majority of the mass is in the base. The carbon fiber arm and aluminum mast make for a remarkably lightweight upper body. While this reduces the risk of crushing, crushing injuries can still occur and should be carefully monitored.</li> <li>Gravity friendly: Due to Stretch\u2019s design, its actuators don't have to counteract gravity on a large lever arm. As a result, the motors and gearboxes are lower torque and lower weight than a conventional mobile manipulator with a comparable reach, avoiding the often dangerously strong shoulder joints of typical robot arms.</li> <li>Low gear ratio: The primary joints of Stretch (base, lift, and arm) have low gear-ratios (approx 5:1), allowing for backdriving of joints when powered off. A low gear-ratio also reduces the effective inertia of each joint, limiting the impacted force during undesired contact with people and the environment.</li> <li>Contact Sensitivity: The four primary joints of Stretch (base, lift, and arm) have contact sensitivity. We measure motor currents to estimate contact forces. Because Stretch is a low gear-ratio robot, current sensing provides a fairly sensitive measure of contact forces.</li> <li>Firmware limits: Motor torques are limited at the lowest level of the firmware to configured bounds.</li> <li>Velocity limits: Fast motions of the base are restricted when the arm is up high and the tool is outside the base footprint. This limits the likelihood of toppling or snagging the tool during base motion.</li> <li>Tilt detection: The robot can detect when its body is tilted beyond a safe threshold. The robot can be configured to trigger a runstop event during an over-tilt event.</li> </ul>"},{"location":"stretch-tutorials/getting_started/safety_guide/#safety-markings","title":"Safety Markings","text":"<p>Stretch has the following safety markings: Top of the shoulder, indicating potential pinch point between rollers and mast.</p> <p></p> <p>Top of the base, indicating potential pinch point between arm and base.</p> <p></p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#runstop","title":"Runstop","text":"<p>The runstop allows the user to pause the motion of the four primary actuators (base, lift, and arm) by tapping the illuminated button on the head. An experienced operator should always keep the runstop within reach, allowing them to stop the motion of the robot if it is deemed unsafe.</p> <p>Warning</p> <p>The runstop is not equivalent to an Emergency Stop found on industrial equipment and no safety guarantees are made by its function.</p> <p>When the runstop is enabled, these actuators are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume.</p> <p>The runstop logic is:</p> Action Runstop State Button Illumination Robot startup Motion enabled Solid Tap runstop button Motion disabled Flashing at 1Hz Hold down runstop button for &gt;2s Motion enabled Solid"},{"location":"stretch-tutorials/getting_started/safety_guide/#safety-hazard-details","title":"Safety Hazard Details","text":""},{"location":"stretch-tutorials/getting_started/safety_guide/#sharp-edges","title":"Sharp Edges","text":"<p>The Stretch robot is a piece of laboratory equipment. As such, its structure has moderately sharp edges and corners that can be unsafe. These edges can get snagged during motion, or they may cause lacerations when sufficient force is applied to a person. Care should be taken when grasping or otherwise making contact with Stretch that a sharp corner or edge is not contacted.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#toppling","title":"Toppling","text":"<p>Stretch is a relatively lightweight robot. In some kinematic configurations, a high center of gravity can make it prone to toppling. Toppling can occur when:</p> <ul> <li>The mobile base is moving at a moderate or fast speed and hits a bump, threshold, or other change in floor property.</li> <li>The arm is raised high and pushes or pulls on the environment with sufficient force.</li> <li>The robot drives over a drop-off such as a stair or a curb.</li> </ul> <p>Warning</p> <p>While Stretch has cliff sensors, they do not currently inhibit motion of the base. During typical use, the robot will not attempt to stop itself at a cliff, and can fall down stairs and hurt itself or a person.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#pinch-points","title":"Pinch Points","text":"<p>Pinch points around the robot's head, gripper, and wrist can cause discomfort and care should be taken when handling these joints as they move.</p> <p>The shoulder, which travels up and down on the lift, has a series of rollers that ride along the mast. While the shoulder shells can prevent large objects from getting pinched by the rollers, small and thin objects can be pulled into and crushed.</p> <p>The telescoping arm, which extends and retracts, has rollers that ride along the telescoping elements. While the arm link cuffs can reduce the chance of large objects getting pinched, small and thin objects, such as hair, can be pulled in.</p> <p>Extra care should be taken with long hair, clothing, and small fingers around the shoulder rollers.</p>"},{"location":"stretch-tutorials/getting_started/safety_guide/#crush-points","title":"Crush Points","text":"<p>The lift degree of freedom is the strongest joint on the robot and as such can apply potentially unsafe forces to a person.</p> <p></p> <p>The lift, while in motion, may trap or crush objects between the \u2018shoulder\u2019 and another surface. As such, best practices for lift safety should always be used when using the lift degree of freedom.  </p> <p>The lift has a max theoretical strength of nearly 200N of linear force. In practice, this force is limited by the lift\u2019s Guarded Move function, which places the lift in Safety Mode when the actuator forces exceed a threshold. </p> <p>The diagrams below show the potential crush points at the top and bottom of the lift range of motion.</p> <p></p> <p></p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/","title":"Stretch Troubleshooting Guide","text":"<p>This guide covers common issues and ways to resolve them. Please check the Hello Robot Forum for additional topics not covered here.</p>"},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/#xbox-teleoperation-is-not-working","title":"Xbox teleoperation is not working","text":"<p>The provided Easy SMX wireless controller can accidentally be placed in the wrong mode. The mode is indicated by the round illuminated ring (shown as Connect below). The top 2 LEDs only should be illuminated. If a different LED pattern is shown then the button mapping expected by stretch_xbox_controller_teleop.py will be incorrect.</p> <p>To set the controller into the correct mode:</p> <ul> <li>Hold the center button down for 5s. It will switch modes. Release.</li> <li>Repeat until the top half of the ring (upper two lights) is illuminated.</li> </ul> <p>In addition, check that the provided USB dongle is plugged into the robot USB port in its trunk.</p> <p></p>"},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/#battery-is-not-staying-charged","title":"Battery is not staying charged","text":"Stretch RE1Stretch 2 <p>Please review the troubleshooting section of the Stretch RE1 Battery Maintenance Guide.</p> <p>Please review the troubleshooting section of the Stretch 2 Battery Maintenance Guide.</p>"},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/#port-busy-or-rpc-transport-errors","title":"Port Busy or RPC Transport Errors","text":"<p>If more than one instance of Stretch Body's Robot class is instantiated at a time, Stretch Body will report communication errors and will not always execute motion commands as expected. This is because the Robot class manages communications with the robot hardware and it doesn't support multiple writes to the USB devices.</p> Examples of busy port or RPC errors <pre><code>[ERROR] [pimu]: Port /dev/hello-pimu is busy. Check if another Stretch Body process is already running\n[WARNING] [pimu]: Unable to open serial port for device /dev/hello-pimu\n[ERROR] [hello-motor-left-wheel]: Port /dev/hello-motor-left-wheel is busy. Check if another Stretch Body process is already running\n[WARNING] [hello-motor-left-wheel]: Unable to open serial port for device /dev/hello-motor-left-wheel\n[ERROR] [hello-motor-right-wheel]: Port /dev/hello-motor-right-wheel is busy. Check if another Stretch Body process is already running\n[WARNING] [hello-motor-right-wheel]: Unable to open serial port for device /dev/hello-motor-right-wheel\n[ERROR] [hello-motor-lift]: Port /dev/hello-motor-lift is busy. Check if another Stretch Body process is already running\n[WARNING] [hello-motor-lift]: Unable to open serial port for device /dev/hello-motor-lift\n[ERROR] [hello-motor-arm]: Port /dev/hello-motor-arm is busy. Check if another Stretch Body process is already running\n[WARNING] [hello-motor-arm]: Unable to open serial port for device /dev/hello-motor-arm\n[ERROR] [wacc]: Port /dev/hello-wacc is busy. Check if another Stretch Body process is already running\n[WARNING] [wacc]: Unable to open serial port for device /dev/hello-wacc\n[ERROR] [pimu]: Port /dev/hello-pimu is busy. Check if another Stretch Body process is already running\n[WARNING] [pimu]: Unable to open serial port for device /dev/hello-pimu\n[ERROR] [hello-motor-left-wheel]: Port /dev/hello-motor-left-wheel is busy. Check if another Stretch Body process is already running\n[WARNING] [hello-motor-left-wheel]: Unable to open serial port for device /dev/hello-motor-left-wheel\n[ERROR] [hello-motor-lift]: Port /dev/hello-motor-lift is busy. Check if another Stretch Body process is already running\n[WARNING] [hello-motor-lift]: Unable to open serial port for device /dev/hello-motor-lift\n[ERROR] [hello-motor-arm]: Port /dev/hello-motor-arm is busy. Check if another Stretch Body process is already running\n[WARNING] [hello-motor-arm]: Unable to open serial port for device /dev/hello-motor-arm\n</code></pre>      or      <pre><code>[WARNING] [head_tilt]: DynamixelHelloXL430 Ping failed... head_tilt\nDynamixelHelloXL430 Ping failed... head_tilt\n</code></pre>      or      <pre><code>serial.serialutil.SerialException: device reports readiness to read but returned no data (device disconnected or multiple access on port?)\n</code></pre>      or      <pre><code>Traceback (most recent call last):\nFile \"/home/hello-robot/.local/bin/stretch_robot_system_check.py\", line 42, in &lt;module&gt;\n    r.startup()\nFile \"/home/hello-robot/repos/stretch_body/body/stretch_body/robot.py\", line 167, in startup\n    if not self.devices[k].startup(threaded=False):\nFile \"/home/hello-robot/repos/stretch_body/body/stretch_body/end_of_arm.py\", line 26, in startup\n    return DynamixelXChain.startup(self, threaded=threaded)\nFile \"/home/hello-robot/repos/stretch_body/body/stretch_body/dynamixel_X_chain.py\", line 80, in startup\n    if not self.motors[mk].startup(threaded=False):\nFile \"/home/hello-robot/repos/stretch_body/body/stretch_body/stretch_gripper.py\", line 22, in startup\n    return DynamixelHelloXL430.startup(self, threaded=threaded)\nFile \"/home/hello-robot/repos/stretch_body/body/stretch_body/dynamixel_hello_XL430.py\", line 193, in startup\n    self.motor.set_temperature_limit(self.params['temperature_limit'])\nFile \"/home/hello-robot/repos/stretch_body/body/stretch_body/dynamixel_XL430.py\", line 707, in set_temperature_limit\n    dxl_comm_result, dxl_error =   self.packet_handler.write1ByteTxRx(self.port_handler, self.dxl_id, XL430_ADDR_TEMPERATURE_LIMIT, int(x))\nFile \"/home/hello-robot/.local/lib/python3.8/site-packages/dynamixel_sdk/protocol2_packet_handler.py\", line 653, in write1ByteTxRx\n    return self.writeTxRx(port, dxl_id, address, 1, data_write)\nFile \"/home/hello-robot/.local/lib/python3.8/site-packages/dynamixel_sdk/protocol2_packet_handler.py\", line 643, in writeTxRx\n    rxpacket, result, error = self.txRxPacket(port, txpacket)\nFile \"/home/hello-robot/.local/lib/python3.8/site-packages/dynamixel_sdk/protocol2_packet_handler.py\", line 346, in txRxPacket\n    rxpacket, result = self.rxPacket(port)\nFile \"/home/hello-robot/.local/lib/python3.8/site-packages/dynamixel_sdk/protocol2_packet_handler.py\", line 257, in rxPacket\n    rxpacket.extend(port.readPort(wait_length - rx_length))\nFile \"/home/hello-robot/.local/lib/python3.8/site-packages/dynamixel_sdk/port_handler.py\", line 78, in readPort\n    return self.ser.read(length)\nFile \"/usr/lib/python3/dist-packages/serial/serialposix.py\", line 509, in read\n    raise SerialException('read failed: {}'.format(e))\nserial.serialutil.SerialException: read failed: device reports readiness to read but returned no data (device disconnected or multiple access on port?)\n</code></pre>      or      <pre><code>Transport RX Error on RPC_ACK_SEND_BLOCK_MORE False 0 102\n---- Debug Exception\n--------------- New RPC -------------------------\nFramer sent RPC_START_NEW_RPC\n...\n</code></pre>      or      <pre><code>IOError(None): None\n...\n</code></pre> <p>To check if an instance of Robot is already instantiated, you may use the Unix top command to monitor active processes. You may use the Unix pkill command to end the background instance of Robot.</p> <pre><code>pkill -9 python\n</code></pre> <p>As shipped, Stretch launches stretch_xbox_controller_teleop.py upon boot. It is necessary to turn off this automatic launch feature, otherwise, your own Robot instance will conflict with this script. Additionally, if you are logged into multiple accounts, a Robot instance may be active in another user account.</p> <p>To turn it off, search for 'Startup' from Ubuntu Activities. Uncheck the box for 'hello_robot_xbox_teleop'.</p> <p></p>"},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/#invalid-homing-offset-for-single-turn-mode-dynamixel","title":"Invalid homing offset for single-turn mode Dynamixel","text":"<p>If you see a warning similar to:</p> <pre><code>[WARNING] [head_tilt]: Dynamixel head_tilt: Servo is in single-turn mode yet has invalid homing offset of -1682.\n This may cause unexpected results if not set to zero (using REx_dynamixel_jog.py)\n Please contact Hello Robot support for more information\n</code></pre> <p>You have a \"single-turn\" Dynamixel servo (e.g. <code>head_tilt</code> in the above warning) that has an invalid \"homing offset\" saved to the servo's flash memory. This can lead to the motor returning its joint position incorrectly. In order to reset the homing offset to zero, create an instance of the Dynamixel class in Stretch Body and set the homing offset to zero. For example, doing this for the <code>head_tilt</code> servo looks like:</p> <pre><code>hello-robot@stretch-re1-xxxx:~$ ipython3\nPython 3.8.10 (default, Mar 13 2023, 10:26:41)\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.7.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import stretch_body.dynamixel_hello_XL430\n\nIn [2]: servo = stretch_body.dynamixel_hello_XL430.DynamixelHelloXL430(name=\"head_tilt\", chain=None)\n\nIn [3]: servo.startup()\n[WARNING] [head_tilt]: Dynamixel head_tilt: Servo is in single-turn mode yet has invalid homing offset of -1682.\n This may cause unexpected results if not set to zero (using REx_dynamixel_jog.py)\n Please contact Hello Robot support for more information\nOut[3]: True\n\nIn [4]: servo.disable_torque()\n\nIn [5]: servo.motor.set_homing_offset(0)\n\nIn [6]: servo.enable_torque()\n\nIn [7]: servo.stop()\n\nIn [8]:\nDo you really want to exit ([y]/n)?\n</code></pre>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/getting_started/untethered_operation/","title":"Untethered Operation","text":"<p>As a mobile manipulator,  Stretch can only go so far when tethered to the monitor, keyboard, and mouse setup. This guide will explain three methods of setting up the Stretch for untethered usage.</p> <p>These methods typically require a wireless network, but it is possible to set up any of these methods without a wireless network by setting up a hotspot.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#remote-desktop","title":"Remote Desktop","text":""},{"location":"stretch-tutorials/getting_started/untethered_operation/#requirements","title":"Requirements","text":"<p>This is the recommended approach if you are running Windows or MacOS. This method requires a Virtual Network Computing (VNC) package. Using any of the free or paid options available for Windows, MacOS, and Chrome will be fine since they all use the Remote Frame Buffer (RFB) protocol to communicate with the robot. If you're using Ubuntu, Remmina Remote Desktop Client will be installed by default.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#how-to","title":"How To","text":"<p>While Stretch is tethered to the monitor, keyboard, and mouse setup, first verify that the robot is connected to the wireless network then install Vino VNC server using the following command:</p> <pre><code>sudo apt install vino\n</code></pre> <p>Go to System Settings. Select the Sharing tab and turn it on then, turn on Screen Sharing and choose a password. If you plan to connect to the robot from a Windows or MacOS machine, then open a terminal and run the following command.</p> <pre><code>sudo gsettings set org.gnome.Vino require-encryption false\n</code></pre> <p>Finally, we need the robot's IP address, username, and password. Open a terminal and run <code>ifconfig</code>, which will print out the network information of the machine. In the wireless section (typically named wlp2s0), look for something that looks like \"inet 10.0.0.15\". The four numbers represent the IP address of the robot on the local network. The robot's default username and password are printed on papers that came in the tools box alongside the robot.</p> <p>VNC will only function properly with an external display attached to the robot. Using a dummy HDMI dongle when operating the robot untethered via VNC is recommended. One possible dummy HDMI dongle can be found on Amazon here. On your computer, connect to the same wireless network as the robot and open the VNC package being used. Using the robot's IP address and username, initialize a new connection to the robot. The robot's desktop will open in a new window.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#ssh-x-server","title":"SSH &amp; X Server","text":""},{"location":"stretch-tutorials/getting_started/untethered_operation/#requirements_1","title":"Requirements","text":"<p>This is the recommended approach if you are running a Unix-based operating system, like Ubuntu or Arch Linux. This method requires both SSH and X Server to be installed. While most Unix-based operating systems have both installed by default, MacOS will only have SSH installed and Windows has neither installed by default. It is possible to install these tools for MacOS or Windows.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#how-to_1","title":"How To","text":"<p>While the Remote Desktop approach is easy to set up, graphics and interaction with the remote desktop are often slow. In this method, we will use SSH and X Server to accomplish the same a bit faster. SSH stands for Secure Shell, enabling one to remotely use the terminal (shell) of another machine. X Server is used on many Unix variants to render the Windowed GUI of applications. With SSH and X Server, it is possible to render a Windowed GUI of an application running on the robot on your computer's screen.</p> <p>The first step is to identify the robot's IP address on the local network. While  Stretch is tethered to the monitor, keyboard, and mouse, verify that the robot is connected to a wireless network. Then, open a terminal and run <code>ifconfig</code>, which will print out the network information of the machine. In the wireless section (typically named wlp2s0), look for something that looks like \"inet 10.0.0.15\". The four numbers represent the IP address of the robot on the local network. Using any other machine on the same local network, I can SSH into the robot using this IP address. Take note of the username and password of the robot. The default combo is printed on papers that came in the tools box alongside the robot.</p> <p>To SSH into the robot, run the following. It will require the password and may ask you to add the robot to the known hosts.</p> <pre><code>ssh -X username@ip-address\n</code></pre> <p>Now that you're SSH-ed into the robot, you can disconnect any wires from the robot. You can accomplish any of the same tasks through the terminal. For example, you can type in <code>ipython</code> and interact with the robot using Stretch Body, as explained in the Quick Start Guide.</p> <p>Furthermore, Windowed GUI applications that would have been displayed on the monitor will now display on your SSH-ed machine. For example, we can open Rviz to visualize what the robot is seeing. Open two terminals and SSH into the robot as explained above. In the first, run <code>roslaunch stretch_core stretch_driver.launch</code>. You should see some information print out in the terminal. In the second, run <code>rviz</code>. A window will pop up and information about the robot can be visualized by clicking on <code>Add -&gt; RobotModel</code> and <code>Add -&gt; By Topic -&gt; /Scan</code>. Additional information on how to use ROS tools can be found in ROS's tutorials or in our Stretch ROS guides.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#moving-files-tofrom-the-robot-wirelessly","title":"Moving files to/from the robot wirelessly","text":"<p>It's common to need to move files to/from the robot wirelessly and a tool similar to SSH can help with this: Secure Copy (SCP).</p> <p>To send the files from your computer to the robot, run: <pre><code>scp ./filename username@ip-address:~/path/to/put/it/\n</code></pre></p> <p>To copy the files from the robot to your computer, run the reverse: <pre><code>scp username@ip-address:/path/to/filename ~/path/to/put/it/\n</code></pre></p> <p>This works for copying directories and their contents as well.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#ros-remote-master","title":"ROS Remote Master","text":""},{"location":"stretch-tutorials/getting_started/untethered_operation/#requirements_2","title":"Requirements","text":"<p>This is the recommended approach if you are running Ubuntu 16.04/18.04/20.04 with ROS kinetic/melodic/noetic installed on your computer. This method will utilize the local installation of ROS tools, such as Rviz, rostopic, and rosservice, while retrieving data from the robot.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#how-to_2","title":"How To","text":"<p>If you are developing ROS code to test on Stretch and you already have ROS installed on your Ubuntu computer, then there is an easier way of using Rviz than the method described in SSH &amp; X Server. In the ROS world, this concept is known as \"remote master\".</p> <p>First, identify your robot's and computer's IP address on the network (e.g. using <code>ifconfig</code>). These are <code>robot-ip-address</code> and <code>computer-ip-address</code> respectively.</p> <p>Next, run the following on the robot:</p> <pre><code>export ROS_IP=robot-ip-address\nexport ROS_MASTER_URI=http://robot-ip-address:11311/\n</code></pre> <p>Next, start the ROS launch files on the robot as you normally would. Finally, on your computer, run:</p> <pre><code>export ROS_IP=computer-ip-address\nexport ROS_MASTER_URI=http://robot-ip-address:11311\n</code></pre> <p>If you use ROS Remote Master often, you can export these environment variables in your bashrc.</p> <p>Tools like rostopic and rosservice can now be used on your computer as you would have on the robot. For example, you can use <code>rostopic list</code> on your computer to print out the topics available on the robot. Additional information can be found in the ROS Multiple Machines Tutorial.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#visualizing-remotely-with-rviz","title":"Visualizing remotely with RViz","text":"<p>If you'd like to visualize the robot model on your computer using Rviz, you'll need to set up a ROS workspace with the Stretch Description package. First, copy over the <code>~/stretch_user</code> directory from the robot to your computer (e.g. using Secure Copy). Second, clone Stretch Install, and checkout the noetic branch if you are running ROS Noetic on the robot. Finally, run the stretch_create_ros_workspace.sh script. A ROS Workspace with the Stretch ROS packages is now set up on your computer. Furthermore, Stretch Description has been set up with your robot's calibrated URDF.</p> <p>We can now use remote master and Rviz to visualize what the robot is seeing on your computer. Open two terminals. First, SSH into the robot and run <code>roslaunch stretch_core stretch_driver.launch</code>. You should see some information print out in the terminal. In the second, run <code>rviz</code>. A window will pop up and information about the robot can be visualized by clicking on <code>Add -&gt; RobotModel</code> and <code>Add -&gt; By Topic -&gt; /Scan</code>. Additional information on how to use Rviz can be found in ROS's tutorials or our Stretch ROS guides.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#additional-ideas","title":"Additional Ideas","text":"<p>Although the methods described above will enable you to wirelessly control the robot, there are several ways to improve the usability and security of your wireless connection. These ideas are listed here.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#hotspot","title":"Hotspot","text":"<p>Often the trouble with wirelessly controlling the robot is the network. If your network is using industrial security like 2-factor authentication, there may be trouble connecting the robot to the network. If the network is servicing a large number of users, the connection may feel sluggish. The alternative is to skip the network by connecting directly to the robot. After starting a hotspot on the robot, you can follow instructions for any of the methods described above to control the robot. The trade-off is that while connected to the robot's hotspot, you will be unable to connect to the internet.</p> <p>To set up the robot's hotspot, visit the Ubuntu Wifi Settings page in the robot. Click on the hamburger menu in the top right and select \"Enable hotspot\". From your local machine, connect to the robot's hotspot and save the credentials. To change the hotspot's password or enable the hotspot automatically whenever the robot boots, see the following Stackoverflow post.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#vs-code-remote-development","title":"VS Code Remote Development","text":"<p>It is possible to simultaneously develop code on the robot while running wireless experiments using the Remote Development Extension provided by the VS Code IDE. If you're already using the VS Code IDE, navigate to the Extensions tab and search for Remote Development Extension by Microsoft. After installing, click on a green button in the bottom left of the screen and then select \"Remote-SSH: Connect to Host\". Setting this up for the first time will require you to know the robot's IP address and username. Add a new host with the information. While connecting, VS Code will ask you for the password of the robot. Once you are connected, you can open any folder and edit the code remotely. Combined with the method explained in SSH &amp; X Server, this is a powerful method of iteratively developing code while testing it.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#static-ip-address","title":"Static IP Address","text":"<p>Routers that serve wireless networks often dynamically assign IP addressess to machines that connect to the network. This means that your robot's IP address may have changed since the last time you turned it on. Since it becomes a pain to connect to the monitor, keyboard, and mouse setup every time to run <code>ifconfig</code>, many users prefer to assign the robot a static IP address. If you control the router, visit the router's settings page to set up the robot's static IP address. It is common at universities and companies to have staff dedicated to the management of the network. This staff will often be able to set up a static IP address for the robot.</p>"},{"location":"stretch-tutorials/getting_started/untethered_operation/#public-key-authentication","title":"Public Key Authentication","text":"<p>The method of SSH described in SSH &amp; X Server uses basic password authentication when connecting. There is a better and more secure method of SSH-ing into the robot called Public Key Authentication. This method will allow multiple developers to SSH into the robot without having to share the robot's admin password.</p> <p>The first step is to generate public and private keys on your computer. Linux and MacOS machines can simply open the terminal and run:</p> <pre><code>ssh-keygen -t ed25519 -f &lt;key_filepath_without_extension&gt; -C \"&lt;some comment&gt;\"\n</code></pre> <p>It will prompt you to enter a password. If you do, you'll need it to use the private key when you SSH into the robot. Next, we give the robot the public key. Linux and MacOS machines can run:</p> <pre><code>ssh-copy-id -i &lt;key_filepath_without_extension&gt; username@ip-address\n</code></pre> <p>This requires you to know the username and IP address of the robot. Instructions on how to find this information are found in the SSH &amp; X Server section. You may now SSH into the robot as normal, and no prompt for the robot's password will appear.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/getting_started/updating_software/","title":"Updating Stretch Software","text":"<p>Stretch's software is improved with new features and bug fixes with each update. In this guide, we cover when and how to update the various software components on your Stretch.</p>"},{"location":"stretch-tutorials/getting_started/updating_software/#when-to-update","title":"When to Update","text":"<p>We develop our software publicly on GitHub, allowing anyone to follow and propose the development of a code feature or bug fix. While we wholeheartedly welcome collaboration on GitHub, it is not necessary to be active on GitHub to follow our software releases. We announce every major release of software on our forum. These are stable releases with code that has been extensively tested on many Stretch robots. To be notified of new releases, create an account on the forum and click the bell icon in the top left of the announcements section. The forum is also available to report issues and ask questions about any of our software packages.</p>"},{"location":"stretch-tutorials/getting_started/updating_software/#how-to-update","title":"How to Update","text":"<p>Each Stretch is shipped with firmware, a Python SDK, and ROS packages developed specifically for Stretch. At the moment, there are three separate processes for updating each of these components.</p>"},{"location":"stretch-tutorials/getting_started/updating_software/#stretch-ros","title":"Stretch ROS","text":"<p>Stretch ROS is the Robot Operating System (ROS) interface to the robot. Many robotics developers find ROS useful to bootstrap their robotics software developments. Depending on whether you want to set up a ROS or ROS 2 workspace, the easiest way to download the most recent updates in the stretch_ros and stretch_ros2 code repositories, while resolving all source-built dependencies at the same time, is by following the instructions in the Creating a New ROS Workspace section in the stretch_install repo. </p> <p>Warning</p> <p>Before you proceed, please ensure that all your personal files in the catkin or ament workspace have been backed up safely. This is important because executing the following set of commands deletes your existing workspace and replaces it with a fresh one.</p> <p>To download the stretch_install repo, execute: <pre><code>cd ~/\ngit clone https://github.com/hello-robot/stretch_install.git\ncd stretch_install\ngit pull\n</code></pre></p> <p>To replace the ROS Melodic catkin_ws in Ubuntu 18.04, execute: <pre><code>./factory/18.04/stretch_create_catkin_workspace.sh -w &lt;optional-path-to-ws&gt;\n</code></pre></p> <p>To replace the ROS Noetic catkin_ws in Ubuntu 20.04, execute: <pre><code>./factory/20.04/stretch_create_catkin_workspace.sh -w &lt;optional-path-to-ws&gt;\n</code></pre></p> <p>To replace the ROS 2 Galactic ament_ws in Ubuntu 20.04, execute: <pre><code>./factory/20.04/stretch_create_ament_workspace.sh -w &lt;optional-path-to-ws&gt;\n</code></pre></p>"},{"location":"stretch-tutorials/getting_started/updating_software/#stretch-body-python-sdk","title":"Stretch Body Python SDK","text":"<p>Stretch Body is the Python SDK for the robot. It abstracts away the low-level details of communication with the embedded devices and provides an intuitive API for working with the robot. You may update it using the following commands depending on the Python version.</p> <p>If you are using Python2, execute: <pre><code>pip install -U hello-robot-stretch-body\npip install -U hello-robot-stretch-body-tools\npip install -U hello-robot-stretch-factory\npip3 install -U hello_robot_stretch_body_tools_py3\n</code></pre></p> <p>For Python3, execute: <pre><code>python3 -m pip -q install --no-warn-script-location hello-robot-stretch-body\npython3 -m pip -q install --no-warn-script-location hello-robot-stretch-body-tools\npython3 -m pip -q install --no-warn-script-location hello-robot-stretch-factory\npython3 -m pip -q install --no-warn-script-location hello-robot-stretch-tool-share\n</code></pre></p>"},{"location":"stretch-tutorials/getting_started/updating_software/#stretch-firmware","title":"Stretch Firmware","text":"<p>The firmware and the Python SDK (called Stretch Body) communicate on an established protocol. Therefore, it is important to maintain a protocol match between the different firmware and Stretch Body versions. Fortunately, there is a script that handles this automatically. In the command line, run the following command:</p> <pre><code>REx_firmware_updater.py --status\n</code></pre> <p>This script will automatically determine what version is currently running on the robot and provide a recommendation for the next step. Follow the next steps provided by the firmware updater script.</p>"},{"location":"stretch-tutorials/getting_started/updating_software/#ubuntu","title":"Ubuntu","text":"<p>The operating system upon which Stretch is built is called Ubuntu. This operating system provides the underlying packages that power Stretch's software packages. Furthermore, users of Stretch depend on this operating system and the underlying packages to develop software on Stretch. Therefore, it is important to keep the OS and these underlying packages up to date. In the command line, run the following command:</p> <pre><code>sudo apt update\nsudo apt upgrade\n</code></pre> <p>Apt is the package manager that handles updates for all Ubuntu packages.</p>"},{"location":"stretch-tutorials/getting_started/updating_software/#troubleshooting","title":"Troubleshooting","text":""},{"location":"stretch-tutorials/getting_started/updating_software/#firmware-mismatch-error","title":"Firmware Mismatch Error","text":"<p>When working with Stretch Body, if you see the following error:</p> <pre><code>----------------\nFirmware protocol mismatch on /dev/XXXX.\nProtocol on board is pX.\nValid protocol is: pX.\nDisabling device.\nPlease upgrade the firmware and/or version of Stretch Body.\n----------------\n</code></pre> <p>This error appears because the low-level Python SDK and the firmware cannot communicate with each other. There is a protocol mismatch preventing communication between the two. Simply run the following script and follow its recommendations to upgrade/downgrade the firmware as necessary to match the protocol level of Stretch Body.</p> <pre><code>REx_firmware_updater.py --status\n</code></pre>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/ros1/","title":"Overview","text":""},{"location":"stretch-tutorials/ros1/#tutorial-track-stretch-ros","title":"Tutorial Track: Stretch ROS","text":""},{"location":"stretch-tutorials/ros1/#robot-operating-system-ros","title":"Robot Operating System (ROS)","text":"<p>Despite the name, ROS is not an operating system. ROS is a middleware framework that is a collection of transport protocols, development and debugging tools, and open-source packages. As a transport protocol, ROS enables distributed communication via messages between nodes. As a development and debugging toolkit, ROS provides build systems that allows for writing applications in a wide variety of languages (Python and C++ are used in this tutorial track), a launch system to manage the execution of mutiple nodes simultaneously, and command line tools to interact with the running system. Finally, as a popular ecosystem, there are many open-source ROS packages that allow users to quickly prototype with new sensors, actuators, planners, perception stacks, and more.</p> <p>This tutorial track is for users looking to get familiar with programming Stretch robots via ROS. We recommend going through the tutorials in the following order:</p>"},{"location":"stretch-tutorials/ros1/#basics","title":"Basics","text":"Tutorial Description 1 Getting Started Setup instructions for ROS on Stretch. 2 Gazebo Basics Use Stretch in a simulated environment with Gazebo. 3 Teleoperating Stretch Control Stretch with a keyboard or xbox controller. 4 Internal State of Stretch Monitor the joint states of Stretch. 5 RViz Basics Visualize topics in Stretch. 6 Navigation Stack Motion planning and control for the mobile base using Nav stack. 7 Follow Joint Trajectory Commands Control joints using joint trajectory server. 8 Perception Use the Realsense D435i camera to visualize the environment. 9 ArUco Marker Detection Localize objects using ArUco markers. 10 ReSpeaker Microphone Array Learn to use the ReSpeaker Microphone Array. 11 FUNMAP Fast Unified Navigation, Manipulation and Planning."},{"location":"stretch-tutorials/ros1/#other-examples","title":"Other Examples","text":"<p>To help get you get started on your software development, here are examples of nodes to have the stretch perform simple tasks.</p> Tutorial Description 1 Teleoperate Stretch with a Node Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Print Joint States Print the joint states of Stretch. 6 Store Effort Values Print, store, and plot the effort values of the Stretch robot. 7 Capture Image Capture images from the RealSense camera data. 8 Voice to Text Interpret speech and save transcript to a text file. 9 Voice Teleoperation of Base Use speech to teleoperate the mobile base. 10 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 11 PointCloud Transformation Convert PointCloud2 data to a PointCloud and transform to a different frame. 12 ArUco Tag Locator Actuate the head to locate a requested ArUco marker tag and return a transform. 13 2D Navigation Goals Send 2D navigation goals to the move_base ROS node."},{"location":"stretch-tutorials/ros1/aruco_marker_detection/","title":"ArUco Marker Detection","text":""},{"location":"stretch-tutorials/ros1/aruco_marker_detection/#aruco-marker-detector","title":"ArUco Marker Detector","text":"<p>For this tutorial, we will go over how to detect Stretch's ArUco markers and review the files that hold the information for the tags.</p>"},{"location":"stretch-tutorials/ros1/aruco_marker_detection/#visualize-aruco-markers-in-rviz","title":"Visualize ArUco Markers in RViz","text":"<p>Begin by running the stretch driver launch file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>roslaunch stretch_core d435i_low_resolution.launch\n</code></pre> <p>Next, in a new terminal, run the stretch ArUco launch file which will bring up the detect_aruco_markers node.</p> <pre><code>roslaunch stretch_core stretch_aruco.launch\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz\n</code></pre> <p>You are going to need to teleoperate Stretch's head to detect the ArUco marker tags. Run the following command in a new terminal and control the head to point the camera toward the markers.   </p> <pre><code>rosrun stretch_core keyboard_teleop\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1/aruco_marker_detection/#the-aruco-marker-dictionary","title":"The ArUco Marker Dictionary","text":"<p>When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml, that holds the information about the markers.</p> <p>If detect_aruco_markers node doesn\u2019t find an entry in stretch_marker_dict.yaml for a particular ArUco marker ID number, it uses the default entry. For example, most robots have shipped with the following default entry:</p> <pre><code>'default':\n  'length_mm': 24\n  'use_rgb_only': False\n  'name': 'unknown'\n  'link': None\n</code></pre> <p>and the following entry for the ArUco marker on the top of the wrist</p> <pre><code>'133':\n  'length_mm': 23.5\n  'use_rgb_only': False\n  'name': 'wrist_top'\n  'link': 'link_aruco_top_wrist'\n</code></pre> <p>Dictionary Breakdown</p> <pre><code>'133':\n</code></pre> <p>The dictionary key for each entry is the ArUco marker\u2019s ID number or <code>default</code>. For example, the entry shown above for the ArUco marker on the top of the wrist assumes that the marker\u2019s ID number is <code>133</code>.</p> <pre><code>'length_mm': 23.5\n</code></pre> <p>The <code>length_mm</code> value used by detect_aruco_markers is important for estimating the pose of an ArUco marker.</p> <p>Note</p> <p>If the actual width and height of the marker do not match this value, then pose estimation will be poor. Thus, carefully measure custom Aruco markers.</p> <pre><code>'use_rgb_only': False\n</code></pre> <p>If <code>use_rgb_only</code> is <code>True</code>, detect_aruco_markers will ignore depth images from the Intel RealSense D435i depth camera when estimating the pose of the marker and will instead only use RGB images from the D435i.</p> <pre><code>'name': 'wrist_top'\n</code></pre> <p><code>name</code> is used for the text string of the ArUco marker\u2019s ROS Marker in the ROS MarkerArray Message published by the detect_aruco_markers ROS node.</p> <pre><code>'link': 'link_aruco_top_wrist'\n</code></pre> <p><code>link</code> is currently used by stretch_calibration. It is the name of the link associated with a body-mounted ArUco marker in the robot\u2019s URDF.</p> <p>It\u2019s good practice to add an entry to stretch_marker_dict.yaml for each ArUco marker you use.</p>"},{"location":"stretch-tutorials/ros1/aruco_marker_detection/#create-a-new-aruco-marker","title":"Create a New ArUco Marker","text":"<p>At Hello Robot, we\u2019ve used the following guide when generating new ArUco markers.</p> <p>We generate ArUco markers using a 6x6-bit grid (36 bits) with 250 unique codes. This corresponds with DICT_6X6_250 defined in OpenCV. We generate markers using this online ArUco marker generator by setting the Dictionary entry to 6x6 and then setting the Marker ID and Marker size, mm as appropriate for the specific application. We strongly recommend measuring the actual marker by hand before adding an entry for it to stretch_marker_dict.yaml.</p> <p>We select marker ID numbers using the following ranges.</p> <ul> <li>0 - 99: reserved for users</li> <li>100 - 249: reserved for official use by Hello Robot Inc.</li> <li>100 - 199: reserved for robots with distinct sets of body-mounted markers<ul> <li>Allows different robots near each other to use distinct sets of body-mounted markers to avoid confusion. This could be valuable for various uses of body-mounted markers, including calibration, visual servoing, visual motion capture, and multi-robot tasks.</li> <li>5 markers per robot = 2 on the mobile base + 2 on the wrist + 1 on the shoulder</li> <li>20 distinct sets = 100 available ID numbers / 5 ID numbers per robot</li> </ul> </li> <li>200 - 249: reserved for official accessories<ul> <li>245 for the prototype docking station</li> <li>246-249 for large floor markers</li> </ul> </li> </ul> <p>When coming up with this guide, we expected the following:</p> <ul> <li>Body-mounted accessories with the same ID numbers mounted to different robots could be disambiguated using the expected range of 3D locations of the ArUco markers on the calibrated body.</li> <li>Accessories in the environment with the same ID numbers could be disambiguated using a map or nearby observable features of the environment.</li> </ul>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/","title":"Autodocking","text":"<p>Note</p> <p>Please be advised that the code in this tutorial is currently in beta and is under active development.</p>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/#autodocking-with-nav-stack","title":"Autodocking with Nav Stack","text":"<p>Wouldn't it be awesome if after a hard day's work, Stretch would just go charge itself without you having to worry about it? In this tutorial we will explore an experimental code that allows Stretch to locate a charging station and charge itself autonomously. This demo will build on top of some of the tutorials that we have explored earlier like ArUco detection, base teleoperation and using the Nav Stack. Be sure to check them out.</p>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/#docking-station","title":"Docking Station","text":"<p>The Stretch Docking Station is a Stretch accessory that allows one to autonomously charge the robot. The top part of the docking station has an ArUco marker number 245 from the 6x6, 250 dictionary. To understand why this is important, refer to this handy guide provided by OpenCV. This marker enables Stretch to accurately locate the docking station in its environment. The docking station also has a Y-shaped recess in its base plate to guide the caster wheel of the robot towards the charging port in case of minor misalignments while backing up. Overall, it's a minimal yet effective way to allow Stretch to charge itself.</p>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/#behaviour-trees","title":"Behaviour Trees","text":"<p>Traditionally, high level task planning has been achieved using Finite State Machines or FSMs which break down each functional element of the task into states that have to be traversed in a cyclic manner to accomplish the major task. This approach has recently gone out of vogue in favour of Behavior Trees or BTs. BTs, also known as Directed Acyclic Graphs, have been popularized through their use in the gaming industry to impart complex behaviors to NPCs. BTs organize behaviors in a tree representation where the control flow is achieved not through cyclic state transitions but in a tree traversal fashion. Additionally, conditions and actions form distinct leafs in the tree which results in better modularity. This ensures that augmenting behaviors by way of additional leafs in the tree does not require a restructuring of previous and following leafs but only the final tree graph. This also means that actions in complex behaviors can be developed independently resulting in higher scalability.</p>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/#py-trees","title":"Py-trees","text":"<p>We decided to implement this demo using the open-source behvaior trees library called py-trees because of its following features:</p> <ul> <li>Open-source</li> <li>Pythonic for quicker adoption and turnaround</li> <li>Well-documented to enable users to self-learn</li> <li>Scalable to work equally well on all levels of our software stack, including ROS 2</li> <li>Well-supported to allow us to continue to build on top for the forseebale future</li> </ul>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/#prerequisites","title":"Prerequisites","text":"<ol> <li>Since this demo uses the ROS Navigation Stack to navigate to the docking station, it requires a pregenerated map that can be utilized to localize the robot. To know how to generate the map, refer to the Nav Stack tutorial.</li> <li>To understand the underlying implementation, it is important to review the concept of Behavior Trees. Although this demo does not use some of its more useful features such as a blackboard or tree visualization, a preliminary read on the concept should be sufficient to understand what's happening under the hood.</li> <li>This demo requires the Behavior Tree library called py-trees to be installed. To do this, execute the following command:</li> </ol> <pre><code>sudo apt install ros-noetic-py-trees-ros ros-noetic-rqt-py-trees\n</code></pre> <p>Once you have the above covered, we are ready to setup the demo.</p>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/#setup-and-launch","title":"Setup and Launch","text":"<p>The demo requires the docking station to be rested against a wall with the charger connected to the back and the arm to be stowed for safety. It is also necessary for the robot to be close to the origin of the map for the robot to have the correct pose estimate at startup. If not, the pose estimate will have to be supplied manually using the <code>2D Pose Estimate</code> button in RViz as soon as the demo starts.</p> <p>Let's stow the arm first: <pre><code>stretch_robot_stow.py\n</code></pre></p> <p>To launch the demo, execute the following command: <pre><code>roslaunch stretch_demos autodocking.launch map_yaml:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre></p> <p> </p>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/#how-it-works","title":"How It Works","text":"<p>Below is a simplified version of the behavior tree we implemented for this demo. Please be advised that the actual implementation has minor variations, but the below image should serve well to understand the control flow. Before moving ahead, we recommend supplementing reading this tutorial with the concept of behaviour trees. Let's dig in!</p> <p>The root of our tree is the <code>sequence</code> node (right-pointing arrow) below the <code>repeat</code> decorator node (circular arrow). This sequence node succeeds only when all of its children succeed, else it returns a failure.</p> <p> </p> <p>It has the <code>fallback</code> node (question mark) at the left as its first child. As per rules of tree traversal, this fallback node is the first to be executed. In turn, this fallback nodes has two children - the <code>Predock found?</code> condition node and the <code>Camera scan</code> action node. The <code>Predock found?</code> condition node is a subscriber that waits for the predock pose to be published on a topic called <code>\\predock_pose</code>. At the start of the demo, we expect this to fail as the robot does not know where the pose is. This triggers the <code>Camera scan</code> action node which is an action client for the <code>ArucoHeadScan</code> action server that detects the docking station ArUco marker. If this action node succeeds it published the predock pose and the next child of the sequence node is ticked.</p> <p>The second child of the sequence node is again a <code>fallback</code> node with two children - the <code>At predock?</code> condition node and the <code>Move to predock</code> action node. The <code>At predock?</code> condition node is simply a TF lookup, wrapped in a Behavior Tree class called CheckTF, that checks if the base_link frame is aligned with the predock_pose frame. We expect this to fail initially as the robot needs to travel to the predock pose for this condition to be met. This triggers the <code>Move to predock</code> action node which is an action client for the MoveBase action server from the Nav stack. This action client passes the predock pose as the goal to the robot. If this action succeeds, the robot navigates to the predock pose and the next child of the root node is triggered.</p> <p>The third child of the root node is the <code>Move to dock</code> action node. This is a simple error-based controller wrapped in a Behavior Tree class called <code>VisualServoing</code>. It's working is explained in the image below. This controller enables the robot to back up and align itself to the docking station in case the navigation stack introduces error in getting to the predock pose. </p> <p> </p> <p>The fourth and final child of the sequence node is another <code>fallback</code> node with two children - the <code>Charging?</code> condition node and the <code>Move to predock</code> action node with an <code>inverter</code> decorator node (+/- sign). The <code>Charging?</code> condition node is a subscriber that checks if the 'present' attribute of the <code>BatteryState</code> message is True. If the robot has backed up correctly into the docking station and the charger port latched, this node should return SUCCESS and the autodocking would succeed. If not, the robot moves back to the predock pose through the <code>Move to predock</code> action node and tries again.</p>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/#code-breakdown","title":"Code Breakdown","text":"<p>Let's jump into the code to see how things work under the hood. Follow along here to have a look at the entire script.</p> <p>We start off by importing the dependencies. The ones of interest are those relating to py-trees and the various behaviour classes in autodocking.autodocking_behaviours, namely, MoveBaseActionClient, CheckTF and VisualServoing. We also created custom ROS action messages for the ArucoHeadScan action defined in the action directory of stretch_demos package. <pre><code>import py_trees\nimport py_trees_ros\nimport py_trees.console as console\nimport rospy\nimport sys\nimport functools\nfrom autodocking.autodocking_behaviours import MoveBaseActionClient\nfrom autodocking.autodocking_behaviours import CheckTF\nfrom autodocking.autodocking_behaviours import VisualServoing\nfrom stretch_core.msg import ArucoHeadScanAction, ArucoHeadScanGoal\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import BatteryState\nimport hello_helpers.hello_misc as hm\n</code></pre></p> <p>The main class of this script is the AutodockingBT class which is a subclass of HelloNode. <pre><code>class AutodockingBT(hm.HelloNode):\n</code></pre></p> <p>The create_root() method is where we construct the autodocking behavior tree. As seen in the figure above, the root node of the behavior tree is a sequence node called <code>autodocking_seq_root</code>. This sequence node executes its child nodes sequentially until either all of them succeed or one of them fails. It begins by executing its first child node called <code>dock_found_fb</code>. </p> <p>The <code>dock_found_fb</code> node is a fallback node which starts executing from the left-most child node and only executes the following child node if the child node preceeding it fails. This is useful for executing recovery behaviors in case a required condition is not met. Similarly, <code>at_predock_fb</code> and <code>charging_fb</code> are also fallback nodes. <pre><code>    def create_root(self):\n        # behaviours\n\n        autodocking_seq_root = py_trees.composites.Sequence(\"autodocking\")\n        dock_found_fb = py_trees.composites.Selector(\"dock_found_fb\")\n        at_predock_fb = py_trees.composites.Selector(\"at_predock_fb\")\n        charging_fb = py_trees.composites.Selector(\"charging_fb\")\n</code></pre></p> <p>The node <code>predock_found_sub</code> is a behavior node which is a child of the <code>dock_found_fb</code> fallback node. This node subscribes to the <code>/predock_pose</code> topic to check for incoming messages. It returns SUCCESS when a predock pose is being published. At the start of the demo, since the robot likely does not have the docking station in its view, no messages are received on this topic. The fallback to this condition would be to scan the area using the head camera. The <code>head_scan_action</code> action node sends a goal to the <code>ArucoHeadScan</code> server to look for the marker number 245 at a camera tilt angle of -0.68 rads through ArucoHeadScanGoal(). If this action returns SUCCESS, we start receiving the predock_pose. <pre><code>        predock_found_sub = py_trees_ros.subscribers.CheckData(\n            name=\"predock_found_sub?\",\n            topic_name='/predock_pose',\n            expected_value=None,\n            topic_type=Pose,\n            fail_if_no_data=True,fail_if_bad_comparison=False)\n\n        aruco_goal = ArucoHeadScanGoal()\n        aruco_goal.aruco_id = 245\n        aruco_goal.tilt_angle = -0.68\n        aruco_goal.publish_to_map = True\n        aruco_goal.fill_in_blindspot_with_second_scan = False\n        aruco_goal.fast_scan = False\n        head_scan_action = py_trees_ros.actions.ActionClient( # Publishes predock pose to /predock_pose topic and tf frame called /predock_pose\n            name=\"ArucoHeadScan\",\n            action_namespace=\"ArucoHeadScan\",\n            action_spec=ArucoHeadScanAction,\n            action_goal=aruco_goal,\n            override_feedback_message_on_running=\"rotating\"\n        )\n</code></pre></p> <p>Next, we want to move to the predock_pose. We do this by passing the predock pose as a goal to the Move Base action server using the <code>predock_action</code>. This is followed by the <code>dock_action</code> action node which uses a mock visual servoing controller to back up into the docking station. This action uses the predock pose to align the robot to the docking station. Internally, it publishes Twist messages on the /stretch/cmd_vel topic after computing the linear and angular velocities based on the postional and angular errors as defined by the simple controller in the image above. <pre><code>        predock_action = MoveBaseActionClient(\n            self.tf2_buffer,\n            name=\"predock_action\",\n            override_feedback_message_on_running=\"moving\"\n        )\n        invert_predock = py_trees.decorators.SuccessIsFailure(name='invert_predock', child=predock_action)\n\n        dock_action = VisualServoing(\n            name='dock_action',\n            source_frame='docking_station',\n            target_frame='charging_port',\n            override_feedback_message_on_running=\"docking\"\n        )\n</code></pre></p> <p>Finally, we define the <code>is_charging_sub</code> behavior node which, like the <code>predock_found_sub</code>, subscribes to the <code>\\battery</code> topic and checks for the <code>present</code> attribute of the BatteryState message to turn True. If this behavior node returns SUCCEES, the root node returns SUCCEESS as well. <pre><code>        is_charging_sub = py_trees_ros.subscribers.CheckData(\n            name=\"battery_charging?\",\n            topic_name='/battery',\n            variable_name='present',\n            expected_value=True,\n            topic_type=BatteryState,\n            fail_if_no_data=True,fail_if_bad_comparison=True)\n</code></pre></p> <p>Once we have defined the behavior nodes, the behavior tree can be constructed using the add_child() or add_children() methods. The root node is then returned to the caller. <pre><code>        autodocking_seq_root.add_children([dock_found_fb, at_predock_fb, dock_action, charging_fb])\n        dock_found_fb.add_children([predock_found_sub, head_scan_action])\n        at_predock_fb.add_children([predock_action])\n        charging_fb.add_children([is_charging_sub, invert_predock])\n        return autodocking_seq_root\n</code></pre></p> <p>The main() method is where the behavior tree is ticked. First, we create an instance of the BehaviorTree class using the root of the tree we created in the create_root() method. The tick_tock() method then ticks the behavior nodes in order until the root either returns a SUCCESS or a FAILURE. <pre><code>    def main(self):\n        \"\"\"\n        Entry point for the demo script.\n        \"\"\"\n        hm.HelloNode.main(self, 'autodocking', 'autodocking')\n\n        root = self.create_root()\n        self.behaviour_tree = py_trees_ros.trees.BehaviourTree(root)\n        rospy.on_shutdown(functools.partial(self.shutdown, self.behaviour_tree))\n        if not self.behaviour_tree.setup(timeout=15):\n            console.logerror(\"failed to setup the tree, aborting.\")\n            sys.exit(1)\n\n        def print_tree(tree):\n            print(py_trees.display.unicode_tree(root=tree.root, show_status=True))\n\n        try:\n            self.behaviour_tree.tick_tock(\n                500\n                # period_ms=500,\n                # number_of_iterations=py_trees.trees.CONTINUOUS_TICK_TOCK,\n                # pre_tick_handler=None,\n                # post_tick_handler=print_tree\n            )\n        except KeyboardInterrupt:\n            self.behaviour_tree.interrupt()\n</code></pre></p>"},{"location":"stretch-tutorials/ros1/autodocking_nav_stack/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore self-charging with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li>The aruco detection fails often and the user might be required to experiment with different locations for the docking station and lighting for better results</li> <li>The controller implementation is not robust to erroneous predock pose supplied by the camera and friction introduced by floor surfaces like carpet</li> <li>The current design of the docking station is minimal and it is recommended that users find ways to stick the station to the floor to prevent it from moving while docking</li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"stretch-tutorials/ros1/example_1/","title":"Mobile Base Velocity Control","text":""},{"location":"stretch-tutorials/ros1/example_1/#example-1","title":"Example 1","text":"<p>The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending <code>Twist</code> messages to a Stretch robot.</p> <p>Begin by running the following command in a new terminal.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Switch to <code>navigation</code> mode using a rosservice call. Then, in a new terminal, drive the robot forward with the move.py node.</p> <pre><code>rosservice call /switch_to_navigation_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython3 move.py\n</code></pre> <p>To stop the node from sending twist messages, type <code>Ctrl</code> + <code>c</code>.</p>"},{"location":"stretch-tutorials/ros1/example_1/#the-code","title":"The Code","text":"<p>Below is the code which will send <code>Twist</code> messages to drive the robot forward.</p> <pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom geometry_msgs.msg import Twist\n\nclass Move:\n    \"\"\"\n    A class that sends Twist messages to move the Stretch robot forward.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber.\n        :param self: The self reference.\n        \"\"\"\n        self.pub = rospy.Publisher('/stretch/cmd_vel', Twist, queue_size=1) #/stretch_diff_drive_controller/cmd_vel for gazebo\n\n    def move_forward(self):\n        \"\"\"\n        Function that publishes Twist messages\n        :param self: The self reference.\n\n        :publishes command: Twist message.\n        \"\"\"\n        command = Twist()\n        command.linear.x = 0.1\n        command.linear.y = 0.0\n        command.linear.z = 0.0\n        command.angular.x = 0.0\n        command.angular.y = 0.0\n        command.angular.z = 0.0\n        self.pub.publish(command)\n\nif __name__ == '__main__':\n    rospy.init_node('move')\n    base_motion = Move()\n    rate = rospy.Rate(10)\n    while not rospy.is_shutdown():\n        base_motion.move_forward()\n        rate.sleep()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_1/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nfrom geometry_msgs.msg import Twist\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. The <code>geometry_msgs.msg</code> import is so that we can send velocity commands to the robot.</p> <pre><code>class Move:\n    def __init__(self):\n        self.pub = rospy.Publisher('/stretch/cmd_vel', Twist, queue_size=1)#/stretch_diff_drive_controller/cmd_vel for gazebo\n</code></pre> <p>This section of code defines the talker's interface to the rest of ROS. <code>pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1)</code> declares that your node is publishing to the <code>/stretch/cmd_vel</code> topic using the message type <code>Twist</code>. The <code>queue_size</code> argument limits the amount of queued messages if any subscriber is not receiving them fast enough.</p> <pre><code>command = Twist()\n</code></pre> <p>Make a <code>Twist</code> message. We're going to set all of the elements since we can't depend on them defaulting to safe values.</p> <pre><code>command.linear.x = 0.1\ncommand.linear.y = 0.0\ncommand.linear.z = 0.0\n</code></pre> <p>A <code>Twist</code> data structure has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y or the z direction.</p> <pre><code>command.angular.x = 0.0\ncommand.angular.y = 0.0\ncommand.angular.z = 0.0\n</code></pre> <p>A <code>Twist</code> message also has three rotational velocities (in radians per second). Stretch will only respond to rotations around the z (vertical) axis.</p> <pre><code>self.pub.publish(command)\n</code></pre> <p>Publish the <code>Twist</code> commands in the previously defined topic name <code>/stretch/cmd_vel</code>.</p> <pre><code>rospy.init_node('move')\nbase_motion = Move()\nrate = rospy.Rate(10)\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node. Until rospy has this information, it cannot start communicating with the ROS Master. </p> <p>Note</p> <p>the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>The <code>rospy.Rate()</code> function creates a Rate object. With the help of its method <code>sleep()</code>, it offers a convenient way for looping at the desired rate. With its argument of 10, we should expect to go through the loop 10 times per second (as long as our processing time does not exceed 1/10th of a second!).</p> <pre><code>while not rospy.is_shutdown():\n    base_motion.move_forward()\n    rate.sleep()\n</code></pre> <p>This loop is a fairly standard rospy construct: checking the <code>rospy.is_shutdown()</code> flag and then doing work. You have to check <code>is_shutdown()</code> to check if your program should exit (e.g. if there is a <code>Ctrl-C</code> event or otherwise). The loop calls <code>rate.sleep()</code>, which sleeps just long enough to maintain the desired rate through the loop.</p>"},{"location":"stretch-tutorials/ros1/example_1/#move-stretch-in-simulation","title":"Move Stretch in Simulation","text":"<p>Using your preferred text editor, modify the topic name of the published <code>Twist</code> messages. Please review the edit in the move.py script below.</p> <pre><code>self.pub = rospy.Publisher('/stretch_diff_drive_controller/cmd_vel', Twist, queue_size=1)\n</code></pre> <p>After saving the edited node, bring up Stretch in the empty world simulation. To drive the robot with the node, type the following in a new terminal</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 move.py\n</code></pre> <p>To stop the node from sending twist messages, type <code>Ctrl</code> + <code>c</code>.</p>"},{"location":"stretch-tutorials/ros1/example_10/","title":"Example 10","text":"<p>This tutorial we will explain how to create a tf2 static broadcaster and listener.</p>"},{"location":"stretch-tutorials/ros1/example_10/#tf2-static-broadcaster","title":"tf2 Static Broadcaster","text":"<p>For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the <code>link_mast</code>, <code>link_lift</code>, and <code>link_wrist_yaw</code> frames.</p> <p>Begin by starting up the stretch driver launch file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/tf2_broadcaster_example.rviz\n</code></pre> <p>Then run the tf2_broadcaster.py node to visualize three static frames. In a new terminal, execute:</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 tf2_broadcaster.py\n</code></pre> <p>The GIF below visualizes what happens when running the previous node.</p> <p> </p> <p>Tip</p> <p>If you would like to see how the static frames update while the robot is in motion, run the stow_command_node.py and observe the tf frames in RViz.</p> <p>In a terminal, execute:</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 stow_command.py\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1/example_10/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport tf.transformations\nfrom geometry_msgs.msg import TransformStamped\nfrom tf2_ros import StaticTransformBroadcaster\n\nclass FixedFrameBroadcaster():\n    \"\"\"\n    This node publishes three child static frames in reference to their parent frames as below:\n    parent -&gt; link_mast            child -&gt; fk_link_mast\n    parent -&gt; link_lift            child -&gt; fk_link_lift\n    parent -&gt; link_wrist_yaw       child -&gt; fk_link_wrist_yaw\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that creates a broadcast node and publishes three new transform\n        frames.\n        :param self: The self reference.\n        \"\"\"\n        self.br = StaticTransformBroadcaster()\n\n        self.mast = TransformStamped()\n        self.mast.header.stamp = rospy.Time.now()\n        self.mast.header.frame_id = 'link_mast'\n        self.mast.child_frame_id = 'fk_link_mast'\n        self.mast.transform.translation.x = 0.0\n        self.mast.transform.translation.y = 2.0\n        self.mast.transform.translation.z = 0.0\n        q = tf.transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.mast.transform.rotation.x = q[0]\n        self.mast.transform.rotation.y = q[1]\n        self.mast.transform.rotation.z = q[2]\n        self.mast.transform.rotation.w = q[3]\n\n        self.lift = TransformStamped()\n        self.lift.header.stamp = rospy.Time.now()\n        self.lift.header.frame_id = 'link_lift'\n        self.lift.child_frame_id = 'fk_link_lift'\n        self.lift.transform.translation.x = 0.0\n        self.lift.transform.translation.y = 1.0\n        self.lift.transform.translation.z = 0.0\n        q = tf.transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.lift.transform.rotation.x = q[0]\n        self.lift.transform.rotation.y = q[1]\n        self.lift.transform.rotation.z = q[2]\n        self.lift.transform.rotation.w = q[3]\n\n        self.wrist = TransformStamped()\n        self.wrist.header.stamp = rospy.Time.now()\n        self.wrist.header.frame_id = 'link_wrist_yaw'\n        self.wrist.child_frame_id = 'fk_link_wrist_yaw'\n        self.wrist.transform.translation.x = 0.0\n        self.wrist.transform.translation.y = 1.0\n        self.wrist.transform.translation.z = 0.0\n        q = tf.transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.wrist.transform.rotation.x = q[0]\n        self.wrist.transform.rotation.y = q[1]\n        self.wrist.transform.rotation.z = q[2]\n        self.wrist.transform.rotation.w = q[3]\n\n        self.br.sendTransform([self.mast, self.lift, self.wrist])\n\n        rospy.loginfo('Publishing TF frames. Use RViz to visualize')\n\nif __name__ == '__main__':\n    rospy.init_node('tf2_broadcaster')\n    FixedFrameBroadcaster()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_10/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nimport tf.transformations\nfrom geometry_msgs.msg import TransformStamped\nfrom tf2_ros import StaticTransformBroadcaster\n</code></pre> <p>You need to import rospy if you are writing a ROS Node. Import <code>tf.transformations</code> to get quaternion values from Euler angles. Import the <code>TransformStamped</code> from the <code>geometry_msgs.msg</code> package because we will be publishing static frames and it requires this message type. The <code>tf2_ros</code> package provides an implementation of a <code>tf2_ros.StaticTransformBroadcaster</code> to help make the task of publishing transforms easier.</p> <pre><code>def __init__(self):\n    \"\"\"\n    A function that creates a broadcast node and publishes three new transform\n    frames.\n    :param self: The self reference.\n    \"\"\"\n    self.br = StaticTransformBroadcaster()\n</code></pre> <p>Here we create a <code>TransformStamped</code> object which will be the message we will send over once populated.</p> <pre><code>self.mast = TransformStamped()\nself.mast.header.stamp = rospy.Time.now()\nself.mast.header.frame_id = 'link_mast'\nself.mast.child_frame_id = 'fk_link_mast'\n</code></pre> <p>We need to give the transform being published a timestamp, we'll just stamp it with the current time, <code>rospy.Time.now()</code>. Then, we need to set the name of the parent frame of the link we're creating, in this case <code>link_mast</code>. Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is <code>fk_link_mast</code>.</p> <pre><code>self.mast.transform.translation.x = 0.0\nself.mast.transform.translation.y = 2.0\nself.mast.transform.translation.z = 0.0\n</code></pre> <p>Set the translation values for the child frame.</p> <pre><code>q = tf.transformations.quaternion_from_euler(1.5707, 0, -1.5707)\nself.wrist.transform.rotation.x = q[0]\nself.wrist.transform.rotation.y = q[1]\nself.wrist.transform.rotation.z = q[2]\nself.wrist.transform.rotation.w = q[3]\n</code></pre> <p>The <code>quaternion_from_euler()</code> function takes in an Euler angle as an argument and returns a quaternion. Then set the rotation values to the transformed quaternions.</p> <p>This process will be completed for the <code>link_lift</code> and <code>link_wrist_yaw</code> as well.</p> <pre><code>self.br.sendTransform([self.mast, self.lift, self.wrist])\n</code></pre> <p>Send the three transforms using the <code>sendTransform()</code> function.</p> <pre><code>rospy.init_node('tf2_broadcaster')\nFixedFrameBroadcaster()\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. </p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the <code>FixedFrameBroadcaster()</code> class.</p> <pre><code>rospy.spin()\n</code></pre> <p>Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1/example_10/#tf2-static-listener","title":"tf2 Static Listener","text":"<p>In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section, we will create a tf2 listener that will find the transform between <code>fk_link_lift</code> and <code>link_grasp_center</code>.</p> <p>Begin by starting up the stretch driver launch file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Then run the tf2_broadcaster.py node in a new terminal to create the three static frames.</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 tf2_broadcaster.py\n</code></pre> <p>Finally, run the tf2_listener.py node in a separate terminal to print the transform between two links.</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 tf2_listener.py\n</code></pre> <p>Within the terminal, the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames.</p> <pre><code>[INFO] [1659551318.098168]: The pose of target frame link_grasp_center with reference from fk_link_lift is:\ntranslation:\n  x: 1.08415191335\n  y: -0.176147838153\n  z: 0.576720021135\nrotation:\n  x: -0.479004489528\n  y: -0.508053545368\n  z: -0.502884087254\n  w: 0.509454501243\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1/example_10/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom geometry_msgs.msg import TransformStamped\nimport tf2_ros\n\nclass FrameListener():\n    \"\"\"\n    This Class prints the transformation between the fk_link_mast frame and the\n    target frame, link_grasp_center.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes the variables and looks up a transformation\n        between a target and source frame.\n        :param self: The self reference.\n        \"\"\"\n        tf_buffer = tf2_ros.Buffer()\n        listener = tf2_ros.TransformListener(tf_buffer)\n\n        from_frame_rel = 'link_grasp_center'\n        to_frame_rel = 'fk_link_lift'\n\n        rospy.sleep(1.0)\n        rate = rospy.Rate(1)\n\n        while not rospy.is_shutdown():\n            try:\n                trans = tf_buffer.lookup_transform(to_frame_rel,\n                                                   from_frame_rel,\n                                                   rospy.Time())\n                rospy.loginfo('The pose of target frame %s with reference from %s is: \\n %s', from_frame_rel, to_frame_rel, trans.transform)\n            except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n                rospy.logwarn(' Could not transform %s from %s ', to_frame_rel, from_frame_rel)\n\n            rate.sleep()\n\nif __name__ == '__main__':\n    rospy.init_node('tf2_listener')\n    FrameListener()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_10/#the-code-explained_1","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nfrom geometry_msgs.msg import TransformStamped\nimport tf2_ros\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. Import the <code>TransformStamped</code> from the <code>geometry_msgs.msg</code> package because we will be publishing static frames and it requires this message type. The <code>tf2_ros</code> package provides an implementation of a <code>tf2_ros.TransformListener</code> to help make the task of receiving transforms easier.</p> <pre><code>tf_buffer = tf2_ros.Buffer()\nlistener = tf2_ros.TransformListener(tf_buffer)\n</code></pre> <p>Here, we create a <code>TransformListener</code> object. Once the listener is created, it starts receiving tf2 transformations and buffers them for up to 10 seconds.</p> <p><pre><code>from_frame_rel = 'link_grasp_center'\nto_frame_rel = 'fk_link_lift'\n</code></pre> Store frame names in variables that will be used to compute transformations.</p> <pre><code>rospy.sleep(1.0)\nrate = rospy.Rate(1)\n</code></pre> <p>The first line gives the listener some time to accumulate transforms. The second line is the rate at which the node is going to publish information (1 Hz).</p> <pre><code>try:\n    trans = tf_buffer.lookup_transform(to_frame_rel,\n                                       from_frame_rel,\n                                       rospy.Time())\n    rospy.loginfo('The pose of target frame %s with reference from %s is: \\n %s', from_frame_rel, to_frame_rel, trans.transform)\n\nexcept (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n    rospy.logwarn(' Could not transform %s from %s ', to_frame_rel, from_frame_rel)\n</code></pre> <p>Try to look up the transformation we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between <code>from_frame_rel</code> and <code>to_frame_rel</code> frames with the <code>lookup_transform()</code> function.</p> <pre><code>rospy.init_node('tf2_listener')\nFrameListener()\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. </p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the <code>FrameListener()</code> class.</p> <pre><code>rospy.spin()\n</code></pre> <p>Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1/example_11/","title":"PointCloud Transformation","text":""},{"location":"stretch-tutorials/ros1/example_11/#example-11","title":"Example 11","text":"<p>This tutorial highlights how to create a PointCloud message from the data of a PointCloud2 message type, then transform the PointCloud's reference link to a different frame. The data published by RealSense is referencing its <code>camera_color_optical_frame</code> link, and we will be changing its reference to the <code>base_link</code>.</p> <p>Begin by starting up the stretch driver launch file.</p> <p><pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <p><pre><code>roslaunch stretch_core d435i_low_resolution.launch\n</code></pre> Then run the pointCloud_transformer.py node. In a new terminal, execute:</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 pointcloud_transformer.py\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the <code>PointCloud</code> in the Display tree. You can visualize this topic and the robot model by running the command below in a new terminal.</p> <pre><code>rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/PointCloud_transformer_example.rviz\n</code></pre> <p>The GIF below visualizes what happens when running the previous node.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/example_11/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rospy\nimport tf\nimport sensor_msgs.point_cloud2 as pc2\nfrom sensor_msgs.msg import PointCloud2, PointCloud\nfrom geometry_msgs.msg import Point32\nfrom std_msgs.msg import Header\n\nclass PointCloudTransformer:\n    \"\"\"\n    A class that takes in a PointCloud2 message and stores its points into a\n    PointCloud message. Then that PointCloud is transformed to reference the\n    'base_link' frame.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber, publisher, and other variables.\n        :param self: The self reference.\n        \"\"\"\n        self.pointcloud2_sub = rospy.Subscriber(\"/camera/depth/color/points\", PointCloud2, self.callback_pcl2, queue_size=1)\n        self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1)\n        self.pcl2_cloud = None\n        self.listener = tf.TransformListener(True, rospy.Duration(10.0))\n        rospy.loginfo('Publishing transformed PointCloud. Use RViz to visualize')\n\n    def callback_pcl2(self,msg):\n        \"\"\"\n        Callback function that stores the PointCloud2 message.\n        :param self: The self reference.\n        :param msg: The PointCloud2 message type.\n        \"\"\"\n        self.pcl2_cloud = msg\n\n    def pcl_transformer(self):\n        \"\"\"\n        A function that extracts the points from the stored PointCloud2 message\n        and appends those points to a PointCloud message. Then the function transforms\n        the PointCloud from its the header frame id, 'camera_color_optical_frame'\n        to the 'base_link' frame.\n        :param self: The self reference.\n        \"\"\"\n        temp_cloud = PointCloud()\n        temp_cloud.header = self.pcl2_cloud.header\n        for data in pc2.read_points(self.pcl2_cloud, skip_nans=True):\n            temp_cloud.points.append(Point32(data[0],data[1],data[2]))\n\n        transformed_cloud = self.transform_pointcloud(temp_cloud)\n        self.pointcloud_pub.publish(transformed_cloud)\n\n    def transform_pointcloud(self,msg):\n        \"\"\"\n        Function that stores the PointCloud2 message.\n        :param self: The self reference.\n        :param msg: The PointCloud message.\n\n        :returns new_cloud: The transformed PointCloud message.\n        \"\"\"\n        while not rospy.is_shutdown():\n            try:\n                new_cloud = self.listener.transformPointCloud(\"base_link\" ,msg)\n                return new_cloud\n                if new_cloud:\n                    break\n            except (tf.LookupException, tf.ConnectivityException,tf.ExtrapolationException):\n                pass\n\nif __name__==\"__main__\":\n    rospy.init_node('pointcloud_transformer',anonymous=True)\n    PCT = PointCloudTransformer()\n    rate = rospy.Rate(1)\n    rospy.sleep(1)\n\n    while not rospy.is_shutdown():\n        PCT.pcl_transformer()\n        rate.sleep()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_11/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nimport tf\nimport sensor_msgs.point_cloud2 as pc2\nfrom sensor_msgs.msg import PointCloud2, PointCloud\nfrom geometry_msgs.msg import Point32\nfrom std_msgs.msg import Header\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. Import <code>tf</code> to utilize the <code>transformPointCloud</code> function. Import various message types from <code>sensor_msgs</code>.</p> <pre><code>self.pointcloud2_sub = rospy.Subscriber(\"/camera/depth/color/points\", PointCloud2, self.callback_pcl2, queue_size=1)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>/camera/depth/color/points</code>, looking for <code>PointCloud2</code> message.  When a message comes in, ROS is going to pass it to the function <code>callback_pcl2()</code> automatically.</p> <pre><code>self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1)\n</code></pre> <p>This section of code defines the talker's interface to the rest of ROS. <code>self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1)</code> declares that your node is publishing to the <code>/camera_cloud</code> topic using the message type <code>PointCloud</code>.</p> <pre><code>self.pcl2_cloud = None\nself.listener = tf.TransformListener(True, rospy.Duration(10.0))\n</code></pre> <p>The first line of code initializes <code>self.pcl2_cloud</code> to store the <code>PointCloud2</code> message. The second line creates a <code>tf.TransformListener</code> object. Once the listener is created, it starts receiving tf transformations and buffers them for up to 10 seconds.</p> <pre><code>def callback_pcl2(self,msg):\n    \"\"\"\n    Callback function that stores the PointCloud2 message.\n    :param self: The self reference.\n    :param msg: The PointCloud2 message type.\n    \"\"\"\n    self.pcl2_cloud = msg\n</code></pre> <p>The callback function then stores the <code>PointCloud2</code> message.</p> <pre><code>temp_cloud = PointCloud()\ntemp_cloud.header = self.pcl2_cloud.header\n</code></pre> <p>Create a <code>PointCloud</code> for temporary use. Set the temporary PointCloud header to the stored <code>PointCloud2</code> header.</p> <pre><code>for data in pc2.read_points(self.pcl2_cloud, skip_nans=True):\n  temp_cloud.points.append(Point32(data[0],data[1],data[2]))\n</code></pre> <p>Use a for loop to extract <code>PointCloud2</code> data into a list of x, y, and z points and append those values to the <code>PointCloud</code> message, <code>temp_cloud</code>.</p> <pre><code>transformed_cloud = self.transform_pointcloud(temp_cloud)\n</code></pre> <p>Utilize the <code>transform_pointcloud</code> function to transform the points in the <code>PointCloud</code> message to reference the <code>base_link</code></p> <pre><code>while not rospy.is_shutdown():\n        try:\n            new_cloud = self.listener.transformPointCloud(\"base_link\" ,msg)\n            return new_cloud\n            if new_cloud:\n                break\n        except (tf.LookupException, tf.ConnectivityException,tf.ExtrapolationException):\n            pass\n</code></pre> <p>Try to look up and transform the <code>PointCloud</code> input. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Transform the point cloud data from <code>camera_color_optical_frame</code> to <code>base_link</code> with the <code>transformPointCloud()</code> function.</p> <pre><code>self.pointcloud_pub.publish(transformed_cloud)\n</code></pre> <p>Publish the new transformed <code>PointCloud</code>.</p> <pre><code>rospy.init_node('pointcloud_transformer',anonymous=True)\nPCT = PointCloudTransformer()\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. </p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Declare a <code>PointCloudTransformer</code> object.</p> <pre><code>rate = rospy.Rate(1)\nrospy.sleep(1)\n</code></pre> <p>The first line gives the listener some time to accumulate transforms. The second line is the rate at which the node is going to publish information (1 Hz).</p> <pre><code>  while not rospy.is_shutdown():\n      PCT.pcl_transformer()\n      rate.sleep()\n</code></pre> <p>Run a while loop until the node is shut down. Within the while loop run the <code>pcl_transformer()</code> method.</p>"},{"location":"stretch-tutorials/ros1/example_12/","title":"Example 12","text":"<p>For this example, we will send follow joint trajectory commands for the head camera to search and locate an ArUco tag. In this instance, a Stretch robot will try to locate the docking station's ArUco tag.</p>"},{"location":"stretch-tutorials/ros1/example_12/#modifying-stretch-marker-dictionary-yaml-file","title":"Modifying Stretch Marker Dictionary YAML File","text":"<p>When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml, that holds the information about the markers. A further breakdown of the YAML file can be found in our Aruco Marker Detection tutorial.</p> <p>Below is what needs to be included in the stretch_marker_dict.yaml file so the detect_aruco_markers node can find the docking station's ArUco tag.</p> <pre><code>'245':\n  'length_mm': 88.0\n  'use_rgb_only': False\n  'name': 'docking_station'\n  'link': None\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_12/#getting-started","title":"Getting Started","text":"<p>Begin by running the stretch driver launch file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>roslaunch stretch_core d435i_high_resolution.launch\n</code></pre> <p>Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. In a new terminal, execute:</p> <pre><code>roslaunch stretch_core stretch_aruco.launch\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz\n</code></pre> <p>Then run the aruco_tag_locator.py node. In a new terminal, execute:</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 aruco_tag_locator.py\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1/example_12/#the-code","title":"The Code","text":"<pre><code>#! /usr/bin/env python3\n\nimport rospy\nimport time\nimport tf2_ros\nimport numpy as np\nfrom math import pi\n\nimport hello_helpers.hello_misc as hm\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom geometry_msgs.msg import TransformStamped\n\nclass LocateArUcoTag(hm.HelloNode):\n    \"\"\"\n    A class that actuates the RealSense camera to find the docking station's\n    ArUco tag and returns a Transform between the `base_link` and the requested tag.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes the subscriber and other needed variables.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n\n        self.joint_states_sub = rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)\n        self.transform_pub = rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10)\n\n        self.joint_state = None\n\n        self.min_pan_position = -4.10\n        self.max_pan_position =  1.50\n        self.pan_num_steps = 10\n        self.pan_step_size = abs(self.min_pan_position - self.max_pan_position)/self.pan_num_steps\n\n        self.min_tilt_position = -0.75\n        self.tilt_num_steps = 3\n        self.tilt_step_size = pi/16\n\n        self.rot_vel = 0.5 # radians per sec\n\n    def joint_states_callback(self, msg):\n        \"\"\"\n        A callback function that stores Stretch's joint states.\n        :param self: The self reference.\n        :param msg: The JointState message type.\n        \"\"\"\n        self.joint_state = msg\n\n    def send_command(self, command):\n        '''\n        Handles single joint control commands by constructing a FollowJointTrajectoryGoal\n        message and sending it to the trajectory_client created in hello_misc.\n        :param self: The self reference.\n        :param command: A dictionary message type.\n        '''\n        if (self.joint_state is not None) and (command is not None):\n            joint_name = command['joint']\n            trajectory_goal = FollowJointTrajectoryGoal()\n            trajectory_goal.trajectory.joint_names = [joint_name]\n            point = JointTrajectoryPoint()\n\n            if 'delta' in command:\n                joint_index = self.joint_state.name.index(joint_name)\n                joint_value = self.joint_state.position[joint_index]\n                delta = command['delta']\n                new_value = joint_value + delta\n                point.positions = [new_value]\n\n            elif 'position' in command:\n                point.positions = [command['position']]\n\n            point.velocities = [self.rot_vel]\n            trajectory_goal.trajectory.points = [point]\n            trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n            trajectory_goal.trajectory.header.frame_id = 'base_link'\n            self.trajectory_client.send_goal(trajectory_goal)\n            self.trajectory_client.wait_for_result()\n\n    def find_tag(self, tag_name='docking_station'):\n        \"\"\"\n        A function that actuates the camera to search for a defined ArUco tag\n        marker. Then the function returns the pose\n        :param self: The self reference.\n        :param tag_name: A string value of the ArUco marker name.\n\n        :returns transform: The docking station's TransformStamped message.\n        \"\"\"\n        pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n        self.send_command(pan_command)\n        tilt_command = {'joint': 'joint_head_tilt', 'position': self.min_tilt_position}\n        self.send_command(tilt_command)\n\n        for i in range(self.tilt_num_steps):\n            for j in range(self.pan_num_steps):\n                pan_command = {'joint': 'joint_head_pan', 'delta': self.pan_step_size}\n                self.send_command(pan_command)\n                rospy.sleep(0.2)\n\n                try:\n                    transform = self.tf_buffer.lookup_transform('base_link',\n                                                                tag_name,\n                                                                rospy.Time())\n                    rospy.loginfo(\"Found Requested Tag: \\n%s\", transform)\n                    self.transform_pub.publish(transform)\n                    return transform\n                except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n                    continue\n\n            pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n            self.send_command(pan_command)\n            tilt_command = {'joint': 'joint_head_tilt', 'delta': self.tilt_step_size}\n            self.send_command(tilt_command)\n            rospy.sleep(.25)\n\n        rospy.loginfo(\"The requested tag '%s' was not found\", tag_name)\n\n    def main(self):\n        \"\"\"\n        Function that initiates the issue_command function.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.main(self, 'aruco_tag_locator', 'aruco_tag_locator', wait_for_first_pointcloud=False)\n        self.static_broadcaster = tf2_ros.StaticTransformBroadcaster()\n        self.tf_buffer = tf2_ros.Buffer()\n        self.listener = tf2_ros.TransformListener(self.tf_buffer)\n        rospy.sleep(1.0)\n        rospy.loginfo('Searching for docking ArUco tag.')\n        pose = self.find_tag(\"docking_station\")\n\nif __name__ == '__main__':\n    try:\n        node = LocateArUcoTag()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_12/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nimport time\nimport tf2_ros\nimport numpy as np\nfrom math import pi\n\nimport hello_helpers.hello_misc as hm\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom geometry_msgs.msg import TransformStamped\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. Import other python modules needed for this node. Import the <code>FollowJointTrajectoryGoal</code> from the control_msgs.msg package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.</p> <pre><code>def __init__(self):\n    \"\"\"\n    A function that initializes the subscriber and other needed variables.\n    :param self: The self reference.\n    \"\"\"\n    hm.HelloNode.__init__(self)\n\n    self.joint_states_sub = rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)\n    self.transform_pub = rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10)\n\n    self.joint_state = None\n</code></pre> <p>The <code>LocateArUcoTag</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is instantiated.</p> <p>Set up a subscriber with <code>rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)</code>.  We're going to subscribe to the topic <code>stretch/joint_states</code>, looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function <code>joint_states_callback()</code> automatically.</p> <p><code>rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10)</code> declares that your node is publishing to the <code>ArUco_transform</code> topic using the message type <code>TransformStamped</code>. The <code>queue_size</code> argument limits the amount of queued messages if any subscriber is not receiving them fast enough.</p> <pre><code>self.min_pan_position = -4.10\nself.max_pan_position =  1.50\nself.pan_num_steps = 10\nself.pan_step_size = abs(self.min_pan_position - self.max_pan_position)/self.pan_num_steps\n</code></pre> <p>Provide the minimum and maximum joint positions for the head pan. These values are needed for sweeping the head to search for the ArUco tag. We also define the number of steps for the sweep, then create the step size for the head pan joint.</p> <pre><code>self.min_tilt_position = -0.75\nself.tilt_num_steps = 3\nself.tilt_step_size = pi/16\n</code></pre> <p>Set the minimum position of the tilt joint, the number of steps, and the size of each step.</p> <pre><code>self.rot_vel = 0.5 # radians per sec\n</code></pre> <p>Define the head actuation rotational velocity.</p> <pre><code>def joint_states_callback(self, msg):\n    \"\"\"\n    A callback function that stores Stretch's joint states.\n    :param self: The self reference.\n    :param msg: The JointState message type.\n    \"\"\"\n    self.joint_state = msg\n</code></pre> <p>The <code>joint_states_callback()</code> function stores Stretch's joint states.</p> <pre><code>def send_command(self, command):\n    '''\n    Handles single joint control commands by constructing a FollowJointTrajectoryGoal\n    message and sending it to the trajectory_client created in hello_misc.\n    :param self: The self reference.\n    :param command: A dictionary message type.\n    '''\n    if (self.joint_state is not None) and (command is not None):\n        joint_name = command['joint']\n        trajectory_goal = FollowJointTrajectoryGoal()\n        trajectory_goal.trajectory.joint_names = [joint_name]\n        point = JointTrajectoryPoint()\n</code></pre> <p>Assign <code>trajectory_goal</code> as a <code>FollowJointTrajectoryGoal</code> message type. Then extract the string value from the <code>joint</code> key. Also, assign <code>point</code> as a <code>JointTrajectoryPoint</code> message type.</p> <pre><code>if 'delta' in command:\n    joint_index = self.joint_state.name.index(joint_name)\n    joint_value = self.joint_state.position[joint_index]\n    delta = command['delta']\n    new_value = joint_value + delta\n    point.positions = [new_value]\n</code></pre> <p>Check to see if <code>delta</code> is a key in the command dictionary. Then get the current position of the joint and add the delta as a new position value.</p> <pre><code>elif 'position' in command:\n    point.positions = [command['position']]\n</code></pre> <p>Check to see if <code>position</code> is a key in the command dictionary. Then extract the position value.</p> <pre><code>point.velocities = [self.rot_vel]\ntrajectory_goal.trajectory.points = [point]\ntrajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\ntrajectory_goal.trajectory.header.frame_id = 'base_link'\nself.trajectory_client.send_goal(trajectory_goal)\nself.trajectory_client.wait_for_result()\n</code></pre> <p>Then <code>trajectory_goal.trajectory.points</code> is defined by the positions set in <code>point</code>. Specify the coordinate frame that we want (base_link) and set the time to be now. Make the action call and send the goal. The last line of code waits for the result before it exits the python script.</p> <pre><code>def find_tag(self, tag_name='docking_station'):\n    \"\"\"\n    A function that actuates the camera to search for a defined ArUco tag\n    marker. Then the function returns the pose\n    :param self: The self reference.\n    :param tag_name: A string value of the ArUco marker name.\n\n    :returns transform: The docking station's TransformStamped message.\n    \"\"\"\n    pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n    self.send_command(pan_command)\n    tilt_command = {'joint': 'joint_head_tilt', 'position': self.min_tilt_position}\n    self.send_command(tilt_command)\n</code></pre> <p>Create a dictionary to get the head in its initial position for its search and send the commands with the <code>send_command()</code> function.</p> <pre><code>for i in range(self.tilt_num_steps):\n    for j in range(self.pan_num_steps):\n        pan_command = {'joint': 'joint_head_pan', 'delta': self.pan_step_size}\n        self.send_command(pan_command)\n        rospy.sleep(0.5)\n</code></pre> <p>Utilize a nested for loop to sweep the pan and tilt in increments. Then update the <code>joint_head_pan</code> position by the <code>pan_step_size</code>. Use <code>rospy.sleep()</code> function to give time to the system to do a Transform lookup before the next step.</p> <pre><code>try:\n    transform = self.tf_buffer.lookup_transform('base_link',\n                                                tag_name,\n                                                rospy.Time())\n    rospy.loginfo(\"Found Requested Tag: \\n%s\", transform)\n    self.transform_pub.publish(transform)\n    return transform\nexcept (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n    continue\n</code></pre> <p>Use a try-except block to look up the transform between the base_link and the requested ArUco tag. Then publish and return the <code>TransformStamped</code> message.</p> <pre><code>pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\nself.send_command(pan_command)\ntilt_command = {'joint': 'joint_head_tilt', 'delta': self.tilt_step_size}\nself.send_command(tilt_command)\nrospy.sleep(.25)\n</code></pre> <p>Begin sweep with new tilt angle.</p> <pre><code>def main(self):\n    \"\"\"\n    Function that initiates the issue_command function.\n    :param self: The self reference.\n    \"\"\"\n    hm.HelloNode.main(self, 'aruco_tag_locator', 'aruco_tag_locator', wait_for_first_pointcloud=False)\n</code></pre> <p>Create a function, <code>main()</code>, to do the setup for the <code>hm.HelloNode</code> class and initialize the <code>aruco_tag_locator</code> node.</p> <pre><code>self.static_broadcaster = tf2_ros.StaticTransformBroadcaster()\nself.tf_buffer = tf2_ros.Buffer()\nself.listener = tf2_ros.TransformListener(self.tf_buffer)\nrospy.sleep(1.0)\n</code></pre> <p>Create a StaticTranformBoradcaster Node. Also, start a tf buffer that will store the tf information for a few seconds. Then set up a tf listener, which will subscribe to all of the relevant tf topics, and keep track of the information. Include <code>rospy.sleep(1.0)</code> to give the listener some time to accumulate transforms.</p> <pre><code>rospy.loginfo('Searching for docking ArUco tag.')\npose = self.find_tag(\"docking_station\")\n</code></pre> <p>Notice Stretch is searching for the ArUco tag with a <code>rospy.loginfo()</code> function. Then search for the ArUco marker for the docking station.</p> <pre><code>if __name__ == '__main__':\n    try:\n        node = LocateArUcoTag()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre> <p>Declare <code>LocateArUcoTag</code> object. Then run the <code>main()</code> method.</p>"},{"location":"stretch-tutorials/ros1/example_13/","title":"Example 13","text":"<p>In this example, we will be utilizing the move_base package, a component of the ROS navigation stack, to send base goals to the Stretch robot.</p>"},{"location":"stretch-tutorials/ros1/example_13/#build-a-map","title":"Build a map","text":"<p>First, begin by building a map of the space the robot will be navigating in. If you need a refresher on how to do this, then check out the Navigation Stack tutorial.</p>"},{"location":"stretch-tutorials/ros1/example_13/#getting-started","title":"Getting Started","text":"<p>Next, with your created map, we can navigate the robot around the mapped space. Run:</p> <pre><code>roslaunch stretch_navigation navigation.launch map_yaml:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p>Where <code>${HELLO_FLEET_PATH}</code> is the path of the <code>&lt;map_name&gt;.yaml</code> file.</p> <p>Note</p> <p>It's likely that the robot's location on the map does not match the robot's location in real space. In the top bar of Rviz, use <code>2D Pose Estimate</code> to lay an arrow down roughly where the robot is located in real space. Below is a gif for reference.</p> <p> </p> <p>Now we are going to use a node to send a move_base goal half a meter in front of the map's origin. run the following command in a new terminal to execute the navigation.py node.</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 navigation.py\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_13/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport actionlib\nimport sys\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\nfrom geometry_msgs.msg import Quaternion\nfrom tf import transformations\n\nclass StretchNavigation:\n    \"\"\"\n    A simple encapsulation of the navigation stack for a Stretch robot.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Create an instance of the simple navigation interface.\n        :param self: The self reference.\n        \"\"\"\n        self.client = actionlib.SimpleActionClient('move_base', MoveBaseAction)\n        self.client.wait_for_server()\n        rospy.loginfo('{0}: Made contact with move_base server'.format(self.__class__.__name__))\n\n        self.goal = MoveBaseGoal()\n        self.goal.target_pose.header.frame_id = 'map'\n        self.goal.target_pose.header.stamp = rospy.Time()\n\n        self.goal.target_pose.pose.position.x = 0.0\n        self.goal.target_pose.pose.position.y = 0.0\n        self.goal.target_pose.pose.position.z = 0.0\n        self.goal.target_pose.pose.orientation.x = 0.0\n        self.goal.target_pose.pose.orientation.y = 0.0\n        self.goal.target_pose.pose.orientation.z = 0.0\n        self.goal.target_pose.pose.orientation.w = 1.0\n\n    def get_quaternion(self,theta):\n        \"\"\"\n        A function to build Quaternians from Euler angles. Since the Stretch only\n        rotates around z, we can zero out the other angles.\n        :param theta: The angle (radians) the robot makes with the x-axis.\n        \"\"\"\n        return Quaternion(*transformations.quaternion_from_euler(0.0, 0.0, theta))\n\n    def go_to(self, x, y, theta):\n        \"\"\"\n        Drive the robot to a particular pose on the map. The Stretch only needs\n        (x, y) coordinates and a heading.\n        :param x: x coordinate in the map frame.\n        :param y: y coordinate in the map frame.\n        :param theta: heading (angle with the x-axis in the map frame)\n        \"\"\"\n        rospy.loginfo('{0}: Heading for ({1}, {2}) at {3} radians'.format(self.__class__.__name__,\n        x, y, theta))\n\n        self.goal.target_pose.pose.position.x = x\n        self.goal.target_pose.pose.position.y = y\n        self.goal.target_pose.pose.orientation = self.get_quaternion(theta)\n\n        self.client.send_goal(self.goal, done_cb=self.done_callback)\n        self.client.wait_for_result()\n\n    def done_callback(self, status, result):\n        \"\"\"\n        The done_callback function will be called when the joint action is complete.\n        :param self: The self reference.\n        :param status: status attribute from MoveBaseActionResult message.\n        :param result: result attribute from MoveBaseActionResult message.\n        \"\"\"\n        if status == actionlib.GoalStatus.SUCCEEDED:\n            rospy.loginfo('{0}: SUCCEEDED in reaching the goal.'.format(self.__class__.__name__))\n        else:\n            rospy.loginfo('{0}: FAILED in reaching the goal.'.format(self.__class__.__name__))\n\nif __name__ == '__main__':\n    rospy.init_node('navigation', argv=sys.argv)\n    nav = StretchNavigation()\n    nav.go_to(0.5, 0.0, 0.0)\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_13/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nimport actionlib\nimport sys\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\nfrom geometry_msgs.msg import Quaternion\nfrom tf import transformations\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node.</p> <pre><code>self.client = actionlib.SimpleActionClient('move_base', MoveBaseAction)\nself.client.wait_for_server()\nrospy.loginfo('{0}: Made contact with move_base server'.format(self.__class__.__name__))\n</code></pre> <p>Set up a client for the navigation action. On the Stretch, this is called <code>move_base</code>, and has type <code>MoveBaseAction</code>.  Once we make the client, we wait for the server to be ready.</p> <pre><code>self.goal = MoveBaseGoal()\nself.goal.target_pose.header.frame_id = 'map'\nself.goal.target_pose.header.stamp = rospy.Time()\n</code></pre> <p>Make a goal for the action. Specify the coordinate frame that we want, in this instance the <code>map</code> frame. Then we set the time to be now.</p> <pre><code>self.goal.target_pose.pose.position.x = 0.0\nself.goal.target_pose.pose.position.y = 0.0\nself.goal.target_pose.pose.position.z = 0.0\nself.goal.target_pose.pose.orientation.x = 0.0\nself.goal.target_pose.pose.orientation.y = 0.0\nself.goal.target_pose.pose.orientation.z = 0.0\nself.goal.target_pose.pose.orientation.w = 1.0\n</code></pre> <p>Initialize a position in the coordinate frame.</p> <pre><code>def get_quaternion(self,theta):\n    \"\"\"\n    A function to build Quaternians from Euler angles. Since the Stretch only\n    rotates around z, we can zero out the other angles.\n    :param theta: The angle (radians) the robot makes with the x-axis.\n    \"\"\"\n    return Quaternion(*transformations.quaternion_from_euler(0.0, 0.0, theta))\n</code></pre> <p>A function that transforms Euler angles to quaternions and returns those values.</p> <pre><code>def go_to(self, x, y, theta, wait=False):\n    \"\"\"\n    Drive the robot to a particular pose on the map. The Stretch only needs\n    (x, y) coordinates and a heading.\n    :param x: x coordinate in the map frame.\n    :param y: y coordinate in the map frame.\n    :param theta: heading (angle with the x-axis in the map frame)\n    \"\"\"\n    rospy.loginfo('{0}: Heading for ({1}, {2}) at {3} radians'.format(self.__class__.__name__,\n    x, y, theta))\n</code></pre> <p>The <code>go_to()</code> function takes in the 3 arguments, the x and y coordinates in the <code>map</code> frame, and the heading.</p> <pre><code>self.goal.target_pose.pose.position.x = x\nself.goal.target_pose.pose.position.y = y\nself.goal.target_pose.pose.orientation = self.get_quaternion(theta)\n</code></pre> <p>The <code>MoveBaseGoal()</code> data structure has three goal positions (in meters), along each of the axes. For Stretch, it will only pay attention to the x and y coordinates, since it can't move in the z-direction.</p> <pre><code>self.client.send_goal(self.goal, done_cb=self.done_callback)\nself.client.wait_for_result()\n</code></pre> <p>Send the goal and include the <code>done_callback()</code> function in one of the arguments in <code>send_goal()</code>.</p> <pre><code>def done_callback(self, status, result):\n    \"\"\"\n    The done_callback function will be called when the joint action is complete.\n    :param self: The self reference.\n    :param status: status attribute from MoveBaseActionResult message.\n    :param result: result attribute from MoveBaseActionResult message.\n    \"\"\"\n    if status == actionlib.GoalStatus.SUCCEEDED:\n        rospy.loginfo('{0}: SUCCEEDED in reaching the goal.'.format(self.__class__.__name__))\n    else:\n        rospy.loginfo('{0}: FAILED in reaching the goal.'.format(self.__class__.__name__))\n</code></pre> <p>Conditional statement to print whether the goal status in the <code>MoveBaseActionResult</code> succeeded or failed.</p> <pre><code>rospy.init_node('navigation', argv=sys.argv)\nnav = StretchNavigation()\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master.</p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Declare the <code>StretchNavigation</code> object.</p> <pre><code>nav.go_to(0.5, 0.0, 0.0)\n</code></pre> <p>Send a move base goal half a meter in front of the map's origin.</p>"},{"location":"stretch-tutorials/ros1/example_2/","title":"Filter Laser Scans","text":""},{"location":"stretch-tutorials/ros1/example_2/#example-2","title":"Example 2","text":"<p>This example aims to provide instructions on how to filter scan messages.</p> <p>For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specifications:</p> <pre><code># Laser scans angles are measured counter clockwise, with Stretch's LiDAR having\n# both angle_min and angle_max facing forward (very closely along the x-axis)\n\nHeader header\nfloat32 angle_min        # start angle of the scan [rad]\nfloat32 angle_max        # end angle of the scan [rad]\nfloat32 angle_increment  # angular distance between measurements [rad]\nfloat32 time_increment   # time between measurements [seconds]\nfloat32 scan_time        # time between scans [seconds]\nfloat32 range_min        # minimum range value [m]\nfloat32 range_max        # maximum range value [m]\nfloat32[] ranges         # range data [m] (Note: values &lt; range_min or &gt; range_max should be discarded)\nfloat32[] intensities    # intensity data [device-specific units]\n</code></pre> <p>The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type.</p> <p> </p> <p>For a Stretch robot the start angle of the scan, <code>angle_min</code>, and end angle, <code>angle_max</code>, are closely located along the x-axis of Stretch's frame. <code>angle_min</code> and <code>angle_max</code> are set at -3.1416 and 3.1416, respectively. This is illustrated by the images below.</p> <p> </p> <p>Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot.</p> <p>First, open a terminal and run the stretch driver launch file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Then in a new terminal run the <code>rplidar.launch</code> file from <code>stretch_core</code>.</p> <pre><code>roslaunch stretch_core rplidar.launch\n</code></pre> <p>To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan_filter.py node by typing the following in a new terminal.</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 scan_filter.py\n</code></pre> <p>Then run the following command in a separate terminal to bring up a simple RViz configuration of the Stretch robot.</p> <pre><code>rosrun rviz rviz -d `rospack find stretch_core`/rviz/stretch_simple_test.rviz\n</code></pre> <p>Change the topic name from the LaserScan display from /scan to /filter_scan.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/example_2/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom numpy import linspace, inf\nfrom math import sin\nfrom sensor_msgs.msg import LaserScan\n\nclass ScanFilter:\n    \"\"\"\n    A class that implements a LaserScan filter that removes all of the points\n    that are not in front of the robot.\n    \"\"\"\n    def __init__(self):\n        self.width = 1.0\n        self.extent = self.width / 2.0\n        self.sub = rospy.Subscriber('/scan', LaserScan, self.callback)\n        self.pub = rospy.Publisher('filtered_scan', LaserScan, queue_size=10)\n        rospy.loginfo(\"Publishing the filtered_scan topic. Use RViz to visualize.\")\n\n    def callback(self,msg):\n        \"\"\"\n        Callback function to deal with incoming LaserScan messages.\n        :param self: The self reference.\n        :param msg: The subscribed LaserScan message.\n\n        :publishes msg: updated LaserScan message.\n        \"\"\"\n        angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n        msg.ranges = new_ranges\n        self.pub.publish(msg)\n\nif __name__ == '__main__':\n    rospy.init_node('scan_filter')\n    ScanFilter()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_2/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nfrom numpy import linspace, inf\nfrom math import sin\nfrom sensor_msgs.msg import LaserScan\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. There are functions from <code>numpy</code> and <code>math</code> that are required within this code, that's why <code>linspace</code>, <code>inf</code>, and <code>sin</code> are imported. The <code>sensor_msgs.msg</code> import is so that we can subscribe and publish <code>LaserScan</code> messages.</p> <pre><code>self.width = 1\nself.extent = self.width / 2.0\n</code></pre> <p>We're going to assume that the robot is pointing up the x-axis so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered.</p> <pre><code>self.sub = rospy.Subscriber('/scan', LaserScan, self.callback)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>scan</code>, looking for <code>LaserScan</code> messages.  When a message comes in, ROS is going to pass it to the function <code>callback</code> automatically.</p> <pre><code>self.pub = rospy.Publisher('filtered_scan', LaserScan, queue_size=10)\n</code></pre> <p><code>pub = rospy.Publisher(\"filtered_scan\", LaserScan, queue_size=10)</code> declares that your node is publishing to the <code>filtered_scan</code> topic using the message type <code>LaserScan</code>. This lets the master tell any nodes listening on <code>filtered_scan</code> that we are going to publish data on that topic.</p> <pre><code>angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n</code></pre> <p>This line of code utilizes <code>linspace</code> to compute each angle of the subscribed scan.</p> <pre><code>points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n</code></pre> <p>Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference.</p> <pre><code>new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n</code></pre> <p>If the absolute value of a point's y-coordinate is under <code>self.extent</code> then keep the range, otherwise use inf, which means \"no return\".</p> <pre><code>msg.ranges = new_ranges\nself.pub.publish(msg)\n</code></pre> <p>Substitute the new ranges in the original message, and republish it.</p> <pre><code>rospy.init_node('scan_filter')\nScanFilter()\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. </p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the class with <code>ScanFilter()</code></p> <pre><code>rospy.spin()\n</code></pre> <p>Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1/example_3/","title":"Mobile Base Collision Avoidance","text":""},{"location":"stretch-tutorials/ros1/example_3/#example-3","title":"Example 3","text":"<p>This example aims to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward.</p> <p>Begin by running the following command in a new terminal.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Then, in a new terminal, type the following to activate the LiDAR sensor.</p> <pre><code>roslaunch stretch_core rplidar.launch\n</code></pre> <p>To set <code>navigation</code> mode and to activate the avoider.py node, type the following in a new terminal.</p> <pre><code>rosservice call /switch_to_navigation_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython3 avoider.py\n</code></pre> <p>To stop the node from sending twist messages, type <code>Ctrl</code> + <code>c</code> in the terminal running the avoider node.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/example_3/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom numpy import linspace, inf, tanh\nfrom math import sin\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\n\nclass Avoider:\n    \"\"\"\n    A class that implements both a LaserScan filter and base velocity control\n    for collision avoidance.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber, publisher, and marker features.\n        :param self: The self reference.\n        \"\"\"\n        self.pub = rospy.Publisher('/stretch/cmd_vel', Twist, queue_size=1) #/stretch_diff_drive_controller/cmd_vel for gazebo\n        self.sub = rospy.Subscriber('/scan', LaserScan, self.set_speed)\n\n        self.width = 1\n        self.extent = self.width / 2.0\n        self.distance = 0.5\n\n        self.twist = Twist()\n        self.twist.linear.x = 0.0\n        self.twist.linear.y = 0.0\n        self.twist.linear.z = 0.0\n        self.twist.angular.x = 0.0\n        self.twist.angular.y = 0.0\n        self.twist.angular.z = 0.0\n\n    def set_speed(self,msg):\n        \"\"\"\n        Callback function to deal with incoming LaserScan messages.\n        :param self: The self reference.\n        :param msg: The subscribed LaserScan message.\n\n        :publishes self.twist: Twist message.\n        \"\"\"\n        angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n        error = min(new_ranges) - self.distance\n\n        self.twist.linear.x = tanh(error) if (error &gt; 0.05 or error &lt; -0.05) else 0\n        self.pub.publish(self.twist)        \n\nif __name__ == '__main__':\n    rospy.init_node('avoider')\n    Avoider()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_3/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nfrom numpy import linspace, inf, tanh\nfrom math import sin\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. There are functions from <code>numpy</code> and <code>math</code> that are required within this code, thus <code>linspace</code>, <code>inf</code>, <code>tanh</code>, and <code>sin</code> are imported. The <code>sensor_msgs.msg</code> import is so that we can subscribe to <code>LaserScan</code> messages. The <code>geometry_msgs.msg</code> import is so that we can send velocity commands to the robot.</p> <pre><code>self.pub = rospy.Publisher('/stretch/cmd_vel', Twist, queue_size=1)#/stretch_diff_drive_controller/cmd_vel for gazebo\n</code></pre> <p>This section of the code defines the talker's interface to the rest of ROS. <code>pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1)</code> declares that your node is publishing to the <code>/stretch/cmd_vel</code> topic using the message type <code>Twist</code>.</p> <pre><code>self.sub = rospy.Subscriber('/scan', LaserScan, self.set_speed)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>scan</code>, looking for <code>LaserScan</code> messages.  When a message comes in, ROS is going to pass it to the function <code>set_speed()</code> automatically.</p> <pre><code>self.width = 1\nself.extent = self.width / 2.0\nself.distance = 0.5\n</code></pre> <p><code>self.width</code> is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing to the x-axis, any points with y coordinates further than half of the defined width (self.extent) from the x-axis are not considered. <code>self.distance</code> defines the stopping distance from an object.</p> <pre><code>self.twist = Twist()\nself.twist.linear.x = 0.0\nself.twist.linear.y = 0.0\nself.twist.linear.z = 0.0\nself.twist.angular.x = 0.0\nself.twist.angular.y = 0.0\nself.twist.angular.z = 0.0\n</code></pre> <p>Allocate a <code>Twist</code> to use, and set everything to zero.  We're going to do this when the class is initiated. Redefining this within the callback function, <code>set_speed()</code> can be more computationally taxing.</p> <pre><code>angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\npoints = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\nnew_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n</code></pre> <p>This line of code utilizes <code>linspace</code> to compute each angle of the subscribed scan. Here we compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under <code>self.extent</code> then keep the range, otherwise use <code>inf</code>, which means \"no return\".</p> <pre><code>error = min(new_ranges) - self.distance\n</code></pre> <p>Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as <code>error</code>.</p> <pre><code>self.twist.linear.x = tanh(error) if (error &gt; 0.05 or error &lt; -0.05) else 0\n</code></pre> <p>Set the speed according to a <code>tanh</code> function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1</p> <pre><code>self.pub.publish(self.twist)\n</code></pre> <p>Publish the <code>Twist</code> message.</p> <pre><code>rospy.init_node('avoider')\nAvoider()\nrospy.spin()\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. </p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate class with <code>Avioder()</code>.</p> <p>Give control to ROS with <code>rospy.spin()</code>. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1/example_4/","title":"Give Stretch a Balloon","text":""},{"location":"stretch-tutorials/ros1/example_4/#example-4","title":"Example 4","text":"<p>Let's bring up Stretch in the Willow Garage world from the gazebo basics tutorial and RViz by using the following command.</p> <pre><code>roslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world rviz:=true\n</code></pre> <p>The <code>rviz</code> flag will open an RViz window to visualize a variety of ROS topics. In a new terminal, run the following commands to execute the marker.py node.</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 marker.py\n</code></pre> <p>The GIF below demonstrates how to add a new <code>Marker</code> display type, and change the topic name from <code>/visualization_marker</code> to <code>/balloon</code>. A red sphere marker should appear above the Stretch robot.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/example_4/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom visualization_msgs.msg import Marker\n\nclass Balloon():\n    \"\"\"\n    A class that attaches a Sphere marker directly above the Stretch robot.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the marker's features.\n        :param self: The self reference.\n        \"\"\"\n        self.publisher = rospy.Publisher('balloon', Marker, queue_size=10)\n        self.marker = Marker()\n        self.marker.header.frame_id = 'base_link'\n        self.marker.header.stamp = rospy.Time()\n        self.marker.type = self.marker.SPHERE\n        self.marker.id = 0\n        self.marker.action = self.marker.ADD\n        self.marker.scale.x = 0.5\n        self.marker.scale.y = 0.5\n        self.marker.scale.z = 0.5\n        self.marker.color.r = 1.0\n        self.marker.color.g = 0.0\n        self.marker.color.b = 0.0\n        self.marker.color.a = 1.0\n        self.marker.pose.position.x = 0.0\n        self.marker.pose.position.y = 0.0\n        self.marker.pose.position.z = 2.0\n        rospy.loginfo(\"Publishing the balloon topic. Use RViz to visualize.\")\n\n    def publish_marker(self):\n        \"\"\"\n        Function that publishes the sphere marker.\n        :param self: The self reference.\n\n        :publishes self.marker: Marker message.\n        \"\"\"\n        self.publisher.publish(self.marker)\n\n\nif __name__ == '__main__':\n    rospy.init_node('marker')\n    balloon = Balloon()\n    rate = rospy.Rate(10)\n\n    while not rospy.is_shutdown():\n        balloon.publish_marker()\n        rate.sleep()        \n</code></pre>"},{"location":"stretch-tutorials/ros1/example_4/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <p><pre><code>import rospy\nfrom visualization_msgs.msg import Marker\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node. Import the <code>Marker</code> type from the <code>visualization_msgs.msg</code> package. This import is required to publish a <code>Marker</code>, which will be visualized in RViz.</p> <pre><code>self.pub = rospy.Publisher('balloon', Marker, queue_size=10)\n</code></pre> <p>This section of code defines the talker's interface to the rest of ROS. <code>pub = rospy.Publisher(\"balloon\", Twist, queue_size=1)</code> declares that your node is publishing to the <code>/ballon</code> topic using the message type <code>Twist</code>.</p> <pre><code>self.marker = Marker()\nself.marker.header.frame_id = 'base_link'\nself.marker.header.stamp = rospy.Time()\nself.marker.type = self.marker.SPHERE\n</code></pre> <p>Create a <code>Marker()</code> message type. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers</p> <pre><code>self.marker.id = 0\n</code></pre> <p>Each marker has a unique ID number. If you have more than one marker that you want to be displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number as an existing marker, it will replace the existing marker with that ID number.</p> <pre><code>self.marker.action = self.marker.ADD\n</code></pre> <p>This line of code sets the action. You can add, delete, or modify markers.</p> <pre><code>self.marker.scale.x = 0.5\nself.marker.scale.y = 0.5\nself.marker.scale.z = 0.5\n</code></pre> <p>These are the size parameters for the marker. These will vary by marker type.</p> <pre><code>self.marker.color.r = 1.0\nself.marker.color.g = 0.0\nself.marker.color.b = 0.0\n</code></pre> <p>Color of the object, specified as r/g/b/a, with values in the range of [0, 1].</p> <pre><code>self.marker.color.a = 1.0\n</code></pre> <p>The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible.</p> <pre><code>self.marker.pose.position.x = 0.0\nself.marker.pose.position.y = 0.0\nself.marker.pose.position.z = 2.0\n</code></pre> <p>Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in <code>frame_id</code>. In this case, the position will always be directly 2 meters above the frame_id (base_link), and will move with it.</p> <pre><code>def publish_marker(self):\n        self.publisher.publish(self.marker)\n</code></pre> <p>Publish the Marker data structure to be visualized in RViz.</p> <pre><code>rospy.init_node('marker', argv=sys.argv)\nballoon = Balloon()\nrate = rospy.Rate(10)\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. </p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate class with <code>Balloon()</code>.</p> <p>The <code>rospy.rate()</code> is the rate at which the node is going to publish information (10 Hz).</p> <pre><code>while not rospy.is_shutdown():\n    balloon.publish_marker()\n    rate.sleep()\n</code></pre> <p>This loop is a fairly standard rospy construct: checking the <code>rospy.is_shutdown()</code> flag and then doing work. You have to check <code>is_shutdown()</code> to check if your program should exit (e.g. if there is a <code>Ctrl+c</code> or otherwise). The loop calls <code>rate.sleep()</code>, which sleeps just long enough to maintain the desired rate through the loop.</p>"},{"location":"stretch-tutorials/ros1/example_5/","title":"Print Joint States","text":""},{"location":"stretch-tutorials/ros1/example_5/#example-5","title":"Example 5","text":"<p>In this example, we will review a Python script that prints out the positions of a selected group of Stretch joints. This script is helpful if you need the joint positions after you teleoperated Stretch with the Xbox controller or physically moved the robot to the desired configuration after hitting the run stop button.</p> <p>If you are looking for a continuous print of the joint states while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial.</p> <p>Begin by starting up the stretch driver launch file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>You can then hit the run-stop button (you should hear a beep and the LED light in the button blink) and move the robot's joints to a desired configuration. Once you are satisfied with the configuration, hold the run-stop button until you hear a beep. Then run the following command to execute the joint_state_printer.py which will print the joint positions of the lift, arm, and wrist. In a new terminal, execute:</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 joint_state_printer.py\n</code></pre> <p>Your terminal will output the <code>position</code> information of the previously mentioned joints shown below. <pre><code>name: ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\nposition: [0.6043133175850597, 0.19873586673129257, 0.017257283863713464]\n</code></pre></p> <p>Note</p> <p>Stretch's arm has four prismatic joints and the sum of their positions gives the wrist_extension distance. The wrist_extension is needed when sending joint trajectory commands to the robot. Further, you can not actuate an individual arm joint. Here is an image of the arm joints for reference:</p> <p> </p>"},{"location":"stretch-tutorials/ros1/example_5/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport sys\nfrom sensor_msgs.msg import JointState\n\nclass JointStatePublisher():\n    \"\"\"\n    A class that prints the positions of desired joints in Stretch.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber.\n        :param self: The self reference.\n        \"\"\"\n        self.sub = rospy.Subscriber('joint_states', JointState, self.callback)\n\n    def callback(self, msg):\n        \"\"\"\n        Callback function to deal with the incoming JointState messages.\n        :param self: The self reference.\n        :param msg: The JointState message.\n        \"\"\"\n        self.joint_states = msg\n\n    def print_states(self, joints):\n        \"\"\"\n        print_states function to deal with the incoming JointState messages.\n        :param self: The self reference.\n        :param joints: A list of string values of joint names.\n        \"\"\"\n        joint_positions = []\n        for joint in joints:\n            if joint == \"wrist_extension\":\n                index = self.joint_states.name.index('joint_arm_l0')\n                joint_positions.append(4*self.joint_states.position[index])\n                continue\n            index = self.joint_states.name.index(joint)\n            joint_positions.append(self.joint_states.position[index])\n        print(\"name: \" + str(joints))\n        print(\"position: \" + str(joint_positions))\n        rospy.signal_shutdown(\"done\")\n        sys.exit(0)\n\nif __name__ == '__main__':\n    rospy.init_node('joint_state_printer', anonymous=True)\n    JSP = JointStatePublisher()\n    rospy.sleep(.1)\n    joints = [\"joint_lift\", \"wrist_extension\", \"joint_wrist_yaw\"]\n    #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"]\n    JSP.print_states(joints)\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_5/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nimport sys\nfrom sensor_msgs.msg import JointState\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. Import <code>sensor_msgs.msg</code> so that we can subscribe to <code>JointState</code> messages.</p> <pre><code>self.sub = rospy.Subscriber('joint_states', JointState, self.callback)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>joint_states</code>, looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function \"callback\" automatically</p> <pre><code>def callback(self, msg):\n    self.joint_states = msg\n</code></pre> <p>This is the callback function where the <code>JointState</code> messages are stored as <code>self.joint_states</code>. Further information about this message type can be found here: JointState Message</p> <pre><code>def print_states(self, joints):\n    joint_positions = []\n</code></pre> <p>This is the <code>print_states()</code> function which takes in a list of joints of interest as its argument. the is also an empty list set as <code>joint_positions</code> and this is where the positions of the requested joints will be appended.</p> <pre><code>for joint in joints:\n  if joint == \"wrist_extension\":\n    index = self.joint_states.name.index('joint_arm_l0')\n    joint_positions.append(4*self.joint_states.position[index])\n    continue\n  index = self.joint_states.name.index(joint)\n  joint_positions.append(self.joint_states.position[index])\n</code></pre> <p>In this section of the code, a for loop is used to parse the names of the requested joints from the <code>self.joint_states</code> list. The <code>index()</code> function returns the index of the name of the requested joint and appends the respective position to the <code>joint_positions</code> list.</p> <pre><code>rospy.signal_shutdown(\"done\")\nsys.exit(0)\n</code></pre> <p>The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter.</p> <pre><code>rospy.init_node('joint_state_printer', anonymous=True)\nJSP = JointStatePublisher()\nrospy.sleep(.1)\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Declare object, JSP, from the <code>JointStatePublisher</code> class.</p> <p>The use of the <code>rospy.sleep()</code> function is to allow the JSP class to initialize all of its features before requesting to publish joint positions of desired joints (running the <code>print_states()</code> method).</p> <pre><code>joints = [\"joint_lift\", \"wrist_extension\", \"joint_wrist_yaw\"]\n#joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"]\nJSP.print_states(joints)\n</code></pre> <p>Create a list of the desired joints that you want positions to be printed. Then use that list as an argument for the <code>print_states()</code> method.</p> <pre><code>rospy.spin()\n</code></pre> <p>Give control to ROS with <code>rospy.spin()</code>. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1/example_6/","title":"Store Effort Values","text":""},{"location":"stretch-tutorials/ros1/example_6/#example-6","title":"Example 6","text":"<p>In this example, we will review a Python script that prints and stores the effort values from a specified joint. If you are looking for a continuous print of the joint state efforts while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial.</p> <p> </p> <p>Begin by running the following command in a terminal.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Switch the mode to <code>position</code> mode using a rosservice call. Then run the effort_sensing.py node. In a new terminal, execute:</p> <pre><code>rosservice call /switch_to_position_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython3 effort_sensing.py\n</code></pre> <p>This will send a <code>FollowJointTrajectory</code> command to move Stretch's arm or head while also printing the effort of the lift.</p>"},{"location":"stretch-tutorials/ros1/example_6/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rospy\nimport time\nimport actionlib\nimport os\nimport csv\nfrom datetime import datetime\n\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom sensor_msgs.msg import JointState\nimport hello_helpers.hello_misc as hm\n\nclass JointActuatorEffortSensor(hm.HelloNode):\n    \"\"\"\n    A class that sends multiple joint trajectory goals to a single joint.\n    \"\"\"\n    def __init__(self, export_data=False):\n        \"\"\"\n        Function that initializes the subscriber,and other features.\n        :param self: The self reference.\n        :param export_data: A boolean message type.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n        self.sub = rospy.Subscriber('joint_states', JointState, self.callback)\n        self.joints = ['joint_lift']\n        self.joint_effort = []\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n        self.export_data = export_data\n\n    def callback(self, msg):\n        \"\"\"\n        Callback function to update and store JointState messages.\n        :param self: The self reference.\n        :param msg: The JointState message.\n        \"\"\"\n        self.joint_states = msg\n\n    def issue_command(self):\n        \"\"\"\n        Function that makes an action call and sends joint trajectory goals\n        to a single joint.\n        :param self: The self reference.\n        \"\"\"\n        trajectory_goal = FollowJointTrajectoryGoal()\n        trajectory_goal.trajectory.joint_names = self.joints\n\n        point0 = JointTrajectoryPoint()\n        point0.positions = [0.9]\n\n        trajectory_goal.trajectory.points = [point0]\n        trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n        self.trajectory_client.send_goal(trajectory_goal, feedback_cb=self.feedback_callback, done_cb=self.done_callback)\n        rospy.loginfo('Sent position goal = {0}'.format(trajectory_goal))\n        self.trajectory_client.wait_for_result()\n\n    def feedback_callback(self,feedback):\n        \"\"\"\n        The feedback_callback function deals with the incoming feedback messages\n        from the trajectory_client. Although, in this function, we do not use the\n        feedback information.\n        :param self: The self reference.\n        :param feedback: FollowJointTrajectoryActionFeedback message.\n        \"\"\"\n        if 'wrist_extension' in self.joints:\n            self.joints.remove('wrist_extension')\n            self.joints.append('joint_arm_l0')\n\n        current_effort = []\n        for joint in self.joints:\n            index = self.joint_states.name.index(joint)\n            current_effort.append(self.joint_states.effort[index])\n\n        if not self.export_data:\n            print(\"name: \" + str(self.joints))\n            print(\"effort: \" + str(current_effort))\n        else:\n            self.joint_effort.append(current_effort)\n\n\n    def done_callback(self, status, result):\n        \"\"\"\n        The done_callback function will be called when the joint action is complete.\n        Within this function we export the data to a .txt file in  the /stored_data directory.\n        :param self: The self reference.\n        :param status: status attribute from FollowJointTrajectoryActionResult message.\n        :param result: result attribute from FollowJointTrajectoryActionResult message.\n        \"\"\"\n        if status == actionlib.GoalStatus.SUCCEEDED:\n            rospy.loginfo('Succeeded')\n        else:\n            rospy.loginfo('Failed')\n\n        if self.export_data:\n            file_name = datetime.now().strftime(\"%Y-%m-%d_%I:%M:%S-%p\")\n            completeName = os.path.join(self.save_path, file_name)\n            with open(completeName, \"w\") as f:\n                writer = csv.writer(f)\n                writer.writerow(self.joints)\n                writer.writerows(self.joint_effort)\n\n    def main(self):\n        \"\"\"\n        Function that initiates the issue_command function.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.main(self, 'issue_command', 'issue_command', wait_for_first_pointcloud=False)\n        rospy.loginfo('issuing command...')\n        self.issue_command()\n        time.sleep(2)\n\nif __name__ == '__main__':\n    try:\n        node = JointActuatorEffortSensor(export_data=True)\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_6/#the-code-explained","title":"The Code Explained","text":"<p>This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nimport time\nimport actionlib\nimport os\nimport csv\nfrom datetime import datetime\n\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom sensor_msgs.msg import JointState\nimport hello_helpers.hello_misc as hm\n</code></pre> <p>You need to import rospy if you are writing a ROS Node. Import the <code>FollowJointTrajectoryGoal</code> from the <code>control_msgs.msg</code> package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the <code>trajectory_msgs</code> package to define robot trajectories. The <code>hello_helpers</code> package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.</p> <pre><code>class JointActuatorEffortSensor(hm.HelloNode):\n    \"\"\"\n    A class that sends multiple joint trajectory goals to a single joint.\n    \"\"\"\n    def __init__(self, export_data=False):\n        \"\"\"\n        Function that initializes the subscriber,and other features.\n        :param self: The self reference.\n        :param export_data: A boolean message type.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n</code></pre> <p>The <code>JointActuatorEffortSensor</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is initialized.</p> <pre><code>self.sub = rospy.Subscriber('joint_states', JointState, self.callback)\nself.joints = ['joint_lift']\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>joint_states</code>, looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function \"callback\" automatically. Create a list of the desired joints you want to print.</p> <pre><code>self.joint_effort = []\nself.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\nself.export_data = export_data\n</code></pre> <p>Create an empty list to store the joint effort values. The <code>self.save_path</code> is the directory path where the .txt file of the effort values will be stored. You can change this path to a preferred directory. The <code>self.export_data</code> is a boolean and its default value is set to <code>False</code>. If set to <code>True</code>, then the joint values will be stored in a .txt file, otherwise, the values will be printed in the terminal where you ran the effort sensing node.</p> <pre><code>self.trajectory_client.send_goal(trajectory_goal, feedback_cb=self.feedback_callback, done_cb=self.done_callback)\n</code></pre> <p>Include the feedback and <code>done_callback</code> functions in the send goal function.</p> <pre><code>def feedback_callback(self,feedback):\n    \"\"\"\n    The feedback_callback function deals with the incoming feedback messages\n    from the trajectory_client. Although, in this function, we do not use the\n    feedback information.\n    :param self: The self reference.\n    :param feedback: FollowJointTrajectoryActionFeedback message.\n    \"\"\"\n</code></pre> <p>The feedback callback function takes in the <code>FollowJointTrajectoryActionFeedback</code> message as its argument.</p> <pre><code>if 'wrist_extension' in self.joints:\n    self.joints.remove('wrist_extension')\n    self.joints.append('joint_arm_l0')\n</code></pre> <p>Use a conditional statement to replace <code>wrist_extenstion</code> with <code>joint_arm_l0</code>. This is because <code>joint_arm_l0</code> has the effort values that the <code>wrist_extension</code> is experiencing.</p> <pre><code>current_effort = []\nfor joint in self.joints:\n    index = self.joint_states.name.index(joint)\n    current_effort.append(self.joint_states.effort[index])\n</code></pre> <p>Create an empty list to store the current effort values. Then use a for loop to parse the joint names and effort values.</p> <pre><code>if not self.export_data:\n    print(\"name: \" + str(self.joints))\n    print(\"effort: \" + str(current_effort))\nelse:\n    self.joint_effort.append(current_effort)\n</code></pre> <p>Use a conditional statement to print effort values in the terminal or store values into a list that will be used for exporting the data in a .txt file.</p> <pre><code>def done_callback(self, status, result):\n      \"\"\"\n      The done_callback function will be called when the joint action is complete.\n      Within this function we export the data to a .txt file in  the /stored_data directory.\n      :param self: The self reference.\n      :param status: status attribute from FollowJointTrajectoryActionResult message.\n      :param result: result attribute from FollowJointTrajectoryActionResult message.\n      \"\"\"\n</code></pre> <p>The done callback function takes in the <code>FollowJointTrajectoryActionResult</code> messages as its arguments.</p> <pre><code>if status == actionlib.GoalStatus.SUCCEEDED:\n    rospy.loginfo('Succeeded')\nelse:\n    rospy.loginfo('Failed')\n</code></pre> <p>Conditional statement to print whether the goal status in the <code>FollowJointTrajectoryActionResult</code> succeeded or failed.</p> <pre><code>if self.export_data:\n    file_name = datetime.now().strftime(\"%Y-%m-%d_%I:%M:%S-%p\")\n    completeName = os.path.join(self.save_path, file_name)\n\n    with open(completeName, \"w\") as f:\n        writer = csv.writer(f)\n        writer.writerow(self.joints)\n        writer.writerows(self.joint_effort)\n</code></pre> <p>A conditional statement is used to export the data to a .txt file. The file's name is set to the date and time the node was executed. That way, no previous files are overwritten.</p>"},{"location":"stretch-tutorials/ros1/example_6/#plottinganimating-effort-data","title":"Plotting/Animating Effort Data","text":"<p>We added a simple python script, stored_data_plotter.py, to this package for plotting the stored data. </p> <p>Note</p> <p>You have to change the name of the file you wish to see in the python script. This is shown below.</p> <pre><code>####################### Copy the file name here! #######################\nfile_name = '2022-06-30_11:26:20-AM'\n</code></pre> <p>Once you have changed the file name, then run the following in a new command.</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 stored_data_plotter.py\n</code></pre> <p>Because this is not a node, you don't need <code>roscore</code> to run this script. Please review the comments in the python script for additional guidance.</p>"},{"location":"stretch-tutorials/ros1/example_7/","title":"Example 7","text":"<p>In this example, we will review the image_view ROS package and a Python script that captures an image from the RealSense camera.</p> <p> </p> <p>Begin by running the stretch <code>driver.launch</code> file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>roslaunch stretch_core d435i_low_resolution.launch\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_7/#capture-image-with-image_view","title":"Capture Image with image_view","text":"<p>There are a couple of methods to save an image using the image_view package.</p> <p>OPTION 1: Use the <code>image_view</code> node to open a simple image viewer for ROS sensor_msgs/image topics. In a new terminal, execute:</p> <pre><code>rosrun image_view image_view image:=/camera/color/image_raw_upright_view\n</code></pre> <p>Then you can save the current image by right-clicking on the display window. By default, images will be saved as frame000.jpg, frame000.jpg, etc. Note, that the image will be saved to the terminal's current work directory.</p> <p>OPTION 2: Use the <code>image_saver</code> node to save an image to the terminal's current work directory. In a new terminal, execute:</p> <pre><code>rosrun image_view image_saver image:=/camera/color/image_raw_upright_view\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_7/#capture-image-with-python-script","title":"Capture Image with Python Script","text":"<p>In this section, we will use a Python node to capture an image from the RealSense camera. Execute the capture_image.py node to save a .jpeg image of the image topic <code>/camera/color/image_raw_upright_view</code>. In a terminal, execute:</p> <p><pre><code>cd ~/catkin_ws/src/stretch_tutorials/src\npython3 capture_image.py\n</code></pre> An image named camera_image.jpeg is saved in the stored_data folder in this package.</p>"},{"location":"stretch-tutorials/ros1/example_7/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport sys\nimport os\nimport cv2\n\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nclass CaptureImage:\n    \"\"\"\n    A class that converts a subscribed ROS image to a OpenCV image and saves\n    the captured image to a predefined directory.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes a CvBridge class, subscriber, and save path.\n        :param self: The self reference.\n        \"\"\"\n        self.bridge = CvBridge()\n        self.sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback, queue_size=1)\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n\n    def callback(self, msg):\n        \"\"\"\n        A callback function that converts the ROS image to a CV2 image and stores the\n        image.\n        :param self: The self reference.\n        :param msg: The ROS image message type.\n        \"\"\"\n        try:\n            image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        except CvBridgeError as e:\n            rospy.logwarn('CV Bridge error: {0}'.format(e))\n\n        file_name = 'camera_image.jpeg'\n        completeName = os.path.join(self.save_path, file_name)\n        cv2.imwrite(completeName, image)\n        rospy.signal_shutdown(\"done\")\n        sys.exit(0)\n\nif __name__ == '__main__':\n    rospy.init_node('capture_image', argv=sys.argv)\n    CaptureImage()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_7/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nimport sys\nimport os\nimport cv2\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. There are functions from <code>sys</code>, <code>os</code>, and <code>cv2</code> that are required within this code. <code>cv2</code> is a library of Python functions that implements computer vision algorithms. Further information about cv2 can be found here: OpenCV Python.</p> <pre><code>from sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n</code></pre> <p>The <code>sensor_msgs.msg</code> is imported so that we can subscribe to ROS <code>Image</code> messages. Import CvBridge to convert between ROS <code>Image</code> messages and OpenCV images.</p> <pre><code>def __init__(self):\n    \"\"\"\n    A function that initializes a CvBridge class, subscriber, and save path.\n    :param self: The self reference.\n    \"\"\"\n    self.bridge = CvBridge()\n    self.sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback, queue_size=1)\n    self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n</code></pre> <p>Initialize the CvBridge class, the subscriber, and the directory where the captured image will be stored.</p> <pre><code>def callback(self, msg):\n    \"\"\"\n    A callback function that converts the ROS image to a cv2 image and stores the\n    image.\n    :param self: The self reference.\n    :param msg: The ROS image message type.\n    \"\"\"\n    try:\n        image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n    except CvBridgeError as e:\n        rospy.logwarn('CV Bridge error: {0}'.format(e))\n</code></pre> <p>Try to convert the ROS Image message to a cv2 Image message using the <code>imgmsg_to_cv2()</code> function.  </p> <pre><code>file_name = 'camera_image.jpeg'\ncompleteName = os.path.join(self.save_path, file_name)\ncv2.imwrite(completeName, image)\n</code></pre> <p>Join the directory and file name using the <code>path.join()</code> function. Then use the <code>imwrite()</code> function to save the image.</p> <pre><code>rospy.signal_shutdown(\"done\")\nsys.exit(0)\n</code></pre> <p>The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter.</p> <pre><code>rospy.init_node('capture_image', argv=sys.argv)\nCaptureImage()\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. </p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the <code>CaptureImage()</code> class.</p> <pre><code>rospy.spin()\n</code></pre> <p>Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1/example_7/#edge-detection","title":"Edge Detection","text":"<p>In this section, we highlight a node that utilizes the Canny Edge filter algorithm to detect the edges from an image and convert it back as a ROS image to be visualized in RViz. In a terminal, execute:</p> <pre><code>cd ~/catkin_ws/src/stretch_tutorials/src\npython3 edge_detection.py\n</code></pre> <p>The node will publish a new Image topic named <code>/image_edge_detection</code>. This can be visualized in RViz and a gif is provided below for reference.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/example_7/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport sys\nimport os\nimport cv2\n\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nclass EdgeDetection:\n    \"\"\"\n    A class that converts a subscribed ROS image to a OpenCV image and saves\n    the captured image to a predefined directory.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes a CvBridge class, subscriber, and other\n        parameter values.\n        :param self: The self reference.\n        \"\"\"\n        self.bridge = CvBridge()\n        self.sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback, queue_size=1)\n        self.pub = rospy.Publisher('/image_edge_detection', Image, queue_size=1)\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n        self.lower_thres = 100\n        self.upper_thres = 200\n        rospy.loginfo(\"Publishing the CV2 Image. Use RViz to visualize.\")\n\n    def callback(self, msg):\n        \"\"\"\n        A callback function that converts the ROS image to a CV2 image and goes\n        through the Canny Edge filter in OpenCV for edge detection. Then publishes\n        that filtered image to be visualized in RViz.\n        :param self: The self reference.\n        :param msg: The ROS image message type.\n        \"\"\"\n        try:\n            image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        except CvBridgeError as e:\n            rospy.logwarn('CV Bridge error: {0}'.format(e))\n\n        image = cv2.Canny(image, self.lower_thres, self.upper_thres)\n        image_msg = self.bridge.cv2_to_imgmsg(image, 'passthrough')\n        image_msg.header = msg.header\n        self.pub.publish(image_msg)\n\nif __name__ == '__main__':\n    rospy.init_node('edge_detection', argv=sys.argv)\n    EdgeDetection()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_7/#the-code-explained_1","title":"The Code Explained","text":"<p>Since there are similarities in the capture image node, we will only break down the different components of the edge detection node.</p> <p>Define the lower and upper bounds of the Hysteresis Thresholds.</p> <pre><code>image = cv2.Canny(image, self.lower_thres, self.upper_thres)\n</code></pre> <p>Run the Canny Edge function to detect edges from the cv2 image.</p> <pre><code>image_msg = self.bridge.cv2_to_imgmsg(image, 'passthrough')\n</code></pre> <p>Convert the cv2 image back to a ROS image so it can be published.</p> <pre><code>image_msg.header = msg.header\nself.pub.publish(image_msg)\n</code></pre> <p>Publish the ROS image with the same header as the subscribed ROS message.</p>"},{"location":"stretch-tutorials/ros1/example_8/","title":"Example 8","text":"<p>This example will showcase how to save the interpreted speech from Stretch's ReSpeaker Mic Array v2.0 to a text file.</p> <p> </p> <p>Begin by running the <code>respeaker.launch</code> file in a terminal.</p> <p><pre><code>roslaunch respeaker_ros respeaker.launch\n</code></pre> Then run the speech_text.py node. In a new terminal, execute:</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 speech_text.py\n</code></pre> <p>The ReSpeaker will be listening and will start to interpret speech and save the transcript to a text file.  To shut down the node, type <code>Ctrl</code> + <code>c</code> in the terminal.</p>"},{"location":"stretch-tutorials/ros1/example_8/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport os\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n\nclass SpeechText:\n    \"\"\"\n    A class that saves the interpreted speech from the ReSpeaker Microphone Array to a text file.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize subscriber and directory to save speech to text file.\n        \"\"\"\n        self.sub = rospy.Subscriber(\"speech_to_text\", SpeechRecognitionCandidates, self.callback)\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n        rospy.loginfo(\"Listening to speech.\")\n\n    def callback(self,msg):\n        \"\"\"\n        A callback function that receives the speech transcript and appends the\n        transcript to a text file.\n        :param self: The self reference.\n        :param msg: The SpeechRecognitionCandidates message type.\n        \"\"\"\n        transcript = ' '.join(map(str,msg.transcript))\n        file_name = 'speech.txt'\n        completeName = os.path.join(self.save_path, file_name)\n\n        with open(completeName, \"a+\") as file_object:\n            file_object.write(\"\\n\")\n            file_object.write(transcript)\n\nif __name__ == '__main__':\n    rospy.init_node('speech_text')\n    SpeechText()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_8/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nimport os\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node.</p> <pre><code>from speech_recognition_msgs.msg import SpeechRecognitionCandidates\n</code></pre> <p>Import <code>SpeechRecognitionCandidates</code> from the <code>speech_recgonition_msgs.msg</code> so that we can receive the interpreted speech.</p> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize subscriber and directory to save speech to text file.\n    \"\"\"\n    self.sub = rospy.Subscriber(\"speech_to_text\", SpeechRecognitionCandidates, self.callback)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>speech_to_text</code>, looking for <code>SpeechRecognitionCandidates</code> messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically.</p> <pre><code>self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data\n</code></pre> <p>Define the directory to save the text file.</p> <pre><code>transcript = ' '.join(map(str,msg.transcript))\n</code></pre> <p>Take all items in the iterable list and join them into a single string named transcript.</p> <pre><code>file_name = 'speech.txt'\ncompleteName = os.path.join(self.save_path, file_name)\n</code></pre> <p>Define the file name and create a complete path directory.</p> <pre><code>with open(completeName, \"a+\") as file_object:\n    file_object.write(\"\\n\")\n    file_object.write(transcript)\n</code></pre> <p>Append the transcript to the text file.</p> <pre><code>rospy.init_node('speech_text')\nSpeechText()\n</code></pre> <p>The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. </p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the <code>SpeechText()</code> class.</p> <pre><code>rospy.spin()\n</code></pre> <p>Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1/example_9/","title":"Voice Teleoperation of Base","text":""},{"location":"stretch-tutorials/ros1/example_9/#example-9","title":"Example 9","text":"<p>This example aims to combine the ReSpeaker Microphone Array and Follow Joint Trajectory tutorials to voice teleoperate the mobile base of the Stretch robot.</p> <p>Begin by running the following command in a new terminal.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Switch the mode to <code>position</code> mode using a rosservice call.</p> <pre><code>rosservice call /switch_to_position_mode\n</code></pre> <p>Then run the <code>respeaker.launch</code> file. In a new terminal, execute:</p> <pre><code>roslaunch stretch_core respeaker.launch\n</code></pre> <p>Then run the voice_teleoperation_base.py node in a new terminal.</p> <pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 voice_teleoperation_base.py\n</code></pre> <p>In terminal 3, a menu of voice commands is printed. You can reference this menu layout below.  </p> <pre><code>------------ VOICE TELEOP MENU ------------\n\nVOICE COMMANDS              \n\"forward\": BASE FORWARD                   \n\"back\"   : BASE BACK                      \n\"left\"   : BASE ROTATE LEFT               \n\"right\"  : BASE ROTATE RIGHT              \n\"stretch\": BASE ROTATES TOWARDS SOUND     \n\nSTEP SIZE                 \n\"big\"    : BIG                            \n\"medium\" : MEDIUM                         \n\"small\"  : SMALL                          \n\n\n\"quit\"   : QUIT AND CLOSE NODE            \n\n-------------------------------------------\n</code></pre> <p>To stop the node from sending twist messages, type <code>Ctrl</code> + <code>c</code> or say \"quit\".</p>"},{"location":"stretch-tutorials/ros1/example_9/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport math\nimport rospy\nimport sys\n\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Int32\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n\nclass GetVoiceCommands:\n    \"\"\"\n    A class that subscribes to the speech to text recognition messages, prints\n    a voice command menu, and defines step size for translational and rotational\n    mobile base motion.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes subscribers and defines the three different\n        step sizes.\n        :param self: The self reference.\n        \"\"\"\n        self.step_size = 'medium'\n        self.rad_per_deg = math.pi/180.0\n\n        self.small_deg = 5.0\n        self.small_rad = self.rad_per_deg * self.small_deg\n        self.small_translate = 0.025\n\n        self.medium_deg = 10.0\n        self.medium_rad = self.rad_per_deg * self.medium_deg\n        self.medium_translate = 0.05\n\n        self.big_deg = 20.0\n        self.big_rad = self.rad_per_deg * self.big_deg\n        self.big_translate = 0.1\n\n        self.voice_command = None\n        self.sound_direction = 0\n        self.speech_to_text_sub  = rospy.Subscriber(\"/speech_to_text\",  SpeechRecognitionCandidates, self.callback_speech)\n        self.sound_direction_sub = rospy.Subscriber(\"/sound_direction\", Int32,                       self.callback_direction)\n\n    def callback_direction(self, msg):\n        \"\"\"\n        A callback function that converts the incoming message, sound direction,\n        from degrees to radians.\n        :param self: The self reference.\n        :param msg: The Int32 message type that represents the sound direction.\n        \"\"\"\n        self.sound_direction = msg.data * -self.rad_per_deg\n\n    def callback_speech(self,msg):\n        \"\"\"\n        A callback function takes the incoming message, a list of the speech to\n        text, and joins all items in that iterable list into a single string.\n        :param self: The self reference.\n        :param msg: The SpeechRecognitionCandidates message type.\n        \"\"\"\n        self.voice_command = ' '.join(map(str,msg.transcript))\n\n    def get_inc(self):\n        \"\"\"\n        A function that sets the increment size for translational and rotational\n        base motion.\n        :param self:The self reference.\n\n        :returns inc: A dictionary type the contains the increment size.\n        \"\"\"\n        if self.step_size == 'small':\n            inc = {'rad': self.small_rad, 'translate': self.small_translate}\n        if self.step_size == 'medium':\n            inc = {'rad': self.medium_rad, 'translate': self.medium_translate}\n        if self.step_size == 'big':\n            inc = {'rad': self.big_rad, 'translate': self.big_translate}\n        return inc\n\n    def print_commands(self):\n        \"\"\"\n        A function that prints the voice teleoperation menu.\n        :param self: The self reference.\n        \"\"\"\n        print('                                           ')\n        print('------------ VOICE TELEOP MENU ------------')\n        print('                                           ')\n        print('               VOICE COMMANDS              ')\n        print(' \"forward\": BASE FORWARD                   ')\n        print(' \"back\"   : BASE BACK                      ')\n        print(' \"left\"   : BASE ROTATE LEFT               ')\n        print(' \"right\"  : BASE ROTATE RIGHT              ')\n        print(' \"stretch\": BASE ROTATES TOWARDS SOUND     ')\n        print('                                           ')\n        print('                 STEP SIZE                 ')\n        print(' \"big\"    : BIG                            ')\n        print(' \"medium\" : MEDIUM                         ')\n        print(' \"small\"  : SMALL                          ')\n        print('                                           ')\n        print('                                           ')\n        print(' \"quit\"   : QUIT AND CLOSE NODE            ')\n        print('                                           ')\n        print('-------------------------------------------')\n\n    def get_command(self):\n        \"\"\"\n        A function that defines the teleoperation command based on the voice command.\n        :param self: The self reference.\n\n        :returns command: A dictionary type that contains the type of base motion.\n        \"\"\"\n        command = None\n        if self.voice_command == 'forward':\n            command = {'joint': 'translate_mobile_base', 'inc': self.get_inc()['translate']}\n        if self.voice_command == 'back':\n            command = {'joint': 'translate_mobile_base', 'inc': -self.get_inc()['translate']}\n        if self.voice_command == 'left':\n            command = {'joint': 'rotate_mobile_base', 'inc': self.get_inc()['rad']}\n        if self.voice_command == 'right':\n            command = {'joint': 'rotate_mobile_base', 'inc': -self.get_inc()['rad']}\n        if self.voice_command == 'stretch':\n            command = {'joint': 'rotate_mobile_base', 'inc': self.sound_direction}\n        if (self.voice_command == \"small\") or (self.voice_command == \"medium\") or (self.voice_command == \"big\"):\n            self.step_size = self.voice_command\n            rospy.loginfo('Step size = {0}'.format(self.step_size))\n        if self.voice_command == 'quit':\n            rospy.signal_shutdown(\"done\")\n            sys.exit(0)\n\n        self.voice_command = None\n        return command\n\n\nclass VoiceTeleopNode(hm.HelloNode):\n    \"\"\"\n    A class that inherits the HelloNode class from hm and sends joint trajectory\n    commands.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that declares object from the GetVoiceCommands class, instantiates\n        the HelloNode class, and set the publishing rate.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n        self.rate = 10.0\n        self.joint_state = None\n        self.speech = GetVoiceCommands()\n\n    def joint_states_callback(self, msg):\n        \"\"\"\n        A callback function that stores Stretch's joint states.\n        :param self: The self reference.\n        :param msg: The JointState message type.\n        \"\"\"\n        self.joint_state = msg\n\n    def send_command(self, command):\n        \"\"\"\n        Function that makes an action call and sends base joint trajectory goals\n        :param self: The self reference.\n        :param command: A dictionary type.\n        \"\"\"\n        joint_state = self.joint_state\n        if (joint_state is not None) and (command is not None):\n            point = JointTrajectoryPoint()\n            point.time_from_start = rospy.Duration(0.0)\n            trajectory_goal = FollowJointTrajectoryGoal()\n            trajectory_goal.goal_time_tolerance = rospy.Time(1.0)\n            joint_name = command['joint']\n            trajectory_goal.trajectory.joint_names = [joint_name]\n\n            inc = command['inc']\n            rospy.loginfo('inc = {0}'.format(inc))\n            new_value = inc\n\n            point.positions = [new_value]\n            trajectory_goal.trajectory.points = [point]\n            trajectory_goal.trajectory.header.stamp = rospy.Time.now()\n            rospy.loginfo('joint_name = {0}, trajectory_goal = {1}'.format(joint_name, trajectory_goal))\n            self.trajectory_client.send_goal(trajectory_goal)\n            rospy.loginfo('Done sending command.')\n            self.speech.print_commands()\n\n    def main(self):\n        \"\"\"\n        The main function that instantiates the HelloNode class, initializes the subscriber,\n        and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.main(self, 'voice_teleop', 'voice_teleop', wait_for_first_pointcloud=False)\n        rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)\n        rate = rospy.Rate(self.rate)\n        self.speech.print_commands()\n\n        while not rospy.is_shutdown():\n            command = self.speech.get_command()\n            self.send_command(command)\n            rate.sleep()\n\nif __name__ == '__main__':\n    try:\n        node = VoiceTeleopNode()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1/example_9/#the-code-explained","title":"The Code Explained","text":"<p>This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import math\nimport rospy\nimport sys\n\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Int32\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. Import the <code>FollowJointTrajectoryGoal</code> from the <code>control_msgs.msg</code> package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the <code>trajectory_msgs</code> package to define robot trajectories. The <code>hello_helpers</code> package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.  Import <code>sensor_msgs.msg</code> so that we can subscribe to JointState messages.</p> <pre><code>class GetVoiceCommands:\n</code></pre> <p>Create a class that subscribes to the <code>speech-to-text</code> recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion.</p> <pre><code>self.step_size = 'medium'\nself.rad_per_deg = math.pi/180.0\n</code></pre> <p>Set the default step size as medium and create a float value, <code>self.rad_per_deg</code>, to convert degrees to radians.</p> <pre><code>self.small_deg = 5.0\nself.small_rad = self.rad_per_deg * self.small_deg\nself.small_translate = 0.025\n\nself.medium_deg = 10.0\nself.medium_rad = self.rad_per_deg * self.medium_deg\nself.medium_translate = 0.05\n\nself.big_deg = 20.0\nself.big_rad = self.rad_per_deg * self.big_deg\nself.big_translate = 0.1\n</code></pre> <p>Define the three rotation and translation step sizes.</p> <pre><code>self.voice_command = None\nself.sound_direction = 0\nself.speech_to_text_sub  = rospy.Subscriber(\"/speech_to_text\",  SpeechRecognitionCandidates, self.callback_speech)\nself.sound_direction_sub = rospy.Subscriber(\"/sound_direction\", Int32,                       self.callback_direction)\n</code></pre> <p>Initialize the voice command and sound direction to values that will not result in moving the base.</p> <p>Set up two subscribers.  The first one subscribes to the topic <code>/speech_to_text</code>, looking for <code>SpeechRecognitionCandidates</code> messages.  When a message comes in, ROS is going to pass it to the function <code>callback_speech</code> automatically. The second subscribes to <code>/sound_direction</code> message and passes it to the <code>callback_direction</code> function.</p> <pre><code>self.sound_direction = msg.data * -self.rad_per_deg\n</code></pre> <p>The <code>callback_direction</code> function converts the <code>sound_direction</code> topic from degrees to radians.</p> <pre><code>if self.step_size == 'small':\n    inc = {'rad': self.small_rad, 'translate': self.small_translate}\nif self.step_size == 'medium':\n    inc = {'rad': self.medium_rad, 'translate': self.medium_translate}\nif self.step_size == 'big':\n    inc = {'rad': self.big_rad, 'translate': self.big_translate}\nreturn inc\n</code></pre> <p>The <code>callback_speech</code> stores the increment size for translational and rotational base motion in <code>inc</code>. The increment size is contingent on the <code>self.step_size</code> string value.</p> <pre><code>command = None\nif self.voice_command == 'forward':\n    command = {'joint': 'translate_mobile_base', 'inc': self.get_inc()['translate']}\nif self.voice_command == 'back':\n    command = {'joint': 'translate_mobile_base', 'inc': -self.get_inc()['translate']}\nif self.voice_command == 'left':\n    command = {'joint': 'rotate_mobile_base', 'inc': self.get_inc()['rad']}\nif self.voice_command == 'right':\n    command = {'joint': 'rotate_mobile_base', 'inc': -self.get_inc()['rad']}\nif self.voice_command == 'stretch':\n    command = {'joint': 'rotate_mobile_base', 'inc': self.sound_direction}\n</code></pre> <p>In the <code>get_command()</code> function, the <code>command</code> is initialized as <code>None</code>, or set as a dictionary where the <code>joint</code> and <code>inc</code> values are stored. The <code>command</code> message type is dependent on the <code>self.voice_command</code> string value.</p> <pre><code>if (self.voice_command == \"small\") or (self.voice_command == \"medium\") or (self.voice_command == \"big\"):\n    self.step_size = self.voice_command\n    rospy.loginfo('Step size = {0}'.format(self.step_size))\n</code></pre> <p>Based on the <code>self.voice_command</code> value set the step size for the increments.</p> <pre><code>if self.voice_command == 'quit':\n    rospy.signal_shutdown(\"done\")\n    sys.exit(0)\n</code></pre> <p>If the <code>self.voice_command</code> is equal to <code>quit</code>, then initiate a clean shutdown of ROS and exit the Python interpreter.</p> <pre><code>class VoiceTeleopNode(hm.HelloNode):\n    \"\"\"\n    A class that inherits the HelloNode class from hm and sends joint trajectory\n    commands.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that declares object from the GetVoiceCommands class, instantiates\n        the HelloNode class, and set the publishing rate.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n        self.rate = 10.0\n        self.joint_state = None\n        self.speech = GetVoiceCommands()\n</code></pre> <p>A class that inherits the <code>HelloNode</code> class from <code>hm</code> declares object from the <code>GetVoiceCommands</code> class and sends joint trajectory commands.</p> <pre><code>def send_command(self, command):\n    \"\"\"\n    Function that makes an action call and sends base joint trajectory goals\n    :param self: The self reference.\n    :param command: A dictionary type.\n    \"\"\"\n    joint_state = self.joint_state\n    if (joint_state is not None) and (command is not None):\n        point = JointTrajectoryPoint()\n        point.time_from_start = rospy.Duration(0.0)\n</code></pre> <p>The <code>send_command</code> function stores the joint state message and uses a conditional statement to send joint trajectory goals. Also, assign <code>point</code> as a <code>JointTrajectoryPoint</code> message type.</p> <pre><code>trajectory_goal = FollowJointTrajectoryGoal()\ntrajectory_goal.goal_time_tolerance = rospy.Time(1.0)\n</code></pre> <p>Assign <code>trajectory_goal</code> as a <code>FollowJointTrajectoryGoal</code> message type.</p> <pre><code>joint_name = command['joint']\ntrajectory_goal.trajectory.joint_names = [joint_name]\n</code></pre> <p>Extract the joint name from the command dictionary.</p> <pre><code>inc = command['inc']\nrospy.loginfo('inc = {0}'.format(inc))\nnew_value = inc\n</code></pre> <p>Extract the increment type from the command dictionary.</p> <pre><code>point.positions = [new_value]\ntrajectory_goal.trajectory.points = [point]\n</code></pre> <p>Assign the new value position to the trajectory goal message type.</p> <pre><code>self.trajectory_client.send_goal(trajectory_goal)\nrospy.loginfo('Done sending command.')\n</code></pre> <p>Make the action call and send the goal of the new joint position.</p> <pre><code>self.speech.print_commands()\n</code></pre> <p>Reprint the voice command menu after the trajectory goal is sent.</p> <pre><code>def main(self):\n      \"\"\"\n      The main function that instantiates the HelloNode class, initializes the subscriber,\n      and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes.\n      :param self: The self reference.\n      \"\"\"\n      hm.HelloNode.main(self, 'voice_teleop', 'voice_teleop', wait_for_first_pointcloud=False)\n      rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)\n      rate = rospy.Rate(self.rate)\n      self.speech.print_commands()\n</code></pre> <p>The main function instantiates the <code>HelloNode</code> class, initializes the subscriber, and calls other methods in both the <code>VoiceTeleopNode</code> and <code>GetVoiceCommands</code> classes.</p> <pre><code>while not rospy.is_shutdown():\n  command = self.speech.get_command()\n  self.send_command(command)\n  rate.sleep()\n</code></pre> <p>Run a while loop to continuously check speech commands and send those commands to execute an action.</p> <pre><code>try:\n  node = VoiceTeleopNode()\n  node.main()\nexcept KeyboardInterrupt:\n  rospy.loginfo('interrupt received, so shutting down')\n</code></pre> <p>Declare a <code>VoiceTeleopNode</code> object. Then execute the <code>main()</code> method.</p>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/","title":"Follow Joint Trajectory Commands","text":""},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#followjointtrajectory-commands","title":"FollowJointTrajectory Commands","text":"<p>Stretch ROS driver offers a <code>FollowJointTrajectory</code> action service for its arm. Within this tutorial, we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute.</p>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#stow-command-example","title":"Stow Command Example","text":"<p>Begin by running the following command in a terminal.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>In a new terminal, switch the mode to <code>position</code> mode using a rosservice call. Then run the stow command node.</p> <pre><code>rosservice call /switch_to_position_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython3 stow_command.py\n</code></pre> <p>This will send a <code>FollowJointTrajectory</code> command to stow Stretch's arm.</p>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nimport time\n\nclass StowCommand(hm.HelloNode):\n  '''\n  A class that sends a joint trajectory goal to stow the Stretch's arm.\n  '''\n  def __init__(self):\n    hm.HelloNode.__init__(self)\n\n  def issue_stow_command(self):\n    '''\n    Function that makes an action call and sends stow position goal.\n    :param self: The self reference.\n    '''\n    stow_point = JointTrajectoryPoint()\n    stow_point.time_from_start = rospy.Duration(0.000)\n    stow_point.positions = [0.2, 0.0, 3.4]\n\n    trajectory_goal = FollowJointTrajectoryGoal()\n    trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n    trajectory_goal.trajectory.points = [stow_point]\n    trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n    trajectory_goal.trajectory.header.frame_id = 'base_link'\n\n    self.trajectory_client.send_goal(trajectory_goal)\n    rospy.loginfo('Sent stow goal = {0}'.format(trajectory_goal))\n    self.trajectory_client.wait_for_result()\n\n  def main(self):\n    '''\n    Function that initiates stow_command function.\n    :param self: The self reference.\n    '''\n    hm.HelloNode.main(self, 'stow_command', 'stow_command', wait_for_first_pointcloud=False)\n    rospy.loginfo('stowing...')\n    self.issue_stow_command()\n    time.sleep(2)\n\n\nif __name__ == '__main__':\n  try:\n    node = StowCommand()\n    node.main()\n  except KeyboardInterrupt:\n    rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rospy\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nimport time\n</code></pre> <p>You need to import <code>rospy</code> if you are writing a ROS Node. Import the <code>FollowJointTrajectoryGoal</code> from the control_msgs.msg package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.</p> <pre><code>class StowCommand(hm.HelloNode):\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n</code></pre> <p>The <code>StowCommand</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is initialized.</p> <pre><code>def issue_stow_command(self):\n    stow_point = JointTrajectoryPoint()\n    stow_point.time_from_start = rospy.Duration(0.000)\n    stow_point.positions = [0.2, 0.0, 3.4]\n</code></pre> <p>The <code>issue_stow_command()</code> is the name of the function that will stow Stretch's arm. Within the function, we set stow_point as a <code>JointTrajectoryPoint</code>and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined next.</p> <pre><code>    trajectory_goal = FollowJointTrajectoryGoal()\n    trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n    trajectory_goal.trajectory.points = [stow_point]\n    trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n    trajectory_goal.trajectory.header.frame_id = 'base_link'\n</code></pre> <p>Set <code>trajectory_goal</code> as a <code>FollowJointTrajectoryGoal</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> is defined by the positions set in <code>stow_point</code>. Specify the coordinate frame that we want (base_link) and set the time to be now.</p> <pre><code>self.trajectory_client.send_goal(trajectory_goal)\nrospy.loginfo('Sent stow goal = {0}'.format(trajectory_goal))\nself.trajectory_client.wait_for_result()\n</code></pre> <p>Make the action call and send the goal. The last line of code waits for the result before it exits the python script.</p> <pre><code>def main(self):\n    hm.HelloNode.main(self, 'stow_command', 'stow_command', wait_for_first_pointcloud=False)\n    rospy.loginfo('stowing...')\n    self.issue_stow_command()\n    time.sleep(2)\n</code></pre> <p>Create a function, <code>main()</code>, to set up the <code>hm.HelloNode</code> class and issue the stow command.</p> <pre><code>if __name__ == '__main__':\n    try:\n        node = StowCommand()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre> <p>Declare object, node, from the <code>StowCommand()</code> class. Then run the <code>main()</code> function.</p>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#multipoint-command-example","title":"Multipoint Command Example","text":"<p>Begin by running the following command in a terminal:</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>In a new terminal, switch the mode to <code>position</code> mode using a rosservice call. Then run the multipoint command node.</p> <pre><code>rosservice call /switch_to_position_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython3 multipoint_command.py\n</code></pre> <p>This will send a list of <code>JointTrajectoryPoint</code> message types to move Stretch's arm.</p>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport time\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\n\nclass MultiPointCommand(hm.HelloNode):\n  \"\"\"\n  A class that sends multiple joint trajectory goals to the stretch robot.\n  \"\"\"\n  def __init__(self):\n    hm.HelloNode.__init__(self)\n\n  def issue_multipoint_command(self):\n    \"\"\"\n    Function that makes an action call and sends multiple joint trajectory goals\n    to the joint_lift, wrist_extension, and joint_wrist_yaw.\n    :param self: The self reference.\n    \"\"\"\n    point0 = JointTrajectoryPoint()\n    point0.positions = [0.2, 0.0, 3.4]\n    point0.velocities = [0.2, 0.2, 2.5]\n    point0.accelerations = [1.0, 1.0, 3.5]\n\n    point1 = JointTrajectoryPoint()\n    point1.positions = [0.3, 0.1, 2.0]\n\n    point2 = JointTrajectoryPoint()\n    point2.positions = [0.5, 0.2, -1.0]\n\n    point3 = JointTrajectoryPoint()\n    point3.positions = [0.6, 0.3, 0.0]\n\n    point4 = JointTrajectoryPoint()\n    point4.positions = [0.8, 0.2, 1.0]\n\n    point5 = JointTrajectoryPoint()\n    point5.positions = [0.5, 0.1, 0.0]\n\n    trajectory_goal = FollowJointTrajectoryGoal()\n    trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n    trajectory_goal.trajectory.points = [point0, point1, point2, point3, point4, point5]\n    trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n    trajectory_goal.trajectory.header.frame_id = 'base_link'\n\n    self.trajectory_client.send_goal(trajectory_goal)\n    rospy.loginfo('Sent list of goals = {0}'.format(trajectory_goal))\n    self.trajectory_client.wait_for_result()\n\n  def main(self):\n    \"\"\"\n    Function that initiates the multipoint_command function.\n    :param self: The self reference.\n    \"\"\"\n    hm.HelloNode.main(self, 'multipoint_command', 'multipoint_command', wait_for_first_pointcloud=False)\n    rospy.loginfo('issuing multipoint command...')\n    self.issue_multipoint_command()\n    time.sleep(2)\n\n\nif __name__ == '__main__':\n  try:\n    node = MultiPointCommand()\n    node.main()\n  except KeyboardInterrupt:\n    rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code-explained_1","title":"The Code Explained","text":"<p>Seeing that there are similarities between the multipoint and stow command nodes, we will only break down the different components of the <code>multipoint_command</code> node.</p> <pre><code>point0 = JointTrajectoryPoint()\npoint0.positions = [0.2, 0.0, 3.4]\n</code></pre> <p>Set <code>point0</code> as a <code>JointTrajectoryPoint</code>and provide desired positions. These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. The lift and wrist extension positions are expressed in meters, whereas the wrist yaw is in radians.</p> <p><pre><code>point0.velocities = [0.2, 0.2, 2.5]\n</code></pre> Provide the desired velocity of the lift (m/s), wrist extension (m/s), and wrist yaw (rad/s) for <code>point0</code>.</p> <pre><code>point0.accelerations = [1.0, 1.0, 3.5]\n</code></pre> <p>Provide desired accelerations of the lift (m/s^2), wrist extension (m/s^2), and wrist yaw (rad/s^2).</p> <p>Note</p> <p>The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated.</p> <pre><code>trajectory_goal = FollowJointTrajectoryGoal()\ntrajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\ntrajectory_goal.trajectory.points = [point0, point1, point2, point3, point4, point5]\ntrajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\ntrajectory_goal.trajectory.header.frame_id = 'base_link'\n</code></pre> <p>Set <code>trajectory_goal</code> as a <code>FollowJointTrajectoryGoal</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> is defined by a list of the 6 points. Specify the coordinate frame that we want (base_link) and set the time to be now.</p>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#single-joint-actuator","title":"Single Joint Actuator","text":"<p>You can also actuate a single joint for the Stretch. Below is the list of joints and their position limit.</p> <pre><code>############################# JOINT LIMITS #############################\njoint_lift:      lower_limit =  0.00,  upper_limit =  1.10  # in meters\nwrist_extension: lower_limit =  0.00,  upper_limit =  0.50  # in meters\njoint_wrist_yaw: lower_limit = -1.75,  upper_limit =  4.00  # in radians\njoint_head_pan:  lower_limit = -3.90,  upper_limit =  1.50  # in radians\njoint_head_tilt: lower_limit = -1.53,  upper_limit =  0.79  # in radians\njoint_gripper_finger_left:  lower_limit = -0.6,  upper_limit =  0.6  # in radians\n\n# INCLUDED JOINTS IN POSITION MODE\ntranslate_mobile_base: No lower or upper limit. Defined by a step size in meters\nrotate_mobile_base:    No lower or upper limit. Defined by a step size in radians\n########################################################################\n</code></pre> <p>Begin by running the following command in a terminal.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>In a new terminal, switch the mode to <code>position</code> mode using a rosservice call. Then run the single joint actuator node.</p> <p><pre><code>rosservice call /switch_to_position_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython3 single_joint_actuator.py\n</code></pre> This will send a list of <code>JointTrajectoryPoint</code> message types to move Stretch's arm.</p> <p>The joint, <code>joint_gripper_finger_left</code>, is only needed when actuating the gripper.</p>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code_2","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport time\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\n\nclass SingleJointActuator(hm.HelloNode):\n    \"\"\"\n    A class that sends multiple joint trajectory goals to a single joint.\n    \"\"\"\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n\n    def issue_command(self):\n        \"\"\"\n        Function that makes an action call and sends joint trajectory goals\n        to a single joint\n        :param self: The self reference.\n        \"\"\"\n        trajectory_goal = FollowJointTrajectoryGoal()\n        trajectory_goal.trajectory.joint_names = ['joint_head_pan']\n\n        point0 = JointTrajectoryPoint()\n        point0.positions = [0.65]\n\n        # point1 = JointTrajectoryPoint()\n        # point1.positions = [0.5]\n\n        trajectory_goal.trajectory.points = [point0]#, point1]\n        trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n        self.trajectory_client.send_goal(trajectory_goal)\n        rospy.loginfo('Sent goal = {0}'.format(trajectory_goal))\n        self.trajectory_client.wait_for_result()\n\n    def main(self):\n        \"\"\"\n        Function that initiates the issue_command function.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.main(self, 'issue_command', 'issue_command', wait_for_first_pointcloud=False)\n        rospy.loginfo('issuing command...')\n        self.issue_command()\n        time.sleep(2)\n\n\nif __name__ == '__main__':\n    try:\n        node = SingleJointActuator()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code-explained_2","title":"The Code Explained","text":"<p>Since the code is quite similar to the <code>multipoint_command</code> code, we will only review the parts that differ.</p> <p>Now let's break the code down.</p> <pre><code>trajectory_goal = FollowJointTrajectoryGoal()\ntrajectory_goal.trajectory.joint_names = ['joint_head_pan']\n</code></pre> <p>Here we only input the joint name that we want to actuate. In this instance, we will actuate the <code>joint_head_pan</code>.</p> <pre><code>point0 = JointTrajectoryPoint()\npoint0.positions = [0.65]\n\n# point1 = JointTrajectoryPoint()\n# point1.positions = [0.5]\n</code></pre> <p>Set <code>point0</code> as a <code>JointTrajectoryPoint</code>and provide the desired position. You also have the option to send multiple point positions rather than one.</p> <pre><code>trajectory_goal.trajectory.points = [point0]#, point1]\ntrajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\ntrajectory_goal.trajectory.header.frame_id = 'base_link'\n</code></pre> <p>Set <code>trajectory_goal</code> as a <code>FollowJointTrajectoryGoal</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> set by your list of points. Specify the coordinate frame that we want (base_link) and set the time to be now.</p>"},{"location":"stretch-tutorials/ros1/gazebo_basics/","title":"Spawning Stretch in Simulation (Gazebo)","text":""},{"location":"stretch-tutorials/ros1/gazebo_basics/#empty-world-simulation","title":"Empty World Simulation","text":"<p>To spawn Stretch in Gazebo's default empty world run the following command in your terminal.</p> <pre><code>roslaunch stretch_gazebo gazebo.launch\n</code></pre> <p>This will bring up the robot in the gazebo simulation similar to the image shown below.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/gazebo_basics/#custom-world-simulation","title":"Custom World Simulation","text":"<p>In Gazebo, you can spawn Stretch in various worlds. First, source the Gazebo world files by running the following command in a terminal:</p> <pre><code>echo \"source /usr/share/gazebo/setup.sh\"\n</code></pre> <p>Then using the world argument, you can spawn Stretch in the Willow Garage world by running the following:</p> <pre><code>roslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1/getting_started/","title":"Getting Started","text":""},{"location":"stretch-tutorials/ros1/getting_started/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Stretch robot (see below for simulation instructions if you don\u2019t have a robot)</li> <li>Follow the Getting Started guide (hello_robot_xbox_teleop must not be running in the background)</li> <li>Interacting with Linux through the command line</li> <li>Basic understanding of ROS</li> <li>Setup untethered operation (optional)</li> </ol>"},{"location":"stretch-tutorials/ros1/getting_started/#connecting-a-monitor","title":"Connecting a Monitor","text":"<p>If you cannot access the robot through ssh due to your network settings, you will need to connect an HDMI monitor, USB keyboard, and mouse to the USB ports in the robot's trunk.</p>"},{"location":"stretch-tutorials/ros1/getting_started/#setting-up-stretch-in-simulation","title":"Setting Up Stretch in Simulation","text":"<p>Users who don\u2019t have a Stretch, but want to try the tutorials can set up their computer with Stretch Gazebo.</p>"},{"location":"stretch-tutorials/ros1/getting_started/#requirements","title":"Requirements","text":"<p>Although lower specifications might be sufficient, for the best experience we recommend the following for running the simulation:</p> <ul> <li>Processor: Intel i7 or comparable</li> <li>Memory: 16 GB</li> <li>Storage: 50 GB</li> <li>OS: Ubuntu 20.04</li> <li>Graphics Card: NVIDIA GTX2060 (optional)</li> </ul>"},{"location":"stretch-tutorials/ros1/getting_started/#setup","title":"Setup","text":"<p>Hello Robot is currently running Stretch on Ubuntu 20.04 and ROS Noetic. To begin the setup, follow the Run the new robot installation script on your system.</p> <p>Finally, follow the Creating a new ROS workspace guide to create a fresh catkin workspace complete with all the dependencies.</p> <p>To begin working with a simulated Stretch, follow the Gazebo basics tutorial.</p>"},{"location":"stretch-tutorials/ros1/internal_state_of_stretch/","title":"Internal State of Stretch","text":""},{"location":"stretch-tutorials/ros1/internal_state_of_stretch/#getting-the-state-of-the-robot","title":"Getting the State of the Robot","text":"<p>Begin by starting up the stretch driver launch file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Then utilize the ROS command-line tool rostopic to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a new terminal.</p> <pre><code>rostopic echo /joint_states -n1\n</code></pre> <p>Note that the flag, <code>-n1</code>, at the end of the command defines the count of how many times you wish to publish the current topic information. Remove the flag if you prefer to continuously print the topic for debugging purposes.</p> <p>Your terminal will output the information associated with the <code>/joint_states</code> topic. Your <code>header</code>, <code>position</code>, <code>velocity</code>, and <code>effort</code> information may vary from what is printed below.</p> <pre><code>header:\n  seq: 70999\n  stamp:\n    secs: 1420\n    nsecs:   2000000\n  frame_id: ''\nname: [joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left,\n  joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift,\n  joint_right_wheel, joint_wrist_yaw]\nposition: [-1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2.9291786329821434e-07, 1.3802900147297237e-06, 0.08154086954434359, 1.4361499260374905e-07, 0.4139061738340768, 9.32603306580404e-07]\nvelocity: [0.00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1.322424459109634e-05, -0.00035084643762840415, 0.0012164337445918797, 0.0002138814988808099, 0.00010419792027496809, 4.0575263146426684e-05, 0.00022487596895736357, -0.0007751929074042957, 0.0002451588607332439]\neffort: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n---\n</code></pre> <p>Let's say you are interested in only seeing the <code>header</code> component of the <code>/joint_states</code> topic, you can output this within the rostopic command-line tool by typing the following command.</p> <pre><code>rostopic echo /joint_states/header -n1\n</code></pre> <p>Your terminal will then output something similar to this:</p> <pre><code>seq: 97277\nstamp:\n  secs: 1945\n  nsecs: 562000000\nframe_id: ''\n---\n</code></pre> <p>Additionally, if you were to type <code>rostopic echo /</code> in the terminal, then press the <code>Tab</code> key on your keyboard, you will see the list of published active topics.</p> <p>A powerful tool to visualize ROS communication is the ROS rqt_graph package. By typing the following in a new terminal, you can see a graph of topics being communicated between nodes.</p> <pre><code>rqt_graph\n</code></pre> <p> </p> <p>The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.</p>"},{"location":"stretch-tutorials/ros1/intro_to_nav/","title":"Intro to Navigation","text":""},{"location":"stretch-tutorials/ros1/intro_to_nav/#display","title":"Display","text":"<p>Visualize the robot in Rviz:</p> <pre><code>roslaunch stretch_core stretch.launch lidar_odom:=false respeaker:=false rviz:=true\n</code></pre> <p>For more details on the arguments, see the API docs.</p>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#simulation","title":"Simulation","text":"<p>Visualize the simulated robot in Rviz:</p> <pre><code>roslaunch stretch_gazebo gazebo.launch rviz:=true\n</code></pre>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#untethered-operation","title":"Untethered Operation","text":"<p>At this point, we want to remove all wires tethered Stretch to our monitor/keyboard/etc. We'll set up \"ROS Remote Master\", which is a feature built into ROS that allows untethered operation. Follow this guide: https://docs.hello-robot.com/0.2/stretch-tutorials/getting_started/untethered_operation/#ros-remote-master</p> <pre><code># on the robot\nroslaunch stretch_core stretch.launch lidar_odom:=false respeaker:=false\n\n# on your personal computer\nrviz -d `rospack find stretch_core`/rviz/stretch.rviz\n</code></pre>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#teleoperation","title":"Teleoperation","text":"<p>Switch to 'navigation' mode:</p> <pre><code>rosservice call /switch_to_navigation_mode\n</code></pre> <p>On your personal computer, plug in the controller dongle and run base keyboard teleop:</p> <pre><code>roslaunch stretch_core teleop_twist.launch\n</code></pre> <p>or if you have the Xbox controller:</p> <pre><code>roslaunch stretch_core teleop_twist.launch teleop_type:=joystick\n</code></pre> <p>The deadman button is the 'A' button (the green one).</p> <p></p>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#stretchcmd_vel","title":"/stretch/cmd_vel","text":"<p>The <code>/stretch/cmd_vel</code> topic accepts Twist msgs, where <code>twist.linear.x</code> and <code>twist.angular.z</code> are the translational and angular velocities the mobile base will execute.</p> <pre><code>$ rostopic echo /stretch/cmd_vel\n---\nlinear: \n  x: -0.04\n  y: 0.0\n  z: 0.0\nangular: \n  x: 0.0\n  y: 0.0\n  z: -0.05731585025787354\n---\nlinear: \n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular: \n  x: 0.0\n  y: 0.0\n  z: 0.0\n---\n</code></pre>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#safety","title":"Safety","text":"<p>Velocity commands must be sent at a regular control rate and must be faster than 2hz. There's two safety behaviors that prevent a runaway robot. A software check smoothly stops base motion after 0.5 seconds if no new command is received. A hardware check abruptly stops base motion after 1 second if no new command is received.</p>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#mapping-slides","title":"Mapping (slides)","text":"<p>Stop all previous ROS commands. Start the following ROS commands on your Stretch.</p> <p>Start the mapping launch file:</p> <pre><code>roslaunch stretch_navigation mapping.launch rviz:=false teleop_type:=none\n</code></pre> <p>Turn on the robot's head camera as well:</p> <pre><code>roslaunch stretch_core stretch_realsense.launch publish_upright_img:=true\n</code></pre> <p>Use keyboard teleop to tilt the head camera downwards to look at the floor in front of the robot:</p> <pre><code>rosrun stretch_core keyboard_teleop\n</code></pre> <p>Now, on your computer, launch Rviz:</p> <pre><code>rviz -d `rospack find stretch_navigation`/rviz/mapping.rviz\n</code></pre> <p>Start controller teleop:</p> <pre><code>roslaunch stretch_core teleop_twist.launch teleop_type:=joystick linear:=0.12 angular:=0.3\n</code></pre> <p>After moving around the environment for some time, you can save the map using:</p> <pre><code>rosrun map_server map_saver -f ${HELLO_FLEET_PATH}/maps/oct24thmap\n</code></pre>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#planning","title":"Planning","text":"<p>Stop all previous ROS commands. Start the following ROS commands on your Stretch.</p> <pre><code>roslaunch stretch_navigation navigation.launch map_yaml:=${HELLO_FLEET_PATH}/maps/oct24thmap.yaml rviz:=false\n</code></pre> <p>Start Rviz on your personal computer:</p> <pre><code>rviz -d `rospack find stretch_navigation`/rviz/navigation.rviz\n</code></pre>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#localization","title":"Localization","text":"<p>AMCL is very commonly used for localization. It's a particle filtering library that works by comparing the robot's motion and sensor updates with a distribution of guesses at the robot's position in order to eliminate unlikely guesses every iteration. Running these motion/sensor update steps will allow the filter to converge on the robot's position as the robot sees landmarks. When the robot \"wakes up\", it doesn't know where it is, and the particles are evenly distributed across the map. We tell the robot where it is using:</p> <ol> <li>The position estimate GUI in Rviz</li> <li>Detecting a unique landmark (e.g. a Aruco marker taped to the wall)</li> <li>Spinning in a 360 degree circle</li> </ol> <p>For example, turn on particle filters visualization in Rviz, use the Pose Estimate GUI to put the robot off somewhere wrong, and run teleop:</p> <pre><code>roslaunch stretch_core teleop_twist.launch teleop_type:=joystick linear:=0.12 angular:=0.3\n</code></pre> <p>Now spin the robot in a 360 degree circle. This doesn't always work, especially in environments with repetitive features.</p>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#costmaps-slides","title":"Costmaps (slides)","text":""},{"location":"stretch-tutorials/ros1/intro_to_nav/#global-plan-slides","title":"Global Plan (slides)","text":"<p>The global planner is called navfn/NavfnROS</p> <p>To visualize the global plan without moving the robot, switch the robot into position mode:</p> <pre><code>rosservice call /switch_to_position_mode\n</code></pre> <p>Use the Nav Goal GUI to send goals to MoveBase and visualize the global plans.</p> <p>Now cancel the plan:</p> <pre><code>rostopic pub /move_base/cancel actionlib_msgs/GoalID \"stamp:\n  secs: 0\n  nsecs: 0\nid: ''\"\n</code></pre>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#plan-follower","title":"Plan Follower","text":"<p>The local planner is called TrajectoryPlannerROS</p>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#code-examples","title":"Code Examples","text":"<ul> <li>https://docs.hello-robot.com/0.2/stretch-tutorials/ros1/example_13/</li> <li>https://docs.hello-robot.com/0.2/stretch-tutorials/ros1/autodocking_nav_stack/</li> </ul>"},{"location":"stretch-tutorials/ros1/intro_to_nav/#references","title":"References","text":"<ul> <li>https://github.com/MetroRobots/navigation_university/</li> <li>https://github.com/hello-robot/stretch_ros/pull/120/files</li> </ul>"},{"location":"stretch-tutorials/ros1/moveit_basics/","title":"MoveIt! Basics","text":"<p>&lt;!--</p>"},{"location":"stretch-tutorials/ros1/moveit_basics/#moveit-on-stretch","title":"MoveIt! on Stretch","text":"<p>To run MoveIt with the actual hardware, (assuming <code>stretch_driver</code> is already running) simply run</p> <pre><code>roslaunch stretch_moveit_config move_group.launch\n</code></pre> <p>This will run all of the planning capabilities, but without the setup, simulation and interface that the above demo provides. To create plans for the robot with the same interface as the offline demo, you can run <pre><code>roslaunch stretch_moveit_config moveit_rviz.launch\n``` --&gt;\n\n## MoveIt! Without Hardware\nTo begin running MoveIt! on stretch, run the demo launch file. This doesn't require any simulator or robot to run.\n\n```{.bash .shell-prompt}\nroslaunch stretch_moveit_config demo.launch\n</code></pre></p> <p>This will bring up an RViz instance where you can move the robot around using interactive markers and create plans between poses. You can reference the bottom gif as a guide to plan and execute motion.</p> <p><p> </p> <p>Additionally, the demo allows a user to select from the three groups, <code>stretch_arm</code>, <code>stretch_gripper</code>, <code>stretch_head</code> to move. A few notes to be kept in mind:</p> <ul> <li> <p>Pre-defined start and goal states can be specified in Start State and Goal State drop-downs in the Planning tab of the Motion Planning RViz plugin.</p> </li> <li> <p>stretch_gripper group does not show markers and is intended to be controlled via the joints tab that is located on the very right of the Motion Planning Rviz plugin.</p> </li> <li> <p>When planning with stretch_head group make sure you select Approx IK Solutions in the Planning tab of the Motion Planning RViz plugin.</p> </li> </ul> <p> <p>Additionally, the demo allows a user to select from the three groups, <code>stretch_arm</code>, <code>stretch_gripper</code>, <code>stretch_head</code> to move. A few notes to be kept in mind:</p> <ul> <li> <p>Pre-defined start and goal states can be specified in Start State and Goal State drop-downs in the Planning tab of the Motion Planning RViz plugin.</p> </li> <li> <p>stretch_gripper group does not show markers and is intended to be controlled via the joints tab that is located on the very right of the Motion Planning Rviz plugin.</p> </li> <li> <p>When planning with stretch_head group make sure you select Approx IK Solutions in the Planning tab of the Motion Planning RViz plugin.</p> </li> </ul> <p>   ## Running Gazebo with MoveIt! and Stretch  To run in Gazebo, execute:  <pre><code>roslaunch stretch_gazebo gazebo.launch\n</code></pre>  Then, in a new terminal, execute:  <pre><code>roslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=keyboard\n</code></pre>  In a separate terminal, launch:  <pre><code>roslaunch stretch_moveit_config demo_gazebo.launch\n</code></pre>  This will launch a Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via the Motion Planning Rviz plugin. Start and goal positions for joints can be selected similarly to [this moveit tutorial](https://ros-planning.github.io/moveit_tutorials/doc/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#choosing-specific-start-goal-states).  <p>"},{"location":"stretch-tutorials/ros1/moveit_basics/#running-gazebo-with-moveit-and-stretch","title":"Running Gazebo with MoveIt! and Stretch","text":"<p>To run in Gazebo, execute:</p> <pre><code>roslaunch stretch_gazebo gazebo.launch\n</code></pre> <p>Then, in a new terminal, execute:</p> <pre><code>roslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=keyboard\n</code></pre> <p>In a separate terminal, launch:</p> <pre><code>roslaunch stretch_moveit_config demo_gazebo.launch\n</code></pre> <p>This will launch a Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via the Motion Planning Rviz plugin. Start and goal positions for joints can be selected similarly to this moveit tutorial.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/navigation_stack/","title":"Navigation Stack","text":""},{"location":"stretch-tutorials/ros1/navigation_stack/#navigation-stack-with-actual-robot","title":"Navigation Stack with Actual robot","text":"<p>stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive Stretch around a mapped space. Running this code will require the robot to be untethered.</p> <p>Then run the following commands to map the space that the robot will navigate in.</p> <pre><code>roslaunch stretch_navigation mapping.launch\n</code></pre> <p>Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures.</p> <p> </p> <p>In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to <code>stretch_user/</code>.</p> <pre><code>mkdir -p ~/stretch_user/maps\nrosrun map_server map_saver -f ${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;\n</code></pre> <p>Note</p> <p>The <code>&lt;map_name&gt;</code> does not include an extension. Map_saver will save two files as <code>&lt;map_name&gt;.pgm</code> and <code>&lt;map_name&gt;.yaml</code>.</p> <p>Next, with <code>&lt;map_name&gt;.yaml</code>, we can navigate the robot around the mapped space. Run:</p> <pre><code>roslaunch stretch_navigation navigation.launch map_yaml:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p>Rviz will show the robot in the previously mapped space; however, the robot's location on the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place.</p> <p>It is also possible to send 2D Pose Estimates and Nav Goals programmatically. In your launch file, you may include <code>navigation.launch</code> to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages to navigate the robot programmatically.</p>"},{"location":"stretch-tutorials/ros1/navigation_stack/#running-in-simulation","title":"Running in Simulation","text":"<p>To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the <code>mapping_gazebo.launch</code> and <code>navigation_gazebo.launch</code> files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch.</p> <pre><code>roslaunch stretch_navigation mapping_gazebo.launch gazebo_world:=worlds/willowgarage.world\n</code></pre>"},{"location":"stretch-tutorials/ros1/navigation_stack/#teleop-using-a-joystick-controller","title":"Teleop using a Joystick Controller","text":"<p>The mapping launch files, <code>mapping.launch</code> and <code>mapping_gazebo.launch</code>, expose the ROS argument <code>teleop_type</code>. By default, this ROS argument is set to <code>keyboard</code>, which launches keyboard teleop in the terminal. If the Xbox controller that ships with Stretch is plugged into your computer, the following command will launch mapping with joystick teleop:</p> <pre><code>roslaunch stretch_navigation mapping.launch teleop_type:=joystick\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1/navigation_stack/#using-ros-remote-master","title":"Using ROS Remote Master","text":"<p>If you have set up ROS Remote Master for untethered operation, you can use Rviz and teleop locally with the following commands:</p> <p>On the robot, execute:</p> <pre><code>roslaunch stretch_navigation mapping.launch rviz:=false teleop_type:=none\n</code></pre> <p>On your machine, execute:</p> <pre><code>rviz -d `rospack find stretch_navigation`/rviz/mapping.launch\n</code></pre> <p>In a separate terminal on your machine, execute:</p> <pre><code>roslaunch stretch_core teleop_twist.launch teleop_type:=keyboard\n</code></pre>"},{"location":"stretch-tutorials/ros1/perception/","title":"Perception","text":""},{"location":"stretch-tutorials/ros1/perception/#perception-introduction","title":"Perception Introduction","text":"<p>The Stretch robot is equipped with the Intel RealSense D435i camera, an essential component that allows the robot to measure and analyze the world around it. In this tutorial, we are going to showcase how to visualize the various topics published by the camera.</p> <p>Begin by running the stretch <code>driver.launch</code> file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>roslaunch stretch_core d435i_low_resolution.launch\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz\n</code></pre>"},{"location":"stretch-tutorials/ros1/perception/#pointcloud2-display","title":"PointCloud2 Display","text":"<p>A list of displays on the left side of the interface can visualize the camera data. Each display has its properties and status that notify a user if topic messages are received.</p> <p>For the <code>PointCloud2</code> display, a sensor_msgs/pointCloud2 message named <code>/camera/depth/color/points</code> is received and the GIF below demonstrates the various display properties when visualizing the data.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/perception/#image-display","title":"Image Display","text":"<p>The <code>Image</code> display when toggled creates a new rendering window that visualizes a sensor_msgs/Image messaged, /camera/color/image_raw. This feature shows the image data from the camera; however, the image comes out sideways. Thus, you can select the /camera/color/image_raw_upright_view from the Image Topic options to get an upright view of the image.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/perception/#camera-display","title":"Camera Display","text":"<p>The <code>Camera</code> display is similar to that of the <code>Image</code> display. In this setting, the rendering window also visualizes other displays, such as the PointCloud2, the RobotModel, and Grid Displays. The <code>visibility</code> property can toggle what displays you are interested in visualizing.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/perception/#depthcloud-display","title":"DepthCloud Display","text":"<p>The <code>DepthCloud</code> display is visualized in the main RViz window. This display takes in the depth image and RGB image provided by RealSense to visualize and register a point cloud.</p> <p> </p>"},{"location":"stretch-tutorials/ros1/perception/#deep-perception","title":"Deep Perception","text":"<p>Hello Robot also has a ROS package that uses deep learning models for various detection demos. A link to the package is provided: stretch_deep_perception.</p>"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/","title":"ReSpeaker Microphone Array","text":""},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#respeaker-microphone-array","title":"Respeaker Microphone Array","text":"<p>In this tutorial, we will go over how to use Stretch's Respeaker Mic Array v2.0.  </p> <p> </p>"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#stretch-respeaker-tool","title":"Stretch Respeaker Tool","text":"<p>We'll begin by using the command-line Stretch Respeaker Tool to quickly try out the robot's microphone array and speakers. This tool doesn't use ROS, but we'll cover how to use the Respeaker from ROS in the next section. Type the following command in a new terminal:</p> <pre><code>stretch_respeaker_test.py\n</code></pre> <p>The following will be outputted in your terminal:</p> <pre><code>hello-robot@stretch-re1-1005:~$ stretch_respeaker_test.py\nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n* waiting for audio...\n* recording 3 seconds\n* done\n* playing audio\n* done\n\n* waiting for audio...\n</code></pre> <p>The Stretch Respeaker tool will wait until it hears audio loud enough to trigger VAD (voice activity detection). Then, the tool will record audio for 3 seconds, replay it through its speakers, and go back to waiting for audio. This tool is a good way to confirm the hardware is working correctly.</p> <p>To stop the tool, type Ctrl + C in the terminal.</p>"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#respeaker-ros-package","title":"Respeaker ROS Package","text":""},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#prerequisite","title":"Prerequisite","text":"<p>Before getting started with the Respeaker ROS package, we'll confirm it is available to use. Type the following into a new terminal:</p> <pre><code>rospack find respeaker_ros\n</code></pre> <p>If you get an error like <code>[rospack] Error: package 'respeaker_ros' not found</code>, refresh your ROS workspace using the Creating a New ROS Workspace tutorial.</p>"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#getting-started","title":"Getting started","text":"<p>Run the <code>respeaker.launch</code> file in a new terminal using:</p> <p><pre><code>roslaunch respeaker_ros respeaker.launch\n</code></pre> This will bring up the necessary ROS nodes to interface with the robot's microphone array and speakers. After initialization, you will see the following outputted in your terminal:</p> <pre><code>[INFO] [1672818306.618280]: Initializing Respeaker device (takes 10 seconds)\n[INFO] [1672818317.082498]: Respeaker device initialized (Version: 16)\n[INFO] [1672818317.521955]: Found 6: ReSpeaker 4 Mic Array (UAC1.0): USB Audio (hw:1,0) (channels: 6)\n[INFO] [1672818317.526263]: Using channels range(0, 6)\n</code></pre> <p>Explore some of the notable ROS topics being published using the following commands: </p> <pre><code>rostopic echo /audio              # Raw audio data\nrostopic echo /speech_audio       # Raw audio data when there is speech\nrostopic echo /sound_direction    # Direction (in Degrees) of audio source\nrostopic echo /sound_localization # Direction (in Quaternion part of SE3 pose) of audio source\nrostopic echo /is_speeching       # Result of Voice Activity Detector (VAD)\nrostopic echo /speech_to_text     # Voice recognition\n</code></pre>"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#speech-recognition","title":"Speech Recognition","text":"<p>In particular, the <code>/speech_to_text</code> topic can be helpful for prototyping voice command based programs. In a new terminal, run <code>rostopic echo /speech_to_text</code>. Then, stand near the robot and say \"hello robot\". You will see something like this outputted to the terminal:</p> <pre><code>transcript:\n  - hello robot\nconfidence: [0.9876290559768677]\n</code></pre> <p>A internet connection is required since the audio is being transcribed by the Google Speech API. To switch to using offline transciption, you can swap the speech recognizer used in the speech_to_text node. See the SpeechRecognition docs for other available speech recognizers.</p>"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#direction-of-arrival","title":"Direction of Arrival","text":"<p>The <code>/sound_direction</code> and <code>/sound_localization</code> topics give the DOA, or direction of arrival, in degrees and quaternion respectively. New messages are published when the DOA changes. And the 12 Respeaker LEDs show the DOA coarsely by coloring the LED closest to the DOA teal, while the rest of the LEDs show a darker blue color. In a new terminal, run <code>rostopic echo /sound_direction</code> or <code>rostopic echo /sound_localization</code>. Then, stand in front of the robot and say anything. You will see something like this outputted to the terminal:</p> <pre><code># /sound_direction\ndata: 0\n\n# or /sound_localization\nheader:\n  seq: 1\n  stamp:\n    secs: 1672863834\n    nsecs: 646940469\n  frame_id: \"respeaker_base\"\npose:\n  position:\n    x: 0.0\n    y: 0.0\n    z: 0.0\n  orientation:\n    x: 0.0\n    y: 0.0\n    z: 1.0\n    w: 0.0\n</code></pre> <p>The DOA range in degrees is [-180, 180], and the diagram below shows the DOA range visually:</p> <pre><code>TODO\n</code></pre> <p>Due to echo-ing within the shell covering the Respeaker, the DOA can be inaccurate. If you are experiencing this issue, contact Hello Robot Support for instructions on removing the head shell, which will reduce echo. Furthermore, it's possible to get more accurate DOA with audio post-processing. The Respeaker calculates the DOA on-hardware, however audio streams from the four microphones are published, so it is possible to use real-time audio source localization software like ODAS to better determine the DOA.</p>"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#dynamic-reconfigure","title":"Dynamic Reconfigure","text":"<p>You can also set various parameters via <code>dynamic_reconfigure</code>, by running the following command in a new terminal:</p> <pre><code>rosrun rqt_reconfigure rqt_reconfigure\n</code></pre>"},{"location":"stretch-tutorials/ros1/rviz_basics/","title":"RViz Basics","text":""},{"location":"stretch-tutorials/ros1/rviz_basics/#visualizing-with-rviz","title":"Visualizing with RViz","text":"<p>You can utilize RViz to visualize Stretch's sensor information. To begin, in a terminal, run the stretch driver launch file.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Then, run the following command in a separate terminal to bring up a simple RViz configuration of the Stretch robot.</p> <pre><code>rosrun rviz rviz -d `rospack find stretch_core`/rviz/stretch_simple_test.rviz\n</code></pre> <p>An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window.</p> <p> </p> <p>If you want to visualize Stretch's tf transform tree, you need to add the display type to the RViz window. First, click the Add button and include the TF  type in the display. You will then see all of the transform frames of the Stretch robot, and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference.</p> <p> </p> <p>There are further tutorials for RViz which can be found here.</p>"},{"location":"stretch-tutorials/ros1/rviz_basics/#running-rviz-and-gazebo-simulation","title":"Running RViz and Gazebo (Simulation)","text":"<p>Let's bring up Stretch in the willow garage world from the gazebo basics tutorial and RViz by using the following command.</p> <pre><code>roslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world rviz:=true\n</code></pre> <p>The <code>rviz</code> flag will open an RViz window to visualize a variety of ROS topics.</p> <p> </p> <p>Bring up the keyboard teleop node to drive Stretch and observe its sensor input.</p>"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/","title":"Teleoperating Stretch","text":""},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#teleoperating-stretch","title":"Teleoperating Stretch","text":""},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#xbox-controller-teleoperating","title":"Xbox Controller Teleoperating","text":"<p>If you have not already had a look at the Xbox Controller Teleoperation section in the Quick Start guide, now might be a good time to try it.</p>"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#keyboard-teleoperating-full-body","title":"Keyboard Teleoperating: Full Body","text":"<p>For full-body teleoperation with the keyboard, you first need to run the <code>stretch_driver.launch</code> in a terminal.</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Then in a new terminal, type the following command</p> <pre><code>rosrun stretch_core keyboard_teleop\n</code></pre> <p>Below are the keyboard commands that allow a user to control all of Stretch's joints.</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------\n\n              i HEAD UP                    \n j HEAD LEFT            l HEAD RIGHT       \n              , HEAD DOWN                  \n\n\n 7 BASE ROTATE LEFT     9 BASE ROTATE RIGHT\n home                   page-up            \n\n\n              8 LIFT UP                    \n              up-arrow                     \n 4 BASE FORWARD         6 BASE BACK        \n left-arrow             right-arrow        \n              2 LIFT DOWN                  \n              down-arrow                   \n\n\n              w ARM OUT                    \n a WRIST FORWARD        d WRIST BACK       \n              x ARM IN                     \n\n\n              5 GRIPPER CLOSE              \n              0 GRIPPER OPEN               \n\n  step size:  b BIG, m MEDIUM, s SMALL     \n\n              q QUIT                       \n\n-------------------------------------------\n</code></pre> <p>To stop the node from sending twist messages, press <code>Ctrl</code> + <code>c</code> in the terminal.</p>"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#keyboard-teleoperating-mobile-base","title":"Keyboard Teleoperating: Mobile Base","text":"<p>Begin by running the following command in your terminal:</p> <pre><code>roslaunch stretch_core stretch_driver.launch\n</code></pre> <p>To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to <code>navigation</code> for the robot to receive <code>Twist</code> messages. This is done using a rosservice call in a new terminal. In the same terminal run the teleop_twist_keyboard node with the argument remapping the <code>cmd_vel</code> topic name to <code>stretch/cmd_vel</code>.</p> <pre><code>rosservice call /switch_to_navigation_mode\nrosrun teleop_twist_keyboard teleop_twist_keyboard.py cmd_vel:=stretch/cmd_vel\n</code></pre> <p>Below are the keyboard commands that allow a user to move Stretch's base.  </p> <pre><code>Reading from the keyboard  and Publishing to Twist!\n---------------------------\nMoving around:\n   u    i    o\n   j    k    l\n   m    ,    .\n\nFor Holonomic mode (strafing), hold down the shift key:\n---------------------------\n   U    I    O\n   J    K    L\n   M    &lt;    &gt;\n\nt : up (+z)\nb : down (-z)\n\nanything else : stop\n\nq/z : increase/decrease max speeds by 10%\nw/x : increase/decrease only linear speed by 10%\ne/c : increase/decrease only angular speed by 10%\n\nCTRL-C to quit\n\ncurrently:  speed 0.5   turn 1.0\n</code></pre> <p>To stop the node from sending twist messages, type <code>Ctrl</code> + <code>c</code>.</p>"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#create-a-node-for-mobile-base-teleoperating","title":"Create a node for Mobile Base Teleoperating","text":"<p>To move Stretch's mobile base using a python script, please look at Teleoperate Stretch with a node for reference.</p>"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#teleoperating-in-gazebo","title":"Teleoperating in Gazebo","text":""},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#keyboard-teleoperating-mobile-base_1","title":"Keyboard Teleoperating: Mobile Base","text":"<p>For keyboard teleoperation of the Stretch's mobile base, first, startup Stretch in simulation. Then run the following command in a new terminal.</p> <pre><code>roslaunch stretch_gazebo gazebo.launch\n</code></pre> <p>In a new terminal, type the following</p> <pre><code>roslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=keyboard\n</code></pre> <p>The same keyboard commands will be presented to a user to move the robot.</p>"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#xbox-controller-teleoperating_1","title":"Xbox Controller Teleoperating","text":"<p>An alternative for robot base teleoperation is to use an Xbox controller. Stop the keyboard teleoperation node by typing <code>Ctrl</code> + <code>c</code> in the terminal where the command was executed. Then connect the Xbox controller device to your local machine and run the following command.</p> <pre><code>roslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=joystick\n</code></pre> <p>Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless pressed. For a Logitech F310 joystick, this button is A.</p>"},{"location":"stretch-tutorials/ros1_melodic/","title":"Index","text":""},{"location":"stretch-tutorials/ros1_melodic/#deprecated-ros-melodic-with-stretch-watch-ros1-documentation-with-ros-noetic","title":"Deprecated ROS Melodic with Stretch, watch ROS1 documentation with ROS Noetic!","text":""},{"location":"stretch-tutorials/ros1_melodic/#tutorial-track-stretch-ros1","title":"Tutorial Track: Stretch ROS1","text":"<p>This tutorial track is for users looking to become familiar with programming the Stretch RE1 and RE2 via ROS1 Melodic. We recommend going through the tutorials in the following order:</p>"},{"location":"stretch-tutorials/ros1_melodic/#basics","title":"Basics","text":"Tutorial Description 1 Getting Started 2 Gazebo Basics 3 Teleoperating Stretch 4 Internal State of Stretch 5 RViz Basics 6 Navigation Stack 7 MoveIt! Basics 8 Follow Joint Trajectory Commands 9 Perception 10 ArUco Marker Detection 11 ReSpeaker Microphone Array 12 FUNMAP"},{"location":"stretch-tutorials/ros1_melodic/#other-examples","title":"Other Examples","text":"<p>To help get you get started on your software development, here are examples of nodes to have the stretch perform simple tasks.</p> Tutorial Description 1 Teleoperate Stretch with a Node Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Print Joint States Print the joint states of Stretch. 6 Store Effort Values Print, store, and plot the effort values of the Stretch robot. 7 Capture Image Capture images from the RealSense camera data. 8 Voice to Text Interpret speech and save transcript to a text file. 9 Voice Teleoperation of Base Use speech to teleoperate the mobile base. 10 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 11 PointCloud Transformation Convert PointCloud2 data to a PointCloud and transform to a different frame. 12 ArUco Tag Locator Actuate the head to locate a requested ArUco marker tag and return a transform. 13 2D Navigation Goals Send 2D navigation goals to the move_base ROS node."},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/","title":"Aruco marker detection","text":""},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/#aruco-marker-detector","title":"ArUco Marker Detector","text":"<p>For this tutorial, we will go over how to detect Stretch's ArUco markers and how to files the hold the information for each tag.</p>"},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/#visualize-aruco-markers-in-rviz","title":"Visualize ArUco Markers in RViz","text":"<p>Begin by running the stretch driver launch file.</p> <pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code># Terminal 2\nroslaunch stretch_core d435i_low_resolution.launch\n</code></pre> <p>Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node.</p> <pre><code># Terminal 3\nroslaunch stretch_core stretch_aruco.launch\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code># Terminal 4\nrosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz\n</code></pre> <p>You are going to need to teleoperate Stretch's head to detect the ArUco marker tags. Run the following command in a new terminal and control the head in order to point the camera towards the markers.   </p> <pre><code># Terminal 5\nrosrun stretch_core keyboard_teleop\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/#the-aruco-marker-dictionary","title":"The ArUco Marker Dictionary","text":"<p>When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml, that holds the information about the markers.</p> <p>If detect_aruco_markers node doesn\u2019t find an entry in stretch_marker_dict.yaml for a particular ArUco marker ID number, it uses the default entry. For example, most robots have shipped with the following default entry:</p> <pre><code>'default':\n  'length_mm': 24\n  'use_rgb_only': False\n  'name': 'unknown'\n  'link': None\n</code></pre> <p>and the following entry for the ArUco marker on the top of the wrist</p> <pre><code>'133':\n  'length_mm': 23.5\n  'use_rgb_only': False\n  'name': 'wrist_top'\n  'link': 'link_aruco_top_wrist'\n</code></pre> <p>Dictionary Breakdown</p> <pre><code>'133':\n</code></pre> <p>The dictionary key for each entry is the ArUco marker\u2019s ID number or <code>default</code>. For example, the entry shown above for the ArUco marker on the top of the wrist assumes that the marker\u2019s ID number is <code>133</code>.</p> <pre><code>'length_mm': 23.5\n</code></pre> <p>The <code>length_mm</code> value used by detect_aruco_markers is important for estimating the pose of an ArUco marker.</p> <p>IMPORTANT NOTE: If the actual width and height of the marker do not match this value, then pose estimation will be poor. Thus, carefully measure custom Aruco markers.</p> <pre><code>'use_rgb_only': False\n</code></pre> <p>If <code>use_rgb_only</code> is <code>True</code>, detect_aruco_markers will ignore depth images from the Intel RealSense D435i depth camera when estimating the pose of the marker and will instead only use RGB images from the D435i.</p> <p><pre><code>'name': 'wrist_top'\n</code></pre> <code>name</code> is used for the text string of the ArUco marker\u2019s ROS Marker in the ROS MarkerArray Message published by the detect_aruco_markers ROS node.</p> <pre><code>'link': 'link_aruco_top_wrist'\n</code></pre> <p><code>link</code> is currently used by stretch_calibration. It is the name of the link associated with a body-mounted ArUco marker in the robot\u2019s URDF.</p> <p>It\u2019s good practice to add an entry to stretch_marker_dict.yaml for each ArUco marker you use.</p>"},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/#create-a-new-aruco-marker","title":"Create a New ArUco Marker","text":"<p>At Hello Robot, we\u2019ve used the following guide when generating new ArUco markers.</p> <p>We generate ArUco markers using a 6x6 bit grid (36 bits) with 250 unique codes. This corresponds with DICT_6X6_250 defined in OpenCV. We generate markers using this online ArUco marker generator by setting the Dictionary entry to 6x6 and then setting the Marker ID and Marker size, mm as appropriate for the specific application. We strongly recommend to measure the actual marker by hand prior to adding an entry for it to stretch_marker_dict.yaml.</p> <p>We select marker ID numbers using the following ranges.</p> <ul> <li>0 - 99 : reserved for users</li> <li>100 - 249 : reserved for official use by Hello Robot Inc.</li> <li>100 - 199 : reserved for robots with distinct sets of body-mounted markers<ul> <li>Allows different robots near each other to use distinct sets of body-mounted markers to avoid confusion. This could be valuable for various uses of body-mounted markers, including calibration, visual servoing, visual motion capture, and multi-robot tasks.</li> <li>5 markers per robot = 2 on the mobile base + 2 on the wrist + 1 on the shoulder</li> <li>20 distinct sets = 100 available ID numbers / 5 ID numbers per robot</li> </ul> </li> <li>200 - 249 : reserved for official accessories<ul> <li>245 for the prototype docking station</li> <li>246-249 for large floor markers</li> </ul> </li> </ul> <p>When coming up with this guide, we expected the following:</p> <ul> <li>Body-mounted accessories with the same ID numbers mounted to different robots could be disambiguated using the expected range of 3D locations of the ArUco markers on the calibrated body.</li> <li>Accessories in the environment with the same ID numbers could be disambiguated using a map or nearby observable features of the environment.</li> </ul>"},{"location":"stretch-tutorials/ros1_melodic/example_1/","title":"Example 1","text":""},{"location":"stretch-tutorials/ros1_melodic/example_1/#example-1","title":"Example 1","text":"<p>The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending <code>Twist</code> messages to a Stretch robot.</p> <p>Begin by running the following command in a new terminal.</p> <pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Switch the mode to navigation mode using a rosservice call. Then drive the robot forward with the move.py node.</p> <p><pre><code># Terminal 2\nrosservice call /switch_to_navigation_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython move.py\n</code></pre> To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_1/#the-code","title":"The Code","text":"<p>Below is the code which will send Twist messages to drive the robot forward.</p> <pre><code>#!/usr/bin/env python\n\nimport rospy\nfrom geometry_msgs.msg import Twist\n\nclass Move:\n    \"\"\"\n    A class that sends Twist messages to move the Stretch robot forward.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber.\n        :param self: The self reference.\n        \"\"\"\n        self.pub = rospy.Publisher('/stretch/cmd_vel', Twist, queue_size=1) #/stretch_diff_drive_controller/cmd_vel for gazebo\n\n    def move_forward(self):\n        \"\"\"\n        Function that publishes Twist messages\n        :param self: The self reference.\n\n        :publishes command: Twist message.\n        \"\"\"\n        command = Twist()\n        command.linear.x = 0.1\n        command.linear.y = 0.0\n        command.linear.z = 0.0\n        command.angular.x = 0.0\n        command.angular.y = 0.0\n        command.angular.z = 0.0\n        self.pub.publish(command)\n\nif __name__ == '__main__':\n    rospy.init_node('move')\n    base_motion = Move()\n    rate = rospy.Rate(10)\n    while not rospy.is_shutdown():\n        base_motion.move_forward()\n        rate.sleep()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_1/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nfrom geometry_msgs.msg import Twist\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node. The <code>geometry_msgs.msg</code> import is so that we can send velocity commands to the robot.</p> <p><pre><code>class Move:\n    def __init__(self):\n        self.pub = rospy.Publisher('/stretch/cmd_vel', Twist, queue_size=1)#/stretch_diff_drive_controller/cmd_vel for gazebo\n</code></pre> This section of code defines the talker's interface to the rest of ROS. <code>pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1)</code> declares that your node is publishing to the /stretch/cmd_vel topic using the message type <code>Twist</code>. The <code>queue_size</code> argument limits the amount of queued messages if any subscriber is not receiving them fast enough.</p> <p><pre><code>command = Twist()\n</code></pre> Make a <code>Twist</code> message. We're going to set all of the elements, since we can't depend on them defaulting to safe values.</p> <p><pre><code>command.linear.x = 0.1\ncommand.linear.y = 0.0\ncommand.linear.z = 0.0\n</code></pre> A <code>Twist</code> data structure has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y direction or the z direction.</p> <p><pre><code>command.angular.x = 0.0\ncommand.angular.y = 0.0\ncommand.angular.z = 0.0\n</code></pre> A <code>Twist</code> message also has three rotational velocities (in radians per second). The Stretch will only respond to rotations around the z (vertical) axis.</p> <p><pre><code>self.pub.publish(command)\n</code></pre> Publish the <code>Twist</code> commands in the previously defined topic name /stretch/cmd_vel.</p> <p><pre><code>rospy.init_node('move')\nbase_motion = Move()\nrate = rospy.Rate(10)\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>The <code>rospy.Rate()</code> function creates a Rate object. With the help of its method <code>sleep()</code>, it offers a convenient way for looping at the desired rate. With its argument of 10, we should expect to go through the loop 10 times per second (as long as our processing time does not exceed 1/10th of a second!)</p> <p><pre><code>while not rospy.is_shutdown():\n    base_motion.move_forward()\n    rate.sleep()\n</code></pre> This loop is a fairly standard rospy construct: checking the <code>rospy.is_shutdown()</code> flag and then doing work. You have to check <code>is_shutdown()</code> to check if your program should exit (e.g. if there is a Ctrl-C or otherwise). The loop calls <code>rate.sleep()</code>, which sleeps just long enough to maintain the desired rate through the loop.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_1/#move-stretch-in-simulation","title":"Move Stretch in Simulation","text":"<p>Using your preferred text editor, modify the topic name of the published <code>Twist</code> messages. Please review the edit in the move.py script below.</p> <pre><code>self.pub = rospy.Publisher('/stretch_diff_drive_controller/cmd_vel', Twist, queue_size=1)\n</code></pre> <p>After saving the edited node, bringup Stretch in the empty world simulation. To drive the robot with the node, type the following in a new terminal</p> <p><pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython move.py\n</code></pre> To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_10/","title":"Example 10","text":"<p>This tutorial provides you an idea of what tf2 can do in the Python track. We will elaborate how to create a tf2 static broadcaster and listener.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_10/#tf2-static-broadcaster","title":"tf2 Static Broadcaster","text":"<p>For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the link_mast, link_lift, and link_wrist_yaw frames.</p> <p>Begin by starting up the stretch driver launch file.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> Within this tutorial package, there is an RViz config file with the topics for transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <p><pre><code># Terminal 2\nrosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/tf2_broadcaster_example.rviz\n</code></pre> Then run the tf2_broadcaster.py node to visualize three static frames.</p> <pre><code># Terminal 3\ncd catkin_ws/src/stretch_tutorials/src/\npython tf2_broadcaster.py\n</code></pre> <p>The gif below visualizes what happens when running the previous node.</p> <p> </p> <p>OPTIONAL: If you would like to see how the static frames update while the robot is in motion, run the stow_command_node.py and observe the tf frames in RViz.</p> <pre><code># Terminal 4\ncd catkin_ws/src/stretch_tutorials/src/\npython stow_command.py\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/example_10/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nimport tf.transformations\nfrom geometry_msgs.msg import TransformStamped\nfrom tf2_ros import StaticTransformBroadcaster\n\nclass FixedFrameBroadcaster():\n    \"\"\"\n    This node publishes three child static frames in reference to their parent frames as below:\n    parent -&gt; link_mast            child -&gt; fk_link_mast\n    parent -&gt; link_lift            child -&gt; fk_link_lift\n    parent -&gt; link_wrist_yaw       child -&gt; fk_link_wrist_yaw\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that creates a broadcast node and publishes three new transform\n        frames.\n        :param self: The self reference.\n        \"\"\"\n        self.br = StaticTransformBroadcaster()\n\n        self.mast = TransformStamped()\n        self.mast.header.stamp = rospy.Time.now()\n        self.mast.header.frame_id = 'link_mast'\n        self.mast.child_frame_id = 'fk_link_mast'\n        self.mast.transform.translation.x = 0.0\n        self.mast.transform.translation.y = 2.0\n        self.mast.transform.translation.z = 0.0\n        q = tf.transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.mast.transform.rotation.x = q[0]\n        self.mast.transform.rotation.y = q[1]\n        self.mast.transform.rotation.z = q[2]\n        self.mast.transform.rotation.w = q[3]\n\n        self.lift = TransformStamped()\n        self.lift.header.stamp = rospy.Time.now()\n        self.lift.header.frame_id = 'link_lift'\n        self.lift.child_frame_id = 'fk_link_lift'\n        self.lift.transform.translation.x = 0.0\n        self.lift.transform.translation.y = 1.0\n        self.lift.transform.translation.z = 0.0\n        q = tf.transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.lift.transform.rotation.x = q[0]\n        self.lift.transform.rotation.y = q[1]\n        self.lift.transform.rotation.z = q[2]\n        self.lift.transform.rotation.w = q[3]\n\n        self.wrist = TransformStamped()\n        self.wrist.header.stamp = rospy.Time.now()\n        self.wrist.header.frame_id = 'link_wrist_yaw'\n        self.wrist.child_frame_id = 'fk_link_wrist_yaw'\n        self.wrist.transform.translation.x = 0.0\n        self.wrist.transform.translation.y = 1.0\n        self.wrist.transform.translation.z = 0.0\n        q = tf.transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.wrist.transform.rotation.x = q[0]\n        self.wrist.transform.rotation.y = q[1]\n        self.wrist.transform.rotation.z = q[2]\n        self.wrist.transform.rotation.w = q[3]\n\n        self.br.sendTransform([self.mast, self.lift, self.wrist])\n\n        rospy.loginfo('Publishing TF frames. Use RViz to visualize')\n\nif __name__ == '__main__':\n    rospy.init_node('tf2_broadcaster')\n    FixedFrameBroadcaster()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_10/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nimport tf.transformations\nfrom geometry_msgs.msg import TransformStamped\nfrom tf2_ros import StaticTransformBroadcaster\n</code></pre> You need to import rospy if you are writing a ROS Node. Import <code>tf.transformations</code> to get quaternion values from Euler angles. Import the <code>TransformStamped</code> from the <code>geometry_msgs.msg</code> package because we will be publishing static frames and it requires this message type. The <code>tf2_ros</code> package provides an implementation of a <code>tf2_ros.StaticTransformBroadcaster</code> to help make the task of publishing transforms easier.</p> <p><pre><code>def __init__(self):\n    \"\"\"\n    A function that creates a broadcast node and publishes three new transform\n    frames.\n    :param self: The self reference.\n    \"\"\"\n    self.br = StaticTransformBroadcaster()\n</code></pre> Here we create a <code>TransformStamped</code> object which will be the message we will send over once populated.</p> <p><pre><code>self.mast = TransformStamped()\nself.mast.header.stamp = rospy.Time.now()\nself.mast.header.frame_id = 'link_mast'\nself.mast.child_frame_id = 'fk_link_mast'\n</code></pre> We need to give the transform being published a timestamp, we'll just stamp it with the current time, <code>rospy.Time.now()</code>. Then, we need to set the name of the parent frame of the link we're creating, in this case link_mast. Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is fk_link_mast.</p> <p><pre><code>self.mast.transform.translation.x = 0.0\nself.mast.transform.translation.y = 2.0\nself.mast.transform.translation.z = 0.0\n</code></pre> Set the translation values for the child frame.</p> <p><pre><code>q = tf.transformations.quaternion_from_euler(1.5707, 0, -1.5707)\nself.wrist.transform.rotation.x = q[0]\nself.wrist.transform.rotation.y = q[1]\nself.wrist.transform.rotation.z = q[2]\nself.wrist.transform.rotation.w = q[3]\n</code></pre> The <code>quaternion_from_euler()</code> function takes in a Euler angle argument and returns a quaternion values. Then set the rotation values to the transformed quaternions.</p> <p>This process will be completed for the link_lift and link_wrist_yaw as well.</p> <p><pre><code>self.br.sendTransform([self.mast, self.lift, self.wrist])\n</code></pre> Send the three transforms using the <code>sendTransform()</code> function.</p> <p><pre><code>rospy.init_node('tf2_broadcaster')\nFixedFrameBroadcaster()\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the <code>FixedFrameBroadcaster()</code> class.</p> <p><pre><code>rospy.spin()\n</code></pre> Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_10/#tf2-static-listener","title":"tf2 Static Listener","text":"<p>In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section we will create a tf2 listener that will find the transform between fk_link_lift and link_grasp_center.</p> <p>Begin by starting up the stretch driver launch file.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> Then run the tf2_broadcaster.py node to create the three static frames.</p> <p><pre><code># Terminal 2\ncd catkin_ws/src/stretch_tutorials/src/\npython tf2_broadcaster.py\n</code></pre> Finally, run the tf2_listener.py node to print the transform between two links.</p> <p><pre><code># Terminal 3\ncd catkin_ws/src/stretch_tutorials/src/\npython tf2_listener.py\n</code></pre> Within the terminal the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames.</p> <pre><code>[INFO] [1659551318.098168]: The pose of target frame link_grasp_center with reference from fk_link_lift is:\ntranslation:\n  x: 1.08415191335\n  y: -0.176147838153\n  z: 0.576720021135\nrotation:\n  x: -0.479004489528\n  y: -0.508053545368\n  z: -0.502884087254\n  w: 0.509454501243\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/example_10/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nfrom geometry_msgs.msg import TransformStamped\nimport tf2_ros\n\nclass FrameListener():\n    \"\"\"\n    This Class prints the transformation between the fk_link_mast frame and the\n    target frame, link_grasp_center.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes the variables and looks up a transformation\n        between a target and source frame.\n        :param self: The self reference.\n        \"\"\"\n        tf_buffer = tf2_ros.Buffer()\n        listener = tf2_ros.TransformListener(tf_buffer)\n\n        from_frame_rel = 'link_grasp_center'\n        to_frame_rel = 'fk_link_lift'\n\n        rospy.sleep(1.0)\n        rate = rospy.Rate(1)\n\n        while not rospy.is_shutdown():\n            try:\n                trans = tf_buffer.lookup_transform(to_frame_rel,\n                                                   from_frame_rel,\n                                                   rospy.Time())\n                rospy.loginfo('The pose of target frame %s with reference from %s is: \\n %s', from_frame_rel, to_frame_rel, trans.transform)\n            except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n                rospy.logwarn(' Could not transform %s from %s ', to_frame_rel, from_frame_rel)\n\n            rate.sleep()\n\nif __name__ == '__main__':\n    rospy.init_node('tf2_listener')\n    FrameListener()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_10/#the-code-explained_1","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rospy\nfrom geometry_msgs.msg import TransformStamped\nimport tf2_ros\n</code></pre> <p>You need to import rospy if you are writing a ROS Node. Import the <code>TransformStamped</code> from the <code>geometry_msgs.msg</code> package because we will be publishing static frames and it requires this message type. The <code>tf2_ros</code> package provides an implementation of a <code>tf2_ros.TransformListener</code> to help make the task of receiving transforms easier.</p> <p><pre><code>tf_buffer = tf2_ros.Buffer()\nlistener = tf2_ros.TransformListener(tf_buffer)\n</code></pre> Here, we create a <code>TransformListener</code> object. Once the listener is created, it starts receiving tf2 transformations over the wire, and buffers them for up to 10 seconds.</p> <p><pre><code>from_frame_rel = 'link_grasp_center'\nto_frame_rel = 'fk_link_lift'\n</code></pre> Store frame names in variables that will be used to compute transformations.</p> <p><pre><code>rospy.sleep(1.0)\nrate = rospy.Rate(1)\n</code></pre> The first line gives the listener some time to accumulate transforms. The second line is the rate the node is going to publish information (1 Hz).</p> <p><pre><code>try:\n    trans = tf_buffer.lookup_transform(to_frame_rel,\n                                       from_frame_rel,\n                                       rospy.Time())\n    rospy.loginfo('The pose of target frame %s with reference from %s is: \\n %s', from_frame_rel, to_frame_rel, trans.transform)\n\nexcept (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n    rospy.logwarn(' Could not transform %s from %s ', to_frame_rel, from_frame_rel)\n</code></pre> Try to look up the transform we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between from_frame_rel and to_frame_rel frames with the <code>lookup_transform()</code> function.</p> <p><pre><code>rospy.init_node('tf2_listener')\nFrameListener()\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the <code>FrameListener()</code> class.</p> <p><pre><code>rospy.spin()\n</code></pre> Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_11/","title":"Example 11","text":""},{"location":"stretch-tutorials/ros1_melodic/example_11/#example-11","title":"Example 11","text":"<p>This tutorial highlights how to create a PointCloud message from the data of a PointCloud2 message type, then transform the PointCloud's reference link to a different frame. The data published by the RealSense is referencing its camera_color_optical_frame link, and we will be changing its reference to the base_link.</p> <p>Begin by starting up the stretch driver launch file.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <p><pre><code># Terminal 2\nroslaunch stretch_core d435i_low_resolution.launch\n</code></pre> Then run the pointCloud_transformer.py node.</p> <p><pre><code># Terminal 3\ncd catkin_ws/src/stretch_tutorials/src/\npython pointcloud_transformer.py\n</code></pre> Within this tutorial package, there is an RViz config file with the <code>PointCloud</code> in the Display tree. You can visualize this topic and the robot model by running the command below in a new terminal.</p> <p><pre><code># Terminal 4\nrosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/PointCloud_transformer_example.rviz\n</code></pre> The gif below visualizes what happens when running the previous node.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/example_11/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\nimport rospy\nimport tf\nimport sensor_msgs.point_cloud2 as pc2\nfrom sensor_msgs.msg import PointCloud2, PointCloud\nfrom geometry_msgs.msg import Point32\nfrom std_msgs.msg import Header\n\nclass PointCloudTransformer:\n    \"\"\"\n    A class that takes in a PointCloud2 message and stores its points into a\n    PointCloud message. Then that PointCloud is transformed to reference the\n    'base_link' frame.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber, publisher, and other variables.\n        :param self: The self reference.\n        \"\"\"\n        self.pointcloud2_sub = rospy.Subscriber(\"/camera/depth/color/points\", PointCloud2, self.callback_pcl2, queue_size=1)\n        self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1)\n        self.pcl2_cloud = None\n        self.listener = tf.TransformListener(True, rospy.Duration(10.0))\n        rospy.loginfo('Publishing transformed PointCloud. Use RViz to visualize')\n\n    def callback_pcl2(self,msg):\n        \"\"\"\n        Callback function that stores the PointCloud2 message.\n        :param self: The self reference.\n        :param msg: The PointCloud2 message type.\n        \"\"\"\n        self.pcl2_cloud = msg\n\n    def pcl_transformer(self):\n        \"\"\"\n        A function that extracts the points from the stored PointCloud2 message\n        and appends those points to a PointCloud message. Then the function transforms\n        the PointCloud from its the header frame id, 'camera_color_optical_frame'\n        to the 'base_link' frame.\n        :param self: The self reference.\n        \"\"\"\n        temp_cloud = PointCloud()\n        temp_cloud.header = self.pcl2_cloud.header\n        for data in pc2.read_points(self.pcl2_cloud, skip_nans=True):\n            temp_cloud.points.append(Point32(data[0],data[1],data[2]))\n\n        transformed_cloud = self.transform_pointcloud(temp_cloud)\n        self.pointcloud_pub.publish(transformed_cloud)\n\n    def transform_pointcloud(self,msg):\n        \"\"\"\n        Function that stores the PointCloud2 message.\n        :param self: The self reference.\n        :param msg: The PointCloud message.\n\n        :returns new_cloud: The transformed PointCloud message.\n        \"\"\"\n        while not rospy.is_shutdown():\n            try:\n                new_cloud = self.listener.transformPointCloud(\"base_link\" ,msg)\n                return new_cloud\n                if new_cloud:\n                    break\n            except (tf.LookupException, tf.ConnectivityException,tf.ExtrapolationException):\n                pass\n\nif __name__==\"__main__\":\n    rospy.init_node('pointcloud_transformer',anonymous=True)\n    PCT = PointCloudTransformer()\n    rate = rospy.Rate(1)\n    rospy.sleep(1)\n\n    while not rospy.is_shutdown():\n        PCT.pcl_transformer()\n        rate.sleep()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_11/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nimport tf\nimport sensor_msgs.point_cloud2 as pc2\nfrom sensor_msgs.msg import PointCloud2, PointCloud\nfrom geometry_msgs.msg import Point32\nfrom std_msgs.msg import Header\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node. Import <code>tf</code> to utilize the <code>transformPointCloud</code> function. Import various the message types from <code>sensor_msgs</code>.</p> <p><pre><code>self.pointcloud2_sub = rospy.Subscriber(\"/camera/depth/color/points\", PointCloud2, self.callback_pcl2, queue_size=1)\n</code></pre> Set up a subscriber.  We're going to subscribe to the topic /camera/depth/color/points, looking for <code>PointCloud2</code> message.  When a message comes in, ROS is going to pass it to the function <code>callback_pcl2()</code> automatically.</p> <p><pre><code>self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1)\n</code></pre> This section of code defines the talker's interface to the rest of ROS. <code>self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1)</code> declares that your node is publishing to the /camera_cloud topic using the message type <code>PointCloud</code>.</p> <p><pre><code>self.pcl2_cloud = None\nself.listener = tf.TransformListener(True, rospy.Duration(10.0))\n</code></pre> The first line of code initializes self.pcl2_cloud to store the <code>PointCloud2</code> message. The second line creates a <code>tf.TransformListener</code> object. Once the listener is created, it starts receiving tf transformations over the wire, and buffers them for up to 10 seconds.</p> <p><pre><code>def callback_pcl2(self,msg):\n    \"\"\"\n    Callback function that stores the PointCloud2 message.\n    :param self: The self reference.\n    :param msg: The PointCloud2 message type.\n    \"\"\"\n    self.pcl2_cloud = msg\n</code></pre> The callback function that stores the the <code>PointCloud2</code> message.</p> <p><pre><code>temp_cloud = PointCloud()\ntemp_cloud.header = self.pcl2_cloud.header\n</code></pre> Create a <code>PointCloud</code> for temporary use. Set the temporary PointCloud's header to the stored <code>PointCloud2</code> header.</p> <p><pre><code>for data in pc2.read_points(self.pcl2_cloud, skip_nans=True):\n  temp_cloud.points.append(Point32(data[0],data[1],data[2]))\n</code></pre> Use a for loop to extract <code>PointCloud2</code> data into a list of x, y, z points and append those values to the <code>PointCloud</code> message, temp_cloud.</p> <p><pre><code>transformed_cloud = self.transform_pointcloud(temp_cloud)\n</code></pre> Utilize the <code>transform_pointcloud</code> function to transform the points in the <code>PointCloud</code> message to reference the base_link</p> <p><pre><code>while not rospy.is_shutdown():\n        try:\n            new_cloud = self.listener.transformPointCloud(\"base_link\" ,msg)\n            return new_cloud\n            if new_cloud:\n                break\n        except (tf.LookupException, tf.ConnectivityException,tf.ExtrapolationException):\n            pass\n</code></pre> Try to look up and transform the <code>PointCloud</code> input. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Transform the point cloud data from camera_color_optical_frame to base_link with the <code>transformPointCloud()</code> function.</p> <p><pre><code>self.pointcloud_pub.publish(transformed_cloud)\n</code></pre> Publish the new transformed <code>PointCloud</code>.</p> <p><pre><code>rospy.init_node('pointcloud_transformer',anonymous=True)\nPCT = PointCloudTransformer()\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Declare a <code>PointCloudTransformer</code> object.</p> <p><pre><code>rate = rospy.Rate(1)\nrospy.sleep(1)\n</code></pre> The first line gives the listener some time to accumulate transforms. The second line is the rate the node is going to publish information (1 Hz).</p> <p><pre><code>  while not rospy.is_shutdown():\n      PCT.pcl_transformer()\n      rate.sleep()\n</code></pre> Run a while loop until the node is shutdown. Within the while loop run the <code>pcl_transformer()</code> method.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_12/","title":"Example 12","text":"<p>For this example, we will send follow joint trajectory commands for the head camera to search and locate an ArUco tag. In this instance, a Stretch robot will try to locate the docking station's ArUco tag.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_12/#modifying-stretch-marker-dictionary-yaml-file","title":"Modifying Stretch Marker Dictionary YAML File.","text":"<p>When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml, that holds the information about the markers. A further breakdown of the yaml file can be found in our Aruco Marker Detection tutorial.</p> <p>Below is what the needs to be included in the stretch_marker_dict.yaml file so the detect_aruco_markers node can find the docking station's ArUco tag.</p> <pre><code>'245':\n  'length_mm': 88.0\n  'use_rgb_only': False\n  'name': 'docking_station'\n  'link': None\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_12/#getting-started","title":"Getting Started","text":"<p>Begin by running the stretch driver launch file.</p> <pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code># Terminal 2\nroslaunch stretch_core d435i_high_resolution.launch\n</code></pre> <p>Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node.</p> <pre><code># Terminal 3\nroslaunch stretch_core stretch_aruco.launch\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code># Terminal 4\nrosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz\n</code></pre> <p>Then run the aruco_tag_locator.py node.</p> <pre><code># Terminal 5\ncd catkin_ws/src/stretch_tutorials/src/\npython aruco_tag_locator.py\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/example_12/#the-code","title":"The Code","text":"<pre><code>#! /usr/bin/env python\n\nimport rospy\nimport time\nimport tf2_ros\nimport numpy as np\nfrom math import pi\n\nimport hello_helpers.hello_misc as hm\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom geometry_msgs.msg import TransformStamped\n\nclass LocateArUcoTag(hm.HelloNode):\n    \"\"\"\n    A class that actuates the RealSense camera to find the docking station's\n    ArUco tag and returns a Transform between the `base_link` and the requested tag.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes the subscriber and other needed variables.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n\n        self.joint_states_sub = rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)\n        self.transform_pub = rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10)\n\n        self.joint_state = None\n\n        self.min_pan_position = -4.10\n        self.max_pan_position =  1.50\n        self.pan_num_steps = 10\n        self.pan_step_size = abs(self.min_pan_position - self.max_pan_position)/self.pan_num_steps\n\n        self.min_tilt_position = -0.75\n        self.tilt_num_steps = 3\n        self.tilt_step_size = pi/16\n\n        self.rot_vel = 0.5 # radians per sec\n\n    def joint_states_callback(self, msg):\n        \"\"\"\n        A callback function that stores Stretch's joint states.\n        :param self: The self reference.\n        :param msg: The JointState message type.\n        \"\"\"\n        self.joint_state = msg\n\n    def send_command(self, command):\n        '''\n        Handles single joint control commands by constructing a FollowJointTrajectoryGoal\n        message and sending it to the trajectory_client created in hello_misc.\n        :param self: The self reference.\n        :param command: A dictionary message type.\n        '''\n        if (self.joint_state is not None) and (command is not None):\n            joint_name = command['joint']\n            trajectory_goal = FollowJointTrajectoryGoal()\n            trajectory_goal.trajectory.joint_names = [joint_name]\n            point = JointTrajectoryPoint()\n\n            if 'delta' in command:\n                joint_index = self.joint_state.name.index(joint_name)\n                joint_value = self.joint_state.position[joint_index]\n                delta = command['delta']\n                new_value = joint_value + delta\n                point.positions = [new_value]\n\n            elif 'position' in command:\n                point.positions = [command['position']]\n\n            point.velocities = [self.rot_vel]\n            trajectory_goal.trajectory.points = [point]\n            trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n            trajectory_goal.trajectory.header.frame_id = 'base_link'\n            self.trajectory_client.send_goal(trajectory_goal)\n            self.trajectory_client.wait_for_result()\n\n    def find_tag(self, tag_name='docking_station'):\n        \"\"\"\n        A function that actuates the camera to search for a defined ArUco tag\n        marker. Then the function returns the pose\n        :param self: The self reference.\n        :param tag_name: A string value of the ArUco marker name.\n\n        :returns transform: The docking station's TransformStamped message.\n        \"\"\"\n        pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n        self.send_command(pan_command)\n        tilt_command = {'joint': 'joint_head_tilt', 'position': self.min_tilt_position}\n        self.send_command(tilt_command)\n\n        for i in range(self.tilt_num_steps):\n            for j in range(self.pan_num_steps):\n                pan_command = {'joint': 'joint_head_pan', 'delta': self.pan_step_size}\n                self.send_command(pan_command)\n                rospy.sleep(0.2)\n\n                try:\n                    transform = self.tf_buffer.lookup_transform('base_link',\n                                                                tag_name,\n                                                                rospy.Time())\n                    rospy.loginfo(\"Found Requested Tag: \\n%s\", transform)\n                    self.transform_pub.publish(transform)\n                    return transform\n                except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n                    continue\n\n            pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n            self.send_command(pan_command)\n            tilt_command = {'joint': 'joint_head_tilt', 'delta': self.tilt_step_size}\n            self.send_command(tilt_command)\n            rospy.sleep(.25)\n\n        rospy.loginfo(\"The requested tag '%s' was not found\", tag_name)\n\n    def main(self):\n        \"\"\"\n        Function that initiates the issue_command function.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.main(self, 'aruco_tag_locator', 'aruco_tag_locator', wait_for_first_pointcloud=False)\n        self.static_broadcaster = tf2_ros.StaticTransformBroadcaster()\n        self.tf_buffer = tf2_ros.Buffer()\n        self.listener = tf2_ros.TransformListener(self.tf_buffer)\n        rospy.sleep(1.0)\n        rospy.loginfo('Searching for docking ArUco tag.')\n        pose = self.find_tag(\"docking_station\")\n\nif __name__ == '__main__':\n    try:\n        node = LocateArUcoTag()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_12/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nimport time\nimport tf2_ros\nimport numpy as np\nfrom math import pi\n\nimport hello_helpers.hello_misc as hm\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom geometry_msgs.msg import TransformStamped\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node. Import other python modules needed for this node. Import the <code>FollowJointTrajectoryGoal</code> from the control_msgs.msg package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module the provides various Python scripts used across stretch_ros. In this instance we are importing the <code>hello_misc</code> script.</p> <p><pre><code>def __init__(self):\n    \"\"\"\n    A function that initializes the subscriber and other needed variables.\n    :param self: The self reference.\n    \"\"\"\n    hm.HelloNode.__init__(self)\n\n    self.joint_states_sub = rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)\n    self.transform_pub = rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10)\n\n    self.joint_state = None\n</code></pre> The <code>LocateArUcoTag</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is instantiated.</p> <p>Set up a subscriber with <code>rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)</code>.  We're going to subscribe to the topic \"stretch/joint_states\", looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function <code>joint_states_callback()</code> automatically.</p> <p><code>rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10)</code> declares that your node is publishing to the ArUco_transform topic using the message type <code>TransformStamped</code>. The <code>queue_size</code> argument limits the amount of queued messages if any subscriber is not receiving them fast enough.</p> <p><pre><code>self.min_pan_position = -4.10\nself.max_pan_position =  1.50\nself.pan_num_steps = 10\nself.pan_step_size = abs(self.min_pan_position - self.max_pan_position)/self.pan_num_steps\n</code></pre> Provide the minimum and maximum joint positions for the head pan. These values are needed for sweeping the head to search for the ArUco tag. We also define the number of steps for the sweep, then create the step size for the head pan joint.</p> <p><pre><code>self.min_tilt_position = -0.75\nself.tilt_num_steps = 3\nself.tilt_step_size = pi/16\n</code></pre> Set the minimum position of the tilt joint, the number of steps, and the size of each step.</p> <p><pre><code>self.rot_vel = 0.5 # radians per sec\n</code></pre> Define the head actuation rotational velocity.</p> <p><pre><code>def joint_states_callback(self, msg):\n    \"\"\"\n    A callback function that stores Stretch's joint states.\n    :param self: The self reference.\n    :param msg: The JointState message type.\n    \"\"\"\n    self.joint_state = msg\n</code></pre> The <code>joint_states_callback()</code> function stores Stretch's joint states.</p> <p><pre><code>def send_command(self, command):\n    '''\n    Handles single joint control commands by constructing a FollowJointTrajectoryGoal\n    message and sending it to the trajectory_client created in hello_misc.\n    :param self: The self reference.\n    :param command: A dictionary message type.\n    '''\n    if (self.joint_state is not None) and (command is not None):\n        joint_name = command['joint']\n        trajectory_goal = FollowJointTrajectoryGoal()\n        trajectory_goal.trajectory.joint_names = [joint_name]\n        point = JointTrajectoryPoint()\n</code></pre> Assign trajectory_goal as a <code>FollowJointTrajectoryGoal</code> message type. Then extract the string value from the <code>joint</code> key. Also, assign point as a <code>JointTrajectoryPoint</code> message type.</p> <p><pre><code>if 'delta' in command:\n    joint_index = self.joint_state.name.index(joint_name)\n    joint_value = self.joint_state.position[joint_index]\n    delta = command['delta']\n    new_value = joint_value + delta\n    point.positions = [new_value]\n</code></pre> Check to see if <code>delta</code> is a key in the command dictionary. Then get the current position of the joint and add the delta as a a new position value.</p> <p><pre><code>elif 'position' in command:\n    point.positions = [command['position']]\n</code></pre> Check to see if <code>position</code>is a key in the command dictionary. Then extract the position value.</p> <p><pre><code>point.velocities = [self.rot_vel]\ntrajectory_goal.trajectory.points = [point]\ntrajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\ntrajectory_goal.trajectory.header.frame_id = 'base_link'\nself.trajectory_client.send_goal(trajectory_goal)\nself.trajectory_client.wait_for_result()\n</code></pre> Then <code>trajectory_goal.trajectory.points</code> is defined by the positions set in point. Specify the coordinate frame that we want (base_link) and set the time to be now. Make the action call and send the goal. The last line of code waits for the result before it exits the python script.</p> <p><pre><code>def find_tag(self, tag_name='docking_station'):\n    \"\"\"\n    A function that actuates the camera to search for a defined ArUco tag\n    marker. Then the function returns the pose\n    :param self: The self reference.\n    :param tag_name: A string value of the ArUco marker name.\n\n    :returns transform: The docking station's TransformStamped message.\n    \"\"\"\n    pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n    self.send_command(pan_command)\n    tilt_command = {'joint': 'joint_head_tilt', 'position': self.min_tilt_position}\n    self.send_command(tilt_command)\n</code></pre> Create a dictionaries to get the head in its initial position for its search and send the commands the the <code>send_command()</code> function.</p> <p><pre><code>for i in range(self.tilt_num_steps):\n    for j in range(self.pan_num_steps):\n        pan_command = {'joint': 'joint_head_pan', 'delta': self.pan_step_size}\n        self.send_command(pan_command)\n        rospy.sleep(0.5)\n</code></pre> Utilize nested for loop to sweep the pan and tilt in increments. Then update the joint_head_pan position by the pan_step_size. Use <code>rospy.sleep()</code> function to give time for system to do a Transform lookup before next step.</p> <p><pre><code>try:\n    transform = self.tf_buffer.lookup_transform('base_link',\n                                                tag_name,\n                                                rospy.Time())\n    rospy.loginfo(\"Found Requested Tag: \\n%s\", transform)\n    self.transform_pub.publish(transform)\n    return transform\nexcept (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException):\n    continue\n</code></pre> Use a try-except block to look up the transform between the base_link and requested ArUco tag. Then publish and return the <code>TransformStamped</code> message.</p> <p><pre><code>pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\nself.send_command(pan_command)\ntilt_command = {'joint': 'joint_head_tilt', 'delta': self.tilt_step_size}\nself.send_command(tilt_command)\nrospy.sleep(.25)\n</code></pre> Begin sweep with new tilt angle.</p> <p><pre><code>def main(self):\n    \"\"\"\n    Function that initiates the issue_command function.\n    :param self: The self reference.\n    \"\"\"\n    hm.HelloNode.main(self, 'aruco_tag_locator', 'aruco_tag_locator', wait_for_first_pointcloud=False)\n</code></pre> Create a funcion, <code>main()</code>, to do all of the setup for the <code>hm.HelloNode</code> class and initialize the <code>aruco_tag_locator</code> node.</p> <p><pre><code>self.static_broadcaster = tf2_ros.StaticTransformBroadcaster()\nself.tf_buffer = tf2_ros.Buffer()\nself.listener = tf2_ros.TransformListener(self.tf_buffer)\nrospy.sleep(1.0)\n</code></pre> Create a StaticTranformBoradcaster Node. Also, start a tf buffer that will store the tf information for a few seconds.Then set up a tf listener, which will subscribe to all of the relevant tf topics, and keep track of the information. Include <code>rospy.sleep(1.0)</code> to give the listener some time to accumulate transforms.</p> <p><pre><code>rospy.loginfo('Searching for docking ArUco tag.')\npose = self.find_tag(\"docking_station\")\n</code></pre> Notify Stretch is searching for the ArUco tag with a <code>rospy.loginfo()</code> function. Then search for the ArUco marker for the docking station.</p> <p><pre><code>if __name__ == '__main__':\n    try:\n        node = LocateArUcoTag()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre> Declare <code>LocateArUcoTag</code> object. Then run the <code>main()</code> method.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_13/","title":"Example 13","text":"<p>In this example, we will be utilizing the move_base ROS node, a component of the ROS navigation stack to send base goals to the Stretch robot.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_13/#build-a-map","title":"Build a map","text":"<p>First, begin by building a map of the space the robot will be navigating in. If you need a refresher on how to do this, then check out the Navigation Stack tutorial.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_13/#getting-started","title":"Getting Started","text":"<p>Next, with your created map, we can navigate the robot around the mapped space. Run:</p> <p><pre><code>roslaunch stretch_navigation navigation.launch map_yaml:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> Where <code>${HELLO_FLEET_PATH}</code> is the path of the <code>&lt;map_name&gt;.yaml</code> file.</p> <p>IMPORTANT NOTE: It's likely that the robot's location in the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. Below is a gif for reference.</p> <p> </p> <p>Now we are going to use a node to send a a move base goal half a meter in front of the map's origin. run the following command to execute the navigation.py node.</p> <pre><code># Terminal 2\ncd catkin_ws/src/stretch_tutorials/src/\npython navigation.py\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_13/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nimport actionlib\nimport sys\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\nfrom geometry_msgs.msg import Quaternion\nfrom tf import transformations\n\nclass StretchNavigation:\n    \"\"\"\n    A simple encapsulation of the navigation stack for a Stretch robot.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Create an instance of the simple navigation interface.\n        :param self: The self reference.\n        \"\"\"\n        self.client = actionlib.SimpleActionClient('move_base', MoveBaseAction)\n        self.client.wait_for_server()\n        rospy.loginfo('{0}: Made contact with move_base server'.format(self.__class__.__name__))\n\n        self.goal = MoveBaseGoal()\n        self.goal.target_pose.header.frame_id = 'map'\n        self.goal.target_pose.header.stamp = rospy.Time()\n\n        self.goal.target_pose.pose.position.x = 0.0\n        self.goal.target_pose.pose.position.y = 0.0\n        self.goal.target_pose.pose.position.z = 0.0\n        self.goal.target_pose.pose.orientation.x = 0.0\n        self.goal.target_pose.pose.orientation.y = 0.0\n        self.goal.target_pose.pose.orientation.z = 0.0\n        self.goal.target_pose.pose.orientation.w = 1.0\n\n    def get_quaternion(self,theta):\n        \"\"\"\n        A function to build Quaternians from Euler angles. Since the Stretch only\n        rotates around z, we can zero out the other angles.\n        :param theta: The angle (radians) the robot makes with the x-axis.\n        \"\"\"\n        return Quaternion(*transformations.quaternion_from_euler(0.0, 0.0, theta))\n\n    def go_to(self, x, y, theta):\n        \"\"\"\n        Drive the robot to a particular pose on the map. The Stretch only needs\n        (x, y) coordinates and a heading.\n        :param x: x coordinate in the map frame.\n        :param y: y coordinate in the map frame.\n        :param theta: heading (angle with the x-axis in the map frame)\n        \"\"\"\n        rospy.loginfo('{0}: Heading for ({1}, {2}) at {3} radians'.format(self.__class__.__name__,\n        x, y, theta))\n\n        self.goal.target_pose.pose.position.x = x\n        self.goal.target_pose.pose.position.y = y\n        self.goal.target_pose.pose.orientation = self.get_quaternion(theta)\n\n        self.client.send_goal(self.goal, done_cb=self.done_callback)\n        self.client.wait_for_result()\n\n    def done_callback(self, status, result):\n        \"\"\"\n        The done_callback function will be called when the joint action is complete.\n        :param self: The self reference.\n        :param status: status attribute from MoveBaseActionResult message.\n        :param result: result attribute from MoveBaseActionResult message.\n        \"\"\"\n        if status == actionlib.GoalStatus.SUCCEEDED:\n            rospy.loginfo('{0}: SUCCEEDED in reaching the goal.'.format(self.__class__.__name__))\n        else:\n            rospy.loginfo('{0}: FAILED in reaching the goal.'.format(self.__class__.__name__))\n\nif __name__ == '__main__':\n    rospy.init_node('navigation', argv=sys.argv)\n    nav = StretchNavigation()\n    nav.go_to(0.5, 0.0, 0.0)\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_13/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nimport actionlib\nimport sys\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\nfrom geometry_msgs.msg import Quaternion\nfrom tf import transformations\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node.</p> <p><pre><code>self.client = actionlib.SimpleActionClient('move_base', MoveBaseAction)\nself.client.wait_for_server()\nrospy.loginfo('{0}: Made contact with move_base server'.format(self.__class__.__name__))\n</code></pre> Set up a client for the navigation action. On the Stretch, this is called <code>move_base</code>, and has type <code>MoveBaseAction</code>.  Once we make the client, we wait for the server to be ready.</p> <p><pre><code>self.goal = MoveBaseGoal()\nself.goal.target_pose.header.frame_id = 'map'\nself.goal.target_pose.header.stamp = rospy.Time()\n</code></pre> Make a goal for the action. Specify the coordinate frame that we want, in this instance the map. Then we set the time to be now.</p> <p><pre><code>self.goal.target_pose.pose.position.x = 0.0\nself.goal.target_pose.pose.position.y = 0.0\nself.goal.target_pose.pose.position.z = 0.0\nself.goal.target_pose.pose.orientation.x = 0.0\nself.goal.target_pose.pose.orientation.y = 0.0\nself.goal.target_pose.pose.orientation.z = 0.0\nself.goal.target_pose.pose.orientation.w = 1.0\n</code></pre> Initialize a position in the coordinate frame.</p> <p><pre><code>def get_quaternion(self,theta):\n    \"\"\"\n    A function to build Quaternians from Euler angles. Since the Stretch only\n    rotates around z, we can zero out the other angles.\n    :param theta: The angle (radians) the robot makes with the x-axis.\n    \"\"\"\n    return Quaternion(*transformations.quaternion_from_euler(0.0, 0.0, theta))\n</code></pre> A function that transforms Euler angles to quaternions and returns those values.</p> <p><pre><code>def go_to(self, x, y, theta, wait=False):\n    \"\"\"\n    Drive the robot to a particular pose on the map. The Stretch only needs\n    (x, y) coordinates and a heading.\n    :param x: x coordinate in the map frame.\n    :param y: y coordinate in the map frame.\n    :param theta: heading (angle with the x-axis in the map frame)\n    \"\"\"\n    rospy.loginfo('{0}: Heading for ({1}, {2}) at {3} radians'.format(self.__class__.__name__,\n    x, y, theta))\n</code></pre> The <code>go_to()</code> function takes in the 3 arguments, the x and y coordinates in the map frame, and the heading.</p> <p><pre><code>self.goal.target_pose.pose.position.x = x\nself.goal.target_pose.pose.position.y = y\nself.goal.target_pose.pose.orientation = self.get_quaternion(theta)\n</code></pre> The <code>MoveBaseGoal()</code> data structure has three goal positions (in meters), along each of the axes. For Stretch, it will only pay attention to the x and y coordinates, since it can't move in the z direction.</p> <p><pre><code>self.client.send_goal(self.goal, done_cb=self.done_callback)\nself.client.wait_for_result()\n</code></pre> Send the goal and include the <code>done_callback()</code> function in one of the arguments in <code>send_goal()</code>.</p> <p><pre><code>def done_callback(self, status, result):\n    \"\"\"\n    The done_callback function will be called when the joint action is complete.\n    :param self: The self reference.\n    :param status: status attribute from MoveBaseActionResult message.\n    :param result: result attribute from MoveBaseActionResult message.\n    \"\"\"\n    if status == actionlib.GoalStatus.SUCCEEDED:\n        rospy.loginfo('{0}: SUCCEEDED in reaching the goal.'.format(self.__class__.__name__))\n    else:\n        rospy.loginfo('{0}: FAILED in reaching the goal.'.format(self.__class__.__name__))\n</code></pre> Conditional statement to print whether the goal status in the <code>MoveBaseActionResult</code> succeeded or failed.</p> <p><pre><code>rospy.init_node('navigation', argv=sys.argv)\nnav = StretchNavigation()\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Declare the <code>StretchNavigation</code> object.</p> <p><pre><code>nav.go_to(0.5, 0.0, 0.0)\n</code></pre> Send a move base goal half a meter in front of the map's origin.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_2/","title":"Example 2","text":""},{"location":"stretch-tutorials/ros1_melodic/example_2/#example-2","title":"Example 2","text":"<p>The aim of this example is to provide instruction on how to filter scan messages.</p> <p>For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specification itself:</p> <p><pre><code># Laser scans angles are measured counter clockwise, with Stretch's LiDAR having\n# both angle_min and angle_max facing forward (very closely along the x-axis)\n\nHeader header\nfloat32 angle_min        # start angle of the scan [rad]\nfloat32 angle_max        # end angle of the scan [rad]\nfloat32 angle_increment  # angular distance between measurements [rad]\nfloat32 time_increment   # time between measurements [seconds]\nfloat32 scan_time        # time between scans [seconds]\nfloat32 range_min        # minimum range value [m]\nfloat32 range_max        # maximum range value [m]\nfloat32[] ranges         # range data [m] (Note: values &lt; range_min or &gt; range_max should be discarded)\nfloat32[] intensities    # intensity data [device-specific units]\n</code></pre> The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type.</p> <p> </p> <p>For a Stretch robot the start angle of the scan, <code>angle_min</code>, and end angle, <code>angle_max</code>, are closely located along the x-axis of Stretch's frame. <code>angle_min</code> and <code>angle_max</code> are set at -3.1416 and 3.1416, respectively. This is illustrated by the images below.</p> <p> </p> <p>Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot.</p> <p>First, open a terminal and run the stretch driver launch file.</p> <pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> <p>Then in a new terminal run the <code>rplidar.launch</code> file from <code>stretch_core</code>. <pre><code># Terminal 2\nroslaunch stretch_core rplidar.launch\n</code></pre></p> <p>To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan_filter.py node by typing the following in a new terminal.</p> <pre><code># Terminal 3\ncd catkin_ws/src/stretch_tutorials/src/\npython scan_filter.py\n</code></pre> <p>Then run the following command to bring up a simple RViz configuration of the Stretch robot. <pre><code># Terminal 4\nrosrun rviz rviz -d `rospack find stretch_core`/rviz/stretch_simple_test.rviz\n</code></pre> Change the topic name from the LaserScan display from /scan to /filter_scan.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/example_2/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nfrom numpy import linspace, inf\nfrom math import sin\nfrom sensor_msgs.msg import LaserScan\n\nclass ScanFilter:\n    \"\"\"\n    A class that implements a LaserScan filter that removes all of the points\n    that are not in front of the robot.\n    \"\"\"\n    def __init__(self):\n        self.width = 1.0\n        self.extent = self.width / 2.0\n        self.sub = rospy.Subscriber('/scan', LaserScan, self.callback)\n        self.pub = rospy.Publisher('filtered_scan', LaserScan, queue_size=10)\n        rospy.loginfo(\"Publishing the filtered_scan topic. Use RViz to visualize.\")\n\n    def callback(self,msg):\n        \"\"\"\n        Callback function to deal with incoming LaserScan messages.\n        :param self: The self reference.\n        :param msg: The subscribed LaserScan message.\n\n        :publishes msg: updated LaserScan message.\n        \"\"\"\n        angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n        msg.ranges = new_ranges\n        self.pub.publish(msg)\n\nif __name__ == '__main__':\n    rospy.init_node('scan_filter')\n    ScanFilter()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_2/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nfrom numpy import linspace, inf\nfrom math import sin\nfrom sensor_msgs.msg import LaserScan\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node. There are functions from <code>numpy</code> and <code>math</code> that are required within this code, thus why linspace, inf, and sin are imported. The <code>sensor_msgs.msg</code> import is so that we can subscribe and publish <code>LaserScan</code> messages.</p> <p><pre><code>self.width = 1\nself.extent = self.width / 2.0\n</code></pre> We're going to assume that the robot is pointing up the x-axis, so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered.</p> <p><pre><code>self.sub = rospy.Subscriber('/scan', LaserScan, self.callback)\n</code></pre> Set up a subscriber.  We're going to subscribe to the topic scan, looking for <code>LaserScan</code> messages.  When a message comes in, ROS is going to pass it to the function \"callback\" automatically.</p> <p><pre><code>self.pub = rospy.Publisher('filtered_scan', LaserScan, queue_size=10)\n</code></pre> <code>pub = rospy.Publisher(\"filtered_scan\", LaserScan, queue_size=10)</code> declares that your node is publishing to the filtered_scan topic using the message type <code>LaserScan</code>. This lets the master tell any nodes listening on filtered_scan that we are going to publish data on that topic.</p> <p><pre><code>angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n</code></pre> This line of code utilizes linspace to compute each angle of the subscribed scan.</p> <p><pre><code>points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n</code></pre> Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference.</p> <p><pre><code>new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n</code></pre> If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\".</p> <p><pre><code>msg.ranges = new_ranges\nself.pub.publish(msg)\n</code></pre> Substitute in the new ranges in the original message, and republish it.</p> <p><pre><code>rospy.init_node('scan_filter')\nScanFilter()\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the class with <code>ScanFilter()</code></p> <p><pre><code>rospy.spin()\n</code></pre> Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_3/","title":"Example 3","text":""},{"location":"stretch-tutorials/ros1_melodic/example_3/#example-3","title":"Example 3","text":"<p>The aim of example 3 is to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward.</p> <p>Begin by running the following commands in a new terminal.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> Then in a new terminal type the following to activate the LiDAR sensor. <pre><code># Terminal 2\nroslaunch stretch_core rplidar.launch\n</code></pre></p> <p>To set navigation mode and to activate the avoider.py node, type the following in a new terminal.</p> <p><pre><code># Terminal 3\nrosservice call /switch_to_navigation_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython avoider.py\n</code></pre> To stop the node from sending twist messages, type Ctrl + c in the terminal running the avoider node.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/example_3/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nfrom numpy import linspace, inf, tanh\nfrom math import sin\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\n\nclass Avoider:\n    \"\"\"\n    A class that implements both a LaserScan filter and base velocity control\n    for collision avoidance.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber, publisher, and marker features.\n        :param self: The self reference.\n        \"\"\"\n        self.pub = rospy.Publisher('/stretch/cmd_vel', Twist, queue_size=1) #/stretch_diff_drive_controller/cmd_vel for gazebo\n        self.sub = rospy.Subscriber('/scan', LaserScan, self.set_speed)\n\n        self.width = 1\n        self.extent = self.width / 2.0\n        self.distance = 0.5\n\n        self.twist = Twist()\n        self.twist.linear.x = 0.0\n        self.twist.linear.y = 0.0\n        self.twist.linear.z = 0.0\n        self.twist.angular.x = 0.0\n        self.twist.angular.y = 0.0\n        self.twist.angular.z = 0.0\n\n    def set_speed(self,msg):\n        \"\"\"\n        Callback function to deal with incoming LaserScan messages.\n        :param self: The self reference.\n        :param msg: The subscribed LaserScan message.\n\n        :publishes self.twist: Twist message.\n        \"\"\"\n        angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n        error = min(new_ranges) - self.distance\n\n        self.twist.linear.x = tanh(error) if (error &gt; 0.05 or error &lt; -0.05) else 0\n        self.pub.publish(self.twist)        \n\nif __name__ == '__main__':\n    rospy.init_node('avoider')\n    Avoider()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_3/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nfrom numpy import linspace, inf, tanh\nfrom math import sin\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\n</code></pre> You need to import rospy if you are writing a ROS Node. There are functions from <code>numpy</code> and <code>math</code> that are required within this code, thus linspace, inf, tanh, and sin are imported. The <code>sensor_msgs.msg</code> import is so that we can subscribe to <code>LaserScan</code> messages. The <code>geometry_msgs.msg</code> import is so that we can send velocity commands to the robot.</p> <p><pre><code>self.pub = rospy.Publisher('/stretch/cmd_vel', Twist, queue_size=1)#/stretch_diff_drive_controller/cmd_vel for gazebo\n</code></pre> This section of code defines the talker's interface to the rest of ROS. <code>pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1)</code> declares that your node is publishing to the /stretch/cmd_vel topic using the message type <code>Twist</code>.</p> <p><pre><code>self.sub = rospy.Subscriber('/scan', LaserScan, self.set_speed)\n</code></pre> Set up a subscriber.  We're going to subscribe to the topic \"scan\", looking for <code>LaserScan</code> messages.  When a message comes in, ROS is going to pass it to the function \"set_speed\" automatically.</p> <p><pre><code>self.width = 1\nself.extent = self.width / 2.0\nself.distance = 0.5\n</code></pre> self.width is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing in the x-axis, any points with y coordinates further than half of the defined width (self.extent) from the x-axis are not considered. self.distance defines the stopping distance from an object.</p> <p><pre><code>self.twist = Twist()\nself.twist.linear.x = 0.0\nself.twist.linear.y = 0.0\nself.twist.linear.z = 0.0\nself.twist.angular.x = 0.0\nself.twist.angular.y = 0.0\nself.twist.angular.z = 0.0\n</code></pre> Allocate a <code>Twist</code> to use, and set everything to zero.  We're going to do this when the class is initiating. Redefining this within the callback function, <code>set_speed()</code> can be more computationally taxing.</p> <p><pre><code>angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\npoints = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\nnew_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n</code></pre> This line of code utilizes linspace to compute each angle of the subscribed scan. Here we compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\".</p> <p><pre><code>error = min(new_ranges) - self.distance\n</code></pre> Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as error.</p> <p><pre><code>self.twist.linear.x = tanh(error) if (error &gt; 0.05 or error &lt; -0.05) else 0\n</code></pre> Set the speed according to a tanh function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1</p> <p><pre><code>self.pub.publish(self.twist)\n</code></pre> Publish the <code>Twist</code> message.</p> <p><pre><code>rospy.init_node('avoider')\nAvoider()\nrospy.spin()\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate class with <code>Avioder()</code></p> <p>Give control to ROS with <code>rospy.spin()</code>. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_4/","title":"Example 4","text":""},{"location":"stretch-tutorials/ros1_melodic/example_4/#example-4","title":"Example 4","text":"<p>Let's bringup stretch in the willowgarage world from the gazebo basics tutorial and RViz by using the following command.</p> <p><pre><code># Terminal 1\nroslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world rviz:=true\n</code></pre> the <code>rviz</code> flag will open an RViz window  to visualize a variety of ROS topics. In a new terminal run the following commands to execute the marker.py node.</p> <p><pre><code># Terminal 2\ncd catkin_ws/src/stretch_tutorials/src/\npython marker.py\n</code></pre> The gif below demonstrates how to add a new <code>Marker</code> display type, and change the topic name from /visualization_marker to /balloon. A red sphere Marker should appear above the Stretch robot.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/example_4/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nfrom visualization_msgs.msg import Marker\n\nclass Balloon():\n    \"\"\"\n    A class that attaches a Sphere marker directly above the Stretch robot.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the marker's features.\n        :param self: The self reference.\n        \"\"\"\n        self.publisher = rospy.Publisher('balloon', Marker, queue_size=10)\n        self.marker = Marker()\n        self.marker.header.frame_id = 'base_link'\n        self.marker.header.stamp = rospy.Time()\n        self.marker.type = self.marker.SPHERE\n        self.marker.id = 0\n        self.marker.action = self.marker.ADD\n        self.marker.scale.x = 0.5\n        self.marker.scale.y = 0.5\n        self.marker.scale.z = 0.5\n        self.marker.color.r = 1.0\n        self.marker.color.g = 0.0\n        self.marker.color.b = 0.0\n        self.marker.color.a = 1.0\n        self.marker.pose.position.x = 0.0\n        self.marker.pose.position.y = 0.0\n        self.marker.pose.position.z = 2.0\n        rospy.loginfo(\"Publishing the balloon topic. Use RViz to visualize.\")\n\n    def publish_marker(self):\n        \"\"\"\n        Function that publishes the sphere marker.\n        :param self: The self reference.\n\n        :publishes self.marker: Marker message.\n        \"\"\"\n        self.publisher.publish(self.marker)\n\n\nif __name__ == '__main__':\n    rospy.init_node('marker')\n    balloon = Balloon()\n    rate = rospy.Rate(10)\n\n    while not rospy.is_shutdown():\n        balloon.publish_marker()\n        rate.sleep()        \n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_4/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nfrom visualization_msgs.msg import Marker\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node. Import the <code>Marker</code> type from the <code>visualization_msgs.msg</code> package. This import is required to publish a <code>Marker</code>, which will be visualized in RViz.</p> <p><pre><code>self.pub = rospy.Publisher('balloon', Marker, queue_size=10)\n</code></pre> This section of code defines the talker's interface to the rest of ROS. <code>pub = rospy.Publisher(\"balloon\", Twist, queue_size=1)</code> declares that your node is publishing to the /ballon topic using the message type <code>Twist</code>.</p> <p><pre><code>self.marker = Marker()\nself.marker.header.frame_id = 'base_link'\nself.marker.header.stamp = rospy.Time()\nself.marker.type = self.marker.SPHERE\n</code></pre> Create a <code>Marker()</code> message type. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers</p> <p><pre><code>self.marker.id = 0\n</code></pre> Each marker has a unique ID number. If you have more than one marker that you want displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number of an existing marker, it will replace the existing marker with that ID number.</p> <p><pre><code>self.marker.action = self.marker.ADD\n</code></pre> This line of code sets the action. You can add, delete, or modify markers.</p> <p><pre><code>self.marker.scale.x = 0.5\nself.marker.scale.y = 0.5\nself.marker.scale.z = 0.5\n</code></pre> These are the size parameters for the marker. These will vary by marker type.</p> <p><pre><code>self.marker.color.r = 1.0\nself.marker.color.g = 0.0\nself.marker.color.b = 0.0\n</code></pre> Color of the object, specified as r/g/b/a, with values in the range of [0, 1].</p> <p><pre><code>self.marker.color.a = 1.0\n</code></pre> The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible.</p> <p><pre><code>self.marker.pose.position.x = 0.0\nself.marker.pose.position.y = 0.0\nself.marker.pose.position.z = 2.0\n</code></pre> Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in <code>frame_id</code>. In this case, the position will always be directly 2 meters above the frame_id (base_link), and will move with it.</p> <p><pre><code>def publish_marker(self):\n        self.publisher.publish(self.marker)\n</code></pre> Publish the Marker data structure to be visualized in RViz.</p> <p><pre><code>rospy.init_node('marker', argv=sys.argv)\nballoon = Balloon()\nrate = rospy.Rate(10)\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate class with <code>Balloon()</code></p> <p>Give control to ROS with <code>rospy.spin()</code>. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p> <p><pre><code>while not rospy.is_shutdown():\n    balloon.publish_marker()\n    rate.sleep()\n</code></pre> This loop is a fairly standard rospy construct: checking the <code>rospy.is_shutdown()</code> flag and then doing work. You have to check <code>is_shutdown()</code> to check if your program should exit (e.g. if there is a Ctrl-C or otherwise). The loop calls <code>rate.sleep()</code>, which sleeps just long enough to maintain the desired rate through the loop.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_5/","title":"Example 5","text":""},{"location":"stretch-tutorials/ros1_melodic/example_5/#example-5","title":"Example 5","text":"<p>In this example, we will review a Python script that prints out the positions of a selected group of Stretch's joints. This script is helpful if you need the joint positions after you teleoperated Stretch with the Xbox controller or physically moved the robot to the desired configuration after hitting the run stop button.</p> <p>If you are looking for a continuous print of the joint states while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial.</p> <p>Begin by starting up the stretch driver launch file.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> You can then hit the run-stop button (you should hear a beep and the LED light in the button blink) and move the robot's joints to a desired configuration. Once you are satisfied with the configuration, hold the run-stop button until you hear a beep. Then run the following command to excecute the joint_state_printer.py which will print the joint positions of the lift, arm, and wrist.</p> <p><pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 joint_state_printer.py\n</code></pre> Your terminal will output the <code>position</code> information of the previously mentioned joints shown below. <pre><code>name: ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\nposition: [0.6043133175850597, 0.19873586673129257, 0.017257283863713464]\n</code></pre> IMPORTANT NOTE: Stretch's arm has 4 prismatic joints and the sum of these positions gives the wrist_extension distance. The wrist_extension is needed when sending joint trajectory commands to the robot. Further, you can not actuate an individual arm joint. Here is an image of the arm joints for reference:</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/example_5/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nimport sys\nfrom sensor_msgs.msg import JointState\n\nclass JointStatePublisher():\n    \"\"\"\n    A class that prints the positions of desired joints in Stretch.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber.\n        :param self: The self reference.\n        \"\"\"\n        self.sub = rospy.Subscriber('joint_states', JointState, self.callback)\n\n    def callback(self, msg):\n        \"\"\"\n        Callback function to deal with the incoming JointState messages.\n        :param self: The self reference.\n        :param msg: The JointState message.\n        \"\"\"\n        self.joint_states = msg\n\n    def print_states(self, joints):\n        \"\"\"\n        print_states function to deal with the incoming JointState messages.\n        :param self: The self reference.\n        :param joints: A list of string values of joint names.\n        \"\"\"\n        joint_positions = []\n        for joint in joints:\n            if joint == \"wrist_extension\":\n                index = self.joint_states.name.index('joint_arm_l0')\n                joint_positions.append(4*self.joint_states.position[index])\n                continue\n            index = self.joint_states.name.index(joint)\n            joint_positions.append(self.joint_states.position[index])\n        print(\"name: \" + str(joints))\n        print(\"position: \" + str(joint_positions))\n        rospy.signal_shutdown(\"done\")\n        sys.exit(0)\n\nif __name__ == '__main__':\n    rospy.init_node('joint_state_printer', anonymous=True)\n    JSP = JointStatePublisher()\n    rospy.sleep(.1)\n    joints = [\"joint_lift\", \"wrist_extension\", \"joint_wrist_yaw\"]\n    #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"]\n    JSP.print_states(joints)\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_5/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nimport sys\nfrom sensor_msgs.msg import JointState\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node. Import <code>sensor_msgs.msg</code> so that we can subscribe to <code>JointState</code> messages.</p> <p><pre><code>self.sub = rospy.Subscriber('joint_states', JointState, self.callback)\n</code></pre> Set up a subscriber.  We're going to subscribe to the topic \"joint_states\", looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function \"callback\" automatically</p> <p><pre><code>def callback(self, msg):\n    self.joint_states = msg\n</code></pre> This is the callback function where he <code>JointState</code> messages are stored as self.joint_states. Further information about the this message type can be found here: JointState Message</p> <p><pre><code>def print_states(self, joints):\n    joint_positions = []\n</code></pre> This is the <code>print_states()</code> function which takes in a list of joints of interest as its argument. the is also an empty list set as joint_positions and this is where the positions of the requested joints will be appended.</p> <p><pre><code>for joint in joints:\n  if joint == \"wrist_extension\":\n    index = self.joint_states.name.index('joint_arm_l0')\n    joint_positions.append(4*self.joint_states.position[index])\n    continue\n  index = self.joint_states.name.index(joint)\n  joint_positions.append(self.joint_states.position[index])\n</code></pre> In this section of the code, a forloop is used to parse the names of the requested joints from the self.joint_states list. The <code>index()</code> function returns the index a of the name of the requested joint and appends the respective position to our joint_positions list.</p> <p><pre><code>rospy.signal_shutdown(\"done\")\nsys.exit(0)\n</code></pre> The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter.</p> <p><pre><code>rospy.init_node('joint_state_printer', anonymous=True)\nJSP = JointStatePublisher()\nrospy.sleep(.1)\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Declare object, JSP, from the <code>JointStatePublisher</code> class.</p> <p>The use of the <code>rospy.sleep()</code> function is to allow the JSP class to initialize all of its features before requesting to publish joint positions of desired joints (running the <code>print_states()</code> method).</p> <p><pre><code>joints = [\"joint_lift\", \"wrist_extension\", \"joint_wrist_yaw\"]\n#joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"]\nJSP.print_states(joints)\n</code></pre> Create a list of the desired joints that you want positions to be printed. Then use that list as an argument for the <code>print_states()</code> method.</p> <p><pre><code>rospy.spin()\n</code></pre> Give control to ROS with <code>rospy.spin()</code>. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_6/","title":"Example 6","text":""},{"location":"stretch-tutorials/ros1_melodic/example_6/#example-6","title":"Example 6","text":"<p>In this example, we will review a Python script that prints out and stores the effort values from a specified joint. If you are looking for a continuous print of the joint state efforts while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial.</p> <p> </p> <p>Begin by running the following command in the terminal in a terminal.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> Switch the mode to position mode using a rosservice call. Then run the effort_sensing.py node.</p> <p><pre><code># Terminal 2\nrosservice call /switch_to_position_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython effort_sensing.py\n</code></pre> This will send a <code>FollowJointTrajectory</code> command to move Stretch's arm or head while also printing the effort of the lift.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_6/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\nimport rospy\nimport time\nimport actionlib\nimport os\nimport csv\nfrom datetime import datetime\n\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom sensor_msgs.msg import JointState\nimport hello_helpers.hello_misc as hm\n\nclass JointActuatorEffortSensor(hm.HelloNode):\n    \"\"\"\n    A class that sends multiple joint trajectory goals to a single joint.\n    \"\"\"\n    def __init__(self, export_data=False):\n        \"\"\"\n        Function that initializes the subscriber,and other features.\n        :param self: The self reference.\n        :param export_data: A boolean message type.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n        self.sub = rospy.Subscriber('joint_states', JointState, self.callback)\n        self.joints = ['joint_lift']\n        self.joint_effort = []\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n        self.export_data = export_data\n\n    def callback(self, msg):\n        \"\"\"\n        Callback function to update and store JointState messages.\n        :param self: The self reference.\n        :param msg: The JointState message.\n        \"\"\"\n        self.joint_states = msg\n\n    def issue_command(self):\n        \"\"\"\n        Function that makes an action call and sends joint trajectory goals\n        to a single joint.\n        :param self: The self reference.\n        \"\"\"\n        trajectory_goal = FollowJointTrajectoryGoal()\n        trajectory_goal.trajectory.joint_names = self.joints\n\n        point0 = JointTrajectoryPoint()\n        point0.positions = [0.9]\n\n        trajectory_goal.trajectory.points = [point0]\n        trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n        self.trajectory_client.send_goal(trajectory_goal, feedback_cb=self.feedback_callback, done_cb=self.done_callback)\n        rospy.loginfo('Sent position goal = {0}'.format(trajectory_goal))\n        self.trajectory_client.wait_for_result()\n\n    def feedback_callback(self,feedback):\n        \"\"\"\n        The feedback_callback function deals with the incoming feedback messages\n        from the trajectory_client. Although, in this function, we do not use the\n        feedback information.\n        :param self: The self reference.\n        :param feedback: FollowJointTrajectoryActionFeedback message.\n        \"\"\"\n        if 'wrist_extension' in self.joints:\n            self.joints.remove('wrist_extension')\n            self.joints.append('joint_arm_l0')\n\n        current_effort = []\n        for joint in self.joints:\n            index = self.joint_states.name.index(joint)\n            current_effort.append(self.joint_states.effort[index])\n\n        if not self.export_data:\n            print(\"name: \" + str(self.joints))\n            print(\"effort: \" + str(current_effort))\n        else:\n            self.joint_effort.append(current_effort)\n\n\n    def done_callback(self, status, result):\n        \"\"\"\n        The done_callback function will be called when the joint action is complete.\n        Within this function we export the data to a .txt file in  the /stored_data directory.\n        :param self: The self reference.\n        :param status: status attribute from FollowJointTrajectoryActionResult message.\n        :param result: result attribute from FollowJointTrajectoryActionResult message.\n        \"\"\"\n        if status == actionlib.GoalStatus.SUCCEEDED:\n            rospy.loginfo('Succeeded')\n        else:\n            rospy.loginfo('Failed')\n\n        if self.export_data:\n            file_name = datetime.now().strftime(\"%Y-%m-%d_%I:%M:%S-%p\")\n            completeName = os.path.join(self.save_path, file_name)\n            with open(completeName, \"w\") as f:\n                writer = csv.writer(f)\n                writer.writerow(self.joints)\n                writer.writerows(self.joint_effort)\n\n    def main(self):\n        \"\"\"\n        Function that initiates the issue_command function.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.main(self, 'issue_command', 'issue_command', wait_for_first_pointcloud=False)\n        rospy.loginfo('issuing command...')\n        self.issue_command()\n        time.sleep(2)\n\nif __name__ == '__main__':\n    try:\n        node = JointActuatorEffortSensor(export_data=True)\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_6/#the-code-explained","title":"The Code Explained","text":"<p>This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nimport time\nimport actionlib\nimport os\nimport csv\nfrom datetime import datetime\n\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom sensor_msgs.msg import JointState\nimport hello_helpers.hello_misc as hm\n</code></pre> You need to import rospy if you are writing a ROS Node. Import the <code>FollowJointTrajectoryGoal</code> from the <code>control_msgs.msg</code> package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the <code>trajectory_msgs</code> package to define robot trajectories. The <code>hello_helpers</code> package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.</p> <p><pre><code>class JointActuatorEffortSensor(hm.HelloNode):\n    \"\"\"\n    A class that sends multiple joint trajectory goals to a single joint.\n    \"\"\"\n    def __init__(self, export_data=False):\n        \"\"\"\n        Function that initializes the subscriber,and other features.\n        :param self: The self reference.\n        :param export_data: A boolean message type.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n</code></pre> The <code>JointActuatorEffortSensor</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is initialized.</p> <p><pre><code>self.sub = rospy.Subscriber('joint_states', JointState, self.callback)\nself.joints = ['joint_lift']\n</code></pre> Set up a subscriber.  We're going to subscribe to the topic \"joint_states\", looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function \"callback\" automatically. Create a list of the desired joints you want to print.</p> <p><pre><code>self.joint_effort = []\nself.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\nself.export_data = export_data\n</code></pre> Create an empty list to store the joint effort values. The self.save_path is the directory path where the .txt file of the effort values will be stored. You can change this path to a preferred directory. The self.export_data is a boolean and its default value is set to False. If set to True, then the joint values will be stored in a .txt file, otherwise, the values will be printed in the terminal where you ran the effort sensing node.</p> <p><pre><code>self.trajectory_client.send_goal(trajectory_goal, feedback_cb=self.feedback_callback, done_cb=self.done_callback)\n</code></pre> Include the feedback and done call back functions in the send goal function.</p> <p><pre><code>def feedback_callback(self,feedback):\n    \"\"\"\n    The feedback_callback function deals with the incoming feedback messages\n    from the trajectory_client. Although, in this function, we do not use the\n    feedback information.\n    :param self: The self reference.\n    :param feedback: FollowJointTrajectoryActionFeedback message.\n    \"\"\"\n</code></pre> The feedback callback function takes in the <code>FollowJointTrajectoryActionFeedback</code> message as its argument.</p> <p><pre><code>if 'wrist_extension' in self.joints:\n    self.joints.remove('wrist_extension')\n    self.joints.append('joint_arm_l0')\n</code></pre> Use a conditional statement to replace <code>wrist_extenstion</code> to <code>joint_arm_l0</code>. This is because <code>joint_arm_l0</code> has the effort values that the <code>wrist_extension</code> is experiencing.</p> <p><pre><code>current_effort = []\nfor joint in self.joints:\n    index = self.joint_states.name.index(joint)\n    current_effort.append(self.joint_states.effort[index])\n</code></pre> Create an empty list to store the current effort values. Then use a for loop to parse the joint names and effort values.</p> <p><pre><code>if not self.export_data:\n    print(\"name: \" + str(self.joints))\n    print(\"effort: \" + str(current_effort))\nelse:\n    self.joint_effort.append(current_effort)\n</code></pre> Use a conditional statement to print effort values in the terminal or store values into a list that will be used for exporting the data in a .txt file.</p> <p><pre><code>def done_callback(self, status, result):\n      \"\"\"\n      The done_callback function will be called when the joint action is complete.\n      Within this function we export the data to a .txt file in  the /stored_data directory.\n      :param self: The self reference.\n      :param status: status attribute from FollowJointTrajectoryActionResult message.\n      :param result: result attribute from FollowJointTrajectoryActionResult message.\n      \"\"\"\n</code></pre> The done callback function takes in the <code>FollowJointTrajectoryActionResult</code> messages as its arguments.</p> <p><pre><code>if status == actionlib.GoalStatus.SUCCEEDED:\n    rospy.loginfo('Succeeded')\nelse:\n    rospy.loginfo('Failed')\n</code></pre> Conditional statement to print whether the goal status in the <code>FollowJointTrajectoryActionResult</code> succeeded or failed.</p> <p><pre><code>if self.export_data:\n    file_name = datetime.now().strftime(\"%Y-%m-%d_%I:%M:%S-%p\")\n    completeName = os.path.join(self.save_path, file_name)\n\n    with open(completeName, \"w\") as f:\n        writer = csv.writer(f)\n        writer.writerow(self.joints)\n        writer.writerows(self.joint_effort)\n</code></pre> A conditional statement is used to export the data to a .txt file. The file's name is set to the date and time the node was executed. That way, no previous files are overwritten.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_6/#plottinganimating-effort-data","title":"Plotting/Animating Effort Data","text":"<p>We added a simple python script, stored_data_plotter.py, to this package for plotting the stored data. Note you have to change the name of the file you wish to see in the python script. This is shown below:</p> <p><pre><code>####################### Copy the file name here! #######################\nfile_name = '2022-06-30_11:26:20-AM'\n</code></pre> Once you have changed the file name, then run the following in a new command.</p> <p><pre><code>cd catkin_ws/src/stretch_tutorials/src/\npython3 stored_data_plotter.py\n</code></pre> Because this is not a node, you don't need <code>roscore</code> to run this script. Please review the comments in the python script for additional guidance.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_7/","title":"Example 7","text":"<p>In this example, we will review the image_view ROS package and a Python script that captures an image from the RealSense camera.</p> <p> </p> <p>BBegin by checking out the feature/upright_camera_view branch in the stretch_ros repository. The configuration of the camera results in the images being displayed sideways. Thus, this branch publishes a new topic that rotates the raw image upright.</p> <p><pre><code>cd ~/catkin_ws/src/stretch_ros/stretch_core\ngit checkout feature/upright_camera_view\n</code></pre> Then run the stretch driver launch file.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <p><pre><code># Terminal 2\nroslaunch stretch_core d435i_low_resolution.launch\n</code></pre> Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code># Terminal 3\nrosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_7/#capture-image-with-image_view","title":"Capture Image with image_view","text":"<p>There are a couple of methods to save an image using the image_view package.</p> <p>OPTION 1: Use the <code>image_view</code> node to open a simple image viewer for ROS sensor_msgs/image topics.</p> <p><pre><code># Terminal 4\nrosrun image_view image_view image:=/camera/color/image_raw_upright_view\n</code></pre> Then you can save the current image by right-clicking on the display window. By deafult, images will be saved as frame000.jpg, frame000.jpg, etc. Note, that the image will be saved to the terminal's current work directory.</p> <p>OPTION 2: Use the <code>image_saver</code> node to save an image to the terminals current work directory. <pre><code># Terminal 4\nrosrun image_view image_saver image:=/camera/color/image_raw_upright_view\n</code></pre></p>"},{"location":"stretch-tutorials/ros1_melodic/example_7/#capture-image-with-python-script","title":"Capture Image with Python Script","text":"<p>In this section, you can use a Python node to capture an image from the RealSense camera. Execute the capture_image.py node to save a .jpeg image of the image topic /camera/color/image_raw_upright_view.</p> <p><pre><code># Terminal 4\ncd ~/catkin_ws/src/stretch_tutorials/src\npython capture_image.py\n</code></pre> An image named camera_image.jpeg is saved in the stored_data folder in this package.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_7/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nimport sys\nimport os\nimport cv2\n\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nclass CaptureImage:\n    \"\"\"\n    A class that converts a subscribed ROS image to a OpenCV image and saves\n    the captured image to a predefined directory.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes a CvBridge class, subscriber, and save path.\n        :param self: The self reference.\n        \"\"\"\n        self.bridge = CvBridge()\n        self.sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback, queue_size=1)\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n\n    def callback(self, msg):\n        \"\"\"\n        A callback function that converts the ROS image to a CV2 image and stores the\n        image.\n        :param self: The self reference.\n        :param msg: The ROS image message type.\n        \"\"\"\n        try:\n            image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        except CvBridgeError, e:\n            rospy.logwarn('CV Bridge error: {0}'.format(e))\n\n        file_name = 'camera_image.jpeg'\n        completeName = os.path.join(self.save_path, file_name)\n        cv2.imwrite(completeName, image)\n        rospy.signal_shutdown(\"done\")\n        sys.exit(0)\n\nif __name__ == '__main__':\n    rospy.init_node('capture_image', argv=sys.argv)\n    CaptureImage()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_7/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nimport sys\nimport os\nimport cv2\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node. There are functions from sys, os, and cv2 that are required within this code. cv2 is a library of Python functions that implements computer vision algorithms. Further information about cv2 can be found here: OpenCV Python.</p> <p><pre><code>from sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n</code></pre> The <code>sensor_msgs.msg</code> is imported so that we can subscribe to ROS <code>Image</code> messages. Import CvBridge to convert between ROS <code>Image</code> messages and OpenCV images.</p> <p><pre><code>def __init__(self):\n    \"\"\"\n    A function that initializes a CvBridge class, subscriber, and save path.\n    :param self: The self reference.\n    \"\"\"\n    self.bridge = CvBridge()\n    self.sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback, queue_size=1)\n    self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n</code></pre> Initialize the CvBridge class, the subscriber, and the directory of where the captured image will be stored.</p> <p><pre><code>def callback(self, msg):\n    \"\"\"\n    A callback function that converts the ROS image to a cv2 image and stores the\n    image.\n    :param self: The self reference.\n    :param msg: The ROS image message type.\n    \"\"\"\n    try:\n        image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n    except CvBridgeError as e:\n        rospy.logwarn('CV Bridge error: {0}'.format(e))\n</code></pre> Try to convert the ROS Image message to a cv2 Image message using the <code>imgmsg_to_cv2()</code> function.  </p> <p><pre><code>file_name = 'camera_image.jpeg'\ncompleteName = os.path.join(self.save_path, file_name)\ncv2.imwrite(completeName, image)\n</code></pre> Join the directory and file name using the <code>path.join()</code> function. Then use the <code>imwrite()</code> function to save the image.</p> <p><pre><code>rospy.signal_shutdown(\"done\")\nsys.exit(0)\n</code></pre> The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter.</p> <p><pre><code>rospy.init_node('capture_image', argv=sys.argv)\nCaptureImage()\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the <code>CaptureImage()</code> class.</p> <p><pre><code>rospy.spin()\n</code></pre> Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process  any messages.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_7/#edge-detection","title":"Edge Detection","text":"<p>In this section, we highlight a node that utilizes the Canny Edge filter algorithm to detect the edges from an image and convert it back as a ROS image to be visualized in RViz. Begin by running the following commands.</p> <p><pre><code># Terminal 4\ncd ~/catkin_ws/src/stretch_tutorials/src\npython edge_detection.py\n</code></pre> The node will publish a new Image topic named /image_edge_detection. This can be visualized in RViz and a gif is provided below for reference.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/example_7/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nimport sys\nimport os\nimport cv2\n\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nclass EdgeDetection:\n    \"\"\"\n    A class that converts a subscribed ROS image to a OpenCV image and saves\n    the captured image to a predefined directory.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes a CvBridge class, subscriber, and other\n        parameter values.\n        :param self: The self reference.\n        \"\"\"\n        self.bridge = CvBridge()\n        self.sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback, queue_size=1)\n        self.pub = rospy.Publisher('/image_edge_detection', Image, queue_size=1)\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n        self.lower_thres = 100\n        self.upper_thres = 200\n        rospy.loginfo(\"Publishing the CV2 Image. Use RViz to visualize.\")\n\n    def callback(self, msg):\n        \"\"\"\n        A callback function that converts the ROS image to a CV2 image and goes\n        through the Canny Edge filter in OpenCV for edge detection. Then publishes\n        that filtered image to be visualized in RViz.\n        :param self: The self reference.\n        :param msg: The ROS image message type.\n        \"\"\"\n        try:\n            image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        except CvBridgeError as e:\n            rospy.logwarn('CV Bridge error: {0}'.format(e))\n\n        image = cv2.Canny(image, self.lower_thres, self.upper_thres)\n        image_msg = self.bridge.cv2_to_imgmsg(image, 'passthrough')\n        image_msg.header = msg.header\n        self.pub.publish(image_msg)\n\nif __name__ == '__main__':\n    rospy.init_node('edge_detection', argv=sys.argv)\n    EdgeDetection()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_7/#the-code-explained_1","title":"The Code Explained","text":"<p>Since that there are similarities in the capture image node, we will only breakdown the different components of the edge detection node.</p> <p><pre><code>self.lower_thres = 100\nself.upper_thres = 200\n</code></pre> Define lower and upper bounds of the Hysteresis Thresholds.</p> <p><pre><code>image = cv2.Canny(image, self.lower_thres, self.upper_thres)\n</code></pre> Run the Canny Edge function to detect edges from the cv2 image.</p> <p><pre><code>image_msg = self.bridge.cv2_to_imgmsg(image, 'passthrough')\n</code></pre> Convert the cv2 image back to a ROS image so it can be published.</p> <p><pre><code>image_msg.header = msg.header\nself.pub.publish(image_msg)\n</code></pre> Publish the ROS image with the same header as the subscribed ROS message.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_8/","title":"Example 8","text":"<p>This example will showcase how to save the interpreted speech from Stretch's ReSpeaker Mic Array v2.0 to a text file.</p> <p> </p> <p>Begin by running the <code>respeaker.launch</code> file in a terminal. <pre><code># Terminal 1\nroslaunch respeaker_ros sample_respeaker.launch\n</code></pre> Then run the speech_text.py node.</p> <p><pre><code># Terminal 2\ncd catkin_ws/src/stretch_tutorials/src/\npython speech_text.py\n</code></pre> The ReSpeaker will be listening and will start to interpret speech and save the transcript to a text file.  To stop shutdown the node, type Ctrl + c in the terminal.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_8/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nimport os\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n\nclass SpeechText:\n    \"\"\"\n    A class that saves the interpreted speech from the ReSpeaker Microphone Array to a text file.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize subscriber and directory to save speech to text file.\n        \"\"\"\n        self.sub = rospy.Subscriber(\"speech_to_text\", SpeechRecognitionCandidates, self.callback)\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n        rospy.loginfo(\"Listening to speech.\")\n\n    def callback(self,msg):\n        \"\"\"\n        A callback function that receives the speech transcript and appends the\n        transcript to a text file.\n        :param self: The self reference.\n        :param msg: The SpeechRecognitionCandidates message type.\n        \"\"\"\n        transcript = ' '.join(map(str,msg.transcript))\n        file_name = 'speech.txt'\n        completeName = os.path.join(self.save_path, file_name)\n\n        with open(completeName, \"a+\") as file_object:\n            file_object.write(\"\\n\")\n            file_object.write(transcript)\n\nif __name__ == '__main__':\n    rospy.init_node('speech_text')\n    SpeechText()\n    rospy.spin()\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_8/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rospy\nimport os\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node.</p> <p><pre><code>from speech_recognition_msgs.msg import SpeechRecognitionCandidates\n</code></pre> Import <code>SpeechRecognitionCandidates</code> from the <code>speech_recgonition_msgs.msg</code> so that we can receive the interpreted speech.</p> <p><pre><code>def __init__(self):\n    \"\"\"\n    Initialize subscriber and directory to save speech to text file.\n    \"\"\"\n    self.sub = rospy.Subscriber(\"speech_to_text\", SpeechRecognitionCandidates, self.callback)\n</code></pre> Set up a subscriber.  We're going to subscribe to the topic \"speech_to_text\", looking for <code>SpeechRecognitionCandidates</code> messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically.</p> <p><pre><code>self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data\n</code></pre> Define the directory to save the text file.</p> <p><pre><code>transcript = ' '.join(map(str,msg.transcript))\n</code></pre> Take all items in the iterable list and join them into a single string named transcript.</p> <p><pre><code>file_name = 'speech.txt'\ncompleteName = os.path.join(self.save_path, file_name)\n</code></pre> Define the file name and create a complete path directory.</p> <p><pre><code>with open(completeName, \"a+\") as file_object:\n    file_object.write(\"\\n\")\n    file_object.write(transcript)\n</code></pre> Append the transcript to the text file.</p> <p><pre><code>rospy.init_node('speech_text')\nSpeechText()\n</code></pre> The next line, <code>rospy.init_node(NAME, ...)</code>, is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Instantiate the <code>SpeechText()</code> class.</p> <p><pre><code>rospy.spin()\n</code></pre> Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros1_melodic/example_9/","title":"Example 9","text":""},{"location":"stretch-tutorials/ros1_melodic/example_9/#example-9","title":"Example 9","text":"<p>The aim of example 9 is to combine the ReSpeaker Microphone Array and Follow Joint Trajectory tutorials to voice teleoperate the mobile base of the Stretch robot.</p> <p>Begin by running the following command in a new terminal.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> Switch the mode to position mode using a rosservice call. Then run the <code>respeaker.launch</code> file.</p> <p><pre><code># Terminal 2\nrosservice call /switch_to_position_mode\nroslaunch stretch_core respeaker.launch\n</code></pre> Then run the voice_teleoperation_base.py node in a new terminal.</p> <p><pre><code># Terminal 3\ncd catkin_ws/src/stretch_tutorials/src/\npython voice_teleoperation_base.py\n</code></pre> In terminal 3, a menu of voice commands is printed. You can reference this menu layout below.  </p> <p><pre><code>------------ VOICE TELEOP MENU ------------\n\nVOICE COMMANDS              \n\"forward\": BASE FORWARD                   \n\"back\"   : BASE BACK                      \n\"left\"   : BASE ROTATE LEFT               \n\"right\"  : BASE ROTATE RIGHT              \n\"stretch\": BASE ROTATES TOWARDS SOUND     \n\nSTEP SIZE                 \n\"big\"    : BIG                            \n\"medium\" : MEDIUM                         \n\"small\"  : SMALL                          \n\n\n\"quit\"   : QUIT AND CLOSE NODE            \n\n-------------------------------------------\n</code></pre> To stop the node from sending twist messages, type Ctrl + c or say \"quit\".</p>"},{"location":"stretch-tutorials/ros1_melodic/example_9/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport math\nimport rospy\nimport sys\n\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Int32\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n\nclass GetVoiceCommands:\n    \"\"\"\n    A class that subscribes to the speech to text recognition messages, prints\n    a voice command menu, and defines step size for translational and rotational\n    mobile base motion.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes subscribers and defines the three different\n        step sizes.\n        :param self: The self reference.\n        \"\"\"\n        self.step_size = 'medium'\n        self.rad_per_deg = math.pi/180.0\n\n        self.small_deg = 5.0\n        self.small_rad = self.rad_per_deg * self.small_deg\n        self.small_translate = 0.025\n\n        self.medium_deg = 10.0\n        self.medium_rad = self.rad_per_deg * self.medium_deg\n        self.medium_translate = 0.05\n\n        self.big_deg = 20.0\n        self.big_rad = self.rad_per_deg * self.big_deg\n        self.big_translate = 0.1\n\n        self.voice_command = None\n        self.sound_direction = 0\n        self.speech_to_text_sub  = rospy.Subscriber(\"/speech_to_text\",  SpeechRecognitionCandidates, self.callback_speech)\n        self.sound_direction_sub = rospy.Subscriber(\"/sound_direction\", Int32,                       self.callback_direction)\n\n    def callback_direction(self, msg):\n        \"\"\"\n        A callback function that converts the incoming message, sound direction,\n        from degrees to radians.\n        :param self: The self reference.\n        :param msg: The Int32 message type that represents the sound direction.\n        \"\"\"\n        self.sound_direction = msg.data * -self.rad_per_deg\n\n    def callback_speech(self,msg):\n        \"\"\"\n        A callback function takes the incoming message, a list of the speech to\n        text, and joins all items in that iterable list into a single string.\n        :param self: The self reference.\n        :param msg: The SpeechRecognitionCandidates message type.\n        \"\"\"\n        self.voice_command = ' '.join(map(str,msg.transcript))\n\n    def get_inc(self):\n        \"\"\"\n        A function that sets the increment size for translational and rotational\n        base motion.\n        :param self:The self reference.\n\n        :returns inc: A dictionary type the contains the increment size.\n        \"\"\"\n        if self.step_size == 'small':\n            inc = {'rad': self.small_rad, 'translate': self.small_translate}\n        if self.step_size == 'medium':\n            inc = {'rad': self.medium_rad, 'translate': self.medium_translate}\n        if self.step_size == 'big':\n            inc = {'rad': self.big_rad, 'translate': self.big_translate}\n        return inc\n\n    def print_commands(self):\n        \"\"\"\n        A function that prints the voice teleoperation menu.\n        :param self: The self reference.\n        \"\"\"\n        print('                                           ')\n        print('------------ VOICE TELEOP MENU ------------')\n        print('                                           ')\n        print('               VOICE COMMANDS              ')\n        print(' \"forward\": BASE FORWARD                   ')\n        print(' \"back\"   : BASE BACK                      ')\n        print(' \"left\"   : BASE ROTATE LEFT               ')\n        print(' \"right\"  : BASE ROTATE RIGHT              ')\n        print(' \"stretch\": BASE ROTATES TOWARDS SOUND     ')\n        print('                                           ')\n        print('                 STEP SIZE                 ')\n        print(' \"big\"    : BIG                            ')\n        print(' \"medium\" : MEDIUM                         ')\n        print(' \"small\"  : SMALL                          ')\n        print('                                           ')\n        print('                                           ')\n        print(' \"quit\"   : QUIT AND CLOSE NODE            ')\n        print('                                           ')\n        print('-------------------------------------------')\n\n    def get_command(self):\n        \"\"\"\n        A function that defines the teleoperation command based on the voice command.\n        :param self: The self reference.\n\n        :returns command: A dictionary type that contains the type of base motion.\n        \"\"\"\n        command = None\n        if self.voice_command == 'forward':\n            command = {'joint': 'translate_mobile_base', 'inc': self.get_inc()['translate']}\n        if self.voice_command == 'back':\n            command = {'joint': 'translate_mobile_base', 'inc': -self.get_inc()['translate']}\n        if self.voice_command == 'left':\n            command = {'joint': 'rotate_mobile_base', 'inc': self.get_inc()['rad']}\n        if self.voice_command == 'right':\n            command = {'joint': 'rotate_mobile_base', 'inc': -self.get_inc()['rad']}\n        if self.voice_command == 'stretch':\n            command = {'joint': 'rotate_mobile_base', 'inc': self.sound_direction}\n        if (self.voice_command == \"small\") or (self.voice_command == \"medium\") or (self.voice_command == \"big\"):\n            self.step_size = self.voice_command\n            rospy.loginfo('Step size = {0}'.format(self.step_size))\n        if self.voice_command == 'quit':\n            rospy.signal_shutdown(\"done\")\n            sys.exit(0)\n\n        self.voice_command = None\n        return command\n\n\nclass VoiceTeleopNode(hm.HelloNode):\n    \"\"\"\n    A class that inherits the HelloNode class from hm and sends joint trajectory\n    commands.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that declares object from the GetVoiceCommands class, instantiates\n        the HelloNode class, and set the publishing rate.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n        self.rate = 10.0\n        self.joint_state = None\n        self.speech = GetVoiceCommands()\n\n    def joint_states_callback(self, msg):\n        \"\"\"\n        A callback function that stores Stretch's joint states.\n        :param self: The self reference.\n        :param msg: The JointState message type.\n        \"\"\"\n        self.joint_state = msg\n\n    def send_command(self, command):\n        \"\"\"\n        Function that makes an action call and sends base joint trajectory goals\n        :param self: The self reference.\n        :param command: A dictionary type.\n        \"\"\"\n        joint_state = self.joint_state\n        if (joint_state is not None) and (command is not None):\n            point = JointTrajectoryPoint()\n            point.time_from_start = rospy.Duration(0.0)\n            trajectory_goal = FollowJointTrajectoryGoal()\n            trajectory_goal.goal_time_tolerance = rospy.Time(1.0)\n            joint_name = command['joint']\n            trajectory_goal.trajectory.joint_names = [joint_name]\n\n            inc = command['inc']\n            rospy.loginfo('inc = {0}'.format(inc))\n            new_value = inc\n\n            point.positions = [new_value]\n            trajectory_goal.trajectory.points = [point]\n            trajectory_goal.trajectory.header.stamp = rospy.Time.now()\n            rospy.loginfo('joint_name = {0}, trajectory_goal = {1}'.format(joint_name, trajectory_goal))\n            self.trajectory_client.send_goal(trajectory_goal)\n            rospy.loginfo('Done sending command.')\n            self.speech.print_commands()\n\n    def main(self):\n        \"\"\"\n        The main function that instantiates the HelloNode class, initializes the subscriber,\n        and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.main(self, 'voice_teleop', 'voice_teleop', wait_for_first_pointcloud=False)\n        rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)\n        rate = rospy.Rate(self.rate)\n        self.speech.print_commands()\n\n        while not rospy.is_shutdown():\n            command = self.speech.get_command()\n            self.send_command(command)\n            rate.sleep()\n\nif __name__ == '__main__':\n    try:\n        node = VoiceTeleopNode()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/example_9/#the-code-explained","title":"The Code Explained","text":"<p>This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import math\nimport rospy\nimport sys\n\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Int32\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n</code></pre> You need to import rospy if you are writing a ROS Node. Import the <code>FollowJointTrajectoryGoal</code> from the <code>control_msgs.msg</code> package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the <code>trajectory_msgs</code> package to define robot trajectories. The <code>hello_helpers</code> package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.  Import <code>sensor_msgs.msg</code> so that we can subscribe to JointState messages.</p> <p><pre><code>class GetVoiceCommands:\n</code></pre> Create a class that subscribes to the speech to text recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion.</p> <p><pre><code>self.step_size = 'medium'\nself.rad_per_deg = math.pi/180.0\n</code></pre> Set the default step size as medium and create a float value, self.rad_per_deg, to convert degrees to radians.</p> <p><pre><code>self.small_deg = 5.0\nself.small_rad = self.rad_per_deg * self.small_deg\nself.small_translate = 0.025\n\nself.medium_deg = 10.0\nself.medium_rad = self.rad_per_deg * self.medium_deg\nself.medium_translate = 0.05\n\nself.big_deg = 20.0\nself.big_rad = self.rad_per_deg * self.big_deg\nself.big_translate = 0.1\n</code></pre> Define the three rotation and translation step sizes.</p> <p><pre><code>self.voice_command = None\nself.sound_direction = 0\nself.speech_to_text_sub  = rospy.Subscriber(\"/speech_to_text\",  SpeechRecognitionCandidates, self.callback_speech)\nself.sound_direction_sub = rospy.Subscriber(\"/sound_direction\", Int32,                       self.callback_direction)\n</code></pre> Initialize the voice command and sound direction to values that will not result in moving the base.</p> <p>Set up two subscribers.  The first one subscribes to the topic /speech_to_text, looking for <code>SpeechRecognitionCandidates</code> messages.  When a message comes in, ROS is going to pass it to the function <code>callback_speech</code> automatically. The second subscribes to /sound_direction message and passes it to the <code>callback_direction</code> function.</p> <p><pre><code>self.sound_direction = msg.data * -self.rad_per_deg\n</code></pre> The <code>callback_direction</code> function converts the sound_direction topic from degrees to radians.</p> <p><pre><code>if self.step_size == 'small':\n    inc = {'rad': self.small_rad, 'translate': self.small_translate}\nif self.step_size == 'medium':\n    inc = {'rad': self.medium_rad, 'translate': self.medium_translate}\nif self.step_size == 'big':\n    inc = {'rad': self.big_rad, 'translate': self.big_translate}\nreturn inc\n</code></pre> The <code>callback_speech</code> stores the increment size for translational and rotational base motion to inc. The increment size is contingent on the self.step_size string value.</p> <p><pre><code>command = None\nif self.voice_command == 'forward':\n    command = {'joint': 'translate_mobile_base', 'inc': self.get_inc()['translate']}\nif self.voice_command == 'back':\n    command = {'joint': 'translate_mobile_base', 'inc': -self.get_inc()['translate']}\nif self.voice_command == 'left':\n    command = {'joint': 'rotate_mobile_base', 'inc': self.get_inc()['rad']}\nif self.voice_command == 'right':\n    command = {'joint': 'rotate_mobile_base', 'inc': -self.get_inc()['rad']}\nif self.voice_command == 'stretch':\n    command = {'joint': 'rotate_mobile_base', 'inc': self.sound_direction}\n</code></pre> In the <code>get_command()</code> function, the command is initialized as None, or set as a dictionary where the joint and inc values are stored. The command message type is dependent on the self.voice_command string value.</p> <p><pre><code>if (self.voice_command == \"small\") or (self.voice_command == \"medium\") or (self.voice_command == \"big\"):\n    self.step_size = self.voice_command\n    rospy.loginfo('Step size = {0}'.format(self.step_size))\n</code></pre> Based on the self.voice_command value, set the step size for the increments.</p> <p><pre><code>if self.voice_command == 'quit':\n    rospy.signal_shutdown(\"done\")\n    sys.exit(0)\n</code></pre> If the self.voice_command is equal to \"quit\", then initiate a clean shutdown of ROS and exit the Python interpreter.</p> <p><pre><code>class VoiceTeleopNode(hm.HelloNode):\n    \"\"\"\n    A class that inherits the HelloNode class from hm and sends joint trajectory\n    commands.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that declares object from the GetVoiceCommands class, instantiates\n        the HelloNode class, and set the publishing rate.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.__init__(self)\n        self.rate = 10.0\n        self.joint_state = None\n        self.speech = GetVoiceCommands()\n</code></pre> A class that inherits the <code>HelloNode</code> class from <code>hm</code>, declares object from the <code>GetVoiceCommands</code> class, and sends joint trajectory commands.</p> <p><pre><code>def send_command(self, command):\n    \"\"\"\n    Function that makes an action call and sends base joint trajectory goals\n    :param self: The self reference.\n    :param command: A dictionary type.\n    \"\"\"\n    joint_state = self.joint_state\n    if (joint_state is not None) and (command is not None):\n        point = JointTrajectoryPoint()\n        point.time_from_start = rospy.Duration(0.0)\n</code></pre> The <code>send_command</code> function stores the joint state message and uses a conditional statement to send joint trajectory goals. Also, assign point as a <code>JointTrajectoryPoint</code> message type.</p> <p><pre><code>trajectory_goal = FollowJointTrajectoryGoal()\ntrajectory_goal.goal_time_tolerance = rospy.Time(1.0)\n</code></pre> Assign trajectory_goal as a <code>FollowJointTrajectoryGoal</code> message type.</p> <p><pre><code>joint_name = command['joint']\ntrajectory_goal.trajectory.joint_names = [joint_name]\n</code></pre> Extract the joint name from the command dictionary.</p> <p><pre><code>inc = command['inc']\nrospy.loginfo('inc = {0}'.format(inc))\nnew_value = inc\n</code></pre> Extract the increment type from the command dictionary.</p> <p><pre><code>point.positions = [new_value]\ntrajectory_goal.trajectory.points = [point]\n</code></pre> Assign the new value position to the trajectory goal message type.</p> <p><pre><code>self.trajectory_client.send_goal(trajectory_goal)\nrospy.loginfo('Done sending command.')\n</code></pre> Make the action call and send goal of the new joint position.</p> <p><pre><code>self.speech.print_commands()\n</code></pre> Reprint the voice command menu after the trajectory goal is sent.</p> <p><pre><code>def main(self):\n      \"\"\"\n      The main function that instantiates the HelloNode class, initializes the subscriber,\n      and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes.\n      :param self: The self reference.\n      \"\"\"\n      hm.HelloNode.main(self, 'voice_teleop', 'voice_teleop', wait_for_first_pointcloud=False)\n      rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback)\n      rate = rospy.Rate(self.rate)\n      self.speech.print_commands()\n</code></pre> The main function instantiates the <code>HelloNode</code> class, initializes the subscriber, and call other methods in both the <code>VoiceTeleopNode</code> and <code>GetVoiceCommands</code> classes.</p> <p><pre><code>while not rospy.is_shutdown():\n  command = self.speech.get_command()\n  self.send_command(command)\n  rate.sleep()\n</code></pre> Run a while loop to continuously check speech commands and send those commands to execute an action.</p> <p><pre><code>try:\n  node = VoiceTeleopNode()\n  node.main()\nexcept KeyboardInterrupt:\n  rospy.loginfo('interrupt received, so shutting down')\n</code></pre> Declare a <code>VoiceTeleopNode</code> object. Then execute the <code>main()</code> method.</p>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/","title":"Follow joint trajectory","text":""},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#followjointtrajectory-commands","title":"FollowJointTrajectory Commands","text":"<p>Stretch ROS driver offers a <code>FollowJointTrajectory</code> action service for its arm. Within this tutorial we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute.</p>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#stow-command-example","title":"Stow Command Example","text":"<p>Begin by running the following command in the terminal in a terminal.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> Switch the mode to position mode using a rosservice call. Then run the stow command node.</p> <p><pre><code># Terminal 2\nrosservice call /switch_to_position_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython stow_command.py\n</code></pre> This will send a <code>FollowJointTrajectory</code> command to stow Stretch's arm.</p>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nimport time\n\nclass StowCommand(hm.HelloNode):\n  '''\n  A class that sends a joint trajectory goal to stow the Stretch's arm.\n  '''\n  def __init__(self):\n    hm.HelloNode.__init__(self)\n\n  def issue_stow_command(self):\n    '''\n    Function that makes an action call and sends stow position goal.\n    :param self: The self reference.\n    '''\n    stow_point = JointTrajectoryPoint()\n    stow_point.time_from_start = rospy.Duration(0.000)\n    stow_point.positions = [0.2, 0.0, 3.4]\n\n    trajectory_goal = FollowJointTrajectoryGoal()\n    trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n    trajectory_goal.trajectory.points = [stow_point]\n    trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n    trajectory_goal.trajectory.header.frame_id = 'base_link'\n\n    self.trajectory_client.send_goal(trajectory_goal)\n    rospy.loginfo('Sent stow goal = {0}'.format(trajectory_goal))\n    self.trajectory_client.wait_for_result()\n\n  def main(self):\n    '''\n    Function that initiates stow_command function.\n    :param self: The self reference.\n    '''\n    hm.HelloNode.main(self, 'stow_command', 'stow_command', wait_for_first_pointcloud=False)\n    rospy.loginfo('stowing...')\n    self.issue_stow_command()\n    time.sleep(2)\n\n\nif __name__ == '__main__':\n  try:\n    node = StowCommand()\n    node.main()\n  except KeyboardInterrupt:\n    rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a python script.</p> <p><pre><code>import rospy\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nimport time\n</code></pre> You need to import <code>rospy</code> if you are writing a ROS Node. Import the <code>FollowJointTrajectoryGoal</code> from the control_msgs.msg package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module the provides various Python scripts used across stretch_ros. In this instance we are importing the <code>hello_misc</code> script.</p> <p><pre><code>class StowCommand(hm.HelloNode):\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n</code></pre> The <code>StowCommand</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is initialized.</p> <p><pre><code>def issue_stow_command(self):\n    stow_point = JointTrajectoryPoint()\n    stow_point.time_from_start = rospy.Duration(0.000)\n    stow_point.positions = [0.2, 0.0, 3.4]\n</code></pre> The <code>issue_stow_command()</code> is the name of the function that will stow Stretch's arm. Within the function, we set stow_point as a <code>JointTrajectoryPoint</code>and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined in the next set of the code.</p> <p><pre><code>    trajectory_goal = FollowJointTrajectoryGoal()\n    trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n    trajectory_goal.trajectory.points = [stow_point]\n    trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n    trajectory_goal.trajectory.header.frame_id = 'base_link'\n</code></pre> Set trajectory_goal as a <code>FollowJointTrajectoryGoal</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> is defined by the positions set in stow_point. Specify the coordinate frame that we want (base_link) and set the time to be now.</p> <p><pre><code>self.trajectory_client.send_goal(trajectory_goal)\nrospy.loginfo('Sent stow goal = {0}'.format(trajectory_goal))\nself.trajectory_client.wait_for_result()\n</code></pre> Make the action call and send the goal. The last line of code waits for the result before it exits the python script.</p> <p><pre><code>def main(self):\n    hm.HelloNode.main(self, 'stow_command', 'stow_command', wait_for_first_pointcloud=False)\n    rospy.loginfo('stowing...')\n    self.issue_stow_command()\n    time.sleep(2)\n</code></pre> Create a funcion, <code>main()</code>, to do all of the setup the <code>hm.HelloNode</code> class and issue the stow command.</p> <p><pre><code>if __name__ == '__main__':\n    try:\n        node = StowCommand()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre> Declare object, node, from the <code>StowCommand()</code> class. Then run the <code>main()</code> function.</p>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#multipoint-command-example","title":"Multipoint Command Example","text":"<p>Begin by running the following command in the terminal in a terminal.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> Switch the mode to position mode using a rosservice call. Then run the multipoint command node.</p> <p><pre><code># Terminal 2\nrosservice call /switch_to_position_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython multipoint_command.py\n</code></pre> This will send a list of <code>JointTrajectoryPoint</code> message types to move Stretch's arm.</p>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nimport time\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\n\nclass MultiPointCommand(hm.HelloNode):\n  \"\"\"\n  A class that sends multiple joint trajectory goals to the stretch robot.\n  \"\"\"\n  def __init__(self):\n    hm.HelloNode.__init__(self)\n\n  def issue_multipoint_command(self):\n    \"\"\"\n    Function that makes an action call and sends multiple joint trajectory goals\n    to the joint_lift, wrist_extension, and joint_wrist_yaw.\n    :param self: The self reference.\n    \"\"\"\n    point0 = JointTrajectoryPoint()\n    point0.positions = [0.2, 0.0, 3.4]\n    point0.velocities = [0.2, 0.2, 2.5]\n    point0.accelerations = [1.0, 1.0, 3.5]\n\n    point1 = JointTrajectoryPoint()\n    point1.positions = [0.3, 0.1, 2.0]\n\n    point2 = JointTrajectoryPoint()\n    point2.positions = [0.5, 0.2, -1.0]\n\n    point3 = JointTrajectoryPoint()\n    point3.positions = [0.6, 0.3, 0.0]\n\n    point4 = JointTrajectoryPoint()\n    point4.positions = [0.8, 0.2, 1.0]\n\n    point5 = JointTrajectoryPoint()\n    point5.positions = [0.5, 0.1, 0.0]\n\n    trajectory_goal = FollowJointTrajectoryGoal()\n    trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n    trajectory_goal.trajectory.points = [point0, point1, point2, point3, point4, point5]\n    trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n    trajectory_goal.trajectory.header.frame_id = 'base_link'\n\n    self.trajectory_client.send_goal(trajectory_goal)\n    rospy.loginfo('Sent list of goals = {0}'.format(trajectory_goal))\n    self.trajectory_client.wait_for_result()\n\n  def main(self):\n    \"\"\"\n    Function that initiates the multipoint_command function.\n    :param self: The self reference.\n    \"\"\"\n    hm.HelloNode.main(self, 'multipoint_command', 'multipoint_command', wait_for_first_pointcloud=False)\n    rospy.loginfo('issuing multipoint command...')\n    self.issue_multipoint_command()\n    time.sleep(2)\n\n\nif __name__ == '__main__':\n  try:\n    node = MultiPointCommand()\n    node.main()\n  except KeyboardInterrupt:\n    rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code-explained_1","title":"The Code Explained","text":"<p>Seeing that there are similarities between the multipoint and stow command nodes, we will only breakdown the different components of the <code>multipoint_command</code> node.</p> <p><pre><code>point0 = JointTrajectoryPoint()\npoint0.positions = [0.2, 0.0, 3.4]\n</code></pre> Set point0 as a <code>JointTrajectoryPoint</code>and provide desired positions. These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. The lift and wrist extension positions are expressed in meters, where as the wrist yaw is in radians.</p> <p><pre><code>point0.velocities = [0.2, 0.2, 2.5]\n</code></pre> Provide desired velocity of the lift (m/s), wrist extension (m/s), and wrist yaw (rad/s) for point0.</p> <p><pre><code>point0.accelerations = [1.0, 1.0, 3.5]\n</code></pre> Provide desired accelerations of the lift (m/s^2), wrist extension (m/s^2), and wrist yaw (rad/s^2).</p> <p>IMPORTANT NOTE: The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated.</p> <p><pre><code>trajectory_goal = FollowJointTrajectoryGoal()\ntrajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\ntrajectory_goal.trajectory.points = [point0, point1, point2, point3, point4, point5]\ntrajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\ntrajectory_goal.trajectory.header.frame_id = 'base_link'\n</code></pre> Set trajectory_goal as a <code>FollowJointTrajectoryGoal</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> is defined by a list of the 6 points. Specify the coordinate frame that we want (base_link) and set the time to be now.</p>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#single-joint-actuator","title":"Single Joint Actuator","text":"<p>You can also actuate a single joint for the Stretch. Below are the list of joints and their position limit.  </p> <pre><code>############################# JOINT LIMITS #############################\njoint_lift:      lower_limit =  0.15,  upper_limit =  1.10  # in meters\nwrist_extension: lower_limit =  0.00,  upper_limit =  0.50  # in meters\njoint_wrist_yaw: lower_limit = -1.75,  upper_limit =  4.00  # in radians\njoint_head_pan:  lower_limit = -2.80,  upper_limit =  2.90  # in radians\njoint_head_tilt: lower_limit = -1.60,  upper_limit =  0.40  # in radians\njoint_gripper_finger_left:  lower_limit = -0.35,  upper_limit =  0.165  # in radians\n\n# INCLUDED JOINTS IN POSITION MODE\ntranslate_mobile_base: No lower or upper limit. Defined by a step size in meters\nrotate_mobile_base:    No lower or upper limit. Defined by a step size in radians\n########################################################################\n</code></pre> <p>Begin by running the following command in the terminal in a terminal.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> Switch the mode to position mode using a rosservice call. Then run the single joint actuator node.</p> <p><pre><code># Terminal 2\nrosservice call /switch_to_position_mode\ncd catkin_ws/src/stretch_tutorials/src/\npython single_joint_actuator.py\n</code></pre> This will send a list of <code>JointTrajectoryPoint</code> message types to move Stretch's arm.</p> <p>The joint, joint_gripper_finger_left, is only needed when actuating the gripper.</p>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code_2","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rospy\nimport time\nfrom control_msgs.msg import FollowJointTrajectoryGoal\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\n\nclass SingleJointActuator(hm.HelloNode):\n    \"\"\"\n    A class that sends multiple joint trajectory goals to a single joint.\n    \"\"\"\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n\n    def issue_command(self):\n        \"\"\"\n        Function that makes an action call and sends joint trajectory goals\n        to a single joint\n        :param self: The self reference.\n        \"\"\"\n        trajectory_goal = FollowJointTrajectoryGoal()\n        trajectory_goal.trajectory.joint_names = ['joint_head_pan']\n\n        point0 = JointTrajectoryPoint()\n        point0.positions = [0.65]\n\n        # point1 = JointTrajectoryPoint()\n        # point1.positions = [0.5]\n\n        trajectory_goal.trajectory.points = [point0]#, point1]\n        trajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n        self.trajectory_client.send_goal(trajectory_goal)\n        rospy.loginfo('Sent goal = {0}'.format(trajectory_goal))\n        self.trajectory_client.wait_for_result()\n\n    def main(self):\n        \"\"\"\n        Function that initiates the issue_command function.\n        :param self: The self reference.\n        \"\"\"\n        hm.HelloNode.main(self, 'issue_command', 'issue_command', wait_for_first_pointcloud=False)\n        rospy.loginfo('issuing command...')\n        self.issue_command()\n        time.sleep(2)\n\n\nif __name__ == '__main__':\n    try:\n        node = SingleJointActuator()\n        node.main()\n    except KeyboardInterrupt:\n        rospy.loginfo('interrupt received, so shutting down')\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code-explained_2","title":"The Code Explained","text":"<p>Since the code is quite similar to the <code>multipoint_command</code> code, we will only review the parts that differ.</p> <p>Now let's break the code down.</p> <p><pre><code>trajectory_goal = FollowJointTrajectoryGoal()\ntrajectory_goal.trajectory.joint_names = ['joint_head_pan']\n</code></pre> Here we only input joint name that we want to actuate. In this instance, we will actuate the joint_head_pan.</p> <p><pre><code>point0 = JointTrajectoryPoint()\npoint0.positions = [0.65]\n\n# point1 = JointTrajectoryPoint()\n# point1.positions = [0.5]\n</code></pre> Set point0 as a <code>JointTrajectoryPoint</code>and provide desired position. You also have the option to send multiple point positions rather than one.</p> <p><pre><code>trajectory_goal.trajectory.points = [point0]#, point1]\ntrajectory_goal.trajectory.header.stamp = rospy.Time(0.0)\ntrajectory_goal.trajectory.header.frame_id = 'base_link'\n</code></pre> Set trajectory_goal as a <code>FollowJointTrajectoryGoal</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> set by your list of points. Specify the coordinate frame that we want (base_link) and set the time to be now.</p>"},{"location":"stretch-tutorials/ros1_melodic/gazebo_basics/","title":"Spawning Stretch in Simulation (Gazebo)","text":""},{"location":"stretch-tutorials/ros1_melodic/gazebo_basics/#empty-world-simulation","title":"Empty World Simulation","text":"<p>To spawn the Stretch in gazebo's default empty world run the following command in your terminal. <pre><code>roslaunch stretch_gazebo gazebo.launch\n</code></pre> This will bringup the robot in the gazebo simulation similar to the image shown below.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/gazebo_basics/#custom-world-simulation","title":"Custom World Simulation","text":"<p>In gazebo, you are able to spawn Stretch in various worlds. First, source the gazebo world files by running the following command in a terminal <pre><code>echo \"source /usr/share/gazebo/setup.sh\"\n</code></pre></p> <p>Then using the world argument, you can spawn the stretch in the willowgarage world by running the following</p> <pre><code>roslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/getting_started/","title":"Getting Started","text":""},{"location":"stretch-tutorials/ros1_melodic/getting_started/#ubuntu","title":"Ubuntu","text":"<p>Hello Robot utilizes Ubuntu, an open-source Linux operating system, for the Stretch platform. If you are unfamiliar with the operating system, we encourage you to review a tutorial provided by Ubuntu. Additionally, the Linux command line, BASH, is used to execute commands and is needed to run ROS on the Stretch robot. Here is a tutorial on getting started with BASH.</p>"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#creating-workspace","title":"Creating Workspace","text":"<p>Create a catkin workspace for your ROS packages. Here is an installation guide for creating a workspace. Once your system is set up, clone the stretch_tutorials to your workspace and build the package in your workspace. This can be done by copying the commands below and pasting them into your terminal.</p> <pre><code>cd ~/catkin_ws/src\ngit clone https://github.com/hello-robot/stretch_tutorials.git\ncd ~/catkin_ws\ncatkin_make\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#connecting-a-monitor","title":"Connecting a Monitor","text":"<p>If you cannot access the robot through ssh due to your network settings, you will need to connect an HDMI monitor, USB keyboard, and mouse to the USB ports in the robot's trunk.</p>"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#ros-setup-on-local-computer","title":"ROS Setup on Local Computer","text":"<p>Hello Robot is currently running Stretch on Ubuntu 20.04 and ROS Noetic. To begin the setup, start with installing Ubuntu desktop on your local machine. Then follow the installation guide for ROS Noetic on your system.</p> <p>Currently, the Realsense2_description package isn't installed by rosdep and requires a user to install the package manually. Run the following command in your terminal</p> <pre><code>sudo apt-get install ros-noetic-realsense2-camera\n</code></pre> <p>After your system is setup, clone the stretch_ros, stretch_tutorials, and realsense_gazebo_plugin packages to your src folder in your preferred workspace. <pre><code>cd ~/catkin_ws/src\ngit clone https://github.com/hello-robot/stretch_ros\ngit clone https://github.com/pal-robotics/realsense_gazebo_plugin\ngit clone https://github.com/hello-robot/stretch_tutorials.git\n</code></pre></p> <p>Change the directory to that of your catkin workspace and install system dependencies of the ROS packages. Then build your workspace. <pre><code>cd ~/catkin_ws\nrosdep install --from-paths src --ignore-src -r -y\ncatkin_make\n</code></pre></p> <p>Once <code>caktin_make</code> has finished compiling,source your workspace and .bashrc file <pre><code>echo \"source /home/USER_NAME/catkin_ws/devel/setup.bash\"\nsource ~/.bashrc\n</code></pre></p>"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#robomaker","title":"RoboMaker","text":"<p>If you cannot dual boot and install ubuntu on your local machine, an alternative is to use AWS RoboMaker. AWS RoboMaker extends the ROS framework with cloud services. The service provides a robotics simulation service, allowing for testing the Stretch platform. If you are a first-time user of AWS RoboMaker, follow the guide here to get up and running with the service.</p>"},{"location":"stretch-tutorials/ros1_melodic/internal_state_of_stretch/","title":"Internal state of stretch","text":""},{"location":"stretch-tutorials/ros1_melodic/internal_state_of_stretch/#getting-the-state-of-the-robot","title":"Getting the State of the Robot","text":"<p>Begin by starting up the stretch driver launch file. <pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre></p> <p>Then utilize the ROS command-line tool, rostopic, to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a terminal. <pre><code># Terminal 2\nrostopic echo /joint_states -n1\n</code></pre> Note that the flag, <code>-n1</code>, at the end of the command defines the count of how many times you wish to publish the current topic information. Remove the flag if you prefer to continuously print the topic for debugging purposes.</p> <p>Your terminal will output the information associated with the <code>/joint_states</code> topic. Your <code>header</code>, <code>position</code>, <code>velocity</code>, and <code>effort</code> information may vary from what is printed below. <pre><code>header:\n  seq: 70999\n  stamp:\n    secs: 1420\n    nsecs:   2000000\n  frame_id: ''\nname: [joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left,\n  joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift,\n  joint_right_wheel, joint_wrist_yaw]\nposition: [-1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2.9291786329821434e-07, 1.3802900147297237e-06, 0.08154086954434359, 1.4361499260374905e-07, 0.4139061738340768, 9.32603306580404e-07]\nvelocity: [0.00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1.322424459109634e-05, -0.00035084643762840415, 0.0012164337445918797, 0.0002138814988808099, 0.00010419792027496809, 4.0575263146426684e-05, 0.00022487596895736357, -0.0007751929074042957, 0.0002451588607332439]\neffort: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n---\n</code></pre></p> <p>Let's say you are interested in only seeing the <code>header</code> component of the <code>/joint_states</code> topic, you can output this within the rostopic command-line tool by typing the following command. <pre><code># Terminal 2\nrostopic echo /joint_states/header -n1\n</code></pre> Your terminal will then output something similar to this:</p> <pre><code>seq: 97277\nstamp:\n  secs: 1945\n  nsecs: 562000000\nframe_id: ''\n---\n</code></pre> <p>Additionally, if you were to type <code>rostopic echo /</code> in the terminal, then press your Tab key on your keyboard, you will see the list of published active topics.</p> <p>A powerful tool to visualize the ROS communication is the ROS rqt_graph package. By typing the following, you can see a graph of topics being communicated between nodes.</p> <pre><code># Terminal 3\nrqt_graph\n</code></pre> <p> </p> <p>The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.</p>"},{"location":"stretch-tutorials/ros1_melodic/moveit_basics/","title":"MoveIt! Basics","text":"<p>&lt;!--</p>"},{"location":"stretch-tutorials/ros1_melodic/moveit_basics/#moveit-on-stretch","title":"MoveIt! on Stretch","text":"<p>To run MoveIt with the actual hardware, (assuming <code>stretch_driver</code> is already running) simply run</p> <pre><code>roslaunch stretch_moveit_config move_group.launch\n</code></pre> <p>This will runs all of the planning capabilities, but without the setup, simulation and interface that the above demo provides. In order to create plans for the robot with the same interface as the offline demo, you can run <pre><code>roslaunch stretch_moveit_config moveit_rviz.launch\n``` --&gt;\n\n## MoveIt! Without Hardware\nTo begin running MoveIt! on stretch, run the demo launch file. This doesn't require any simulator or robot to run.\n\n```bash\nroslaunch stretch_moveit_config demo.launch\n</code></pre> This will brining up an RViz instance where you can move the robot around using interactive markers and create plans between poses. You can reference the bottom gif as a guide to plan and execute motion.</p> <p><p> </p> <p>Additionally, the demo allows a user to select from the three groups, stretch_arm, stretch_gripper, stretch_head to move. The option to change groups in the in Planning Request section in the Displays tree. A few notes to be kept in mind:</p> <ul> <li> <p>Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning RViz plugin.</p> </li> <li> <p>stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin.</p> </li> <li> <p>When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning RViz plugin.</p> </li> </ul> <p> <p>Additionally, the demo allows a user to select from the three groups, stretch_arm, stretch_gripper, stretch_head to move. The option to change groups in the in Planning Request section in the Displays tree. A few notes to be kept in mind:</p> <ul> <li> <p>Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning RViz plugin.</p> </li> <li> <p>stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin.</p> </li> <li> <p>When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning RViz plugin.</p> </li> </ul> <p>   ## Running Gazebo with MoveIt! and Stretch   <pre><code># Terminal 1:\nroslaunch stretch_gazebo gazebo.launch\n# Terminal 2:\nroslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=keyboard # or use teleop_type:=joystick if you have a controller\n# Terminal 3\nroslaunch stretch_moveit_config demo_gazebo.launch\n</code></pre>   This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to [this moveit tutorial](https://ros-planning.github.io/moveit_tutorials/doc/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#choosing-specific-start-goal-states).   <p>"},{"location":"stretch-tutorials/ros1_melodic/moveit_basics/#running-gazebo-with-moveit-and-stretch","title":"Running Gazebo with MoveIt! and Stretch","text":"<pre><code># Terminal 1:\nroslaunch stretch_gazebo gazebo.launch\n# Terminal 2:\nroslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=keyboard # or use teleop_type:=joystick if you have a controller\n# Terminal 3\nroslaunch stretch_moveit_config demo_gazebo.launch\n</code></pre> <p>This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to this moveit tutorial.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/","title":"Navigation stack","text":""},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/#navigation-stack-with-actual-robot","title":"Navigation Stack with Actual robot","text":"<p>stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive Stretch around a mapped space. Running this code will require the robot to be untethered.</p> <p>Then run the following commands to map the space that the robot will navigate in. <pre><code>roslaunch stretch_navigation mapping.launch\n</code></pre> Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures.</p> <p> </p> <p>In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to <code>stretch_user/</code>.</p> <pre><code>mkdir -p ~/stretch_user/maps\nrosrun map_server map_saver -f ${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;\n</code></pre> <p>The <code>&lt;map_name&gt;</code> does not include an extension. Map_saver will save two files as <code>&lt;map_name&gt;.pgm</code> and <code>&lt;map_name&gt;.yaml</code>.</p> <p>Next, with <code>&lt;map_name&gt;.yaml</code>, we can navigate the robot around the mapped space. Run:</p> <pre><code>roslaunch stretch_navigation navigation.launch map_yaml:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p>Rviz will show the robot in the previously mapped space; however, the robot's location on the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place.</p> <p>It is also possible to send 2D Pose Estimates and Nav Goals programmatically. In your launch file, you may include <code>navigation.launch</code> to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages in order to navigate the robot programmatically.</p>"},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/#running-in-simulation","title":"Running in Simulation","text":"<p>To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the <code>mapping_gazebo.launch</code> and <code>navigation_gazebo.launch</code> launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch.</p> <pre><code># Terminal 1\nroslaunch stretch_navigation mapping_gazebo.launch gazebo_world:=worlds/willowgarage.world\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/#teleop-using-a-joystick-controller","title":"Teleop using a Joystick Controller","text":"<p>The mapping launch files, <code>mapping.launch</code> and <code>mapping_gazebo.launch</code> expose the ROS argument, \"teleop_type\". By default, this ROS arg is set to \"keyboard\", which launches keyboard teleop in the terminal. If the xbox controller that ships with Stretch is plugged into your computer, the following command will launch mapping with joystick teleop:</p> <pre><code># Terminal 2\nroslaunch stretch_navigation mapping.launch teleop_type:=joystick\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/#using-ros-remote-master","title":"Using ROS Remote Master","text":"<p>If you have set up ROS Remote Master for untethered operation, you can use Rviz and teleop locally with the following commands:</p> <pre><code># On Robot\nroslaunch stretch_navigation mapping.launch rviz:=false teleop_type:=none\n\n# On your machine, Terminal 1:\nrviz -d `rospack find stretch_navigation`/rviz/mapping.launch\n# On your machine, Terminal 2:\nroslaunch stretch_core teleop_twist.launch teleop_type:=keyboard # or use teleop_type:=joystick if you have a controller\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/perception/","title":"Perception","text":""},{"location":"stretch-tutorials/ros1_melodic/perception/#perception-introduction","title":"Perception Introduction","text":"<p>The Stretch robot is equipped with the Intel RealSense D435i camera, an essential component that allows the robot to measure and analyze the world around it. In this tutorial, we are going to showcase how to visualize the various topics published from the camera.</p> <p>Begin by running the stretch <code>driver.launch</code> file.</p> <pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code># Terminal 2\nroslaunch stretch_core d435i_low_resolution.launch\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code># Terminal 3\nrosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/perception/#pointcloud2-display","title":"PointCloud2 Display","text":"<p>A list of displays on the left side of the interface can visualize the camera data. Each display has its properties and status that notify a user if topic messages are received.</p> <p>For the <code>PointCloud2</code> display, a sensor_msgs/pointCloud2 message named /camera/depth/color/points, is received and the gif below demonstrates the various display properties when visualizing the data.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/perception/#image-display","title":"Image Display","text":"<p>The <code>Image</code> display when toggled creates a new rendering window that visualizes a sensor_msgs/Image messaged, /camera/color/image_raw. This feature shows the image data from the camera; however, the image comes out sideways. Thus, you can select the /camera/color/image_raw_upright_view from the Image Topic options to get an upright view of the image.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/perception/#camera-display","title":"Camera Display","text":"<p>The <code>Camera</code> display is similar to that of the <code>Image</code> display. In this setting, the rendering window also visualizes other displays, such as the PointCloud2, the RobotModel, and Grid Displays. The visibility property can toggle what displays your are interested in visualizing.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/perception/#depthcloud-display","title":"DepthCloud Display","text":"<p>The <code>DepthCloud</code> display is visualized in the main RViz window. This display takes in the depth image and RGB image, provided by the RealSense, to visualize and register a point cloud.</p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/perception/#deep-perception","title":"Deep Perception","text":"<p>Hello Robot also has a ROS package that uses deep learning models for various detection demos. A link to the package is provided: stretch_deep_perception.</p>"},{"location":"stretch-tutorials/ros1_melodic/respeaker_microphone_array/","title":"Respeaker microphone array","text":""},{"location":"stretch-tutorials/ros1_melodic/respeaker_microphone_array/#respeaker-microphone-array","title":"ReSpeaker Microphone Array","text":"<p>For this tutorial, we will go over on a high level how to use Stretch's ReSpeaker Mic Array v2.0.  </p> <p> </p>"},{"location":"stretch-tutorials/ros1_melodic/respeaker_microphone_array/#stretch-body-package","title":"Stretch Body Package","text":"<p>In this tutorial's section we will use command line tools in the Stretch_Body package, a low level Python API for Stretch's hardware, to directly interact with the ReSpeaker.</p> <p>Begin by typing the following command in a new terminal.</p> <pre><code>stretch_respeaker_test.py\n</code></pre> <p>The following will be displayed in your terminal <pre><code>hello-robot@stretch-re1-1005:~$ stretch_respeaker_test.py\nFor use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n\n* waiting for audio...\n* recording 3 seconds\n* done\n* playing audio\n* done\n</code></pre></p> <p>The ReSpeaker Mico Array will wait until it hears audio loud enough to trigger its recording feature. Stretch will record audio for 3 seconds and then replay it through its speakers. This command line is a good method to see if the hardware is working correctly.</p> <p>To stop the python script, type Ctrl + c in the terminal.</p>"},{"location":"stretch-tutorials/ros1_melodic/respeaker_microphone_array/#respeaker_ros-package","title":"ReSpeaker_ROS Package","text":"<p>A ROS package for the ReSpeaker is utilized for this tutorial's section.</p> <p>Begin by running the <code>sample_respeaker.launch</code> file in a terminal.</p> <p><pre><code># Terminal 1\nroslaunch respeaker_ros sample_respeaker.launch\n</code></pre> This will bring up the necessary nodes that will allow the ReSpeaker to implement a voice and sound interface with the robot.</p> <p>Below are executables you can run and see the ReSpeaker results.</p> <pre><code>rostopic echo /sound_direction    # Result of Direction (in Radians) of Audio\nrostopic echo /sound_localization # Result of Direction as Pose (Quaternion values)\nrostopic echo /is_speeching       # Result of Voice Activity Detector\nrostopic echo /audio              # Raw audio data\nrostopic echo /speech_audio       # Raw audio data when there is speech\nrostopic echo /speech_to_text     # Voice recognition\n</code></pre> <p>An example is when you run the <code>speech_to_text</code> executable and speak near the microphone array. In this instance, \"hello robot\" was said.</p> <pre><code># Terminal 2\nhello-robot@stretch-re1-1005:~$ rostopic echo /speech_to_text\ntranscript:\n  - hello robot\nconfidence: []\n---\n</code></pre> <p>You can also set various parameters via<code>dynamic_reconfigure</code> running the following command in a new terminal.</p> <pre><code># Terminal 3\nrosrun rqt_reconfigure rqt_reconfigure\n</code></pre>"},{"location":"stretch-tutorials/ros1_melodic/rviz_basics/","title":"Rviz basics","text":""},{"location":"stretch-tutorials/ros1_melodic/rviz_basics/#visualizing-with-rviz","title":"Visualizing with RViz","text":"<p>You can utilize RViz to visualize Stretch's sensor information. To begin, run the stretch driver launch file.</p> <pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.roslaunch\n</code></pre> <p>Then run the following command to bring up a simple RViz configuration of the Stretch robot. <pre><code># Terminal 2\nrosrun rviz rviz -d `rospack find stretch_core`/rviz/stretch_simple_test.rviz\n</code></pre> An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window.</p> <p> </p> <p>If you want the visualize Stretch's tf transform tree, you need to add the display type to the RViz window. First, click the Add button and include the TF  type in the display. You will then see all of the transform frames of the Stretch robot, and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference.</p> <p> </p> <p>There are further tutorials for RViz and can be found here.</p>"},{"location":"stretch-tutorials/ros1_melodic/rviz_basics/#running-rviz-and-gazebo-simulation","title":"Running RViz and Gazebo (Simulation)","text":"<p>Let's bringup stretch in the willowgarage world from the gazebo basics tutorial and RViz by using the following command.</p> <p><pre><code>roslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world rviz:=true\n</code></pre> the <code>rviz</code> flag will open an RViz window  to visualize a variety of ROS topics.</p> <p> </p> <p>Bringup the keyboard teleop to drive Stretch and observe its sensor input.</p>"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/","title":"Teleoperating stretch","text":""},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#teleoperating-stretch","title":"Teleoperating Stretch","text":""},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#xbox-controller-teleoperating","title":"Xbox Controller Teleoperating","text":"<p>Stretch comes ready to run out of the box. The Xbox Teleoperation demo will let you quickly test out the robot capabilities by teleoperating it with an Xbox Controller.</p> <p>Note: Make sure the USB Dongle is plugged into the the USB port of the base trunk.</p> <p>To start the demo:</p> <ul> <li>Remove the 'trunk' cover and power on the robot Wait for about 45 seconds. You will hear the Ubuntu startup sound, followed by two beeps (indicating the demo is running).</li> <li>Hit the Connect button on the controller. The upper two LEDs of the ring will illuminate.</li> <li>Hit the Home Robot button. Stretch will go through its homing calibration routine.</li> <li>Note: make sure the space around the robot is clear before running the Home function</li> </ul> <p>You're ready to go! A few things to try:</p> <ul> <li>Hit the Stow Robot button. The robot will assume the stow pose.</li> <li>Practice driving the robot around.</li> <li>Pull the Fast Base trigger while driving. When stowed, it will make Stretch drive faster</li> <li>Manually stop the arm or lift from moving to make it stop upon contact.</li> <li>Try picking up your cellphone from the floor</li> <li>Try grasping cup from a counter top</li> <li>Try delivering an object to a person</li> </ul> <p>If you're done, hold down the Shutdown PC button for 2 seconds. This will cause the PC to turn off. You can then power down the robot.</p>"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#keyboard-teleoperating-mobile-base","title":"Keyboard Teleoperating: Mobile Base","text":"<p>Begin by running the following command in your terminal:</p> <pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> <p>To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to nagivation for the robot to receive Twist messages. This is done using a rosservice call in a new terminal. In the same terminal run the teleop_twist_keyboard node with the argument remapping the cmd_vel topic name to stretch/cmd_vel.</p> <pre><code># Terminal 2\nrosservice call /switch_to_navigation_mode\nrosrun teleop_twist_keyboard teleop_twist_keyboard.py cmd_vel:=stretch/cmd_vel\n</code></pre> <p>Below are the keyboard commands that allow a user to move Stretch's base. <pre><code>Reading from the keyboard  and Publishing to Twist!\n---------------------------\nMoving around:\n   u    i    o\n   j    k    l\n   m    ,    .\n\nFor Holonomic mode (strafing), hold down the shift key:\n---------------------------\n   U    I    O\n   J    K    L\n   M    &lt;    &gt;\n\nt : up (+z)\nb : down (-z)\n\nanything else : stop\n\nq/z : increase/decrease max speeds by 10%\nw/x : increase/decrease only linear speed by 10%\ne/c : increase/decrease only angular speed by 10%\n\nCTRL-C to quit\n\ncurrently:  speed 0.5   turn 1.0\n</code></pre> To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#create-a-node-for-mobile-base-teleoperating","title":"Create a node for Mobile Base Teleoperating","text":"<p>To move Stretch's mobile base using a python script, please look at example 1 for reference.</p>"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#keyboard-teleoperating-full-body","title":"Keyboard Teleoperating: Full Body","text":"<p>For full body teleoperation with the keyboard, you first need to run the <code>stretch_driver.launch</code> in a terminal.</p> <p><pre><code># Terminal 1\nroslaunch stretch_core stretch_driver.launch\n</code></pre> Then in a new terminal, type the following command</p> <pre><code># Terminal 2\nrosrun stretch_core keyboard_teleop\n</code></pre> <p>Below are the keyboard commands that allow a user to control all of Stretch's joints. <pre><code>---------- KEYBOARD TELEOP MENU -----------\n\n              i HEAD UP                    \n j HEAD LEFT            l HEAD RIGHT       \n              , HEAD DOWN                  \n\n\n 7 BASE ROTATE LEFT     9 BASE ROTATE RIGHT\n home                   page-up            \n\n\n              8 LIFT UP                    \n              up-arrow                     \n 4 BASE FORWARD         6 BASE BACK        \n left-arrow             right-arrow        \n              2 LIFT DOWN                  \n              down-arrow                   \n\n\n              w ARM OUT                    \n a WRIST FORWARD        d WRIST BACK       \n              x ARM IN                     \n\n\n              5 GRIPPER CLOSE              \n              0 GRIPPER OPEN               \n\n  step size:  b BIG, m MEDIUM, s SMALL     \n\n              q QUIT                       \n\n-------------------------------------------\n</code></pre> To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#teleoperating-in-gazebo","title":"Teleoperating in Gazebo","text":""},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#keyboard-teleoperating-mobile-base_1","title":"Keyboard Teleoperating: Mobile Base","text":"<p>For keyboard teleoperation of the Stretch's mobile base, first startup Stretch in simulation. Then run the following command in a new terminal.</p> <pre><code># Terminal 1\nroslaunch stretch_gazebo gazebo.launch\n</code></pre> <p>In a new terminal, type the following</p> <p><pre><code># Terminal 2\nroslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=keyboard # or use teleop_type:=joystick if you have a controller\n</code></pre> The same keyboard commands will be presented to a user to move the robot.</p>"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#xbox-controller-teleoperating_1","title":"Xbox Controller Teleoperating","text":"<p>An alternative for robot base teleoperation is to use an Xbox controller. Stop the keyboard teleoperation node by typing Ctrl + c in the terminal where the command was executed. Then connect the Xbox controller device to your local machine and run the following command.</p> <p><pre><code># Terminal 2\nroslaunch stretch_core teleop_twist.launch twist_topic:=/stretch_diff_drive_controller/cmd_vel linear:=1.0 angular:=2.0 teleop_type:=joystick\n</code></pre> Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless pressed. For a Logitech F310 joystick, this button is A.</p>"},{"location":"stretch-tutorials/ros2/","title":"Overview","text":""},{"location":"stretch-tutorials/ros2/#tutorial-track-stretch-ros-2-beta","title":"Tutorial Track: Stretch ROS 2 (Beta)","text":"<p>Note</p> <p>Stretch's ROS2 packages and this ROS2 tutorial track are both under active development. They are considered 'beta', and we welcome any feedback. If you find any issues or bugs in the stretch_ros2 repository, please see the Stretch ROS2 and Stretch Tutorials issue trackers.</p>"},{"location":"stretch-tutorials/ros2/#robot-operating-system-2-ros-2","title":"Robot Operating System 2 (ROS 2)","text":"<p>Despite the name, ROS is not an operating system. ROS is a middleware framework that is a collection of transport protocols, development and debugging tools, and open-source packages. As a transport protocol, ROS enables distributed communication via messages between nodes. As a development and debugging toolkit, ROS provides build systems that allow for writing applications in a wide variety of languages (Python and C++ are used in this tutorial track), a launch system to manage the execution of multiple nodes simultaneously, and command line tools to interact with the running system. Finally, as a popular ecosystem, there are many open-source ROS packages that allow users to quickly prototype with new sensors, actuators, planners, perception stacks, and more.</p> <p>This tutorial track is for users looking to get familiar with programming Stretch robots via ROS 2. We recommend going through the tutorials in the following order:</p>"},{"location":"stretch-tutorials/ros2/#basics","title":"Basics","text":"Tutorial Description 1 Getting Started Setup instructions for ROS 2 on Stretch 2 Introduction to ROS 2 Explore the client library used in ROS2 3 Introduction to HelloNode Explore the Hello Node class to create a ROS2 node for Stretch 4 Teleoperating Stretch Control Stretch with a Keyboard or a Gamepad controller. 5 Internal State of Stretch Monitor the joint states of Stretch. 6 RViz Basics Visualize topics in Stretch. 7 Nav2 Stack Motion planning and control for mobile base. 8 Follow Joint Trajectory Commands Control joints using joint trajectory server. 9 Perception Use the Realsense D435i camera to visualize the environment. 10 ArUco Marker Detection Localize objects using ArUco markers. 11 ReSpeaker Microphone Array Learn to use the ReSpeaker Microphone Array. 12 FUNMAP Fast Unified Navigation, Manipulation and Planning."},{"location":"stretch-tutorials/ros2/#other-examples","title":"Other Examples","text":"<p>To help get you started on your software development, here are examples of nodes to have Stretch perform simple tasks.</p> Tutorial Description 1 Mobile Base Velocity Control Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Print Joint States Print the joint states of Stretch. 6 Store Effort Values Print, store, and plot the effort values of the Stretch robot. 7 Capture Image Capture images from the RealSense camera data. 8 Voice to Text Interpret speech and save transcript to a text file. 9 Voice Teleoperation of Base Use speech to teleoperate the mobile base. 10 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 11 ArUco Tag Locator Actuate the head to locate a requested ArUco marker tag and return a transform. 12 Obstacle Avoider Avoid obstacles using the planar lidar. 13 Align to ArUco Detect ArUco fiducials using OpenCV and align to them. 14 Deep Perception Use YOLOv5 to detect 3D objects in a point cloud. 15 Avoiding Race Conditions and Deadlocks Learn how to avoid Race Conditions and Deadlocks"},{"location":"stretch-tutorials/ros2/align_to_aruco/","title":"Align to ArUco","text":"<p>ArUco markers are a type of fiducials that are used extensively in robotics for identification and pose estimation. In this tutorial we will learn how to identify ArUco markers with the ArUco detection node and enable Stretch to navigate and align itself with respect to the marker.</p>"},{"location":"stretch-tutorials/ros2/align_to_aruco/#aruco-detection","title":"ArUco Detection","text":"<p>Stretch uses the OpenCV ArUco detection library and is configured to identify a specific set of ArUco markers belonging to the 6x6, 250 dictionary. To understand why this is important, please refer to this handy guide provided by OpenCV.</p> <p>Stretch comes preconfigured to identify ArUco markers. The ROS node that enables this is the detect_aruco_markers node in the stretch_core package. Thanks to this node, identifying and estimating the pose of a marker is as easy as pointing the camera at the marker and running the detection node. It is also possible and quite convenient to visualize the detections with RViz.</p>"},{"location":"stretch-tutorials/ros2/align_to_aruco/#computing-transformations","title":"Computing Transformations","text":"<p>If you have not already done so, now might be a good time to review the tf listener tutorial. Go on, we can wait\u2026 Now that we know how to program stretch to return the transform between known reference frames, we can use this knowledge to compute the transform between the detected marker and the robot base_link. From its current pose, for Stretch to align itself in front of the marker, we need to command it to reach there. But even before that, we need to program Stretch to know the goal pose. We define the goal pose to be 0.5 metre outward from the marker in the marker negative y-axis (Green axis). This is easier to visualize through the figure below.</p> <p> </p> <p>By monitoring the /aruco/marker_array and /aruco/axes topics, we can visualize the markers in RViz. The detection node also publishes the tf pose of the detected markers. This can be visualized by using the TF plugin and selecting the detected marker to inspect the pose. Next, we will use exactly that to compute the transform between the detected marker and the base_link of the robot.</p> <p>Now, we can compute the transformation from the robot base_link frame to the goal pose and pass this as an SE2 pose to the mobile base.</p> <p>Since we want Stretch to stop at a fixed distance with respect to the marker, we define a 0.5m offset in the marker y-axis where Stretch would come to a stop. At the same time, we also want Stretch to align its orientation to point its arm towards the marker so as to make the subsequent manipulation tasks easier to accomplish. This would result in the end pose of the base_link as shown in the above figure. Sweet! The next task is to generate a simple motion plan for the mobile base to reach this end pose. We do this in three steps: 1. Turn theta degrees towards the goal position. This would be the angle formed between the robot x-axis and the line connecting the start and the goal positions. 2. Travel straight to the goal position. This would be the euclidean distance between the start and the goal positions. 3. Turn phi degrees to attain the goal orientation. This would be the correction angle necessary to align the robot y-axis with the marker x-axis.</p> <p>Luckily, we know how to command Stretch to execute a trajectory using the joint trajectory server. If you are just starting, have a look at the Follow Joint Trajectory Commands tutorial to know how to command Stretch using the Joint trajectory Server.</p> <p>Warning</p> <p>Since we won't be using the arm for this demo, it's safer to stow Stretch's arm in.</p> <pre><code>stretch_robot_stow.py\n</code></pre>"},{"location":"stretch-tutorials/ros2/align_to_aruco/#see-it-in-action","title":"See It In Action","text":"<p>First, we need to point the camera towards the marker. To do this, you could use the keyboard teleop node. To do this, run: <pre><code>ros2 launch stretch_core keyboard_teleop.launch.py\n</code></pre></p> <p>When you are ready, execute the following command: <pre><code>ros2 launch stretch_core align_to_aruco.launch.py\n</code></pre></p> <p> </p>"},{"location":"stretch-tutorials/ros2/align_to_aruco/#code-breakdown","title":"Code Breakdown","text":"<p>Let's jump into the code to see how things work under the hood. Follow along here to have a look at the entire script.</p> <p>We make use of two separate Python classes for this demo. The FrameListener class is derived from the Node class and is the place where we compute the TF transformations. For an explantion of this class, you can refer to the TF listener tutorial. <pre><code>class FrameListener(Node):\n</code></pre></p> <p>The AlignToAruco class is where we command Stretch to the pose goal. This class is derived from the FrameListener class so that they can both share the node instance. <pre><code>class AlignToAruco(FrameListener):\n</code></pre></p> <p>The constructor initializes the Joint trajectory action client. It also initializes the attribute called offset that determines the end distance between the marker and the robot. <pre><code>    def __init__(self, node, offset=0.75):\n        self.trans_base = TransformStamped()\n        self.trans_camera = TransformStamped()\n        self.joint_state = JointState()\n        self.offset = offset\n        self.node = node\n\n        self.trajectory_client = ActionClient(self.node, FollowJointTrajectory, '/stretch_controller/follow_joint_trajectory')\n        server_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\n        if not server_reached:\n            self.node.get_logger().error('Unable to connect to arm action server. Timeout exceeded.')\n            sys.exit()\n</code></pre></p> <p>The joint_states_callback is the callback method that receives the most recent joint state messages published on the /stretch/joint_states topic. <pre><code>    def joint_states_callback(self, joint_state):\n        self.joint_state = joint_state\n</code></pre></p> <p>The compute_difference() method is where we call the get_transform() method from the FrameListener class to compute the difference between the base_link and base_right frame with an offset of 0.5 m in the negative y-axis. <pre><code>    def compute_difference(self):\n        self.trans_base, self.trans_camera = self.node.get_transforms()\n</code></pre></p> <p>To compute the (x, y) coordinates of the SE2 pose goal, we compute the transformation here. <pre><code>        R = quaternion_matrix((x, y, z, w))\n        P_dash = np.array([[0], [-self.offset], [0], [1]])\n        P = np.array([[self.trans_base.transform.translation.x], [self.trans_base.transform.translation.y], [0], [1]])\n\n        X = np.matmul(R, P_dash)\n        P_base = X + P\n\n        base_position_x = P_base[0, 0]\n        base_position_y = P_base[1, 0]\n</code></pre></p> <p>From this, it is relatively straightforward to compute the angle phi and the euclidean distance dist. We then compute the angle z_rot_base to perform the last angle correction. <pre><code>        phi = atan2(base_position_y, base_position_x)\n\n        dist = sqrt(pow(base_position_x, 2) + pow(base_position_y, 2))\n\n        x_rot_base, y_rot_base, z_rot_base = euler_from_quaternion([x, y, z, w])\n        z_rot_base = -phi + z_rot_base + 3.14159\n</code></pre></p> <p>The align_to_marker() method is where we command Stretch to the pose goal in three steps using the Joint Trajectory action server. For an explanation on how to form the trajectory goal, you can refer to the Follow Joint Trajectory Commands tutorial. <pre><code>    def align_to_marker(self):\n</code></pre></p> <p>If you want to work with a different ArUco marker than the one we used in this tutorial, you can do so by changing line 44 in the code to the one you wish to detect. Also, don't forget to add the marker in the stretch_marker_dict.yaml ArUco marker dictionary.</p>"},{"location":"stretch-tutorials/ros2/aruco_marker_detection/","title":"ArUco Marker Detection","text":""},{"location":"stretch-tutorials/ros2/aruco_marker_detection/#aruco-marker-detector","title":"ArUco Marker Detector","text":"<p>For this tutorial, we will go over how to detect Stretch's ArUco markers and review the files that hold the information for the tags.</p>"},{"location":"stretch-tutorials/ros2/aruco_marker_detection/#visualize-aruco-markers-in-rviz","title":"Visualize ArUco Markers in RViz","text":"<p>Begin by running the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>ros2 launch stretch_core d435i_high_resolution.launch.py\n</code></pre> <p>Next, in a new terminal, run the stretch ArUco launch file which will bring up the detect_aruco_markers node.</p> <pre><code>ros2 launch stretch_core stretch_aruco.launch.py\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>ros2 run rviz2 rviz2 -d /home/hello-robot/ament_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz\n</code></pre> <p>You are going to need to teleoperate Stretch's head to detect the ArUco marker tags. Run the following command in a new terminal and control the head to point the camera toward the markers.   </p> <pre><code>ros2 run stretch_core keyboard_teleop\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros2/aruco_marker_detection/#the-aruco-marker-dictionary","title":"The ArUco Marker Dictionary","text":"<p>When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml, that holds the information about the markers.</p> <p>If detect_aruco_markers node doesn\u2019t find an entry in stretch_marker_dict.yaml for a particular ArUco marker ID number, it uses the default entry. For example, most robots have shipped with the following default entry:</p> <pre><code>'default':\n  'length_mm': 24\n  'use_rgb_only': False\n  'name': 'unknown'\n  'link': None\n</code></pre> <p>and the following entry for the ArUco marker on the top of the wrist</p> <pre><code>'133':\n  'length_mm': 23.5\n  'use_rgb_only': False\n  'name': 'wrist_top'\n  'link': 'link_aruco_top_wrist'\n</code></pre> <p>Dictionary Breakdown</p> <pre><code>'133':\n</code></pre> <p>The dictionary key for each entry is the ArUco marker\u2019s ID number or <code>default</code>. For example, the entry shown above for the ArUco marker on the top of the wrist assumes that the marker\u2019s ID number is <code>133</code>.</p> <pre><code>'length_mm': 23.5\n</code></pre> <p>The <code>length_mm</code> value used by detect_aruco_markers is important for estimating the pose of an ArUco marker.</p> <p>Note</p> <p>If the actual width and height of the marker do not match this value, then pose estimation will be poor. Thus, carefully measure custom Aruco markers.</p> <pre><code>'use_rgb_only': False\n</code></pre> <p>If <code>use_rgb_only</code> is <code>True</code>, detect_aruco_markers will ignore depth images from the Intel RealSense D435i depth camera when estimating the pose of the marker and will instead only use RGB images from the D435i.</p> <pre><code>'name': 'wrist_top'\n</code></pre> <p><code>name</code> is used for the text string of the ArUco marker\u2019s ROS Marker in the ROS MarkerArray Message published by the detect_aruco_markers ROS node.</p> <pre><code>'link': 'link_aruco_top_wrist'\n</code></pre> <p><code>link</code> is currently used by stretch_calibration. It is the name of the link associated with a body-mounted ArUco marker in the robot\u2019s URDF.</p> <p>It\u2019s good practice to add an entry to stretch_marker_dict.yaml for each ArUco marker you use.</p>"},{"location":"stretch-tutorials/ros2/aruco_marker_detection/#create-a-new-aruco-marker","title":"Create a New ArUco Marker","text":"<p>At Hello Robot, we\u2019ve used the following guide when generating new ArUco markers.</p> <p>We generate ArUco markers using a 6x6-bit grid (36 bits) with 250 unique codes. This corresponds with DICT_6X6_250 defined in OpenCV. We generate markers using this online ArUco marker generator by setting the Dictionary entry to 6x6 and then setting the Marker ID and Marker size, mm as appropriate for the specific application. We strongly recommend measuring the actual marker by hand before adding an entry for it to stretch_marker_dict.yaml.</p> <p>We select marker ID numbers using the following ranges.</p> <ul> <li>0 - 99: reserved for users</li> <li>100 - 249: reserved for official use by Hello Robot Inc.</li> <li>100 - 199: reserved for robots with distinct sets of body-mounted markers<ul> <li>Allows different robots near each other to use distinct sets of body-mounted markers to avoid confusion. This could be valuable for various uses of body-mounted markers, including calibration, visual servoing, visual motion capture, and multi-robot tasks.</li> <li>5 markers per robot = 2 on the mobile base + 2 on the wrist + 1 on the shoulder</li> <li>20 distinct sets = 100 available ID numbers / 5 ID numbers per robot</li> </ul> </li> <li>200 - 249: reserved for official accessories<ul> <li>245 for the prototype docking station</li> <li>246-249 for large floor markers</li> </ul> </li> </ul> <p>When coming up with this guide, we expected the following:</p> <ul> <li>Body-mounted accessories with the same ID numbers mounted to different robots could be disambiguated using the expected range of 3D locations of the ArUco markers on the calibrated body.</li> <li>Accessories in the environment with the same ID numbers could be disambiguated using a map or nearby observable features of the environment.</li> </ul>"},{"location":"stretch-tutorials/ros2/avoiding_deadlocks_race_conditions/","title":"Avoiding Race Conditions and Deadlocks","text":"<p>ROS 2 was created with real time safety in mind. Some of the features of ROS 2, although exposed for the convenience of users, can get in the way if one is not aware about them.</p>"},{"location":"stretch-tutorials/ros2/avoiding_deadlocks_race_conditions/#race-conditions","title":"Race conditions","text":"<p>In parallel programming, race conditions can occur when data is being written to and read from a shared memory space concurrently by two threads. This creates a condition where a thread that\u2019s reading from the shared memory gets conflicting data. Such conditions are often difficult to debug as it requires one to introspect multiple threads. Hence, it is always better to recognize and avoid common pitfalls that lead to race conditions.</p>"},{"location":"stretch-tutorials/ros2/avoiding_deadlocks_race_conditions/#deadlocks","title":"Deadlocks","text":"<p>In parallel programming, deadlocks occur when multiple threads try to gain access to the same system resource at the same time. In the context of ROS 2, this can occur when a resource like the executor which is shared by multiple callback methods is asked to service more than one callback concurrently. Again, this is another pitfall that\u2019s difficult to debug as it often manifests itself with nodes becoming unresponsive without any apparent error message. Hence, it is always better to recognize and avoid common pitfalls that lead to deadlocks.</p>"},{"location":"stretch-tutorials/ros2/avoiding_deadlocks_race_conditions/#executor","title":"Executor","text":"<p>We had a brief introduction to the ROS 2 executor in the previous tutorial on Introduction to ROS 2. We looked at ways to invoke the executor with various spin methods. At a high level, the job of the executor is to service callback messages as they arrive and process and relay messages in the callback. In parallel programming, sometimes you might want parts of a process to run concurrently to avoid deadlocks, or in some cases, you might not want certain parts of a process to run concurrently to avoid race conditions. Fortunately, you can control this by defining the executor model.</p> <p>An executor is defined as a SingleThreadedExecutor by default which is perfect for servicing fast running callbacks sequentially. However, if parallelism is desired, an executor can be defined as a MultiThreadedExecutor. This allows long running callbacks to run in parallel with fast running ones. A custom executor model can also be defined, but that is beyond the scope of this tutorial.</p>"},{"location":"stretch-tutorials/ros2/avoiding_deadlocks_race_conditions/#callback-groups","title":"Callback groups","text":"<p>A callback is the method that receives a ROS message and is where a ROS message is processed. Depending on the kind of data that\u2019s being worked upon, a callback method could finish executing in virtually no time or take several seconds to process. Several seconds is a long time in programming and the longer a callback method takes, the longer the next callback has to wait for the executor to service it.</p> <p>With a MultiThreadedExecutor, callbacks can be serviced in parallel. However, this makes the process prone to race conditions as multiple threads work on the same shared memory. Fortunately, this can be avoided by defining callback groups. Grouping callbacks such that the ones that deal with the same shared memory space never execute concurrently ensures that data doesn\u2019t get corrupted due to race conditions.</p> <p>There are two different kinds of callback groups available. A MutuallyExclusiveCallbackGroup, the default, ensures that the callbacks belonging to this group never execute in parallel. You would use this when two callbacks access and write to the same shared memory space and having them execute them together would result in a race condition. A ReentrantCallbackGroup ensures that callbacks belonging to this group are able to execute parallelly. You would use this when a long running callback occupies the bulk of the executors time and you want shorter fast running callbacks to run in parallel.</p> <p>Now, let\u2019s explore what we have learned so far in the form of a real example. You can try them by creating a Python file and run it in your terminal as <code>python3 FILE_NAME</code>.</p>"},{"location":"stretch-tutorials/ros2/avoiding_deadlocks_race_conditions/#race-condition-example","title":"Race Condition Example","text":"<p>It is instructive to see an example code that generates a race condition. The below code simulates a race condition by defining two subscriber callbacks that write and read from shared memory simultaneously.</p> <p>Note</p> <p>Before executing the Race Condition Example you first need to launch the stretch core driver as ros2 launch stretch_core stretch_driver.launch.py</p> <pre><code>import rclpy\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import JointState\nfrom rclpy.executors import MultiThreadedExecutor\nfrom rclpy.executors import SingleThreadedExecutor\nfrom rclpy.callback_groups import MutuallyExclusiveCallbackGroup\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.node import Node\nimport time\n\nclass ParallelExec(Node):\n    def __init__(self, cb_group):\n        super().__init__('parallel_execution')\n        self.shared_memory = True\n        self.joint_state = JointState()\n        self.mode = String()\n        self.cb_group = cb_group\n\n    def callback_one(self, msg):\n        self.joint_states = msg\n        if not self.shared_memory:\n            time.sleep(0.2)\n            self.get_logger().info(\"Switching from {} to True\".format(self.shared_memory))\n            self.shared_memory = True\n\n    def callback_two(self, msg):\n        self.mode = msg\n        if self.shared_memory:\n            time.sleep(0.2)\n            self.get_logger().info(\"Switching from {} to False\".format(self.shared_memory))\n            self.shared_memory = False\n\n    def main(self):         \n        sub_joint_states = self.create_subscription(JointState, '/stretch/joint_states', self.callback_one, 1, callback_group=self.cb_group)\n        sub_mode = self.create_subscription(String, 'mode', self.callback_two, 1, callback_group=self.cb_group)\n\nif __name__ == '__main__':\n    rclpy.init()\n\n    executor = None\n    cb_group = None\n\n    x = input(\"Select the executor model: Press 1 for SingleThreadedExecutor(); Press 2 for MultiThreadedExecutor()\")\n    if x == '1':\n        print(\"Using single-threaded execution\")\n        executor = SingleThreadedExecutor()\n    elif x == '2':\n        print(\"Using multi-threaded execution\")\n        executor = MultiThreadedExecutor(num_threads=2)\n\n    y = input(\"Select the callback group: Press 1 for MutuallyExclusiveCallbackGroup(); Press 2 for ReentrantCallbackGroup()\")\n    if y == '1':\n        print(\"Processing callbacks one after the other\")\n        cb_group = MutuallyExclusiveCallbackGroup()\n    elif y == '2':\n        print(\"Processing callbacks in parallel\")\n        cb_group = ReentrantCallbackGroup()\n\n    node = ParallelExec(cb_group)\n    node.main()\n\n    executor.add_node(node)\n    executor.spin()\n\n    node.destroy_node()\n    rclpy.shutdown()\n</code></pre> <p>Executing the above script, the expected behavior is to see the logged statements conform to the conditional statements in the callback methods. To elaborate, callback_one() receives joint_state messages and should print \"Switching from False to True\" only if shared_memory is False. Whereas callback_two() receives mode messages and should print \"Switching from True to False\" only if shared_memory is True. However, without setting up protection against race conditions we see that this is not what ends up happening.</p> <p>Executing the above code, you are presented with two prompts, first to select the executor, either a SingleThreadedExecutor or a MultiThreadedExecutor; and then to select a callback group type, either a MutuallyExclusiveCallbackGroup or a ReentrantCallbackGroup.</p> <p>Selecting a SingleThreadedExecutor, irrespective of which callback group is selected, results in callbacks being executed sequentially. This is because the executor is spun using a single thread that can only service one callback at a time. In this case, we see that there is no memory corruption and the observed behavior is the same as the expected behavior.</p> <p>Things get interesting when we choose the MultiThreadedExecutor along with a ReentrantCallbackGroup. Multiple threads are used by the executor to service callbacks, while callbacks are allowed to execute in parallel. This allows multiple threads to access the same memory space and execute read/write operations. The observed behavior is that, sometimes you see the callbacks print statements like  \"Switching from True to True\" or \"Switching from False to False\" which go against the conditions set in the callbacks. This is a race condition.</p> <p>Selecting a MultiThreadedExecutor along with a MutuallyExclusiveCallbackGroup allows us to circumvent this problem by using parallelism but still protecting shared memory from race conditions.</p>"},{"location":"stretch-tutorials/ros2/avoiding_deadlocks_race_conditions/#deadlock-example","title":"Deadlock Example","text":"<p>A great example of a deadlock is provided in the official ROS 2 documentation on sync deadlock, so this example will directly build off of the same code. The server side defines a callback method add_two_ints_callback() which returns the sum of two requested numbers. Notice the call to spin in the main() method which persistently executes the callback method as a service request arrives. For the requested numbers you will need to input them in the terminal manually.</p> <pre><code>from example_interfaces.srv import AddTwoInts\n\nimport rclpy\nfrom rclpy.node import Node\n\nclass MinimalService(Node):\n    def __init__(self):\n        super().__init__('minimal_service')\n        self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\n\n    def add_two_ints_callback(self, request, response):\n        response.sum = request.a + request.b\n        self.get_logger().info('Incoming request\\na: %d b: %d' % (request.a, request.b))\n        return response\n\ndef main():\n    rclpy.init()\n    minimal_service = MinimalService()\n    rclpy.spin(minimal_service)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Now let's look at the client side. This script makes a synchronous service call to the add_two_ints service. </p> <pre><code>import sys\nfrom threading import Thread\n\nfrom example_interfaces.srv import AddTwoInts\nimport rclpy\nfrom rclpy.node import Node\n\nclass MinimalClientSync(Node):\n    def __init__(self):\n        super().__init__('minimal_client_sync')\n        self.cli = self.create_client(AddTwoInts, 'add_two_ints')\n        while not self.cli.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info('service not available, waiting again...')\n        self.req = AddTwoInts.Request()\n\n    def send_request(self):\n        self.req.a = int(sys.argv[1])\n        self.req.b = int(sys.argv[2])\n        return self.cli.call(self.req)\n\ndef main():\n    rclpy.init()\n    minimal_client = MinimalClientSync()\n    response = minimal_client.send_request()\n    minimal_client.get_logger().info(\n        'Result of add_two_ints: for %d + %d = %d' %\n        (minimal_client.req.a, minimal_client.req.b, response.sum))\n\n    rclpy.spin()\n    minimal_client.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Every call to a service expects a response which is a ROS message that requires a callback to listen to. Although not explicitly defined, this callback method is created when the client is initialized. Like any other callback method, the executor needs to call the callback method for the response to be received. Notice that the above example only invokes the executor (with rclpy.spin()) after the synchronous service call has been issued through <code>self.cli.call(self.req)</code> in the send_request() method. This creates a deadlock because the execution can't move forward until a response is received, but the executor has not been invoked yet to service the response callback method.</p> <p>The way to get around this is to invoke the executor in a separate thread before the synchronous service call is made. This way, the response callback is called in a separate thread from the main execution thread. Here's how to do this:</p> <pre><code>def main():\n    rclpy.init()\n    minimal_client = MinimalClientSync()\n\n    # Invoke the executor in a separate thread\n    spin_thread = Thread(target=rclpy.spin, args=(minimal_client,))\n    spin_thread.start()\n\n    response = minimal_client.send_request()\n    minimal_client.get_logger().info(\n        'Result of add_two_ints: for %d + %d = %d' %\n        (minimal_client.req.a, minimal_client.req.b, response.sum))\n    minimal_client.destroy_node()\n    rclpy.shutdown()\n</code></pre> <p>An alternative to this, a feature that's new to ROS 2, is to use an asynchronous service call. This allows one to monitor the response through what's called a future object in ROS 2. The future holds the status of whether a service call was accepted by the server and also the returned response. Since the future is returned immediately on making an async service call, the execution is not blocked and the executor can be invoked in the main execution thread. Here's how to do it:</p> <pre><code>class MinimalClientAsync(Node):\n\n    ...\n\n    def send_request(self, a, b):\n        self.req.a = a\n        self.req.b = b\n\n        # Make an async call and wait until the executor receives a response\n        self.future = self.cli.call_async(self.req)\n        rclpy.spin_until_future_complete(self, self.future)\n        return self.future.result()\n\ndef main():\n    rclpy.init()\n\n    minimal_client = MinimalClientAsync()\n\n    # The response is a future object which can be queried to get the result\n    response = minimal_client.send_request(int(sys.argv[1]), int(sys.argv[2]))\n    minimal_client.get_logger().info(\n        'Result of add_two_ints: for %d + %d = %d' %\n        (int(sys.argv[1]), int(sys.argv[2]), response.sum))\n\n    minimal_client.destroy_node()\n    rclpy.shutdown()\n</code></pre>"},{"location":"stretch-tutorials/ros2/coming_soon/","title":"Coming soon","text":"<p> ROS 2 tutorials are still under active development. Coming soon.</p>"},{"location":"stretch-tutorials/ros2/deep_perception/","title":"Deep Perception","text":"<p>Ever wondered if there is a way to make a robot do awesome things without explicitly having to program it to do so? Deep Perception is a branch of Deep Learning that enables sensing the elements that make up an environment with the help of artificial neural networks without writing complicated code. Well, almost. The most wonderful thing about Stretch is that it comes preloaded with software that makes it a breeze to get started with topics such as Deep Learning. In this tutorial, we will deploy deep neural networks on Stretch using two popular Deep Learning frameworks, namely, PyTorch and OpenVino.</p>"},{"location":"stretch-tutorials/ros2/deep_perception/#yolov5-with-pytorch","title":"YOLOv5 with PyTorch","text":"<p>PyTorch is an open-source end-to-end machine learning framework that makes many pretrained production-quality neural networks available for general use. In this tutorial, we will use the YOLOv5s model trained on the COCO dataset.</p> <p>YOLOv5 is a popular object detection model that divides a supplied image into a grid and detects objects in each cell of the grid recursively. The YOLOv5s model that we have deployed on Stretch has been pretrained on the COCO dataset which allows Stretch to detect a wide range of day-to-day objects. However, that\u2019s not all, in this demo we want to go a step further and use this extremely versatile object detection model to extract useful information about the scene.</p>"},{"location":"stretch-tutorials/ros2/deep_perception/#extracting-bounding-boxes-and-depth-information","title":"Extracting Bounding Boxes and Depth Information","text":"<p>Often, it\u2019s not enough to simply identify an object. Stretch is a mobile manipulator and its job is to manipulate objects in its environment. But before it can do that, it needs information of where exactly the object is located with respect to itself so that a motion plan to reach the object can be generated. This is possible by knowing which pixels correspond to the object of interest in the image frame and then using that to extract the depth information in the camera frame. Once we have this information, it is possible to compute a transform of these points in the end effector frame for Stretch to generate a motion plan. </p> <p>For the sake of brevity, we will limit the scope of this tutorial to drawing bounding boxes around objects of interest to point to pixels in the image frame and drawing a detection plane corresponding to depth pixels in the camera frame.</p> <p>Warning</p> <p>Running inference on Stretch results in a continuously high current draw by the CPU. Please ensure proper ventilation with the onboard fan. It is recommended to run the demo in tethered mode.</p>"},{"location":"stretch-tutorials/ros2/deep_perception/#see-it-in-action","title":"See It In Action","text":"<p>Go ahead and execute the following command to run the inference and visualize the detections in RViz:</p> <pre><code>ros2 launch stretch_deep_perception stretch_detect_objects.launch.py\n</code></pre> <p></p> <p>Voila! You just executed your first deep-learning model on Stretch!</p>"},{"location":"stretch-tutorials/ros2/deep_perception/#code-breakdown","title":"Code Breakdown","text":"<p>Luckily, the stretch_deep_pereption package is extremely modular and is designed to work with a wide array of detectors. Although most of the heavy lifting in this tutorial is being done by the neural network, let's attempt to break down the code into functional blocks to understand the detection pipeline.</p> <p>The control flow begins with executing the detect_objects.py script. In the main() function, we create an instance of the ObjectDetector class from the object_detect_pytorch.py script where we configure the YOLOv5s model. Next, we pass this detector to an instance of the DetectionNode class from the detection_node.py script and call the main function. <pre><code>def main():\n    confidence_threshold = 0.0\n    detector = od.ObjectDetector(confidence_threshold=confidence_threshold)\n    default_marker_name = 'object'\n    node_name = 'DetectObjectsNode'\n    topic_base_name = 'objects'\n    fit_plane = False\n    node = dn.DetectionNode(detector, default_marker_name, node_name, topic_base_name, fit_plane)\n    node.main()\n</code></pre></p> <p>Let's skim through the object_detect_pytorch.py script to understand the configuration. The constructor is where we load the pretrained YOLOv5s model using the torch.hub.load() PyTorch wrapper. We set the confidence threshold to be 0.2, which says that a detection is only considered valid if the probability is higher than 0.2. This can be tweaked, although lower numbers often result in false positives and higher numbers often disregard blurry or smaller valid objects. <pre><code>class ObjectDetector:\n    def __init__(self, confidence_threshold=0.2):\n        # Load the models\n        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom\n        self.confidence_threshold = confidence_threshold\n</code></pre></p> <p>The apply_to_image() method passes the stream of RGB images from the realsense camera to the YOLOv5s model and returns detections in the form of a dictionary consisting of class_id, label, confidence and bouding box coordinates. The last part is exactly what we need for further computations. <pre><code>def apply_to_image(self, rgb_image, draw_output=False):\n        results = self.model(rgb_image)\n\n        ...\n\n        if draw_output:\n            output_image = rgb_image.copy()\n            for detection_dict in results:\n                self.draw_detection(output_image, detection_dict)\n\n        return results, output_image\n</code></pre></p> <p>This method calls the draw_detection() method to draw bounding boxes with the object labels and confidence thresholds over detected objects in the image using OpenCV. <pre><code>def draw_detection(self, image, detection_dict):\n        ...\n\n        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, rectangle_line_thickness)\n\n        ...\n\n        cv2.rectangle(image, (label_x_min, label_y_min), (label_x_max, label_y_max), (255, 255, 255), cv2.FILLED)\n        cv2.putText(image, output_string, (text_x, text_y), font, font_scale, line_color, line_width, cv2.LINE_AA)\n</code></pre></p> <p>Next, the script detection_node.py contains the class DetectionNode which is the main ROS node that subscribes to the RGB and depth images from the realsense camera and feeds them to the detector to run inference. The image_callback() method runs in a loop to subscribe to synchronized RGB and depth images. The RGB images are then rotated 90 degrees and passed to the apply_to_image() method. The returned output image is published on the visualize_object_detections_pub publisher, while the detections_2d dictionary is passed to the detections_2d_to_3d() method for further processing and drawing the detection plane. For detectors that also return markers and axes, it also publishes this information. <pre><code>def image_callback(self, ros_rgb_image, ros_depth_image, rgb_camera_info):\n        ...\n\n        detection_box_image = cv2.rotate(self.rgb_image, cv2.ROTATE_90_CLOCKWISE)\n\n        ...\n\n        detections_2d, output_image = self.detector.apply_to_image(detection_box_image, draw_output=debug_output)\n\n        ...\n\n        if output_image is not None:\n            output_image = ros2_numpy.msgify(Image, output_image, encoding='rgb8')\n            if output_image is not None:\n                self.visualize_object_detections_pub.publish(output_image)\n\n        detections_3d = d2.detections_2d_to_3d(detections_2d, self.rgb_image, self.camera_info, self.depth_image, fit_plane=self.fit_plane, min_box_side_m=self.min_box_side_m, max_box_side_m=self.max_box_side_m)\n\n        ...\n\n        if self.publish_marker_point_clouds: \n            for marker in self.marker_collection:\n                marker_points = marker.get_marker_point_cloud()\n                self.add_point_array_to_point_cloud(marker_points)\n                publish_plane_points = False\n                if publish_plane_points: \n                    plane_points = marker.get_plane_fit_point_cloud()\n                    self.add_point_array_to_point_cloud(plane_points)\n            self.publish_point_cloud()\n        self.visualize_markers_pub.publish(marker_array)\n        if axes_array is not None: \n            self.visualize_axes_pub.publish(axes_array)\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/deep_perception/#face-detection-facial-landmarks-detection-and-head-pose-estimation-with-openvino-and-opencv","title":"Face Detection, Facial Landmarks Detection and Head Pose Estimation with OpenVINO and OpenCV","text":"<p>Detecting objects is just one thing Stretch can do well, it can do much more using pretrained models. For this part of the tutorial, we will be using Intel\u2019s OpenVINO toolkit with OpenCV. The cool thing about this demo is that it uses three different models in tandem to not just detect human faces, but also important features of the human face such as the eyes, nose and the lips with head pose information. This is important in the context of precise manipulation tasks such as assisted feeding where we want to know the exact location of the facial features the end effector must reach.</p> <p>OpenVINO is a toolkit popularized by Intel to optimize and deploy machine learning inference that can utilize hardware acceleration dongles such as the Intel Neural Compute Stick with Intel based compute architectures. More convenient is the fact that most of the deep learning models in the Open Model Zoo are accessible and configurable using the familiar OpenCV API with the opencv-python-inference-engine library.</p> <p>With that, let\u2019s jump right into it!</p> <p>Warning</p> <p>Running inference on Stretch results in a continuously high current draw by the CPU. Please ensure proper ventilation with the onboard fan. It is recommended to run the demo in tethered mode.</p>"},{"location":"stretch-tutorials/ros2/deep_perception/#see-it-in-action_1","title":"See It In Action","text":"<p>First, let\u2019s execute the following command to see what it looks like:</p> <pre><code>ros2 launch stretch_deep_perception stretch_detect_faces.launch.py\n</code></pre> <p></p>"},{"location":"stretch-tutorials/ros2/deep_perception/#code-breakdown_1","title":"Code Breakdown","text":"<p>Ain't that something! If you followed the breakdown in object detection, you'll find that the only change if you are looking to detect faces, facial landmarks or estimate head pose instead of detecting objects is in using a different deep learning model that does just that. For this, we will explore how to use the OpenVINO toolkit. Let's head to the detect_faces.py node to begin.</p> <p>In the main() method, we see a similar structure as with the object detction node. We first create an instance of the detector using the HeadPoseEstimator class from the head_estimator.py script to configure the deep learning models. Next, we pass this to an instance of the DetectionNode class from the detection_node.py script and call the main function. <pre><code>    ...\n\n    detector = he.HeadPoseEstimator(models_directory,\n                                    use_neural_compute_stick=use_neural_compute_stick)\n    default_marker_name = 'face'\n    node_name = 'DetectFacesNode'\n    topic_base_name = 'faces'\n    fit_plane = False\n    node = dn.DetectionNode(detector,\n                            default_marker_name,\n                            node_name,\n                            topic_base_name,\n                            fit_plane,\n                            min_box_side_m=min_head_m,\n                            max_box_side_m=max_head_m)\n    node.main()\n</code></pre></p> <p>In addition to detecting faces, this class also enables detecting facial landmarks as well as estimating head pose. The constructor initializes and configures three separate models, namely head_detection_model, head_pose_model and landmarks_model,  with the help of the renamed_cv2.dnn.readNet() wrappers. Note that renamed_cv2 is simply the opencv_python_inference_engine library compiled under a different namespace for use with Stretch so as not to conflict with the regular OpenCV library and have functionalities from both available to users concurrently. <pre><code>class HeadPoseEstimator:\n    def __init__(self, models_directory, use_neural_compute_stick=False):\n        ...\n\n        self.head_detection_model = renamed_cv2.dnn.readNetFromCaffe(head_detection_model_prototxt_filename, head_detection_model_caffemodel_filename)\n        dm.print_model_info(self.head_detection_model, 'head_detection_model')\n\n        ...\n\n        self.head_pose_model = renamed_cv2.dnn.readNet(head_pose_weights_filename, head_pose_config_filename)\n\n        ...\n\n        self.landmarks_model = renamed_cv2.dnn.readNet(landmarks_weights_filename, landmarks_config_filename)\n</code></pre></p> <p>The apply_to_image() method calls individual methods like detect_faces(), estimate_head_pose() and detect_facial_landmarks() that each runs the inference using the models we configured above. The bounding_boxes from the face detection model are used to supply the cropped image of the faces to head pose and facial landmark models to make their job way more efficient. <pre><code> def apply_to_image(self, rgb_image, draw_output=False):\n        ...\n\n        boxes = self.detect_faces(rgb_image)\n\n        facial_landmark_names = self.landmark_names.copy()\n        for bounding_box in boxes:\n            if draw_output: \n                self.draw_bounding_box(output_image, bounding_box)\n            yaw, pitch, roll = self.estimate_head_pose(rgb_image, bounding_box, enlarge_box=True, enlarge_scale=1.15)\n            if yaw is not None: \n                ypr = (yaw, pitch, roll)\n                if draw_output: \n                    self.draw_head_pose(output_image, yaw, pitch, roll, bounding_box)\n            else:\n                ypr = None\n            landmarks, landmark_names = self.detect_facial_landmarks(rgb_image, bounding_box, enlarge_box=True, enlarge_scale=1.15)\n            if (landmarks is not None) and draw_output: \n                self.draw_landmarks(output_image, landmarks)\n            heads.append({'box':bounding_box, 'ypr':ypr, 'landmarks':landmarks})\n\n        return heads, output_image\n</code></pre></p> <p>The detecion_node.py script then takes over as we saw with the object detection tutorial to publish the detections on pertinent topics. </p> <p>Now go ahead and experiment with a few more pretrained models using PyTorch or OpenVINO on Stretch. If you are feeling extra motivated, try creating your own neural networks and training them. Stretch is ready to deploy them!</p>"},{"location":"stretch-tutorials/ros2/demo_grasp_object/","title":"Object Grasping","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore an object-grasping demo using FUNMAP.</p>"},{"location":"stretch-tutorials/ros2/demo_grasp_object/#motivation","title":"Motivation","text":"<p>Through this demo, we demonstrate grasp candidate computation, grasp planning, and navigation using FUNMAP. An object is placed in front of the Stretch at a reasonable distance and height. The arm of the robot is fully retracted to get a good view of the object. We have observed reliable depth inference and object detection whenever the grasp candidate is placed in front of a dark background.</p>"},{"location":"stretch-tutorials/ros2/demo_grasp_object/#workspace-setup","title":"Workspace Setup","text":"<p>Ideally, this demo requires the object to be placed at half the height of a Stretch. The end-effector of the robot is aligned with the target object. The arm is fully retracted to get a good view of the scene and reliable depth computation. The lift position is slightly below the height of the target surface. The demo works best in dim lighting conditions. Finally, ensure that there is no flat vertical surface, such as a wall, close behind the object of interest.</p>"},{"location":"stretch-tutorials/ros2/demo_grasp_object/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the object grasping demo:</p> <pre><code>ros2 launch stretch_demos grasp_object.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, and the grasp_object nodes. </p> <p>In a new terminal window, start keyboard teleoperation:</p> <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p grasp_object_on:=true\n</code></pre> <p>You will be presented with a keyboard teleoperation menu in a new terminal window. Use key commands to get the Stretch configured as per the above workspace setup guidelines. Once the robot is ready, press \u2018\\\u2019 to trigger object grasping.</p>"},{"location":"stretch-tutorials/ros2/demo_grasp_object/#code-explained","title":"Code Explained","text":"<p>The grasp_object node uses the joint_trajectory_server inside stretch_core to send out target joint positions. </p> <pre><code>self.trajectory_client = ActionClient(self,\n    FollowJointTrajectory,\n    '/stretch_controller/follow_joint_trajectory',\n    callback_group=self.callback_group\n)\n\nserver_reached = self.trajectory_client.wait_for_server(\n    timeout_sec=60.0\n)\n\nif not server_reached:\n    self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n    sys.exit()\n</code></pre> <p>Additionally, the node also subscribes to RealSense's depth cloud, tool type, and Stretch\u2019s joint state topics.</p> <pre><code>self.joint_states_subscriber = self.create_subscription(JointState,\n    '/stretch/joint_states',\n    callback=self.joint_states_callback,\n    qos_profile=1,\n    callback_group=self.callback_group\n)\n\nself.point_cloud_subscriber = self.create_subscription(PointCloud2,\n    '/camera/depth/color/points',\n    callback=self.point_cloud_callback,\n    qos_profile=1,\n    callback_group=self.callback_group\n)\n\nself.tool_subscriber = self.create_subscription(String,\n    '/tool',\n    callback=self.tool_callback,\n    qos_profile=1,\n    callback_group=self.callback_group\n)\n</code></pre> <p>Whenever the user triggers the grasp object service, Stretch scans the field of view for potential grasp candidates.</p> <pre><code>self.logger.info('Stow the arm.')\nself.stow_the_robot()\n\n# 1. Scan surface and find grasp target\nself.look_at_surface(scan_time_s = 4.0)\ngrasp_target = self.manipulation_view.get_grasp_target(self.tf2_buffer)\n</code></pre> <p>After determining a suitable grasp candidate, the node generates a pregrasp base and end-effector pose.</p> <pre><code># 2. Move to pregrasp pose\npregrasp_lift_m = self.manipulation_view.get_pregrasp_lift(grasp_target,\n    self.tf2_buffer\n)\n\nif self.tool == \"tool_stretch_dex_wrist\":\n    pregrasp_lift_m += 0.02\nif (self.lift_position is None):\n    return Trigger.Response(\n        success=False,\n        message='lift position unavailable'\n    )\n\nself.logger.info('Raise tool to pregrasp height.')\nlift_to_pregrasp_m = max(self.lift_position + pregrasp_lift_m, 0.1)\nlift_to_pregrasp_m = min(lift_to_pregrasp_m, max_lift_m)\n\npose = {'joint_lift': lift_to_pregrasp_m}\nself.move_to_pose(pose)\n\nif self.tool == \"tool_stretch_dex_wrist\":\n    self.logger.info('Rotate pitch/roll for grasping.')\n    pose = {'joint_wrist_pitch': -0.3, 'joint_wrist_roll': 0.0}\n    self.move_to_pose(pose)\n\npregrasp_yaw = self.manipulation_view.get_pregrasp_yaw(grasp_target,    \n    self.tf2_buffer\n)\n\nself.logger.info('pregrasp_yaw = {0:.2f} rad'.format(pregrasp_yaw))\nself.logger.info('pregrasp_yaw = {0:.2f} deg'.format(pregrasp_yaw * (180.0/np.pi)))\nself.logger.info('Rotate the gripper for grasping.')\n\npose = {'joint_wrist_yaw': pregrasp_yaw}\nself.move_to_pose(pose)\n\nself.logger.info('Open the gripper.')\npose = {'gripper_aperture': 0.125}\nself.move_to_pose(pose)\n\npregrasp_mobile_base_m, pregrasp_wrist_extension_m = self.manipulation_view.get_pregrasp_planar_translation(grasp_target,\n   self.tf2_buffer\n)\nself.logger.info('pregrasp_mobile_base_m = {0:.3f} m'.format(pregrasp_mobile_base_m))\nself.logger.info('pregrasp_wrist_extension_m = {0:.3f} m'.format(pregrasp_wrist_extension_m))\nself.logger.info('Drive to pregrasp location.')\nself.drive(pregrasp_mobile_base_m)\n\nif pregrasp_wrist_extension_m &gt; 0.0:\n    extension_m = max(self.wrist_position + pregrasp_wrist_extension_m, min_extension_m)\n    extension_m = min(extension_m, max_extension_m)\n    self.logger.info('Extend tool above surface.')\n    pose = {'wrist_extension': extension_m}\n    self.move_to_pose(pose)\nelse:\n    self.logger.info('negative wrist extension for pregrasp, so not extending or retracting.')\n</code></pre> <p>A plan to grasp the object is generated using FUNMAP\u2019s ManipulationView class. This class requires the pregrasp pose parameters to generate the final grasp plan.</p> <pre><code># 3. Grasp the object and lift it\ngrasp_mobile_base_m, grasp_lift_m, grasp_wrist_extension_m = self.manipulation_view.get_grasp_from_pregrasp(grasp_target, self.tf2_buffer)\nself.logger.info('grasp_mobile_base_m = {0:3f} m, grasp_lift_m = {1:3f} m, grasp_wrist_extension_m = {2:3f} m'.format(grasp_mobile_base_m, grasp_lift_m, grasp_wrist_extension_m))\nself.logger.info('Move the grasp pose from the pregrasp pose.')\n\nlift_m = max(self.lift_position + grasp_lift_m, 0.1)\nlift_m = min(lift_m, max_lift_m)\nextension_m = max(self.wrist_position + grasp_wrist_extension_m, min_extension_m)\nextension_m = min(extension_m, max_extension_m)\npose = {'translate_mobile_base': grasp_mobile_base_m,\n        'joint_lift': lift_m,\n        'wrist_extension': extension_m}\n</code></pre> <p>The node then proceeds to grasp the object by sending our goal joint positions.</p>"},{"location":"stretch-tutorials/ros2/demo_grasp_object/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore object grasping with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>The grasp object candidate detection often fails due to the quality of the input point cloud.</p> </li> <li> <p>After grasp candidate detection, the segmented point cloud might not cover the entire surface for an object. Hence, the calculated grasp point will have a constant offset from the ideal grasp location.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"stretch-tutorials/ros2/demo_handover_object/","title":"Object Handover Demo","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore an object-handover demo using FUNMAP.</p>"},{"location":"stretch-tutorials/ros2/demo_handover_object/#motivation","title":"Motivation","text":"<p>Through this demo, we demonstrate human mouth detection using the stretch_deep_perception package, and demonstrate object delivery with navigation using FUNMAP. The robot is teleoperated to have a person in the view of its camera. The person requesting the object must face the robot. We use OpenVINO to perform facial recognition.</p>"},{"location":"stretch-tutorials/ros2/demo_handover_object/#workspace-setup","title":"Workspace Setup","text":"<p>Ideally, this demo requires the person requesting the object to be facing the robot\u2019s camera. Use keyboard teleop to place the object in the robot\u2019s gripper.</p>"},{"location":"stretch-tutorials/ros2/demo_handover_object/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the object handover demo:</p> <pre><code>ros2 launch stretch_demos handover_object.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, and the handover_object nodes. </p> <p>In a new terminal, launch keyboard teleoperation:</p> <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p handover_object_on:=true\n</code></pre> <p>You will be presented with a keyboard teleoperation menu in a new terminal window. Use key commands to get the Stretch configured as per the above workspace setup guidelines. Once the robot is ready, press \u2018y\u2019 or \u2018Y\u2019 to trigger object handover.</p>"},{"location":"stretch-tutorials/ros2/demo_handover_object/#code-explained","title":"Code Explained","text":"<p>The object_handover node uses the joint_trajectory_server inside stretch_core to send out target joint positions.</p> <pre><code>self.trajectory_client = ActionClient(self, FollowJointTrajectory, '/stretch_controller/follow_joint_trajectory', callback_group=self.callback_group)\nserver_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\nif not server_reached:\n    self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n    sys.exit()\n</code></pre> <p>Additionally, the node also subscribes to mouth positions detected by stretch_deep_perception, and Stretch\u2019s joint state topics.</p> <pre><code>self.joint_states_subscriber = self.create_subscription(JointState, '/stretch/joint_states', qos_profile=1, callback=self.joint_states_callback, callback_group=self.callback_group)\n\nself.mouth_position_subscriber = self.create_subscription(MarkerArray, '/nearest_mouth/marker_array', qos_profile=1, callback=self.mouth_position_callback, callback_group=self.callback_group)\n</code></pre> <p>Whenever the node receives a mouth position message, it computes a handoff XYZ coordinate depending upon the current wrist and mouth positions:</p> <pre><code>def mouth_position_callback(self, marker_array):\n    with self.move_lock:\n\n        for marker in marker_array.markers:\n            if marker.type == self.mouth_marker_type:\n                mouth_position = marker.pose.position\n                self.mouth_point = PointStamped()\n                self.mouth_point.point = mouth_position\n                header = self.mouth_point.header\n                header.stamp = marker.header.stamp\n                header.frame_id = marker.header.frame_id\n                # header.seq = marker.header.seq\n                self.logger.info('******* new mouth point received *******')\n\n                lookup_time = Time(seconds=0) # return most recent transform\n                timeout_ros = Duration(seconds=0.1)\n\n                old_frame_id = self.mouth_point.header.frame_id\n                new_frame_id = 'base_link'\n                stamped_transform = self.tf2_buffer.lookup_transform(new_frame_id, old_frame_id, lookup_time, timeout_ros)\n                points_in_old_frame_to_new_frame_mat = rn.numpify(stamped_transform.transform)\n                camera_to_base_mat = points_in_old_frame_to_new_frame_mat\n\n                grasp_center_frame_id = 'link_grasp_center'\n                stamped_transform = self.tf2_buffer.lookup_transform(new_frame_id,\n                    grasp_center_frame_id,\n                    lookup_time,\n                    timeout_ros\n                )\n                grasp_center_to_base_mat = rn.numpify(stamped_transform.transform)\n\n                mouth_camera_xyz = np.array([0.0, 0.0, 0.0, 1.0])\n                mouth_camera_xyz[:3] = rn.numpify(self.mouth_point.point)[:3]\n\n                mouth_xyz = np.matmul(camera_to_base_mat, mouth_camera_xyz)[:3]\n                fingers_xyz = grasp_center_to_base_mat[:,3][:3]\n\n                handoff_object = True\n\n                if handoff_object:\n                    # attempt to handoff the object at a location below\n                    # the mouth with respect to the world frame (i.e.,\n                    # gravity)\n                    target_offset_xyz = np.array([0.0, 0.0, -0.2])\n                else:\n                    object_height_m = 0.1\n                    target_offset_xyz = np.array([0.0, 0.0, -object_height_m])\n                target_xyz = mouth_xyz + target_offset_xyz\n\n                fingers_error = target_xyz - fingers_xyz\n                self.logger.info(f'fingers_error = {str(fingers_error)}')\n\n                delta_forward_m = fingers_error[0]\n                delta_extension_m = -fingers_error[1]\n                delta_lift_m = fingers_error[2]\n\n                max_lift_m = 1.0\n                lift_goal_m = self.lift_position + delta_lift_m\n                lift_goal_m = min(max_lift_m, lift_goal_m)\n                self.lift_goal_m = lift_goal_m\n\n                self.mobile_base_forward_m = delta_forward_m\n\n                max_wrist_extension_m = 0.5\n                wrist_goal_m = self.wrist_position + delta_extension_m\n\n                if handoff_object:\n                    # attempt to handoff the object by keeping distance\n                    # between the object and the mouth distance\n                    wrist_goal_m = wrist_goal_m - 0.25 # 25cm from the mouth\n                    wrist_goal_m = max(0.0, wrist_goal_m)\n\n                self.wrist_goal_m = min(max_wrist_extension_m, wrist_goal_m)\n\n                self.handover_goal_ready = True\n</code></pre> <p>The delta between the wrist XYZ and mouth XYZ is used to calculate the lift position, base forward translation, and wrist extension.</p> <p>Once the user triggers the handover object service, the node sends out joint goal positions for the base, lift, and the wrist, to deliver the object near the person\u2019s mouth:</p> <pre><code>self.logger.info(\"Starting object handover!\")\nwith self.move_lock:\n    # First, retract the wrist in preparation for handing out an object.\n    pose = {'wrist_extension': 0.005}\n    self.move_to_pose(pose)\n\n    if self.handover_goal_ready:\n        pose = {'joint_lift': self.lift_goal_m}\n        self.move_to_pose(pose)\n        tolerance_distance_m = 0.01\n        at_goal = self.move_base.forward(self.mobile_base_forward_m,\n            detect_obstacles=False,\n            tolerance_distance_m=tolerance_distance_m\n        )\n        pose = {'wrist_extension': self.wrist_goal_m}\n        self.move_to_pose(pose)\n        self.handover_goal_ready = False\n</code></pre>"},{"location":"stretch-tutorials/ros2/demo_handover_object/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore object delivery with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>The node requires the target user's face to be in the camera view while triggering the demo. As it stands, it does not keep any past face detections in its memory.</p> </li> <li> <p>Facial landmarks detection might not work well for some faces and is highly variable to the deviation from the faces that the algorithm was originally trained on.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"stretch-tutorials/ros2/demo_hello_world/","title":"Hello World Demo","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore a hello world writing demo using FUNMAP.</p>"},{"location":"stretch-tutorials/ros2/demo_hello_world/#motivation","title":"Motivation","text":"<p>Stretch is a contacts-sensitive robot, making it compliant in human spaces. This contact sensitivity can be leveraged for surface and obstacles detection. Through this demo we demonstrate one specific application of contacts detection - surface writing. In this demo, Stretch writes out the letters 'H', 'E', 'L', 'L', and 'O' in a sequential manner.</p>"},{"location":"stretch-tutorials/ros2/demo_hello_world/#workspace-setup","title":"Workspace Setup","text":"<p>Things that you will need  - Whiteboard  - Marker</p> <p>Ideally, this demo requires a whiteboard that is placed at the same height as the Stretch. The writing surface must be flat and reachable by the robot's arm.</p>"},{"location":"stretch-tutorials/ros2/demo_hello_world/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the hello world demo:</p> <pre><code>ros2 launch stretch_demos hello_world.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, and the hello_world nodes. </p> <p>Next, in a separate terminal, run:</p> <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p hello_world_on:=true\n</code></pre> <p>You will be presented with a keyboard teleoperation menu. Teleoperate the robot so that the arm is perpendicular to the writing surface, about 30 cm away. Place a marker pointing outward in the robot's gripper. Once the robot is ready, press \u2018`\u2019 or '~\u2019 to trigger hello world writing.</p>"},{"location":"stretch-tutorials/ros2/demo_hello_world/#code-explained","title":"Code Explained","text":"<p>The clean_surface node uses the joint_trajectory_server inside stretch_core to send out target joint positions.</p> <pre><code>self.trajectory_client = ActionClient(self,\n    FollowJointTrajectory,\n    '/stretch_controller/follow_joint_trajectory',\n    callback_group=self.callback_group\n)\nserver_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\nif not server_reached:\n    self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n    sys.exit()\n</code></pre> <p>Additionally, the node also subscribes to Stretch\u2019s joint states topic.</p> <pre><code>self.joint_states_subscriber = self.create_subscription(JointState,\n    '/stretch/joint_states',\n    callback=self.joint_states_callback,\n    qos_profile=10,\n    callback_group=self.callback_group\n)\n</code></pre> <p>Whenever the user triggers the hello world write service, the robot moves to an initial joint configuration:</p> <pre><code>def move_to_initial_configuration(self):\n    initial_pose = {'wrist_extension': 0.01,\n                    'joint_lift': self.letter_top_lift_m,\n                    'joint_wrist_yaw': 0.0}\n\n    self.logger.info('Move to initial arm pose for writing.')\n    self.move_to_pose(initial_pose)\n</code></pre> <p>Thereafter, the node uses FUNMAP's <code>align_to_nearest_cliff</code> service to align the Stretch w.r.t the whiteboard.</p> <pre><code>def align_to_surface(self):\n    self.logger.info('align_to_surface')\n    trigger_request = Trigger.Request() \n    trigger_result = self.trigger_align_with_nearest_cliff_service.call_async(trigger_request)\n    self.logger.info('trigger_result = {0}'.format(trigger_result))\n</code></pre> <p>After aligning with the whiteboard, the node then proceeds to execute joint position goals to draw out each letter of the word \"hello\":</p> <pre><code>self.letter_h()\nself.space()\nself.letter_e()\nself.space()\nself.letter_l()\nself.space()\nself.letter_l()\nself.space()\nself.letter_o()\nself.space()\n\nreturn Trigger.Response(\n    success=True,\n    message='Completed writing hello!'\n)\n</code></pre>"},{"location":"stretch-tutorials/ros2/demo_hello_world/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore whiteboard writing with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>Each Stretch has unique calibration thresholds. The contacts thresholds defined in the stock demo code might not work for your Stretch. Additional tuning might be necessary.</p> </li> <li> <p>The quality of the written text structure is also highly dependent on mobile base movements. Currently, navigation is open-loop and does not account for accumulated error terms.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"stretch-tutorials/ros2/demo_open_drawer/","title":"Drawer Opening Demo","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore an drawer-opening demo using FUNMAP.</p>"},{"location":"stretch-tutorials/ros2/demo_open_drawer/#motivation","title":"Motivation","text":"<p>Through this demo, we demonstrate opening of common drawers that have handles, and contact sensing with navigation using FUNMAP. FUNMAP provides APIs that allow users to extend the arm or adjust the lift until a contact is detected. We leverage this contact detection API in the demo. Users can trigger the drawer opening demo through an upward or downward motion of the hook attached to the end-effector, explained below.</p>"},{"location":"stretch-tutorials/ros2/demo_open_drawer/#workspace-setup","title":"Workspace Setup","text":"<p>The robot is teleoperated so as to keep its arm perpendicular to the drawer\u2019s frontal surface, slightly above or below the handle. The workspace must be clear of any obstacles between the end-effector and the drawer. Finally, ensure that the wrist can reach the drawer through extension.</p>"},{"location":"stretch-tutorials/ros2/demo_open_drawer/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the drawer opening demo:</p> <pre><code>ros2 launch stretch_demos open_drawer.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, and the open_drawer nodes. </p> <p>In a new terminal, launch keyboard teleoperation:</p> <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p open_drawer_on:=true\n</code></pre> <p>You will be presented with a keyboard teleoperation menu in a new terminal window. Use key commands to get the Stretch configured as per the above workspace setup guidelines. Once the robot is ready, press \u2018.\u2019 or \u2018&gt;\u2019 to trigger the drawer opening with an upward motion. If you want to run the demo with a downward motion, press \u2018z\u2019 or \u2018Z\u2019.</p>"},{"location":"stretch-tutorials/ros2/demo_open_drawer/#code-explained","title":"Code Explained","text":"<p>The open_drawer node uses the joint_trajectory_server inside stretch_core to send out target joint positions. </p> <pre><code>self.trajectory_client = ActionClient(self, FollowJointTrajectory, '/stretch_controller/follow_joint_trajectory', callback_group=self.callback_group)\n        server_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\n        if not server_reached:\n            self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n            sys.exit()\n</code></pre> <p>The node then proceeds to connect to the contact detection services provided by FUNMAP:</p> <pre><code>self.trigger_open_drawer_service = self.create_service(Trigger, '/open_drawer/trigger_open_drawer_down',                                                self.trigger_open_drawer_down_callback, callback_group=self.callback_group)\n\nself.trigger_open_drawer_service = self.create_service(Trigger, '/open_drawer/trigger_open_drawer_up',                                                self.trigger_open_drawer_up_callback, callback_group=self.callback_group)\n\nself.trigger_align_with_nearest_cliff_service = self.create_client(Trigger, '/funmap/trigger_align_with_nearest_cliff', callback_group=self.callback_group)\n\nself.trigger_align_with_nearest_cliff_service.wait_for_service()\nself.logger.info('Node ' + self.get_name() + ' connected to /funmap/trigger_align_with_nearest_cliff.')\n\nself.trigger_reach_until_contact_service = self.create_client(Trigger, '/funmap/trigger_reach_until_contact', callback_group=self.callback_group)\nself.trigger_reach_until_contact_service.wait_for_service()\nself.logger.info('Node ' + self.get_name() + ' connected to /funmap/trigger_reach_until_contact.')\n\nself.trigger_lower_until_contact_service = self.create_client(Trigger, '/funmap/trigger_lower_until_contact', callback_group=self.callback_group)\nself.trigger_lower_until_contact_service.wait_for_service()\nself.logger.info('Node ' + self.get_name() + ' connected to /funmap/trigger_lower_until_contact.')\n</code></pre> <p>Once the user triggers the drawer opening demo, the robot is commanded to move to an initial configuration:</p> <pre><code>initial_pose = {'wrist_extension': 0.01,\n                'joint_wrist_yaw': 1.570796327,\n                'gripper_aperture': 0.0}\nself.logger.info('Move to the initial configuration for drawer opening.')\nself.move_to_pose(initial_pose)\n</code></pre> <p>The robot then proceeds to extend its wrist until it detects a contact, hopefully caused by a drawer surface.</p> <pre><code>self.extend_hook_until_contact()\n</code></pre> <p>It then backs off from the surface by about 5 cm so as to not touch it. </p> <pre><code>success = self.backoff_from_surface()\nif not success:\n    return Trigger.Response(\n        success=False,\n        message='Failed to backoff from the surface.'\n    )\n</code></pre> <p>Depending upon the user\u2019s choice, the robot will either raise or lower its lift so as to hook the handle. </p> <pre><code>if direction == 'down':\n    self.lower_hook_until_contact()\nelif direction == 'up':\n    self.raise_hook_until_contact()\n</code></pre> <p>Finally, the robot will pull open the drawer.</p> <pre><code>success = self.pull_open()\nif not success:\n     return Trigger.Response(\n         success=False,\n         message='Failed to pull open the drawer.'\n     )\n\npush_drawer_closed = False\nif push_drawer_closed:\n    time.sleep(3.0)\n    self.push_closed()\n\nreturn Trigger.Response(\n    success=True,\n    message='Completed opening the drawer!'\n)\n</code></pre>"},{"location":"stretch-tutorials/ros2/demo_open_drawer/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore drawer opening with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>The demo performs a blind hook operation using contacts sensitivity. Special care must be taken so as to not damage the target surface.</p> </li> <li> <p>The code might command the Stretch to lift its arm by a large amount which might cause collisions in the workspace. The code also does not take into account the configuration space of the robot.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"stretch-tutorials/ros2/demo_surface_cleaning/","title":"Surface Cleaning Demo","text":"<p>FUNMAP is a hardware-targeted perception, planning, and navigation framework developed by Hello Robot for ROS developers and researchers. Some of the key features provided by FUNMAP include cliff detection, closed-loop navigation, and mapping. In this tutorial, we will explore a surface cleaning demo using FUNMAP.</p>"},{"location":"stretch-tutorials/ros2/demo_surface_cleaning/#motivation","title":"Motivation","text":"<p>Through this demo, we demonstrate flat surface detection using the head camera, and demonstrate surface cleaning with navigation using FUNMAP. The robot is teleoperated to have a flat surface at a reasonable height in the view of the robot\u2019s camera. The arm of the robot is fully retracted to get a good and entire view of the surface of interest. We have observed reliable depth inference and object detection whenever the surface to be cleaned is placed in front of a dark background.</p>"},{"location":"stretch-tutorials/ros2/demo_surface_cleaning/#workspace-setup","title":"Workspace Setup","text":"<p>Ideally, this demo requires the surface to be present at half the height of a Stretch. Teleoperate your robot so as to get a good view of it. The end-effector of the robot is aligned with the target object. The arm is fully retracted to get a good view of the scene and reliable depth computation. The lift position is slightly below the height of the target surface. The demo works best in dim lighting conditions. Finally, ensure that there is no flat vertical surface, such as a wall, close behind the surface to be cleaned. You can use RViz to visualize the pointcloud data being captured by the camera.</p>"},{"location":"stretch-tutorials/ros2/demo_surface_cleaning/#how-to-run","title":"How-to-run","text":"<p>After building and sourcing the workspace, home the robot:</p> <pre><code>stretch_robot_home.py\n</code></pre> <p>This ensures that the underlying stretch_body package knows the exact joint limits and provides the user with a good starting joint configuration.</p> <p>After homing, launch the surface cleaning demo:</p> <pre><code>ros2 launch stretch_demos clean_surface.launch.py\n</code></pre> <p>This command will launch stretch_driver, stretch_funmap, keyboard_teleop, and the clean_surface nodes. </p> <p>In a new terminal, launch keyboard teleoperation: <pre><code>ros2 run stretch_core keyboard_teleop --ros-args -p clean_surface_on:=true\n</code></pre></p> <p>You will be presented with a keyboard teleoperation menu in a new terminal window. Use key commands to get the Stretch configured as per the above workspace setup guidelines. Once the robot is ready, press \u2018/\u2019 or \u2018?\u2019 to trigger surface cleaning.</p>"},{"location":"stretch-tutorials/ros2/demo_surface_cleaning/#code-explained","title":"Code Explained","text":"<p>The clean_surface node uses the joint_trajectory_server inside stretch_core to send out target joint positions.</p> <pre><code>self.trajectory_client = ActionClient(self,\n    FollowJointTrajectory,\n    '/stretch_controller/follow_joint_trajectory',\n    callback_group=self.callback_group\n)\nserver_reached = self.trajectory_client.wait_for_server(timeout_sec=60.0)\nif not server_reached:\n    self.get_logger().error('Unable to connect to joint_trajectory_server. Timeout exceeded.')\n    sys.exit()\n</code></pre> <p>Additionally, the node also subscribes to RealSense's depth cloud, and Stretch\u2019s joint state topics.</p> <pre><code>self.joint_states_subscriber = self.create_subscription(JointState,\n    '/stretch/joint_states',\n    callback=self.joint_states_callback,\n    qos_profile=10,\n    callback_group=self.callback_group\n)\n\nself.point_cloud_subscriber = self.create_subscription(PointCloud2,\n    '/camera/depth/color/points',\n    callback=self.point_cloud_callback,\n    qos_profile=10, \n    callback_group=self.callback_group\n)\n</code></pre> <p>Whenever the user triggers the clean surface service, Stretch scans the field of view for candidate flat surfaces and generates a surface wiping plan represented by a list of joint position goals. The wiping plan is generated using the ManipulationView class:</p> <pre><code>self.log.info(\"Cleaning initiating!\")\n\ntool_width_m = 0.08\ntool_length_m = 0.08\nstep_size_m = 0.04\nmin_extension_m = 0.01\nmax_extension_m = 0.5\n\nself.look_at_surface()\nstrokes, simple_plan, lift_to_surface_m = self.manipulation_view.get_surface_wiping_plan(self.tf2_buffer,\n    tool_width_m,\n    tool_length_m,\n    step_size_m\n)\nself.log.info(\"Plan:\" + str(simple_plan))\n\nself.log.info('********* lift_to_surface_m = {0} **************'.format(lift_to_surface_m))\n</code></pre> <p>The ManipulationView class holds a max-height image as an internal representation of the surroundings. It creates a segmentation mask to generate a set of linear forward-backward strokes that cover the entire detected surface. See <code>stretch_funmap/segment_max_height_image.py</code> to understand how a flat surface is detected. Obstacles, if present on the surface, are inflated using dilation and erosion to create a margin of safety. These strokes, in the image frame, are then transformed into the base_link frame using tf2. This is how we obtain the wiping plan.</p> <p>After generating the wiping plan, the node validates this plan by asserting that the required strokes are greater than zero and the surface is reachable by adjusting the lift and wrist positions:</p> <pre><code>if True and (strokes is not None) and (len(strokes) &gt; 0):\n    if (self.lift_position is not None) and (self.wrist_position is not None):\n        above_surface_m = 0.1\n        lift_above_surface_m = self.lift_position + lift_to_surface_m + above_surface_m\n</code></pre> <p>After validation, the node then proceeds to execute the wiping plan in a sequential manner.</p>"},{"location":"stretch-tutorials/ros2/demo_surface_cleaning/#results-and-expectations","title":"Results and Expectations","text":"<p>This demo serves as an experimental setup to explore surface cleaning with Stretch. Please be advised that this code is not expected to work perfectly. Some of the shortcomings of the demo include:</p> <ul> <li> <p>The area of surface being cleaned is dependent on the fill rate of the RealSense point cloud. If the segemented cleaning surface has too many holes, Stretch might only attempt to clean a fraction of the total area.</p> </li> <li> <p>As each Stretch is unique in its contacts thresholds after calibration, your robot might attempt too aggressively to press down the cloth upon the surface. Additonal tuning might be necessary.</p> </li> </ul> <p>Users are encouraged to try this demo and submit improvements.</p>"},{"location":"stretch-tutorials/ros2/example_1/","title":"Mobile Base Velocity Control","text":""},{"location":"stretch-tutorials/ros2/example_1/#example-1","title":"Example 1","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p> </p> <p>The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending <code>Twist</code> messages to a Stretch robot.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To drive the robot in circles with the move node, type the following in a new terminal.</p> <p><pre><code>ros2 run stetch_ros_tutorials move\n</code></pre> To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"stretch-tutorials/ros2/example_1/#the-code","title":"The Code","text":"<p>Below is the code which will send Twist messages to drive the robot in circles.</p> <pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nclass Move(Node):\n    def __init__(self):\n        super().__init__('stretch_base_move')\n        self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 10)\n\n        self.get_logger().info(\"Starting to move in circle...\")\n        timer_period = 0.5  # seconds\n        self.timer = self.create_timer(timer_period, self.move_around)\n    def move_around(self):\n        command = Twist()\n        command.linear.x = 0.0\n        command.linear.y = 0.0\n        command.linear.z = 0.0\n        command.angular.x = 0.0\n        command.angular.y = 0.0\n        command.angular.z = 0.5\n        self.publisher_.publish(command)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    base_motion = Move()\n    rclpy.spin(base_motion)\n    base_motion.destroy_node()  \n    rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_1/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <p><pre><code>import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\n</code></pre> You need to import rclpy if you are writing a ROS 2 Node. The geometry_msgs.msg import is so that we can send velocity commands to the robot.</p> <p><pre><code>class Move(Node):\n    def __init__(self):\n        super().__init__('stretch_base_move')\n        self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 10)\n</code></pre> This section of code defines the talker's interface to the rest of ROS. self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 10) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist. The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough.</p> <p><pre><code>        timer_period = 0.5  # seconds\n        self.timer = self.create_timer(timer_period, self.move_around)\n</code></pre> We create a timer with a period of 0.5 seconds. This timer ensures that the function move_around is called every 0.5 seconds. This ensures a constant rate of 2Hz for the execution loop.</p> <p><pre><code>command = Twist()\n</code></pre> Make a Twist message.  We're going to set all of the elements since we can't depend on them defaulting to safe values.</p> <p><pre><code>        command.linear.x = 0.0\n        command.linear.y = 0.0\n        command.linear.z = 0.0\n</code></pre> A Twist has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y direction or the z-direction. We set the linear velocities to 0.</p> <p><pre><code>        command.angular.x = 0.0\n        command.angular.y = 0.0\n        command.angular.z = 0.5\n</code></pre> A Twist also has three rotational velocities (in radians per second). The Stretch will only respond to rotations around the z (vertical) axis. We set this to a non-zero value.</p> <p><pre><code>self.publisher_.publish(command)\n</code></pre> Publish the Twist commands in the previously defined topic name /stretch/cmd_vel.</p> <p><pre><code>def main(args=None):\n    rclpy.init(args=args)\n    base_motion = Move()\n    rclpy.spin(base_motion)\n    base_motion.destroy_node()  \n    rclpy.shutdown()\n</code></pre> The next line, rclpy.init(args=args), is very important as it tells ROS to initialize the node. Until rclpy has this information, it cannot start communicating with the ROS Master. In this case, your node will take on the name 'stretch_base_move'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". We then create an instance called base_motion of the class Move(). This is then spun using the spin function in rclpy to call the callback functions, in our case the timer that ensures the move_around function is called at a steady rate of 2Hz.</p> <p>To stop the node from sending twist messages, type <code>Ctrl + c</code>.</p>"},{"location":"stretch-tutorials/ros2/example_10/","title":"Tf2 Broadcaster and Listener","text":""},{"location":"stretch-tutorials/ros2/example_10/#example-10","title":"Example 10","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>This tutorial provides you with an idea of what tf2 can do in the Python track. We will elaborate on how to create a tf2 static broadcaster and listener.</p>"},{"location":"stretch-tutorials/ros2/example_10/#tf2-static-broadcaster","title":"tf2 Static Broadcaster","text":"<p>For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the link_mast, link_lift, and link_wrist_yaw frames.</p> <p>Begin by starting up the stretch driver launch file.</p> <p><pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> Open RViz in another terminal and add the RobotModel and TF plugins in the left hand panel</p> <p><pre><code>ros2 run rviz2 rviz2\n</code></pre> Then run the tf2 broadcaster node to visualize three static frames.</p> <pre><code>ros2 run stretch_ros_tutorials tf_broadcaster\n</code></pre> <p>The GIF below visualizes what happens when running the previous node.</p> <p> </p> <p>OPTIONAL: If you would like to see how the static frames update while the robot is in motion, run the stow command node and observe the tf frames in RViz.</p> <pre><code>ros2 run stretch_ros_tutorials stow_command\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros2/example_10/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python\n\nimport rclpy\nfrom rclpy.node import Node\nfrom tf2_ros import TransformBroadcaster\nimport tf_transformations\nfrom geometry_msgs.msg import TransformStamped\n\n# This node publishes three child static frames in reference to their parent frames as below:\n# parent -&gt; link_mast            child -&gt; fk_link_mast\n# parent -&gt; link_lift            child -&gt; fk_link_lift\n# parent -&gt; link_wrist_yaw       child -&gt; fk_link_wrist_yaw\n\nclass FixedFrameBroadcaster(Node):\n    def __init__(self):\n        super().__init__('stretch_tf_broadcaster')\n        self.br = TransformBroadcaster(self)\n        time_period = 0.1 # seconds\n        self.timer = self.create_timer(time_period, self.broadcast_timer_callback)\n\n        self.mast = TransformStamped()\n        self.mast.header.frame_id = 'link_mast'\n        self.mast.child_frame_id = 'fk_link_mast'\n        self.mast.transform.translation.x = 0.0\n        self.mast.transform.translation.y = 0.0\n        self.mast.transform.translation.z = 0.0\n        q = tf_transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.mast.transform.rotation.x = q[0]\n        self.mast.transform.rotation.y = q[1]\n        self.mast.transform.rotation.z = q[2]\n        self.mast.transform.rotation.w = q[3]\n\n        self.lift = TransformStamped()\n        self.lift.header.stamp = self.get_clock().now().to_msg()\n        self.lift.header.frame_id = 'link_lift'\n        self.lift.child_frame_id = 'fk_link_lift'\n        self.lift.transform.translation.x = 0.0\n        self.lift.transform.translation.y = 2.0\n        self.lift.transform.translation.z = 0.0\n        q = tf_transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.lift.transform.rotation.x = q[0]\n        self.lift.transform.rotation.y = q[1]\n        self.lift.transform.rotation.z = q[2]\n        self.lift.transform.rotation.w = q[3]\n        self.br.sendTransform(self.lift)\n\n        self.wrist = TransformStamped()\n        self.wrist.header.stamp = self.get_clock().now().to_msg()\n        self.wrist.header.frame_id = 'link_wrist_yaw'\n        self.wrist.child_frame_id = 'fk_link_wrist_yaw'\n        self.wrist.transform.translation.x = 0.0\n        self.wrist.transform.translation.y = 2.0\n        self.wrist.transform.translation.z = 0.0\n        q = tf_transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.wrist.transform.rotation.x = q[0]\n        self.wrist.transform.rotation.y = q[1]\n        self.wrist.transform.rotation.z = q[2]\n        self.wrist.transform.rotation.w = q[3]\n        self.br.sendTransform(self.wrist)\n\n        self.get_logger().info(\"Publishing Tf frames. Use RViz to visualize.\")\n\n    def broadcast_timer_callback(self):\n        self.mast.header.stamp = self.get_clock().now().to_msg()\n        self.br.sendTransform(self.mast)\n\n        self.lift.header.stamp = self.get_clock().now().to_msg()\n        self.br.sendTransform(self.lift)\n\n        self.wrist.header.stamp = self.get_clock().now().to_msg()\n        self.br.sendTransform(self.wrist)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    tf_broadcaster = FixedFrameBroadcaster()\n\n    rclpy.spin(tf_broadcaster)\n\n    tf_broadcaster.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_10/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom tf2_ros import TransformBroadcaster\nimport tf_transformations\nfrom geometry_msgs.msg import TransformStamped\n</code></pre> <p>You need to import rclpy if you are writing a ROS 2 node. Import <code>tf_transformations</code> to get quaternion values from Euler angles. Import the <code>TransformStamped</code> from the <code>geometry_msgs.msg</code> package because we will be publishing static frames and it requires this message type. The <code>tf2_ros</code> package provides an implementation of a <code>TransformBroadcaster.</code> to help make the task of publishing transforms easier.</p> <pre><code>class FixedFrameBroadcaster(Node):\n    def __init__(self):\n        super().__init__('stretch_tf_broadcaster')\n        self.br = TransformBroadcaster(self)\n</code></pre> <p>Here we create a <code>TransformStamped</code> object which will be the message we will send over once populated.</p> <pre><code>        self.lift = TransformStamped()\n        self.lift.header.stamp = self.get_clock().now().to_msg()\n        self.lift.header.frame_id = 'link_lift'\n        self.lift.child_frame_id = 'fk_link_lift'\n</code></pre> <p>We need to give the transform being published a timestamp, we'll just stamp it with the current time, <code>self.get_clock().now().to_msg()</code>. Then, we need to set the name of the parent frame of the link we're creating, in this case link_lift. Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is fk_link_lift.</p> <pre><code>        self.mast.transform.translation.x = 0.0\n        self.mast.transform.translation.y = 0.0\n        self.mast.transform.translation.z = 0.0\n</code></pre> <p>Set the translation values for the child frame.</p> <pre><code>        q = tf_transformations.quaternion_from_euler(1.5707, 0, -1.5707)\n        self.lift.transform.rotation.x = q[0]\n        self.lift.transform.rotation.y = q[1]\n        self.lift.transform.rotation.z = q[2]\n        self.lift.transform.rotation.w = q[3]\n</code></pre> <p>The <code>quaternion_from_euler()</code> function takes in a Euler angle argument and returns a quaternion values. Then set the rotation values to the transformed quaternions.</p> <p>This process will be completed for the link_mast and link_wrist_yaw as well.</p> <pre><code>self.br.sendTransform(self.lift)\n</code></pre> <p>Send the three transforms using the <code>sendTransform()</code> function.</p> <pre><code>def main(args=None):\n    rclpy.init(args=args)\n    tf_broadcaster = FixedFrameBroadcaster()\n</code></pre> <p>Instantiate the <code>FixedFrameBroadcaster()</code> class.</p> <pre><code>rclpy.spin(tf_broadcaster)\n</code></pre> <p>Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros2/example_10/#tf2-static-listener","title":"tf2 Static Listener","text":"<p>In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section we will create a tf2 listener that will find the transform between fk_link_lift and link_grasp_center.</p> <p>Begin by starting up the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then run the tf2 broadcaster node to create the three static frames.</p> <pre><code>ros2 run stretch_ros_tutorials tf_broadcaster\n</code></pre> <p>Finally, run the tf2 listener node to print the transform between two links.</p> <pre><code>ros2 run stretch_ros_tutorials tf_listener\n</code></pre> <p>Within the terminal the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames.</p> <pre><code>[INFO] [1659551318.098168]: The pose of target frame link_grasp_center with reference from fk_link_lift is:\ntranslation:\n  x: 1.08415191335\n  y: -0.176147838153\n  z: 0.576720021135\nrotation:\n  x: -0.479004489528\n  y: -0.508053545368\n  z: -0.502884087254\n  w: 0.509454501243\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros2/example_10/#the-code_1","title":"The Code","text":"<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom rclpy.time import Time\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\n\nclass FrameListener(Node):\n\n    def __init__(self):\n        super().__init__('stretch_tf_listener')\n\n        self.declare_parameter('target_frame', 'link_grasp_center')\n        self.target_frame = self.get_parameter(\n            'target_frame').get_parameter_value().string_value\n\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        time_period = 1.0 # seconds\n        self.timer = self.create_timer(time_period, self.on_timer)\n\n    def on_timer(self):\n        from_frame_rel = self.target_frame\n        to_frame_rel = 'fk_link_mast'\n\n        try:\n            now = Time()\n            trans = self.tf_buffer.lookup_transform(\n                to_frame_rel,\n                from_frame_rel,\n                now)\n        except TransformException as ex:\n            self.get_logger().info(\n                f'Could not transform {to_frame_rel} to {from_frame_rel}: {ex}')\n            return\n\n        self.get_logger().info(\n                        f'the pose of target frame {from_frame_rel} with reference to {to_frame_rel} is: {trans}')\n\n\ndef main():\n    rclpy.init()\n    node = FrameListener()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_10/#the-code-explained_1","title":"The Code Explained","text":"<pre><code>        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n</code></pre> <p>Here, we create a <code>TransformListener</code> object. Once the listener is created, it starts receiving tf2 transformations over the wire, and buffers them for up to 10 seconds.</p> <pre><code>        from_frame_rel = self.target_frame\n        to_frame_rel = 'fk_link_mast'\n</code></pre> <p>Store frame names in variables that will be used to compute transformations.</p> <pre><code>        try:\n            now = Time()\n            trans = self.tf_buffer.lookup_transform(\n                to_frame_rel,\n                from_frame_rel,\n                now)\n        except TransformException as ex:\n            self.get_logger().info(\n                f'Could not transform {to_frame_rel} to {from_frame_rel}: {ex}')\n            return\n</code></pre> <p>Try to look up the transform we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between from_frame_rel and to_frame_rel frames with the <code>lookup_transform()</code> function.</p>"},{"location":"stretch-tutorials/ros2/example_12/","title":"Example 12","text":"<p>For this example, we will send follow joint trajectory commands for the head camera to search and locate an ArUco tag. In this instance, a Stretch robot will try to locate the docking station's ArUco tag.</p>"},{"location":"stretch-tutorials/ros2/example_12/#modifying-stretch-marker-dictionary-yaml-file","title":"Modifying Stretch Marker Dictionary YAML File","text":"<p>When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml, that holds the information about the markers. A further breakdown of the YAML file can be found in our Aruco Marker Detection tutorial.</p> <p>Below is what needs to be included in the stretch_marker_dict.yaml file so the detect_aruco_markers node can find the docking station's ArUco tag.</p> <pre><code>'245':\n  'length_mm': 88.0\n  'use_rgb_only': False\n  'name': 'docking_station'\n  'link': None\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_12/#getting-started","title":"Getting Started","text":"<p>Begin by running the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>ros2 launch stretch_core d435i_high_resolution.launch.py\n</code></pre> <p>Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. In a new terminal, execute:</p> <pre><code>ros2 launch stretch_core stretch_aruco.launch.py\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>ros2 run rviz2 rviz2 -d /home/hello-robot/ament_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz\n</code></pre> <p>Then run the aruco_tag_locator.py node. In a new terminal, execute:</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 aruco_tag_locator.py\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros2/example_12/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\n# Import modules\nimport rclpy\nimport time\nimport tf2_ros\nfrom tf2_ros import TransformException\nfrom rclpy.time import Time\nfrom math import pi\n\n# Import hello_misc script for handling trajectory goals with an action client\nimport hello_helpers.hello_misc as hm\n\n# We're going to subscribe to a JointState message type, so we need to import\n# the definition for it\nfrom sensor_msgs.msg import JointState\n\n# Import the FollowJointTrajectory from the control_msgs.action package to\n# control the Stretch robot\nfrom control_msgs.action import FollowJointTrajectory\n\n# Import JointTrajectoryPoint from the trajectory_msgs package to define\n# robot trajectories\nfrom trajectory_msgs.msg import JointTrajectoryPoint\n\n# Import TransformStamped from the geometry_msgs package for the publisher\nfrom geometry_msgs.msg import TransformStamped\n\nclass LocateArUcoTag(hm.HelloNode):\n    \"\"\"\n    A class that actuates the RealSense camera to find the docking station's\n    ArUco tag and returns a Transform between the `base_link` and the requested tag.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes the subscriber and other needed variables.\n        :param self: The self reference.\n        \"\"\"\n        # Initialize the inhereted hm.Hellonode class\n        hm.HelloNode.__init__(self)\n        hm.HelloNode.main(self, 'aruco_tag_locator', 'aruco_tag_locator', wait_for_first_pointcloud=False)\n        # Initialize subscriber\n        self.joint_states_sub = self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)\n        # Initialize publisher\n        self.transform_pub = self.create_publisher(TransformStamped, 'ArUco_transform', 10)\n\n        # Initialize the variable that will store the joint state positions\n        self.joint_state = None\n\n        # Provide the min and max joint positions for the head pan. These values\n        # are needed for sweeping the head to search for the ArUco tag\n        self.min_pan_position = -3.8\n        self.max_pan_position =  1.50\n\n        # Define the number of steps for the sweep, then create the step size for\n        # the head pan joint\n        self.pan_num_steps = 10\n        self.pan_step_size = abs(self.min_pan_position - self.max_pan_position)/self.pan_num_steps\n\n        # Define the min tilt position, number of steps, and step size\n        self.min_tilt_position = -0.75\n        self.tilt_num_steps = 3\n        self.tilt_step_size = pi/16\n\n        # Define the head actuation rotational velocity\n        self.rot_vel = 0.5 # radians per sec\n\n    def joint_states_callback(self, msg):\n        \"\"\"\n        A callback function that stores Stretch's joint states.\n        :param self: The self reference.\n        :param msg: The JointState message type.\n        \"\"\"\n        self.joint_state = msg\n\n    def send_command(self, command):\n        '''\n        Handles single joint control commands by constructing a FollowJointTrajectoryGoal\n        message and sending it to the trajectory_client created in hello_misc.\n        :param self: The self reference.\n        :param command: A dictionary message type.\n        '''\n        if (self.joint_state is not None) and (command is not None):\n\n            # Extract the string value from the `joint` key\n            joint_name = command['joint']\n\n            # Set trajectory_goal as a FollowJointTrajectory.Goal and define\n            # the joint name\n            trajectory_goal = FollowJointTrajectory.Goal()\n            trajectory_goal.trajectory.joint_names = [joint_name]\n\n            # Create a JointTrajectoryPoint message type\n            point = JointTrajectoryPoint()\n\n            # Check to see if `delta` is a key in the command dictionary\n            if 'delta' in command:\n                # Get the current position of the joint and add the delta as a\n                # new position value\n                joint_index = self.joint_state.name.index(joint_name)\n                joint_value = self.joint_state.position[joint_index]\n                delta = command['delta']\n                new_value = joint_value + delta\n                point.positions = [new_value]\n\n            # Check to see if `position` is a key in the command dictionary\n            elif 'position' in command:\n                # extract the head position value from the `position` key\n                point.positions = [command['position']]\n\n            # Set the rotational velocity\n            point.velocities = [self.rot_vel]\n\n            # Assign goal position with updated point variable\n            trajectory_goal.trajectory.points = [point]\n\n            # Specify the coordinate frame that we want (base_link) and set the time to be now.\n            trajectory_goal.trajectory.header.stamp = self.get_clock().now().to_msg()\n            trajectory_goal.trajectory.header.frame_id = 'base_link'\n\n            # Make the action call and send the goal. The last line of code waits\n            # for the result\n            self.trajectory_client.send_goal(trajectory_goal)\n\n    def find_tag(self, tag_name='docking_station'):\n        \"\"\"\n        A function that actuates the camera to search for a defined ArUco tag\n        marker. Then the function returns the pose.\n        :param self: The self reference.\n        :param tag_name: A string value of the ArUco marker name.\n\n        :returns transform: The docking station's TransformStamped message.\n        \"\"\"\n        # Create dictionaries to get the head in its initial position\n        pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n        self.send_command(pan_command)\n        tilt_command = {'joint': 'joint_head_tilt', 'position': self.min_tilt_position}\n        self.send_command(tilt_command)\n\n        # Nested for loop to sweep the joint_head_pan and joint_head_tilt in increments\n        for i in range(self.tilt_num_steps):\n            for j in range(self.pan_num_steps):\n                # Update the joint_head_pan position by the pan_step_size\n                pan_command = {'joint': 'joint_head_pan', 'delta': self.pan_step_size}\n                self.send_command(pan_command)\n\n                # Give time for system to do a Transform lookup before next step\n                time.sleep(0.2)\n\n                # Use a try-except block\n                try:\n                    now = Time()\n                    # Look up transform between the base_link and requested ArUco tag\n                    transform = self.tf_buffer.lookup_transform('base_link',\n                                                            tag_name,\n                                                            now)\n                    self.get_logger().info(\"Found Requested Tag: \\n%s\", transform)\n\n                    # Publish the transform\n                    self.transform_pub.publish(transform)\n\n                    # Return the transform\n                    return transform\n                except TransformException as ex:\n                    continue\n\n            # Begin sweep with new tilt angle\n            pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n            self.send_command(pan_command)\n            tilt_command = {'joint': 'joint_head_tilt', 'delta': self.tilt_step_size}\n            self.send_command(tilt_command)\n            time.sleep(0.25)\n\n        # Notify that the requested tag was not found\n        self.get_logger().info(\"The requested tag '%s' was not found\", tag_name)\n\n    def main(self):\n        \"\"\"\n        Function that initiates the issue_command function.\n        :param self: The self reference.\n        \"\"\"\n        # Create a StaticTranformBoradcaster Node. Also, start a Tf buffer that\n        # will store the tf information for a few seconds.Then set up a tf listener, which\n        # will subscribe to all of the relevant tf topics, and keep track of the information\n        self.static_broadcaster = tf2_ros.StaticTransformBroadcaster(self)\n        self.tf_buffer = tf2_ros.Buffer()\n        self.listener = tf2_ros.TransformListener(self.tf_buffer, self)\n\n        # Give the listener some time to accumulate transforms\n        time.sleep(1.0)\n\n        # Notify Stretch is searching for the ArUco tag with `get_logger().info()`\n        self.get_logger().info('Searching for docking ArUco tag')\n\n        # Search for the ArUco marker for the docking station\n        pose = self.find_tag(\"docking_station\")\n\ndef main():\n    try:\n        # Instantiate the `LocateArUcoTag()` object\n        node = LocateArUcoTag()\n        # Run the `main()` method\n        node.main()\n        node.new_thread.join()\n    except:\n        node.get_logger().info('Interrupt received, so shutting down')\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_12/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport time\nimport tf2_ros\nfrom tf2_ros import TransformException\nfrom rclpy.time import Time\nfrom math import pi\n\nimport hello_helpers.hello_misc as hm\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom geometry_msgs.msg import TransformStamped\n</code></pre> <p>You need to import <code>rclpy</code> if you are writing a ROS Node. Import other python modules needed for this node. Import the <code>FollowJointTrajectory</code> from the control_msgs.action package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.</p> <pre><code>def __init__(self):\n        # Initialize the inhereted hm.Hellonode class\n        hm.HelloNode.__init__(self)\n        hm.HelloNode.main(self, 'aruco_tag_locator', 'aruco_tag_locator', wait_for_first_pointcloud=False)\n        # Initialize subscriber\n        self.joint_states_sub = self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)\n        # Initialize publisher\n        self.transform_pub = self.create_publisher(TransformStamped, 'ArUco_transform', 10)\n\n        # Initialize the variable that will store the joint state positions\n        self.joint_state = None\n</code></pre> <p>The <code>LocateArUcoTag</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is instantiated.</p> <p>Set up a subscriber with <code>self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)</code>.  We're going to subscribe to the topic <code>stretch/joint_states</code>, looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function <code>joint_states_callback()</code> automatically.</p> <p><code>self.create_publisher(TransformStamped, 'ArUco_transform', 10)</code> declares that your node is publishing to the <code>ArUco_transform</code> topic using the message type <code>TransformStamped</code>. The <code>10</code> argument limits the amount of queued messages if any subscriber is not receiving them fast enough.</p> <pre><code>self.min_pan_position = -4.10\nself.max_pan_position =  1.50\nself.pan_num_steps = 10\nself.pan_step_size = abs(self.min_pan_position - self.max_pan_position)/self.pan_num_steps\n</code></pre> <p>Provide the minimum and maximum joint positions for the head pan. These values are needed for sweeping the head to search for the ArUco tag. We also define the number of steps for the sweep, then create the step size for the head pan joint.</p> <pre><code>self.min_tilt_position = -0.75\nself.tilt_num_steps = 3\nself.tilt_step_size = pi/16\n</code></pre> <p>Set the minimum position of the tilt joint, the number of steps, and the size of each step.</p> <pre><code>self.rot_vel = 0.5 # radians per sec\n</code></pre> <p>Define the head actuation rotational velocity.</p> <pre><code>def joint_states_callback(self, msg):\n    self.joint_state = msg\n</code></pre> <p>The <code>joint_states_callback()</code> function stores Stretch's joint states.</p> <pre><code>def send_command(self, command):\n    if (self.joint_state is not None) and (command is not None):\n        joint_name = command['joint']\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = [joint_name]\n        point = JointTrajectoryPoint()\n</code></pre> <p>Assign <code>trajectory_goal</code> as a <code>FollowJointTrajectory.Goal</code> message type. Then extract the string value from the <code>joint</code> key. Also, assign <code>point</code> as a <code>JointTrajectoryPoint</code> message type.</p> <pre><code>if 'delta' in command:\n    joint_index = self.joint_state.name.index(joint_name)\n    joint_value = self.joint_state.position[joint_index]\n    delta = command['delta']\n    new_value = joint_value + delta\n    point.positions = [new_value]\n</code></pre> <p>Check to see if <code>delta</code> is a key in the command dictionary. Then get the current position of the joint and add the delta as a new position value.</p> <pre><code>elif 'position' in command:\n    point.positions = [command['position']]\n</code></pre> <p>Check to see if <code>position</code> is a key in the command dictionary. Then extract the position value.</p> <pre><code>point.velocities = [self.rot_vel]\ntrajectory_goal.trajectory.points = [point]\ntrajectory_goal.trajectory.header.stamp = self.get_clock().now().to_msg()\ntrajectory_goal.trajectory.header.frame_id = 'base_link'\nself.trajectory_client.send_goal(trajectory_goal)\n</code></pre> <p>Then <code>trajectory_goal.trajectory.points</code> is defined by the positions set in <code>point</code>. Specify the coordinate frame that we want (base_link) and set the time to be now. Make the action call and send the goal.</p> <pre><code>def find_tag(self, tag_name='docking_station'):\n    pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\n    self.send_command(pan_command)\n    tilt_command = {'joint': 'joint_head_tilt', 'position': self.min_tilt_position}\n    self.send_command(tilt_command)\n</code></pre> <p>Create a dictionary to get the head in its initial position for its search and send the commands with the <code>send_command()</code> function.</p> <pre><code>for i in range(self.tilt_num_steps):\n    for j in range(self.pan_num_steps):\n        pan_command = {'joint': 'joint_head_pan', 'delta': self.pan_step_size}\n        self.send_command(pan_command)\n        time.sleep(0.5)\n</code></pre> <p>Utilize a nested for loop to sweep the pan and tilt in increments. Then update the <code>joint_head_pan</code> position by the <code>pan_step_size</code>. Use <code>time.sleep()</code> function to give time to the system to do a Transform lookup before the next step.</p> <pre><code>try:\n    now = Time()\n    transform = self.tf_buffer.lookup_transform('base_link',\n                                                tag_name,\n                                                now)\n    self.get_logger().info(\"Found Requested Tag: \\n%s\", transform)\n    self.transform_pub.publish(transform)\n    return transform\nexcept TransformException as ex:\n    continue\n</code></pre> <p>Use a try-except block to look up the transform between the base_link and the requested ArUco tag. Then publish and return the <code>TransformStamped</code> message.</p> <pre><code>pan_command = {'joint': 'joint_head_pan', 'position': self.min_pan_position}\nself.send_command(pan_command)\ntilt_command = {'joint': 'joint_head_tilt', 'delta': self.tilt_step_size}\nself.send_command(tilt_command)\ntime.sleep(.25)\n</code></pre> <p>Begin sweep with new tilt angle.</p> <pre><code>def main(self):\n    self.static_broadcaster = tf2_ros.StaticTransformBroadcaster(self)\n    self.tf_buffer = tf2_ros.Buffer()\n    self.listener = tf2_ros.TransformListener(self.tf_buffer, self)\n    time.sleep(1.0)\n</code></pre> <p>Create a StaticTranformBoradcaster Node. Also, start a tf buffer that will store the tf information for a few seconds. Then set up a tf listener, which will subscribe to all of the relevant tf topics, and keep track of the information. Include <code>time.sleep(1.0)</code> to give the listener some time to accumulate transforms.</p> <pre><code>self.get_logger().info('Searching for docking ArUco tag')\npose = self.find_tag(\"docking_station\")\n</code></pre> <p>Notice Stretch is searching for the ArUco tag with a <code>self.get_logger().info()</code> function. Then search for the ArUco marker for the docking station.</p> <p><pre><code>def main():\n    try:\n        node = LocateArUcoTag()\n        node.main()\n        node.new_thread.join()\n    except:\n        node.get_logger().info('Interrupt received, so shutting down')\n        node.destroy_node()\n        rclpy.shutdown()\n</code></pre> Instantiate the <code>LocateArUcoTag()</code> object and run the <code>main()</code> method.</p>"},{"location":"stretch-tutorials/ros2/example_2/","title":"Filter Laser Scans","text":""},{"location":"stretch-tutorials/ros2/example_2/#example-2","title":"Example 2","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>This example aims to provide instructions on how to filter scan messages.</p> <p>For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specification itself:</p> <pre><code># Laser scans angles are measured counter clockwise,\n# with Stretch's LiDAR having both angle_min and angle_max facing forward\n# (very closely along the x-axis) of the device frame\n#\nstd_msgs/Header header   # timestamp data in a particular coordinate frame\nfloat32 angle_min        # start angle of the scan [rad]\nfloat32 angle_max        # end angle of the scan [rad]\nfloat32 angle_increment  # angular distance between measurements [rad]\nfloat32 time_increment   # time between measurements [seconds]\nfloat32 scan_time        # time between scans [seconds]\nfloat32 range_min        # minimum range value [m]\nfloat32 range_max        # maximum range value [m]\nfloat32[] ranges         # range data [m] (Note: values &lt; range_min or &gt; range_max should be discarded)\nfloat32[] intensities    # intensity data [device-specific units]\n</code></pre> <p>The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type.</p> <p> </p> <p>For a Stretch robot the start angle of the scan, <code>angle_min</code>, and end angle, <code>angle_max</code>, are closely located along the x-axis of Stretch's frame. <code>angle_min</code> and <code>angle_max</code> are set at -3.1416 and 3.1416, respectively. This is illustrated by the images below.</p> <p> </p> <p>Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot.</p> <p>First, open a terminal and run the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then in a new terminal run the rplidar launch file from <code>stretch_core</code>. <pre><code>ros2 launch stretch_core rplidar.launch.py\n</code></pre></p> <p>To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan filter node by typing the following in a new terminal.</p> <pre><code>ros2 run stretch_ros_tutorials scan_filter\n</code></pre> <p>Then run the following command to bring up a simple RViz configuration of the Stretch robot. <pre><code>ros2 run rviz2 rviz2 -d `ros2 pkg prefix stretch_calibration`/share/stretch_calibration/rviz/stretch_simple_test.rviz\n</code></pre></p> <p>Note</p> <p>If the laser scan points published by the scan or the scan_filtered topic are not visible in RViz, you can visualize them by adding them using the 'Add' button in the left panel, selecting the 'By topic' tab, and then selecting the scan or scan_filtered topic.</p> <p>Change the topic name from the LaserScan display from /scan to /filter_scan.</p> <p> </p>"},{"location":"stretch-tutorials/ros2/example_2/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom numpy import linspace, inf\nfrom math import sin\nfrom sensor_msgs.msg import LaserScan\nclass ScanFilter(Node):\n    def __init__(self):\n        super().__init__('stretch_scan_filter')\n        self.pub = self.create_publisher(LaserScan, '/filtered_scan', 10)\n        self.sub = self.create_subscription(LaserScan, '/scan', self.scan_filter_callback, 10)\n\n        self.width = 1\n        self.extent = self.width / 2.0\n        self.get_logger().info(\"Publishing the filtered_scan topic. Use RViz to visualize.\")\n    def scan_filter_callback(self,msg):\n        angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n        msg.ranges = new_ranges\n        self.pub.publish(msg)\ndef main(args=None):\n    rclpy.init(args=args)\n    scan_filter = ScanFilter()\n    rclpy.spin(scan_filter)\n    scan_filter.destroy_node()\n    rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_2/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom numpy import linspace, inf\nfrom math import sin\nfrom sensor_msgs.msg import LaserScan\n</code></pre> <p>You need to import rclpy if you are writing a ROS Node. There are functions from numpy and math that are required within this code, that's why linspace, inf, and sin are imported. The sensor_msgs.msg import is so that we can subscribe and publish LaserScan messages.</p> <p><pre><code>self.width = 1\nself.extent = self.width / 2.0\n</code></pre> We're going to assume that the robot is pointing up the x-axis, so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered.</p> <pre><code>self.sub = self.create_subscription(LaserScan, '/scan', self.scan_filter_callback, 10)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic \"/scan\", looking for LaserScan messages.  When a message comes in, ROS is going to pass it to the function \"callback\" automatically.</p> <pre><code>self.pub = self.create_publisher(LaserScan, '/filtered_scan', 10)\n</code></pre> <p>This declares that your node is publishing to the filtered_scan topic using the message type LaserScan. This lets any nodes listening on filtered_scan that we are going to publish data on that topic.</p> <p><pre><code>angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n</code></pre> This line of code utilizes linspace to compute each angle of the subscribed scan.</p> <p><pre><code>points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n</code></pre> Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference.</p> <p><pre><code>new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n</code></pre> If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\".</p> <p><pre><code>msg.ranges = new_ranges\nself.pub.publish(msg)\n</code></pre> Substitute the new ranges in the original message, and republish it.</p> <p><pre><code>def main(args=None):\n    rclpy.init(args=args)\n    scan_filter = ScanFilter()\n</code></pre> The next line, rclpy.init_node initializes the node. In this case, your node will take on the name 'stretch_scan_filter'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Setup Scanfilter class with <code>scan_filter = Scanfilter()</code></p> <pre><code>rclpy.spin(scan_filter)\n</code></pre> <p>Give control to ROS.  This will allow the callback to be called whenever new messages come in.  If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros2/example_3/","title":"Mobile Base Collision Avoidance","text":""},{"location":"stretch-tutorials/ros2/example_3/#example-3","title":"Example 3","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>The aim of example 3 is to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py mode:=navigation\n</code></pre> <p>Then in a new terminal type the following to activate the LiDAR sensor.</p> <pre><code>ros2 launch stretch_core rplidar.launch.py\n</code></pre> <p>To activate the avoider node, type the following in a new terminal.</p> <pre><code>ros2 run stretch_ros_tutorials avoider\n</code></pre> <p>To stop the node from sending twist messages, type Ctrl + c in the terminal running the avoider node.</p> <p> </p>"},{"location":"stretch-tutorials/ros2/example_3/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom numpy import linspace, inf, tanh\nfrom math import sin\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nclass Avoider(Node):\n    def __init__(self):\n        super().__init__('stretch_avoider')\n        self.width = 1\n        self.extent = self.width / 2.0\n        self.distance = 0.5\n        self.twist = Twist()\n        self.twist.linear.x = 0.0\n        self.twist.linear.y = 0.0\n        self.twist.linear.z = 0.0\n        self.twist.angular.x = 0.0\n        self.twist.angular.y = 0.0\n        self.twist.angular.z = 0.0\n        self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 1)\n        self.subscriber_ = self.create_subscription(LaserScan, '/scan', self.set_speed, 10)\n    def set_speed(self, msg):\n        angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n        points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        new_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n        error = min(new_ranges) - self.distance\n        self.twist.linear.x = tanh(error) if (error &gt; 0.05 or error &lt; -0.05) else 0\n        self.publisher_.publish(self.twist)\ndef main(args=None):\n    rclpy.init(args=args)\n    avoider = Avoider()\n    rclpy.spin(avoider)\n    avoider.destroy_node()\n    rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_3/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom numpy import linspace, inf, tanh\nfrom math import sin\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\n</code></pre> <p>You need to import rclpy if you are writing a ROS Node. There are functions from numpy and math that are required within this code, thus linspace, inf, tanh, and sin are imported. The sensor_msgs.msg import is so that we can subscribe to LaserScan messages. The geometry_msgs.msg import is so that we can send velocity commands to the robot.</p> <pre><code>self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 1)\n</code></pre> <p>This declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist.</p> <pre><code>self.subscriber_ = self.create_subscription(LaserScan, '/scan', self.set_speed, 10)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic \"/scan\", looking for LaserScan messages.  When a message comes in, ROS is going to pass it to the function \"set_speed\" automatically.</p> <pre><code>self.width = 1\nself.extent = self.width / 2.0\nself.distance = 0.5\n</code></pre> <p>self.width is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing in the x-axis, any points with y coordinates further than half of the defined width (self.extent) from the x-axis are not considered. self.distance defines the stopping distance from an object.</p> <pre><code>self.twist = Twist()\nself.twist.linear.x = 0.0\nself.twist.linear.y = 0.0\nself.twist.linear.z = 0.0\nself.twist.angular.x = 0.0\nself.twist.angular.y = 0.0\nself.twist.angular.z = 0.0\n</code></pre> <p>Allocate a Twist to use, and set everything to zero.  We're going to do this when the class is initiating. Redefining this within the callback function, <code>set_speed()</code> can be more computationally taxing.</p> <pre><code>angles = linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\npoints = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\nnew_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, points)]\n</code></pre> <p>This line of code utilizes linspace to compute each angle of the subscribed scan. Here we  compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\".</p> <pre><code>error = min(new_ranges) - self.distance\n</code></pre> <p>Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as error.</p> <pre><code>self.twist.linear.x = tanh(error) if (error &gt; 0.05 or error &lt; -0.05) else 0\n</code></pre> <p>Set the speed according to a tanh function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1</p> <pre><code>self.publisher_.publish(self.twist)\n</code></pre> <p>Publish the Twist message.</p> <pre><code>def main(args=None):\n    rclpy.init(args=args)\n    avoider = Avoider()\n    rclpy.spin(avoider)\n    avoider.destroy_node()\n    rclpy.shutdown()\n</code></pre> <p>The next line, rclpy.init() method initializes the node. In this case, your node will take on the name 'stretch_avoider'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Setup Avoider class with <code>avoider = Avioder()</code></p> <p>Give control to ROS with <code>rclpy.spin()</code>. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros2/example_4/","title":"Give Stretch a Balloon","text":""},{"location":"stretch-tutorials/ros2/example_4/#example-4","title":"Example 4","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p></p> <p>Let's bring up Stretch in RViz by using the following command.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\nros2 run rviz2 rviz2 -d `ros2 pkg prefix stretch_calibration`/share/stretch_calibration/rviz/stretch_simple_test.rviz\n</code></pre> <p>In a new terminal run the following commands to create a marker.</p> <pre><code>ros2 run stretch_ros_tutorials marker\n</code></pre> <p>The gif below demonstrates how to add a new Marker display type, and change the topic name from <code>visualization_marker</code> to <code>balloon</code>. A red sphere Marker should appear above the Stretch robot.</p> <p></p>"},{"location":"stretch-tutorials/ros2/example_4/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom visualization_msgs.msg import Marker\nclass Balloon(Node):\n    def __init__(self):\n        super().__init__('stretch_marker')\n        self.publisher_ = self.create_publisher(Marker, 'balloon', 10)  \n\n        self.marker = Marker()\n        self.marker.header.frame_id = '/base_link'\n        self.marker.header.stamp = self.get_clock().now().to_msg()\n        self.marker.type = self.marker.SPHERE\n        self.marker.id = 0\n        self.marker.action = self.marker.ADD\n        self.marker.scale.x = 0.5\n        self.marker.scale.y = 0.5\n        self.marker.scale.z = 0.5\n        self.marker.color.r = 1.0\n        self.marker.color.g = 0.0\n        self.marker.color.b = 0.0\n        self.marker.color.a = 1.0\n        self.marker.pose.position.x = 0.0\n        self.marker.pose.position.y = 0.0\n        self.marker.pose.position.z = 2.0\n        self.get_logger().info(\"Publishing the balloon topic. Use RViz to visualize.\")\n    def publish_marker(self):\n        self.publisher_.publish(self.marker)\ndef main(args=None):\n    rclpy.init(args=args)\n    balloon = Balloon()\n    while rclpy.ok():\n        balloon.publish_marker()\n    balloon.destroy_node()  \n    rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_4/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.node import Node\nfrom visualization_msgs.msg import Marker\n</code></pre> <p>You need to import rclpy if you are writing a ROS 2 Node. Import the <code>Marker</code> type from the visualization_msgs.msg package. This import is required to publish a Marker, which will be visualized in RViz.</p> <pre><code>self.publisher_ = self.create_publisher(Marker, 'balloon', 10)  \n</code></pre> <p>This declares that your node is publishing to the /ballon topic using the message type Twist.</p> <pre><code>self.marker = Marker()\nself.marker.header.frame_id = '/base_link'\nself.marker.header.stamp = self.get_clock().now().to_msg()\nself.marker.type = self.marker.SPHERE\n</code></pre> <p>Create a maker. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers</p> <pre><code>self.marker.id = 0\n</code></pre> <p>Each marker has a unique ID number. If you have more than one marker that you want displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number of an existing marker, it will replace the existing marker with that ID number.</p> <pre><code>self.marker.action = self.marker.ADD\n</code></pre> <p>This line of code sets the action. You can add, delete, or modify markers.</p> <pre><code>self.marker.scale.x = 0.5\nself.marker.scale.y = 0.5\nself.marker.scale.z = 0.5\n</code></pre> <p>These are the size parameters for the marker. These will vary by marker type.</p> <pre><code>self.marker.color.r = 1.0\nself.marker.color.g = 0.0\nself.marker.color.b = 0.0\n</code></pre> <p>Color of the object, specified as r/g/b/a, with values in the range of [0, 1].</p> <pre><code>self.marker.color.a = 1.0\n</code></pre> <p>The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible.</p> <pre><code>self.marker.pose.position.x = 0.0\nself.marker.pose.position.y = 0.0\nself.marker.pose.position.z = 2.0\n</code></pre> <p>Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in frame_id. In this case, the position will always be directly 2 meters above the frame_id (base_link), and will move with it.</p> <pre><code>def publish_marker(self):\n        self.publisher_.publish(self.marker)\n</code></pre> <p>Publish the Marker data structure to be visualized in RViz.</p> <pre><code>def main(args=None):\n    rclpy.init(args=args)\n    balloon = Balloon()\n</code></pre> <p>The next line, rospy.init. In this case, your node will take on the name talker. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Setup Balloon class with <code>balloon = Balloon()</code></p> <pre><code>while rclpy.ok():\n    balloon.publish_marker()\n    balloon.destroy_node()  \n    rclpy.shutdown()\n</code></pre> <p>This loop is a fairly standard rclpy construct: checking the rclpy.ok() flag and then doing work. You have to run this check to see if your program should exit (e.g. if there is a Ctrl-C or otherwise).</p>"},{"location":"stretch-tutorials/ros2/example_5/","title":"Print Joint States","text":""},{"location":"stretch-tutorials/ros2/example_5/#example-5","title":"Example 5","text":"<p>In this example, we will review a Python script that prints out the positions of a selected group of Stretch joints. This script is helpful if you need the joint positions after you teleoperated Stretch with the Xbox controller or physically moved the robot to the desired configuration after hitting the run stop button.</p> <p>If you are looking for a continuous print of the joint states while Stretch is in action, then you can use the ros2 topic command-line tool shown in the Internal State of Stretch Tutorial.</p> <p>Begin by starting up the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>You can then hit the run-stop button (you should hear a beep and the LED light in the button blink) and move the robot's joints to a desired configuration. Once you are satisfied with the configuration, hold the run-stop button until you hear a beep. Then run the following command to execute the joint_state_printer.py which will print the joint positions of the lift, arm, and wrist. In a new terminal, execute:</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 joint_state_printer.py\n</code></pre> <p>Your terminal will output the <code>position</code> information of the previously mentioned joints shown below. <pre><code>name: ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\nposition: [0.6043133175850597, 0.19873586673129257, 0.017257283863713464]\n</code></pre></p> <p>Note</p> <p>Stretch's arm has four prismatic joints and the sum of their positions gives the wrist_extension distance. The wrist_extension is needed when sending joint trajectory commands to the robot. Further, you can not actuate an individual arm joint. Here is an image of the arm joints for reference:</p> <p> </p>"},{"location":"stretch-tutorials/ros2/example_5/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport sys\nimport time\n\n# We're going to subscribe to a JointState message type, so we need to import\n# the definition for it\nfrom sensor_msgs.msg import JointState\n\nclass JointStatePublisher(Node):\n    \"\"\"\n    A class that prints the positions of desired joints in Stretch.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Function that initializes the subscriber.\n        :param self: The self reference\n        \"\"\"\n        super().__init__('stretch_joint_state')\n\n        # Set up a subscriber. We're going to subscribe to the topic \"joint_states\"\n        self.sub = self.create_subscription(JointState, 'joint_states', self.callback, 1)\n\n\n    def callback(self, msg):\n        \"\"\"\n        Callback function to deal with the incoming JointState messages.\n        :param self: The self reference.\n        :param msg: The JointState message.\n        \"\"\"\n        # Store the joint messages for later use\n        self.get_logger().info('Receiving JointState messages')\n        self.joint_states = msg\n\n\n    def print_states(self, joints):\n        \"\"\"\n        print_states function to deal with the incoming JointState messages.\n        :param self: The self reference.\n        :param joints: A list of string values of joint names.\n        \"\"\"\n        # Create an empty list that will store the positions of the requested joints\n        joint_positions = []\n\n        # Use of forloop to parse the names of the requested joints list.\n        # The index() function returns the index at the first occurrence of\n        # the name of the requested joint in the self.joint_states.name list\n        for joint in joints:\n            if joint == \"wrist_extension\":\n                index = self.joint_states.name.index('joint_arm_l0')\n                joint_positions.append(4*self.joint_states.position[index])\n                continue\n\n            index = self.joint_states.name.index(joint)\n            joint_positions.append(self.joint_states.position[index])\n\n        # Print the joint position values to the terminal\n        print(\"name: \" + str(joints))\n        print(\"position: \" + str(joint_positions))\n\n        # Sends a signal to rclpy to shutdown the ROS interfaces\n        rclpy.shutdown()\n\n        # Exit the Python interpreter\n        sys.exit(0)\n\ndef main(args=None):\n    # Initialize the node\n    rclpy.init(args=args)\n    joint_publisher = JointStatePublisher()\n    time.sleep(1)\n    rclpy.spin_once(joint_publisher)\n\n    # Create a list of the joints and name them joints. These will be an argument\n    # for the print_states() function\n    joints = [\"joint_lift\", \"wrist_extension\", \"joint_wrist_yaw\"]\n    joint_publisher.print_states(joints)\n\n    # Give control to ROS.  This will allow the callback to be called whenever new\n    # messages come in.  If we don't put this line in, then the node will not work,\n    # and ROS will not process any messages\n    rclpy.spin(joint_publisher)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_5/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport sys\nimport time\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\n</code></pre> <p>You need to import rclpy if you are writing a ROS 2 Node. Import <code>sensor_msgs.msg</code> so that we can subscribe to <code>JointState</code> messages.</p> <pre><code>self.sub = self.create_subscription(JointState, 'joint_states', self.callback, 1)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>joint_states</code>, looking for <code>JointState</code> messages.  When a message comes in, ROS is going to pass it to the function \"callback\" automatically</p> <pre><code>def callback(self, msg):\n    self.joint_states = msg\n</code></pre> <p>This is the callback function where the <code>JointState</code> messages are stored as <code>self.joint_states</code>. Further information about this message type can be found here: JointState Message</p> <pre><code>def print_states(self, joints):\n    joint_positions = []\n</code></pre> <p>This is the <code>print_states()</code> function which takes in a list of joints of interest as its argument. the is also an empty list set as <code>joint_positions</code> and this is where the positions of the requested joints will be appended.</p> <pre><code>for joint in joints:\n  if joint == \"wrist_extension\":\n    index = self.joint_states.name.index('joint_arm_l0')\n    joint_positions.append(4*self.joint_states.position[index])\n    continue\n  index = self.joint_states.name.index(joint)\n  joint_positions.append(self.joint_states.position[index])\n</code></pre> <p>In this section of the code, a for loop is used to parse the names of the requested joints from the <code>self.joint_states</code> list. The <code>index()</code> function returns the index of the name of the requested joint and appends the respective position to the <code>joint_positions</code> list.</p> <pre><code>rclpy.shutdown()\nsys.exit(0)\n</code></pre> <p>The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter.</p> <pre><code>rclpy.init(args=args)\njoint_publisher = JointStatePublisher()\ntime.sleep(1)\n</code></pre> <p>The next line, rclpy.init_node initializes the node. In this case, your node will take on the name 'stretch_joint_state'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p>Declare object, joint_publisher, from the <code>JointStatePublisher</code> class.</p> <p>The use of the <code>time.sleep()</code> function is to allow the joint_publisher class to initialize all of its features before requesting to publish joint positions of desired joints (running the <code>print_states()</code> method).</p> <pre><code>joints = [\"joint_lift\", \"wrist_extension\", \"joint_wrist_yaw\"]\n#joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"]\njoint_publisher.print_states(joints)\n</code></pre> <p>Create a list of the desired joints that you want positions to be printed. Then use that list as an argument for the <code>print_states()</code> method.</p> <pre><code>rclpy.spin(joint_publisher)\n</code></pre> <p>Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros2/example_6/","title":"Store Effort Values","text":""},{"location":"stretch-tutorials/ros2/example_6/#example-6","title":"Example 6","text":"<p>In this example, we will review a Python script that prints and stores the effort values from a specified joint. If you are looking for a continuous print of the joint state efforts while Stretch is in action, then you can use the ros2 topic command-line tool shown in the Internal State of Stretch Tutorial.</p> <p> </p> <p>Begin by running the following command in a terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>There's no need to switch to the position mode in comparison with ROS1 because the default mode of the launcher is this position mode. Then run the effort_sensing.py node. In a new terminal, execute:</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 effort_sensing.py\n</code></pre> <p>This will send a <code>FollowJointTrajectory</code> command to move Stretch's arm or head while also printing the effort of the lift.</p>"},{"location":"stretch-tutorials/ros2/example_6/#the-code","title":"The Code","text":"<pre><code>##!/usr/bin/env python3\n\nimport rclpy\nimport hello_helpers.hello_misc as hm\nimport os\nimport csv\nimport time\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('tkagg')\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom datetime import datetime\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\n\nclass JointActuatorEffortSensor(hm.HelloNode):\n    def __init__(self, export_data=True, animate=True):\n        hm.HelloNode.__init__(self)\n        hm.HelloNode.main(self, 'issue_command', 'issue_command', wait_for_first_pointcloud=False)\n        self.joints = ['joint_lift']\n        self.joint_effort = []\n        self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n        self.export_data = export_data\n        self.result = False\n        self.file_name = datetime.now().strftime(\"effort_sensing_tutorial_%Y%m%d%I\")\n\n\n    def issue_command(self):\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = self.joints\n\n        point0 = JointTrajectoryPoint()\n        point0.positions = [0.9]\n\n        trajectory_goal.trajectory.points = [point0]\n        trajectory_goal.trajectory.header.stamp = self.get_clock().now().to_msg()\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n\n        while self.joint_state is None:\n            time.sleep(0.1)\n        self._send_goal_future = self.trajectory_client.send_goal_async(trajectory_goal, self.feedback_callback)\n        self.get_logger().info('Sent position goal = {0}'.format(trajectory_goal))\n        self._send_goal_future.add_done_callback(self.goal_response_callback)\n\n    def goal_response_callback(self, future):\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info('Failed')\n            return\n\n        self.get_logger().info('Succeded')\n\n        self._get_result_future = goal_handle.get_result_async()\n        self._get_result_future.add_done_callback(self.get_result_callback)\n\n    def get_result_callback(self, future):\n        self.result = future.result().result\n        self.get_logger().info('Sent position goal = {0}'.format(self.result))\n\n    def feedback_callback(self, feedback_msg):\n        if 'wrist_extension' in self.joints:\n            self.joints.remove('wrist_extension')\n            self.joints.append('joint_arm_l0')\n\n        current_effort = []\n        for joint in self.joints:\n            index = self.joint_state.name.index(joint)\n            current_effort.append(self.joint_state.effort[index])\n\n        if not self.export_data:\n            print(\"name: \" + str(self.joints))\n            print(\"effort: \" + str(current_effort))\n        else:\n            self.joint_effort.append(current_effort)\n\n        if self.export_data:\n            file_name = self.file_name\n            completeName = os.path.join(self.save_path, file_name)\n            with open(completeName, \"w\") as f:\n                writer = csv.writer(f)\n                writer.writerow(self.joints)\n                writer.writerows(self.joint_effort)\n\n    def plot_data(self, animate = True):\n        while not self.result:\n            time.sleep(0.1)\n        file_name = self.file_name\n        self.completeName = os.path.join(self.save_path, file_name)\n        self.data = pd.read_csv(self.completeName)\n        self.y_anim = []\n        self.animate = animate\n\n        for joint in self.data.columns:\n\n            # Create figure, labels, and title\n            fig = plt.figure()\n            plt.title(joint + ' Effort Sensing')\n            plt.ylabel('Effort')\n            plt.xlabel('Data Points')\n\n            # Conditional statement for animation plotting\n            if self.animate:\n                self.effort = self.data[joint]\n                frames = len(self.effort)-1\n                anim = animation.FuncAnimation(fig=fig,func=self.plot_animate, repeat=False,blit=False,frames=frames, interval =75)\n                plt.show()\n\n                ## If you want to save a video, make sure to comment out plt.show(),\n                ## right before this comment.\n                # save_path = str(self.completeName + '.mp4')\n                # anim.save(save_path, writer=animation.FFMpegWriter(fps=10))\n\n                # Reset y_anim for the next joint effort animation\n                del self.y_anim[:]\n\n            # Conditional statement for regular plotting (No animation)\n            else:\n                self.data[joint].plot(kind='line')\n                # save_path = str(self.completeName + '.png')\n                # plt.savefig(save_path, bbox_inches='tight')\n                plt.show()\n\n    def plot_animate(self,i):\n        # Append self.effort values for given joint\n        self.y_anim.append(self.effort.values[i])\n        plt.plot(self.y_anim, color='blue')\n\n    def main(self):\n        self.get_logger().info('issuing command')\n        self.issue_command()\n\ndef main():\n    try:\n        node = JointActuatorEffortSensor(export_data=True, animate=True)\n        node.main()\n        node.plot_data()\n        node.new_thread.join()\n\n    except KeyboardInterrupt:\n        node.get_logger().info('interrupt received, so shutting down')\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_6/#the-code-explained","title":"The Code Explained","text":"<p>This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport hello_helpers.hello_misc as hm\nimport os\nimport csv\nimport time\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('tkagg')\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom datetime import datetime\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\n</code></pre> <p>You need to import rclpy if you are writing a ROS Node. Import the <code>FollowJointTrajectory</code> from the <code>control_msgs.action</code> package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the <code>trajectory_msgs</code> package to define robot trajectories. The <code>hello_helpers</code> package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.</p> <pre><code>class JointActuatorEffortSensor(hm.HelloNode):\n    def __init__(self, export_data=False):\n        hm.HelloNode.__init__(self)\n        hm.HelloNode.main(self, 'issue_command', 'issue_command', wait_for_first_pointcloud=False)\n</code></pre> <p>The <code>JointActuatorEffortSensor</code> class inherits the <code>HelloNode</code> class from <code>hm</code> and is initialized also the HelloNode class already have the topic joint_states, thanks to this we don't need to create a subscriber.</p> <pre><code>self.joints = ['joint_lift']\n</code></pre> <p>Create a list of the desired joints you want to print.</p> <pre><code>self.joint_effort = []\nself.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\nself.export_data = export_data\nself.result = False\nself.file_name = datetime.now().strftime(\"effort_sensing_tutorial_%Y%m%d%I\")\n</code></pre> <p>Create an empty list to store the joint effort values. The <code>self.save_path</code> is the directory path where the .txt file of the effort values will be stored. You can change this path to a preferred directory. The <code>self.export_data</code> is a boolean and its default value is set to <code>True</code>. If set to <code>False</code>, then the joint values will be printed in the terminal, otherwise, it will be stored in a .txt file and that's what we want to see the plot graph. Also we want to give our text file a name since the beginning and we use the <code>self.file_name</code> to access this later.</p> <pre><code>self._send_goal_future = self.trajectory_client.send_goal_async(trajectory_goal, self.feedback_callback)\nself._send_goal_future.add_done_callback(self.goal_response_callback)\n</code></pre> <p>The ActionClient.send_goal_async() method returns a future to a goal handle. Include the goal and <code>feedback_callback</code> functions in the send goal function. Also we need to register a <code>goal_response_callback</code> for when the future is complete </p> <p><pre><code>def goal_response_callback(self,future):\n    goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info('Failed')\n            return\n\n        self.get_logger().info('Succeded')\n</code></pre> Looking at the <code>goal_response_callback</code> in more detail we can see if the future is complete with the messages that will appear.</p> <p><pre><code>        self._get_result_future = goal_handle.get_result_async()\n        self._get_result_future.add_done_callback(self.get_result_callback)\n</code></pre> We need the goal_handle to request the result with the method get_result_async. With this we will get a future that will complete when the result is ready so we need a callback for this result.</p> <p><pre><code>def get_result_callback(self, future):\n        self.result = future.result().result\n        self.get_logger().info('Sent position goal = {0}'.format(result))\n</code></pre> In the result callback we log the result of our poistion goal </p> <pre><code>def feedback_callback(self,feedback_msg):\n</code></pre> <p>The feedback callback function takes in the <code>FollowJointTrajectoryActionFeedback</code> message as its argument.</p> <pre><code>if 'wrist_extension' in self.joints:\n    self.joints.remove('wrist_extension')\n    self.joints.append('joint_arm_l0')\n</code></pre> <p>Use a conditional statement to replace <code>wrist_extenstion</code> with <code>joint_arm_l0</code>. This is because <code>joint_arm_l0</code> has the effort values that the <code>wrist_extension</code> is experiencing.</p> <pre><code>current_effort = []\nfor joint in self.joints:\n    index = self.joint_states.name.index(joint)\n    current_effort.append(self.joint_states.effort[index])\n</code></pre> <p>Create an empty list to store the current effort values. Then use a for loop to parse the joint names and effort values.</p> <pre><code>if not self.export_data:\n    print(\"name: \" + str(self.joints))\n    print(\"effort: \" + str(current_effort))\nelse:\n    self.joint_effort.append(current_effort)\n</code></pre> <p>Use a conditional statement to print effort values in the terminal or store values into a list that will be used for exporting the data in a .txt file.</p> <p><pre><code>if self.export_data:\n            file_name = self.file_name\n            completeName = os.path.join(self.save_path, file_name)\n            with open(completeName, \"w\") as f:\n                writer = csv.writer(f)\n                writer.writerow(self.joints)\n                writer.writerows(self.joint_effort)\n</code></pre> A conditional statement is used to export the data to a .txt file. The file's name is set to the one we created at the beginning.</p> <p><pre><code>def plot_data(self, animate = True):\n        while not self.result:\n            time.sleep(0.1)\n        file_name = self.file_name\n        self.completeName = os.path.join(self.save_path, file_name)\n        self.data = pd.read_csv(self.completeName)\n        self.y_anim = []\n        self.animate = animate\n</code></pre> This function will help us initialize some values to plot our data, we need to wait until we get the results to start plotting, because the file could still be storing values and we want to plot every point also we need to create an empty list for the animation.</p> <p><pre><code>for joint in self.data.columns:\n\n            # Create figure, labels, and title\n            fig = plt.figure()\n            plt.title(joint + ' Effort Sensing')\n            plt.ylabel('Effort')\n            plt.xlabel('Data Points')\n</code></pre> Create a for loop to print each joint's effort writing the correct labels for x and y</p> <p><pre><code>if self.animate:\n    self.effort = self.data[joint]\n    frames = len(self.effort)-1\n    anim = animation.FuncAnimation(fig=fig,func=self.plot_animate, repeat=False,blit=False,frames=frames, interval =75)\n    plt.show()\n    del self.y_anim[:]\n\nelse:\n    self.data[joint].plot(kind='line')\n    # save_path = str(self.completeName + '.png')\n    # plt.savefig(save_path, bbox_inches='tight')\n    plt.show()\n</code></pre> This is a conditional statement for the animation plotting</p> <p><pre><code>def plot_animate(self,i):\n        self.y_anim.append(self.effort.values[i])\n        plt.plot(self.y_anim, color='blue')\n</code></pre> We will create another function that will plot every increment in the data frame</p> <p> </p>"},{"location":"stretch-tutorials/ros2/example_7/","title":"Capture Image","text":""},{"location":"stretch-tutorials/ros2/example_7/#example-7","title":"Example 7","text":"<p>In this example, we will review a Python script that captures an image from the RealSense camera.</p> <p> </p> <p>Begin by running the stretch <code>driver.launch</code> file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>ros2 launch stretch_core d435i_low_resolution.launch.py\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>ros2 run rviz2 rviz2 -d /home/hello-robot/ament_ws/src/stretch_tutorials/rviz/perception_example.rviz\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_7/#capture-image-with-python-script","title":"Capture Image with Python Script","text":"<p>In this section, we will use a Python node to capture an image from the RealSense camera. Execute the capture_image.py node to save a .jpeg image of the image topic <code>/camera/color/image_raw</code>. In a terminal, execute:</p> <p><pre><code>cd ~/ament_ws/src/stretch_tutorials/stretch_ros_tutorials\npython3 capture_image.py\n</code></pre> An image named camera_image.jpeg is saved in the stored_data folder in this package, if you don't have this folder you can create it yourself.</p>"},{"location":"stretch-tutorials/ros2/example_7/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nimport sys\nimport os\nimport cv2\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nclass CaptureImage(Node):\n    \"\"\"\n    A class that converts a subscribed ROS image to a OpenCV image and saves\n    the captured image to a predefined directory.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes a CvBridge class, subscriber, and save path.\n        :param self: The self reference.\n        \"\"\"\n        super().__init__('stretch_capture_image')\n        self.bridge = CvBridge()\n        self.sub = self.create_subscription(Image, '/camera/color/image_raw', self.image_callback, 10)\n        self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n        self.br = CvBridge()\n\n    def image_callback(self, msg):\n        \"\"\"\n        A callback function that converts the ROS image to a CV2 image and stores the\n        image.\n        :param self: The self reference.\n        :param msg: The ROS image message type.\n        \"\"\"\n\n        try:\n            image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        except CvBridgeError as e:\n            self.get_logger().warn('CV Bridge error: {0}'.format(e))\n\n        file_name = 'camera_image.jpeg'\n        completeName = os.path.join(self.save_path, file_name)\n        cv2.imwrite(completeName, image)\n        rclpy.shutdown()\n        sys.exit(0)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    capture_image = CaptureImage()\n    rclpy.spin(capture_image)\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_7/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport sys\nimport os\nimport cv2\n</code></pre> <p>You need to import <code>rclpy</code> if you are writing a ROS Node. There are functions from <code>sys</code>, <code>os</code>, and <code>cv2</code> that are required within this code. <code>cv2</code> is a library of Python functions that implements computer vision algorithms. Further information about cv2 can be found here: OpenCV Python.</p> <pre><code>from rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n</code></pre> <p>The <code>sensor_msgs.msg</code> is imported so that we can subscribe to ROS <code>Image</code> messages. Import CvBridge to convert between ROS <code>Image</code> messages and OpenCV images and the <code>Node</code> is neccesary to create a node in ROS2.</p> <pre><code>def __init__(self):\n    \"\"\"\n    A function that initializes a CvBridge class, subscriber, and save path.\n    :param self: The self reference.\n    \"\"\"\n    super().__init__('stretch_capture_image')\n    self.bridge = CvBridge()\n    self.sub = self.create_subscription(Image, '/camera/color/image_raw', self.image_callback, 10)\n    self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n    self.br = CvBridge()\n</code></pre> <p>Initialize the node, CvBridge class, the subscriber, and the directory where the captured image will be stored.</p> <pre><code>def image_callback(self, msg):\n    \"\"\"\n    A callback function that converts the ROS image to a cv2 image and stores the\n    image.\n    :param self: The self reference.\n    :param msg: The ROS image message type.\n    \"\"\"\n    try:\n        image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n    except CvBridgeError as e:\n        rospy.logwarn('CV Bridge error: {0}'.format(e))\n</code></pre> <p>Try to convert the ROS Image message to a cv2 Image message using the <code>imgmsg_to_cv2()</code> function.  </p> <pre><code>file_name = 'camera_image.jpeg'\ncompleteName = os.path.join(self.save_path, file_name)\ncv2.imwrite(completeName, image)\n</code></pre> <p>Join the directory and file name using the <code>path.join()</code> function. Then use the <code>imwrite()</code> function to save the image.</p> <pre><code>rclpy.shutdown()\nsys.exit(0)\n</code></pre> <p>The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter.</p> <pre><code>    rclpy.init(args=args)\n    capture_image = CaptureImage()\n</code></pre> <p>The next line, rclpy.init_node initializes the node. In this case, your node will take on the name 'stretch_capture_image'. Also setup CaptureImage class with <code>capture_image = CaptureImage()</code>.</p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <p><pre><code>rclpy.spin(capture_image)\n</code></pre> Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros2/example_7/#edge-detection","title":"Edge Detection","text":"<p>In this section, we highlight a node that utilizes the Canny Edge filter algorithm to detect the edges from an image and convert it back as a ROS image to be visualized in RViz. In a terminal, execute:</p> <pre><code>cd ~/ament_ws/src/stretch_tutorials/stretch_ros_tutorials\npython3 edge_detection.py\n</code></pre> <p>The node will publish a new Image topic named <code>/image_edge_detection</code>. This can be visualized in RViz and a gif is provided below for reference.</p> <p> </p>"},{"location":"stretch-tutorials/ros2/example_7/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nimport cv2\n\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nclass EdgeDetection(Node):\n    \"\"\"\n    A class that converts a subscribed ROS image to a OpenCV image and saves\n    the captured image to a predefined directory.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        A function that initializes a CvBridge class, subscriber, and other\n        parameter values.\n        :param self: The self reference.\n        \"\"\"\n        super().__init__('stretch_edge_detection')\n        self.bridge = CvBridge()\n        self.sub = self.create_subscription(Image, '/camera/color/image_raw', self.callback, 1)\n        self.pub = self.create_publisher(Image, '/image_edge_detection', 1)\n        self.save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data'\n        self.lower_thres = 100\n        self.upper_thres = 200\n        self.get_logger().info(\"Publishing the CV2 Image. Use RViz to visualize.\")\n\n    def callback(self, msg):\n        \"\"\"\n        A callback function that converts the ROS image to a CV2 image and goes\n        through the Canny Edge filter in OpenCV for edge detection. Then publishes\n        that filtered image to be visualized in RViz.\n        :param self: The self reference.\n        :param msg: The ROS image message type.\n        \"\"\"\n        try:\n            image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        except CvBridgeError as e:\n            self.get_logger().warn('CV Bridge error: {0}'.format(e))\n\n        image = cv2.Canny(image, self.lower_thres, self.upper_thres)\n        image_msg = self.bridge.cv2_to_imgmsg(image, 'passthrough')\n        image_msg.header = msg.header\n        self.pub.publish(image_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    edge_detection = EdgeDetection()\n    rclpy.spin(edge_detection)\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_7/#the-code-explained_1","title":"The Code Explained","text":"<p>Since there are similarities in the capture image node, we will only break down the different components of the edge detection node.</p> <p>Define the lower and upper bounds of the Hysteresis Thresholds.</p> <pre><code>image = cv2.Canny(image, self.lower_thres, self.upper_thres)\n</code></pre> <p>Run the Canny Edge function to detect edges from the cv2 image.</p> <pre><code>image_msg = self.bridge.cv2_to_imgmsg(image, 'passthrough')\n</code></pre> <p>Convert the cv2 image back to a ROS image so it can be published.</p> <pre><code>image_msg.header = msg.header\nself.pub.publish(image_msg)\n</code></pre> <p>Publish the ROS image with the same header as the subscribed ROS message.</p>"},{"location":"stretch-tutorials/ros2/example_8/","title":"Example 8","text":"<p>This example will showcase how to save the interpreted speech from Stretch's ReSpeaker Mic Array v2.0 to a text file.</p> <p> </p> <p>Begin by running the <code>respeaker.launch.py</code> file in a terminal.</p> <p><pre><code>ros2 launch respeaker_ros2 respeaker.launch.py\n</code></pre> Then run the speech_text.py node. In a new terminal, execute:</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 speech_text.py\n</code></pre> <p>The ReSpeaker will be listening and will start to interpret speech and save the transcript to a text file.  To shut down the node, type <code>Ctrl</code> + <code>c</code> in the terminal.</p>"},{"location":"stretch-tutorials/ros2/example_8/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\n# Import modules\nimport rclpy\nimport os\nfrom rclpy.node import Node\n\n# Import SpeechRecognitionCandidates from the speech_recognition_msgs package\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n\nclass SpeechText(Node):\n    def __init__(self):\n        super().__init__('stretch_speech_text')\n        # Initialize subscriber\n        self.sub = self.create_subscription(SpeechRecognitionCandidates, \"speech_to_text\", self.callback, 1)\n\n        # Create path to save captured images to the stored data folder\n        self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n\n        # Create log message\n        self.get_logger().info(\"Listening to speech\")\n\n    def callback(self,msg):\n        # Take all items in the iterable list and join them into a single string\n        transcript = ' '.join(map(str,msg.transcript))\n\n        # Define the file name and create a complete path name\n        file_name = 'speech.txt'\n        completeName = os.path.join(self.save_path, file_name)\n\n        # Append 'hello' at the end of file\n        with open(completeName, \"a+\") as file_object:\n            file_object.write(\"\\n\")\n            file_object.write(transcript)\n\ndef main(args=None):\n    # Initialize the node and name it speech_text\n    rclpy.init(args=args)\n\n    # Instantiate the SpeechText class\n    speech_txt = SpeechText()\n\n    # Give control to ROS.  This will allow the callback to be called whenever new\n    # messages come in.  If we don't put this line in, then the node will not work,\n    # and ROS will not process any messages\n    rclpy.spin(speech_txt)\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_8/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script.</p> <pre><code>import rclpy\nimport os\nfrom rclpy.node import Node\n</code></pre> <p>You need to import rclpy if you are writing a ROS Node.</p> <pre><code>from speech_recognition_msgs.msg import SpeechRecognitionCandidates\n</code></pre> <p>Import <code>SpeechRecognitionCandidates</code> from the <code>speech_recgonition_msgs.msg</code> so that we can receive the interpreted speech.</p> <pre><code>def __init__(self):\n    super().__init__('stretch_speech_text')\n    self.sub = self.create_subscription(SpeechRecognitionCandidates, \"speech_to_text\", self.callback, 1)\n</code></pre> <p>Set up a subscriber.  We're going to subscribe to the topic <code>speech_to_text</code>, looking for <code>SpeechRecognitionCandidates</code> messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically.</p> <pre><code>self.save_path = '/home/hello-robot/ament_ws/src/stretch_tutorials/stored_data'\n</code></pre> <p>Define the directory to save the text file.</p> <pre><code>transcript = ' '.join(map(str,msg.transcript))\n</code></pre> <p>Take all items in the iterable list and join them into a single string named transcript.</p> <pre><code>file_name = 'speech.txt'\ncompleteName = os.path.join(self.save_path, file_name)\n</code></pre> <p>Define the file name and create a complete path directory.</p> <pre><code>with open(completeName, \"a+\") as file_object:\n    file_object.write(\"\\n\")\n    file_object.write(transcript)\n</code></pre> <p>Append the transcript to the text file.</p> <pre><code>def main(args=None):\n    rclpy.init(args=args)\n    speech_txt = SpeechText()\n</code></pre> <p>The next line, rclpy.init() method initializes the node. In this case, your node will take on the name 'stretch_speech_text'. Instantiate the <code>SpeechText()</code> class.</p> <p>Note</p> <p>The name must be a base name, i.e. it cannot contain any slashes \"/\".</p> <pre><code>rclpy.spin(speech_txt)\n</code></pre> <p>Give control to ROS with <code>rclpy.spin()</code>. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.</p>"},{"location":"stretch-tutorials/ros2/example_9/","title":"Voice Teleoperation of Base","text":""},{"location":"stretch-tutorials/ros2/example_9/#example-9","title":"Example 9","text":"<p>This example aims to combine the ReSpeaker Microphone Array and Follow Joint Trajectory tutorials to voice teleoperate the mobile base of the Stretch robot.</p> <p>Begin by running the following command in a new terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then run the <code>respeaker.launch.py</code> file. In a new terminal, execute:</p> <pre><code>ros2 launch respeaker_ros2 respeaker.launch.py\n</code></pre> <p>Then run the voice_teleoperation_base.py node in a new terminal.</p> <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 voice_teleoperation_base.py\n</code></pre> <p>In terminal 3, a menu of voice commands is printed. You can reference this menu layout below.  </p> <pre><code>------------ VOICE TELEOP MENU ------------\n\nVOICE COMMANDS              \n\"forward\": BASE FORWARD                   \n\"back\"   : BASE BACK                      \n\"left\"   : BASE ROTATE LEFT               \n\"right\"  : BASE ROTATE RIGHT              \n\"stretch\": BASE ROTATES TOWARDS SOUND     \n\nSTEP SIZE                 \n\"big\"    : BIG                            \n\"medium\" : MEDIUM                         \n\"small\"  : SMALL                          \n\n\n\"quit\"   : QUIT AND CLOSE NODE            \n\n-------------------------------------------\n</code></pre> <p>To stop the node from sending twist messages, type <code>Ctrl</code> + <code>c</code> or say \"quit\".</p>"},{"location":"stretch-tutorials/ros2/example_9/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\n\n# Import modules\nimport math\nimport rclpy\nimport sys\nfrom rclpy.duration import Duration\n\n# We're going to subscribe to 64-bit integers, so we need to import the definition\n# for them\nfrom sensor_msgs.msg import JointState\n\n# Import Int32 message typs from the std_msgs package\nfrom std_msgs.msg import Int32\n\n# Import the FollowJointTrajectory from the control_msgs.msg package to\n# control the Stretch robot\nfrom control_msgs.action import FollowJointTrajectory\n\n# Import JointTrajectoryPoint from the trajectory_msgs package to define\n# robot trajectories\nfrom trajectory_msgs.msg import JointTrajectoryPoint\n\n# Import hello_misc script for handling trajectory goals with an action client\nimport hello_helpers.hello_misc as hm\n\n# Import SpeechRecognitionCandidates from the speech_recognition_msgs package\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n\nclass GetVoiceCommands:\n    def __init__(self, node):\n        self.node = node\n        # Set step size as medium by default\n        self.step_size = 'medium'\n        self.rad_per_deg = math.pi/180.0\n\n        # Small step size parameters\n        self.small_deg = 5.0\n        self.small_rad = self.rad_per_deg * self.small_deg\n        self.small_translate = 0.025\n\n        # Medium step size parameters\n        self.medium_deg = 10.0\n        self.medium_rad = self.rad_per_deg * self.medium_deg\n        self.medium_translate = 0.05\n\n        # Big step size parameters\n        self.big_deg = 20.0\n        self.big_rad = self.rad_per_deg * self.big_deg\n        self.big_translate = 0.1\n\n        # Initialize the voice command\n        self.voice_command = None\n\n        # Initialize the sound direction\n        self.sound_direction = 0\n\n        # Initialize subscribers\n        self.speech_to_text_sub = self.node.create_subscription(SpeechRecognitionCandidates, \"/speech_to_text\", self.callback_speech, 1)\n        self.sound_direction_sub = self.node.create_subscription(Int32, \"/sound_direction\", self.callback_direction, 1)\n\n    def callback_direction(self, msg):\n        self.sound_direction = msg.data * -self.rad_per_deg\n\n    def callback_speech(self,msg):\n        self.voice_command = ' '.join(map(str,msg.transcript))\n\n    def get_inc(self):\n        if self.step_size == 'small':\n            inc = {'rad': self.small_rad, 'translate': self.small_translate}\n        if self.step_size == 'medium':\n            inc = {'rad': self.medium_rad, 'translate': self.medium_translate}\n        if self.step_size == 'big':\n            inc = {'rad': self.big_rad, 'translate': self.big_translate}\n        return inc\n\n    def print_commands(self):\n        \"\"\"\n        A function that prints the voice teleoperation menu.\n        :param self: The self reference.\n        \"\"\"\n        print('                                           ')\n        print('------------ VOICE TELEOP MENU ------------')\n        print('                                           ')\n        print('               VOICE COMMANDS              ')\n        print(' \"forward\": BASE FORWARD                   ')\n        print(' \"back\"   : BASE BACK                      ')\n        print(' \"left\"   : BASE ROTATE LEFT               ')\n        print(' \"right\"  : BASE ROTATE RIGHT              ')\n        print(' \"stretch\": BASE ROTATES TOWARDS SOUND     ')\n        print('                                           ')\n        print('                 STEP SIZE                 ')\n        print(' \"big\"    : BIG                            ')\n        print(' \"medium\" : MEDIUM                         ')\n        print(' \"small\"  : SMALL                          ')\n        print('                                           ')\n        print('                                           ')\n        print(' \"quit\"   : QUIT AND CLOSE NODE            ')\n        print('                                           ')\n        print('-------------------------------------------')\n\n    def get_command(self):\n        command = None\n        # Move base forward command\n        if self.voice_command == 'forward':\n            command = {'joint': 'translate_mobile_base', 'inc': self.get_inc()['translate']}\n\n        # Move base back command\n        if self.voice_command == 'back':\n            command = {'joint': 'translate_mobile_base', 'inc': -self.get_inc()['translate']}\n\n        # Rotate base left command\n        if self.voice_command == 'left':\n            command = {'joint': 'rotate_mobile_base', 'inc': self.get_inc()['rad']}\n\n        # Rotate base right command\n        if self.voice_command == 'right':\n            command = {'joint': 'rotate_mobile_base', 'inc': -self.get_inc()['rad']}\n\n        # Move base to sound direction command\n        if self.voice_command == 'stretch':\n            command = {'joint': 'rotate_mobile_base', 'inc': self.sound_direction}\n\n        # Set the step size of translational and rotational base motions\n        if (self.voice_command == \"small\") or (self.voice_command == \"medium\") or (self.voice_command == \"big\"):\n            self.step_size = self.voice_command\n            self.node.get_logger().info('Step size = {0}'.format(self.step_size))\n\n        if self.voice_command == 'quit':\n            # Sends a signal to ros to shutdown the ROS interfaces\n            self.node.get_logger().info(\"done\")\n\n            # Exit the Python interpreter\n            sys.exit(0)\n\n        # Reset voice command to None\n        self.voice_command = None\n\n        # return the updated command\n        return command\n\n\nclass VoiceTeleopNode(hm.HelloNode):\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n        self.rate = 10.0\n        self.joint_state = None\n        hm.HelloNode.main(self, 'voice_teleop', 'voice_teleop', wait_for_first_pointcloud=False)\n        self.speech = GetVoiceCommands(self)\n\n\n    def joint_states_callback(self, msg):\n        self.joint_state = msg\n\n    def send_command(self, command):\n        joint_state = self.joint_state\n        # Conditional statement to send  joint trajectory goals\n        if (joint_state is not None) and (command is not None):\n            # Assign point as a JointTrajectoryPoint message type\n            point = JointTrajectoryPoint()\n            point.time_from_start = Duration(seconds=0).to_msg()\n\n            # Assign trajectory_goal as a FollowJointTrajectoryGoal message type\n            trajectory_goal = FollowJointTrajectory.Goal()\n            trajectory_goal.goal_time_tolerance = Duration(seconds=0).to_msg()\n\n            # Extract the joint name from the command dictionary\n            joint_name = command['joint']\n            trajectory_goal.trajectory.joint_names = [joint_name]\n\n            # Extract the increment type from the command dictionary\n            inc = command['inc']\n            self.get_logger().info('inc = {0}'.format(inc))\n            new_value = inc\n\n            # Assign the new_value position to the trajectory goal message type\n            point.positions = [new_value]\n            trajectory_goal.trajectory.points = [point]\n            trajectory_goal.trajectory.header.stamp = self.get_clock().now().to_msg()\n            self.get_logger().info('joint_name = {0}, trajectory_goal = {1}'.format(joint_name, trajectory_goal))\n\n            # Make the action call and send goal of the new joint position\n            self.trajectory_client.send_goal(trajectory_goal)\n            self.get_logger().info('Done sending command.')\n\n            # Reprint the voice command menu\n            self.speech.print_commands()\n\n    def timer_get_command(self):\n        # Get voice command\n            command = self.speech.get_command()\n\n            # Send voice command for joint trajectory goals\n            self.send_command(command)\n    def main(self):\n        self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)\n        rate = self.create_rate(self.rate)\n        self.speech.print_commands()\n\n        self.sleep = self.create_timer(1/self.rate, self.timer_get_command)\n\ndef main(args=None):\n    try:\n        #rclpy.init(args=args)\n        node = VoiceTeleopNode()\n        node.main()\n        node.new_thread.join()\n    except KeyboardInterrupt:\n        node.get_logger().info('interrupt received, so shutting down')\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/example_9/#the-code-explained","title":"The Code Explained","text":"<p>This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down.</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import math\nimport rclpy\nimport sys\nfrom rclpy.duration import Duration\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Int32\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nimport hello_helpers.hello_misc as hm\nfrom speech_recognition_msgs.msg import SpeechRecognitionCandidates\n</code></pre> <p>You need to import <code>rclpy</code> if you are writing a ROS Node. Import the <code>FollowJointTrajectory</code> from the <code>control_msgs.action</code> package to control the Stretch robot. Import <code>JointTrajectoryPoint</code> from the <code>trajectory_msgs</code> package to define robot trajectories. The <code>hello_helpers</code> package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the <code>hello_misc</code> script.  Import <code>sensor_msgs.msg</code> so that we can subscribe to JointState messages.</p> <pre><code>class GetVoiceCommands:\n</code></pre> <p>Create a class that subscribes to the <code>speech-to-text</code> recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion.</p> <pre><code>self.node = node\nself.step_size = 'medium'\nself.rad_per_deg = math.pi/180.0\n</code></pre> <p>Set the default step size as medium and create a float value, <code>self.rad_per_deg</code>, to convert degrees to radians.</p> <pre><code>self.small_deg = 5.0\nself.small_rad = self.rad_per_deg * self.small_deg\nself.small_translate = 0.025\n\nself.medium_deg = 10.0\nself.medium_rad = self.rad_per_deg * self.medium_deg\nself.medium_translate = 0.05\n\nself.big_deg = 20.0\nself.big_rad = self.rad_per_deg * self.big_deg\nself.big_translate = 0.1\n</code></pre> <p>Define the three rotation and translation step sizes.</p> <pre><code>self.voice_command = None\nself.sound_direction = 0\nself.speech_to_text_sub = self.node.create_subscription(SpeechRecognitionCandidates, \"/speech_to_text\", self.callback_speech, 1)\nself.sound_direction_sub = self.node.create_subscription(Int32, \"/sound_direction\", self.callback_direction, 1)\n</code></pre> <p>Initialize the voice command and sound direction to values that will not result in moving the base.</p> <p>Set up two subscribers.  The first one subscribes to the topic <code>/speech_to_text</code>, looking for <code>SpeechRecognitionCandidates</code> messages.  When a message comes in, ROS is going to pass it to the function <code>callback_speech</code> automatically. The second subscribes to <code>/sound_direction</code> message and passes it to the <code>callback_direction</code> function.</p> <pre><code>def callback_direction(self, msg):\n    self.sound_direction = msg.data * -self.rad_per_deg\n</code></pre> <p>The <code>callback_direction</code> function converts the <code>sound_direction</code> topic from degrees to radians.</p> <pre><code>if self.step_size == 'small':\n    inc = {'rad': self.small_rad, 'translate': self.small_translate}\nif self.step_size == 'medium':\n    inc = {'rad': self.medium_rad, 'translate': self.medium_translate}\nif self.step_size == 'big':\n    inc = {'rad': self.big_rad, 'translate': self.big_translate}\nreturn inc\n</code></pre> <p>The <code>callback_speech</code> stores the increment size for translational and rotational base motion in <code>inc</code>. The increment size is contingent on the <code>self.step_size</code> string value.</p> <pre><code>command = None\nif self.voice_command == 'forward':\n    command = {'joint': 'translate_mobile_base', 'inc': self.get_inc()['translate']}\nif self.voice_command == 'back':\n    command = {'joint': 'translate_mobile_base', 'inc': -self.get_inc()['translate']}\nif self.voice_command == 'left':\n    command = {'joint': 'rotate_mobile_base', 'inc': self.get_inc()['rad']}\nif self.voice_command == 'right':\n    command = {'joint': 'rotate_mobile_base', 'inc': -self.get_inc()['rad']}\nif self.voice_command == 'stretch':\n    command = {'joint': 'rotate_mobile_base', 'inc': self.sound_direction}\n</code></pre> <p>In the <code>get_command()</code> function, the <code>command</code> is initialized as <code>None</code>, or set as a dictionary where the <code>joint</code> and <code>inc</code> values are stored. The <code>command</code> message type is dependent on the <code>self.voice_command</code> string value.</p> <pre><code>if (self.voice_command == \"small\") or (self.voice_command == \"medium\") or (self.voice_command == \"big\"):\n    self.step_size = self.voice_command\n    self.node.get_logger().info('Step size = {0}'.format(self.step_size))\n</code></pre> <p>Based on the <code>self.voice_command</code> value set the step size for the increments.</p> <pre><code>if self.voice_command == 'quit':\n    self.node.get_logger().info(\"done\")\n    sys.exit(0)\n</code></pre> <p>If the <code>self.voice_command</code> is equal to <code>quit</code>, then initiate a clean shutdown of ROS and exit the Python interpreter.</p> <pre><code>class VoiceTeleopNode(hm.HelloNode):\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n        self.rate = 10.0\n        self.joint_state = None\n        hm.HelloNode.main(self, 'voice_teleop', 'voice_teleop', wait_for_first_pointcloud=False)\n        self.speech = GetVoiceCommands(self)\n</code></pre> <p>A class that inherits the <code>HelloNode</code> class from <code>hm</code> declares object from the <code>GetVoiceCommands</code> class and sends joint trajectory commands. The main function instantiates the <code>HelloNode</code> class.</p> <pre><code>def send_command(self, command):\n    joint_state = self.joint_state\n    if (joint_state is not None) and (command is not None):\n        point = JointTrajectoryPoint()\n        point.time_from_start = Duration(seconds=0).to_msg()\n</code></pre> <p>The <code>send_command</code> function stores the joint state message and uses a conditional statement to send joint trajectory goals. Also, assign <code>point</code> as a <code>JointTrajectoryPoint</code> message type.</p> <pre><code>trajectory_goal = FollowJointTrajectory.Goal()\ntrajectory_goal.goal_time_tolerance = Duration(seconds=0).to_msg()\n</code></pre> <p>Assign <code>trajectory_goal</code> as a <code>FollowJointTrajectory.Goal</code> message type.</p> <pre><code>joint_name = command['joint']\ntrajectory_goal.trajectory.joint_names = [joint_name]\n</code></pre> <p>Extract the joint name from the command dictionary.</p> <pre><code>inc = command['inc']\nself.get_logger().info('inc = {0}'.format(inc))\nnew_value = inc\n</code></pre> <p>Extract the increment type from the command dictionary.</p> <pre><code>point.positions = [new_value]\ntrajectory_goal.trajectory.points = [point]\n</code></pre> <p>Assign the new value position to the trajectory goal message type.</p> <pre><code>self.trajectory_client.send_goal(trajectory_goal)\nself.get_logger().info('Done sending command.')\n</code></pre> <p>Make the action call and send the goal of the new joint position.</p> <pre><code>self.speech.print_commands()\n</code></pre> <p>Reprint the voice command menu after the trajectory goal is sent.</p> <pre><code>def main(self):\n      self.create_subscription(JointState, '/stretch/joint_states', self.joint_states_callback, 1)\n      rate = self.create_rate(self.rate)\n      self.speech.print_commands()\n</code></pre> <p>The main function initializes the subscriber and we are going to use the publishing rate that we set before.</p> <pre><code>try:\n    #rclpy.init(args=args)\n    node = VoiceTeleopNode()\n    node.main()\n    node.new_thread.join()\nexcept KeyboardInterrupt:\n    node.get_logger().info('interrupt received, so shutting down')\n    node.destroy_node()\n    rclpy.shutdown()\n</code></pre> <p>Declare a <code>VoiceTeleopNode</code> object. Then execute the <code>main()</code> method.</p>"},{"location":"stretch-tutorials/ros2/feature_comparison/","title":"ROS 1 v/s ROS 2 Feature Comparison","text":"<p>This document will help you better understand our progress of bringing parity between our ROS 1 and ROS 2 software offerings. Additional comments indicate changes in the API and/or behavior for a component.</p> Package Node/Launch/Component Item ROS 1 ROS 2 Comments Stretch Install Humble Installation scripts NA \u2713 Stretch Install Humble Mesh and URDF files NA \u2713 Stretch Install Iron Installation scripts NA \u2713 Stretch Install Iron Mesh and URDF files NA \u2713 Stretch Core stretch_driver JointTrajectoryAction Server, Position mode \u2713 \u2713 Stretch Core stretch_driver JointTrajectoryAction Server, Navigation mode \u2713 \u2713 Stretch Core stretch_driver JointTrajectoryAction Server, Trajectory mode \u2715 \u2713 Smooth preemption missing Stretch Core stretch_driver Mode topic \u2715 \u2713 Stretch Core stretch_driver Mode services \u2713 \u2713 ROS 1 does not have trajectory mode Stretch Core stretch_driver Home/stop/stow services \u2713 \u2713 Stretch Core stretch_driver Magnetometer and BatteryState topic \u2713 \u2713 Stretch Core stretch_driver Joint states, joint limits \u2713 \u2713 Stretch Core stretch_driver TF2 \u2713 \u2713 Stretch Core keyboard_teleop Teleoperation \u2713 \u2713 Stretch Core keyboard_teleop Service triggers for demos \u2713 \u2713 Stretch Core detect_aruco_markers Aruco marker detection \u2713 \u2713 Stretch Core rplidar Driver \u2713 \u2713 Stretch Core rplidar LaserScan filtering \u2713 \u2713 Stretch Core d435i Driver \u2713 \u2713 Stretch Core d435i High/Low resolution launch \u2713 \u2713 Stretch Core d435i IMU \u2713 \u2713 Stretch Core d435i Aligned depth \u2713 \u2715 ROS 2 pending Stretch Core respeaker Driver \u2713 \u2715 ROS 2 pending Stretch Core API Docs Documentation \u2715 \u2715 ROS 1 and ROS 2 pending Hello Helpers hello_misc HelloNode \u2713 \u2713 Hello Helpers hello_misc Preprocess trajectories \u2713 \u2713 Hello Helpers hello_ros_viz Create markers \u2713 \u2713 Hello Helpers simple_command_groups SimpleCommandGroup \u2713 \u2713 Hello Helpers gripper_conversion GripperConversion \u2713 \u2713 Hello Helpers fit_plane Fit plane helper functions \u2713 \u2713 Hello Helpers configure_wrist Switch between standard and dex wrist \u2713 \u2713 Hello Helpers API Docs Documentation \u2715 \u2715 ROS 1 and 2 pending Stretch Description stretch_description Stretch URDF \u2713 \u2713 Stretch Description stretch_description Standard Gripper xacro \u2713 \u2713 Stretch Description stretch_description Dex wrist xacro \u2713 \u2713 Stretch Description stretch_description Batch-specific mesh files \u2713 \u2713 Stretch Calibration collect_head_calibration_data CollectHeadCalibrationDataNode \u2713 \u2713 Stretch Calibration process_head_calibration_data HeadCalibrator, ProcessHeadCalibrationNode \u2713 \u2713 Stretch Calibration check_head_calibration check_head_calibration \u2713 \u2713 Stretch Calibration revert_to_previous_calibration revert_to_previous_calibration \u2713 \u2713 Stretch Calibration update_uncalibrated_urdf update_uncalibrated_urdf \u2713 \u2713 Stretch Calibration update_urdf_after_xacro_change update_urdf_after_xacro_change \u2713 \u2713 Stretch Calibration update_with_most_recent_calibration update_with_most_recent_calibration \u2713 \u2713 Stretch Calibration visualize_most_recent_head_calibration visualize_most_recent_head_calibration \u2713 \u2713 Stretch Calibration Revert to position mode Revert to position mode \u2713 \u2713 Stretch Tutorials Getting started Instructions \u2713 \u2713 Stretch Tutorials Modes tutorial Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Tutorials ROS 2 with rclpy Instructions \u2713 \u2713 Stretch Tutorials Race conditions and deadlocks in ROS 2 Instructions \u2713 \u2713 Stretch Tutorials HelloNode Tutorial Instructions \u2713 \u2713 Stretch Tutorials HFollow Joint Trajectory Commands Trajectory mode \u2713 \u2713 Stretch Tutorials HFollow Joint Trajectory Commands Joint Trajectory Server \u2713 \u2713 Stretch Tutorials Internal State of Stretch Command line and RQT graph \u2713 \u2713 Stretch Tutorials RViz basics Robot visualizationa and TF tree \u2713 \u2713 Stretch Tutorials Teleoperation Velocity command of mobile base \u2713 \u2713 Stretch Tutorials Tf2 broadcaster and listener TF2 static broadcaster and listener \u2713 \u2713 Stretch Tutorials Respeaker voice to text Instructions \u2713 \u2715 ROS 2 pending Stretch Tutorials API docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Tutorials Migration guides From ROS1 Noetic &amp; from ROS2 Galactic NA \u2715 ROS 2 pending Stretch Tutorials Nav2 stack Mapping \u2713 \u2713 Stretch Tutorials Nav2 stack Navigation \u2713 \u2713 Stretch Tutorials Filter laser scans RPLidar and laserscan filtering \u2713 \u2713 Stretch Tutorials Deep perception Objects and faces detection \u2713 \u2713 Stretch Tutorials FUNMAP demos Instructions \u2713 \u2713 Stretch Tutorials Align to ArUco ArUco detection, tf transforms and trajectory server \u2713 \u2713 Stretch Tutorials Obstacle avoider RPLidar based sensing and avoidance \u2713 \u2713 Stretch Tutorials Mobile base collision avoidance RPLidar based sensing and avoidance \u2713 \u2715 Stretch FUNMAP manipulation_planning plan_surface_coverage, detect_cliff \u2713 \u2713 Stretch FUNMAP manipulation_planning PlanarRobotModel, ManipulationPlanner \u2713 \u2713 Stretch FUNMAP manipulation_planning ManipulationView \u2713 \u2713 Stretch FUNMAP mapping robot stowing, scanning, &amp; localizing methods \u2713 \u2713 Stretch FUNMAP mapping HeadScan \u2713 \u2713 Stretch FUNMAP navigate ForwardMotionObstacleDetector \u2713 \u2713 Stretch FUNMAP navigate FastSingleViewPlanner \u2713 \u2713 Stretch FUNMAP navigate MoveBase \u2713 \u2713 Stretch FUNMAP ros_max_height_image ROSVolumeOfInterest \u2713 \u2713 Stretch FUNMAP ros_max_height_image ROSMaxHeightImage \u2713 \u2713 Stretch FUNMAP funmap ContactDetector, FunMapNode, services \u2713 \u2713 Stretch FUNMAP API Docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Deep Perception detection_node DetectionNode \u2713 \u2713 Stretch Deep Perception detect_faces OpenVINO Face Detection \u2713 \u2713 Stretch Deep Perception detect_objects PyTorch YOLO object detection \u2713 \u2713 Stretch Deep Perception detect_nearest_mouth OpenVINO Mouth Detection \u2713 \u2713 Stretch Deep Perception detect_nearest_mouth OpenVINO Body Landmark Detection \u2713 \u2713 Stretch Deep Perception API Docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Demos hello_world HelloWorldNode, services \u2713 \u2713 Stretch Demos clean_surface CleanSurfaceNode, services \u2713 \u2713 Stretch Demos grasp_object GraspObjectNode, services \u2713 \u2713 Stretch Demos handover_object HandoverObjectNode, services \u2713 \u2713 Stretch Demos open_drawer OpenDrawerNode, services \u2713 \u2713 Stretch Demos HelloNode API Switch demos to HelloNode API \u2713 \u2713 Stretch Demos autodocking_behaviors MoveBaseActionClient, CheckTF \u2713 \u2715 ROS 2 pending Stretch Demos autodocking_behaviors VisualServoing \u2713 \u2715 ROS 2 pending Stretch Demos API docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch Nav2 offline_mapping SLAM \u2713 \u2713 Stretch Nav2 navigation Pose, Waypoints, Obstacle avoidance \u2713 \u2713 Stretch Nav2 Patrolling Demo Python API, Autonomous waypoint nav \u2713 \u2713 Stretch Nav2 API Docs Instructions \u2715 \u2715 ROS 1 and ROS 2 pending Stretch MoveIt 2 MoveIt Config Files YAML/XML/SRDF/URDF files \u2715 \u2713 Stretch MoveIt 2 moveit RViz plugin \u2715 \u2713 Stretch MoveIt 2 moveit_py Joint Space Goals \u2715 \u2713 In review Stretch MoveIt 2 moveit_py Pose Goals \u2715 \u2715 ROS 2 pending Stretch MoveIt 2 moveit_py Multiplanning pipeline \u2715 \u2713 In review Stretch MoveIt 2 moveit_py Octomap plugin \u2715 \u2713 In review Stretch MoveIt 2 moveit End effector pose goals \u2715 \u2715 ROS 2 pending Stretch MoveIt 2 moveit Hybrid planning \u2715 \u2715 ROS 2 pending Web-based Teleoperation web_interface operator \u2713 \u2715 ROS 2 pending Web-based Teleoperation web_interface robot \u2713 \u2715 ROS 2 pending"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/","title":"Follow Joint Trajectory Commands","text":""},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#followjointtrajectory-commands","title":"FollowJointTrajectory Commands","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. For this exercise you'll need to have Ubuntu 22.04 and ROS Iron for it to work completly.</p> <p>Stretch driver offers a <code>FollowJointTrajectory</code> action service for its arm. Within this tutorial, we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute.</p>"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#stow-command-example","title":"Stow Command Example","text":"<p>Begin by launching stretch_driver in a terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py mode:=trajectory\n</code></pre> <p>In a new terminal type the following commands.</p> <pre><code>ros2 run stretch_ros_tutorials stow_command\n</code></pre> <p>This will send a FollowJointTrajectory command to stow Stretch's arm.</p>"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#the-code","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.duration import Duration\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom hello_helpers.hello_misc import HelloNode\nimport time\nclass StowCommand(HelloNode):\n    def __init__(self):\n        HelloNode.__init__(self)\n        HelloNode.main(self, 'stow_command', 'stow_command', wait_for_first_pointcloud=False)\n    def issue_stow_command(self):\n        while not self.joint_state.position:\n            self.get_logger().info(\"Waiting for joint states message to arrive\")\n            time.sleep(0.1)\n            continue\n        self.get_logger().info('Stowing...')\n        joint_state = self.joint_state\n        stow_point1 = JointTrajectoryPoint()\n        stow_point2 = JointTrajectoryPoint()\n        duration1 = Duration(seconds=0.0)\n        duration2 = Duration(seconds=4.0)\n        stow_point1.time_from_start = duration1.to_msg()\n        stow_point2.time_from_start = duration2.to_msg()\n        lift_index = joint_state.name.index('joint_lift')\n        arm_index = joint_state.name.index('wrist_extension')\n        wrist_yaw_index = joint_state.name.index('joint_wrist_yaw')\n        joint_value1 = joint_state.position[lift_index]\n        joint_value2 = joint_state.position[arm_index]\n        joint_value3 = joint_state.position[wrist_yaw_index]\n\n        stow_point1.positions = [joint_value1, joint_value2, joint_value3]\n        stow_point2.positions = [0.2, 0.0, 3.14]\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n        trajectory_goal.trajectory.points = [stow_point1, stow_point2]\n        self.trajectory_client.send_goal_async(trajectory_goal)\n        self.get_logger().info(\"Goal sent\")\n    def main(self):\n        self.issue_stow_command()\ndef main():\n    try:\n        node = StowCommand()\n        node.main()\n        node.new_thread.join()\n    except:\n        node.get_logger().info(\"Exiting\")\n        node.destroy_node()\n        rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#the-code-explained","title":"The Code Explained","text":"<p>Now let's break the code down.</p> <p><pre><code>#!/usr/bin/env python3\n</code></pre> Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script.</p> <pre><code>import rclpy\nfrom rclpy.duration import Duration\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom hello_helpers.hello_misc import HelloNode\nimport time\n</code></pre> <p>You need to import rclpy if you are writing a ROS 2 Node. Import the FollowJointTrajectory from the control_msgs.action package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories.</p> <pre><code>class StowCommand(HelloNode):\n    def __init__(self):\n        HelloNode.__init__(self)\n        HelloNode.main(self, 'stow_command', 'stow_command', wait_for_first_pointcloud=False)\n</code></pre> <p>The <code>StowCommand</code> class inherits from the <code>HelloNode</code> class and is initialized with the main method in HelloNode by passing the arguments node_name as 'stow_command', node_namespace as 'stow_command' and wait_for_first_pointcloud as False. Refer to the Introduction to HelloNode tutorial if you haven't already to understand how this works.</p> <pre><code>def issue_stow_command(self):\n</code></pre> <p>The <code>issue_stow_command()</code> method will stow Stretch's arm. Within the function, we set stow_point as a <code>JointTrajectoryPoint</code>and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined in the next set of the code.</p> <pre><code>        while not self.joint_state.position:\n            self.get_logger().info(\"Waiting for joint states message to arrive\")\n            time.sleep(0.1)\n            continue\n        self.get_logger().info('Stowing...')\n        joint_state = self.joint_state\n        stow_point1 = JointTrajectoryPoint()\n        stow_point2 = JointTrajectoryPoint()\n        duration1 = Duration(seconds=0.0)\n        duration2 = Duration(seconds=4.0)\n        stow_point1.time_from_start = duration1.to_msg()\n        stow_point2.time_from_start = duration2.to_msg()\n        lift_index = joint_state.name.index('joint_lift')\n        arm_index = joint_state.name.index('wrist_extension')\n        wrist_yaw_index = joint_state.name.index('joint_wrist_yaw')\n        joint_value1 = joint_state.position[lift_index]\n        joint_value2 = joint_state.position[arm_index]\n        joint_value3 = joint_state.position[wrist_yaw_index]\n\n        stow_point1.positions = [joint_value1, joint_value2, joint_value3]\n        stow_point2.positions = [0.2, 0.0, 3.14]\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw']\n        trajectory_goal.trajectory.points = [stow_point1, stow_point2]\n</code></pre> <p>Set trajectory_goal as a <code>FollowJointTrajectory.Goal()</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> is defined by the positions set in stow_point1 and stow_point2.</p> <p><pre><code>        self.trajectory_client.send_goal_async(trajectory_goal)\n        self.get_logger().info(\"Goal sent\")\n</code></pre> Make the action call and send the goal.</p> <pre><code>def main(args=None):\n    try:\n        node = StowCommand()\n        node.main()\n        node.new_thread.join()\n    except:\n        node.get_logger().info(\"Exiting\")\n        node.destroy_node()\n        rclpy.shutdown()\n</code></pre> <p>Create a funcion, <code>main()</code>, to do all of the setup in the class and issue the stow command. Initialize the <code>StowCommand()</code> class and set it to node and run the <code>main()</code> function.</p> <pre><code>if __name__ == '__main__':\n    main()\n</code></pre> <p>To make the script executable call the main() function like above.</p>"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#multipoint-command-example","title":"Multipoint Command Example","text":"<p>If you have killed the above instance of stretch_driver relaunch it again through the terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py mode:=trajectory\n</code></pre> <p>In a new terminal type the following commands.</p> <pre><code>ros2 run stretch_ros_tutorials multipoint_command\n</code></pre> <p>This will send a list of JointTrajectoryPoint's to move Stretch's arm.</p>"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#the-code_1","title":"The Code","text":"<pre><code>#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.duration import Duration\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectoryPoint\nfrom hello_helpers.hello_misc import HelloNode\nimport time\nclass MultiPointCommand(HelloNode):\n    def __init__(self):\n        HelloNode.__init__(self)\n        HelloNode.main(self, 'multipoint_command', 'multipoint_command', wait_for_first_pointcloud=False)\n    def issue_multipoint_command(self):\n        while not self.joint_state.position:\n            self.get_logger().info(\"Waiting for joint states message to arrive\")\n            time.sleep(0.1)\n            continue\n\n        self.get_logger().info('Issuing multipoint command...')\n        joint_state = self.joint_state\n        duration0 = Duration(seconds=0.0)\n        duration1 = Duration(seconds=6.0)\n        duration2 = Duration(seconds=9.0)\n        duration3 = Duration(seconds=12.0)\n        duration4 = Duration(seconds=16.0)\n        duration5 = Duration(seconds=20.0)\n        lift_index = joint_state.name.index('joint_lift')\n        arm_index = joint_state.name.index('wrist_extension')\n        wrist_yaw_index = joint_state.name.index('joint_wrist_yaw')\n        gripper_index = joint_state.name.index('joint_gripper_finger_left')\n        joint_value1 = joint_state.position[lift_index]\n        joint_value2 = joint_state.position[arm_index]\n        joint_value3 = joint_state.position[wrist_yaw_index]\n        joint_value4 = joint_state.position[gripper_index]\n        point0 = JointTrajectoryPoint()\n        point0.positions = [joint_value1, joint_value2, joint_value3, joint_value4]\n        point0.velocities = [0.0, 0.0, 0.0, 0.0]\n        point0.time_from_start = duration0.to_msg()\n        point1 = JointTrajectoryPoint()\n        point1.positions = [0.9, 0.0, 0.0, 0.0] \n        point1.time_from_start = duration1.to_msg()\n        point2 = JointTrajectoryPoint()\n        point2.positions = [0.9, 0.2, 0.0, -0.3]\n        point2.time_from_start = duration2.to_msg()\n        point3 = JointTrajectoryPoint()\n        point3.positions = [0.9, 0.4, 0.0, -0.3]\n        point3.time_from_start = duration3.to_msg()\n        point4 = JointTrajectoryPoint()\n        point4.positions = [0.9, 0.4, 0.0, 0.0]\n        point4.time_from_start = duration4.to_msg()\n        point5 = JointTrajectoryPoint()\n        point5.positions = [0.4, 0.0, 1.54, 0.0]\n        point5.time_from_start = duration5.to_msg()\n        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw', 'joint_gripper_finger_left']\n        trajectory_goal.trajectory.points = [point0, point1, point2, point3, point4, point5]\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n        self.trajectory_client.send_goal_async(trajectory_goal)\n        self.get_logger().info(\"Goal sent\")\n\n    def main(self):\n        self.issue_multipoint_command()\ndef main():\n    try:\n        node = MultiPointCommand()\n        node.main()\n        node.new_thread.join()\n    except KeyboardInterrupt:\n        node.get_logger().info(\"Exiting\")\n        node.destroy_node()\n        rclpy.shutdown()\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#the-code-explained_1","title":"The Code Explained.","text":"<p>Seeing that there are similarities between the multipoint and stow command nodes, we will only breakdown the distinct components of the multipoint_command node.</p> <pre><code>        point1 = JointTrajectoryPoint()\n        point1.positions = [0.9, 0.0, 0.0, 0.0] \n        point1.time_from_start = duration1.to_msg()\n</code></pre> <p>Set point1 as a <code>JointTrajectoryPoint</code>and provide desired positions (in meters). These are the positions of the lift, wrist_extension, wrist_yaw and gripper_aperture joints, respectively.</p> <p>Note</p> <p>The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated.</p> <pre><code>        trajectory_goal = FollowJointTrajectory.Goal()\n        trajectory_goal.trajectory.joint_names = ['joint_lift', 'wrist_extension', 'joint_wrist_yaw', 'joint_gripper_finger_left']\n        trajectory_goal.trajectory.points = [point0, point1, point2, point3, point4, point5]\n        trajectory_goal.trajectory.header.frame_id = 'base_link'\n        self.trajectory_client.send_goal_async(trajectory_goal)\n        self.get_logger().info(\"Goal sent\")\n</code></pre> <p>Set trajectory_goal as a <code>FollowJointTrajectory.Goal()</code> and define the joint names as a list. Then <code>trajectory_goal.trajectory.points</code> is defined by a list of the 6 points.</p>"},{"location":"stretch-tutorials/ros2/gazebo_basics/","title":"Spawning Stretch in Simulation (Gazebo)","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>Note</p> <p>Simulation support for Stretch in ROS 2 is under active development. Please reach out to us if you want to work with Stretch in a simulated environment like Gazebo/Ignition in ROS 2.</p> <p>Refer to the instructions below if you want to test this functionality in ROS 1.</p>"},{"location":"stretch-tutorials/ros2/gazebo_basics/#empty-world-simulation","title":"Empty World Simulation","text":"<p>To spawn the Stretch in gazebo's default empty world run the following command in your terminal.</p> <pre><code>roslaunch stretch_gazebo gazebo.launch\n</code></pre> <p>This will bringup the robot in the gazebo simulation similar to the image shown below.</p> <p></p>"},{"location":"stretch-tutorials/ros2/gazebo_basics/#custom-world-simulation","title":"Custom World Simulation","text":"<p>In gazebo, you are able to spawn Stretch in various worlds. First, source the gazebo world files by running the following command in a terminal</p> <pre><code>echo \"source /usr/share/gazebo/setup.sh\"\n</code></pre> <p>Then using the world argument, you can spawn the stretch in the willowgarage world by running the following</p> <pre><code>roslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world\n</code></pre> <p></p>"},{"location":"stretch-tutorials/ros2/getting_started/","title":"Getting Started","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p>"},{"location":"stretch-tutorials/ros2/getting_started/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Stretch RE1 or Stretch 2 robot, turned on and connected to a keyboard, mouse, and monitor<ul> <li>Alternatively, setup untethered operation. This avoids the HDMI/USB cables getting pulled while the robot is moving.</li> </ul> </li> <li>Running the Ubuntu 20.04 software stack<ul> <li>All Stretch 2 robots ship with Ubuntu 20.04, however RE1s had shipped with Ubuntu 18.04 until summer 2022. RE1 users should run <code>lsb_release -sd</code> in a terminal and confirm \"Ubuntu 20.04.5 LTS\" or similar is printed out. If you are running Ubuntu 18.04, follow the upgrade guide.</li> </ul> </li> <li>Already went through the Start Coding section of the Getting Started guide (hello_robot_xbox_teleop must not be running in the background)</li> </ol>"},{"location":"stretch-tutorials/ros2/getting_started/#switching-to-ros2","title":"Switching to ROS2","text":"<p>It's recommended that ROS1 and ROS2 systems not run at the same time. Therefore, the default installation starts with ROS1 enabled and ROS2 disabled. This is configured in the \"STRETCH BASHRC SETUP\", which you can see by running <code>gedit ~/.bashrc</code> in a terminal and scrolling to the bottom.</p> <p></p> <p>We will disable ROS1 by commenting out the ROS1 related lines by adding '#' in front of them, and enable ROS2 by uncommenting the ROS2 related lines by deleting the '#' in front of them. The result will look like:</p> <p></p> <p>Save this configuration using Ctrl + S. Close out of the current terminal and open a new one. ROS2 is now enabled!</p>"},{"location":"stretch-tutorials/ros2/getting_started/#refreshing-the-ros2-workspace","title":"Refreshing the ROS2 workspace","text":"<p>While Stretch ROS2 is in beta, there will be frequent updates to the ROS2 software. Therefore, it makes sense to refresh the ROS2 software to the latest available release. In the ROS and ROS2 world, software is organized into \"ROS Workspaces\", where packages can be developed, compiled, and be made available to run from the command line. We are going to refresh the ROS2 workspace, which is called \"~/ament_ws\" and available in the home directory. Follow the Create a new ROS Workspace guide to run the <code>stretch_create_ament_workspace.sh</code> script. This will delete the existing \"~/ament_ws\", create a new one with all of the required ROS2 packages for Stretch, and compile it. Also we need to take into account that building the workspace is different in ROS2, we need to type colcon build instead of catkin make for it to work.</p>"},{"location":"stretch-tutorials/ros2/getting_started/#testing-keyboard-teleop","title":"Testing Keyboard Teleop","text":"<p>We can test whether the ROS2 workspace was enabled successfully by testing out the ROS2 drivers package, called \"stretch_core\", with keyboard teleop. In one terminal, we'll launch Stretch's ROS2 drivers using:</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>In the second terminal, launch the keyboard teleop node using: <pre><code>ros2 run stretch_core keyboard_teleop\n</code></pre></p> <p>The following menu will be outputted to the terminal and you can press a key to move the corresponding joint on the robot. When you're ready to exit, press Ctrl + C.</p> <pre><code>[INFO] [1672878953.011453154] [keyboard_teleop]: keyboard_teleop started\n[INFO] [1672878953.041154084] [keyboard_teleop]: Node keyboard_teleop connected to /stop_the_robot service.\n---------- KEYBOARD TELEOP MENU -----------\n\n              i HEAD UP                    \n j HEAD LEFT            l HEAD RIGHT       \n              , HEAD DOWN                  \n\n\n 7 BASE ROTATE LEFT     9 BASE ROTATE RIGHT\n home                   page-up            \n\n\n              8 LIFT UP                    \n              up-arrow                     \n 4 BASE FORWARD         6 BASE BACK        \n left-arrow             right-arrow        \n              2 LIFT DOWN                  \n              down-arrow                   \n\n\n              w ARM OUT                    \n a WRIST FORWARD        d WRIST BACK       \n              x ARM IN                     \n\n\n              5 GRIPPER CLOSE              \n              0 GRIPPER OPEN               \n\n  step size:  b BIG, m MEDIUM, s SMALL     \n\n              q QUIT                       \n\n-------------------------------------------\n</code></pre> <p>ROS2 is setup! Move onto the next tutorial: Follow Joint Trajectory Commands.</p>"},{"location":"stretch-tutorials/ros2/internal_state_of_stretch/","title":"Internal State of Stretch","text":""},{"location":"stretch-tutorials/ros2/internal_state_of_stretch/#getting-the-state-of-the-robot","title":"Getting the State of the Robot","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>Begin by starting up the stretch driver launch file by typing the following in a terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then utilize the ROS command-line tool, ros2 topic, to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a terminal.</p> <pre><code>ros2 topic echo /stretch/joint_states\n</code></pre> <p>Your terminal will then output the information associated with the <code>/stretch/joint_states</code> topic. Your <code>header</code>, <code>position</code>, <code>velocity</code>, and <code>effort</code> information may vary from what is printed below.</p> <pre><code>header:\n  seq: 70999\n  stamp:\n    secs: 1420\n    nsecs:   2000000\n  frame_id: ''\nname: [joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left,\n  joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift,\n  joint_right_wheel, joint_wrist_yaw]\nposition: [-1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2.9291786329821434e-07, 1.3802900147297237e-06, 0.08154086954434359, 1.4361499260374905e-07, 0.4139061738340768, 9.32603306580404e-07]\nvelocity: [0.00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1.322424459109634e-05, -0.00035084643762840415, 0.0012164337445918797, 0.0002138814988808099, 0.00010419792027496809, 4.0575263146426684e-05, 0.00022487596895736357, -0.0007751929074042957, 0.0002451588607332439]\neffort: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n---\n</code></pre> <p>Additionally, if you type <code>ros2 topic list</code> in the terminal, you will see the list of active topics being published.</p> <p>A powerful tool to visualize the ROS communication is through the rqt_graph package. You can see a graph of topics being communicated between nodes by typing the following.</p> <pre><code>ros2 run rqt_graph rqt_graph\n</code></pre> <p></p> <p>The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.</p>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/","title":"Introduction to HelloNode","text":"<p>HelloNode is a convenience class for creating a ROS 2 node for Stretch. The most common way to use this class is to extend it. In your extending class, the main funcion would call <code>HelloNode</code>'s main function. This would look like:</p> <pre><code>import hello_helpers.hello_misc as hm\n\nclass MyNode(hm.HelloNode):\n    def __init__(self):\n        hm.HelloNode.__init__(self)\n\n    def main(self):\n        hm.HelloNode.main(self, 'my_node', 'my_node', wait_for_first_pointcloud=False)\n        # my_node's main logic goes here\n\nnode = MyNode()\nnode.main()\n</code></pre> <p>There is also a one-liner class method for instantiating a <code>HelloNode</code> for easy prototyping. One example where this is handy in sending pose commands from iPython:</p> <pre><code># roslaunch the stretch launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.move_to_pose({'joint_lift': 0.4})\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#attributes","title":"Attributes","text":""},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#dryrun","title":"<code>dryrun</code>","text":"<p>This attribute allows you to control whether the robot actually moves when calling <code>move_to_pose()</code>, <code>home_the_robot()</code>, <code>stow_the_robot()</code>, or other motion methods in this class. When <code>dryrun</code> is set to True, these motion methods return immediately. This attribute is helpful when you want to run just the perception/planning part of your node without actually moving the robot. For example, you could replace the following verbose snippet:</p> <pre><code># launch the stretch driver launch file beforehand\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\nactually_move = False\n[...]\nif actually_move:\n    temp.move_to_pose({'translate_mobile_base': 1.0})\n</code></pre> <p>to be more consise:</p> <pre><code># launch the stretch driver launch file beforehand\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\n[...]\ntemp.dryrun = True\ntemp.move_to_pose({'translate_mobile_base': 1.0})\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#methods","title":"Methods","text":""},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#move_to_posepose-blockingfalse-custom_contact_thresholdsfalse-duration20","title":"<code>move_to_pose(pose, blocking=False, custom_contact_thresholds=False, duration=2.0)</code>","text":"<p>This method takes in a dictionary that describes a desired pose for the robot and communicates with stretch_driver to execute it. The basic format of this dictionary is string/number key/value pairs, where the keys are joint names and the values are desired position goals. For example, <code>{'joint_lift': 0.5}</code> would put the lift at 0.5m in its joint range. A full list of command-able joints is published to the <code>/stretch/joint_states</code> topic. Used within a node extending <code>HelloNode</code>, calling this method would look like:</p> <pre><code>self.move_to_pose({'joint_lift': 0.5})\n</code></pre> <p>Internally, this dictionary is converted into a JointTrajectory message that is sent to a FollowJointTrajectory action server in stretch_driver. This method waits by default for the server to report that the goal has completed executing. However, you can return before the goal has completed by setting the <code>blocking</code> argument to False. This can be useful for preempting goals.</p> <p>When the robot is in <code>position</code> mode, if you set <code>custom_contact_thresholds</code> to True, this method expects a different format dictionary: string/tuple key/value pairs, where the keys are still joint names, but the values are <code>(position_goal, effort_threshold)</code>. The addition of a effort threshold enables you to detect when a joint has made contact with something in the environment, which is useful for manipulation or safe movements. For example, <code>{'joint_arm': (0.5, 20)}</code> commands the telescoping arm fully out (the arm is nearly fully extended at 0.5 meters) but with a low enough effort threshold (20% of the arm motor's max effort) that the motor will stop when the end of arm has made contact with something. Again, in a node, this would look like:</p> <pre><code>self.move_to_pose({'joint_arm': (0.5, 40)}, custom_contact_thresholds=True)\n</code></pre> <p>When the robot is in <code>trajectory</code> mode, if you set argument <code>duration</code> as <code>ts</code>, this method will ensure that the target joint positions are achieved over <code>ts</code> seconds. For example, the below would put the lift at 0.5m from its current position in <code>5.0</code> seconds:</p> <pre><code>self.move_to_pose({'joint_lift': 0.5}, duration=5.0)\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#home_the_robot","title":"<code>home_the_robot()</code>","text":"<p>This is a convenience method to interact with the driver's <code>/home_the_robot</code> service.</p>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#stow_the_robot","title":"<code>stow_the_robot()</code>","text":"<p>This is a convenience method to interact with the driver's <code>/stow_the_robot</code> service.</p>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#stop_the_robot","title":"<code>stop_the_robot()</code>","text":"<p>This is a convenience method to interact with the driver's <code>/stop_the_robot</code> service.</p>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#get_tffrom_frame-to_frame","title":"<code>get_tf(from_frame, to_frame)</code>","text":"<p>Use this method to get the transform (geometry_msgs/TransformStamped) between two frames. This method is blocking. For example, this method can do forward kinematics from the base_link to the link between the gripper fingers, link_grasp_center, using:</p> <pre><code># launch the stretch driver launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\nt = temp.get_tf('base_link', 'link_grasp_center')\nprint(t.transform.translation)\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#get_robot_floor_pose_xyafloor_frameodom","title":"<code>get_robot_floor_pose_xya(floor_frame='odom')</code>","text":"<p>Returns the current estimated x, y position and angle of the robot on the floor. This is typically called with respect to the odom frame or the map frame. x and y are in meters and the angle is in radians.</p> <p>Note</p> <p>To get the robot pose with respect to the odom frame we need to launch stretch_driver along with the broadcast_odom_tf parameter set to True. To do this execute the command:  <code>ros2 launch stretch_core stretch_driver.launch.py broadcast_odom_tf:=True</code></p> <pre><code># launch the stretch driver launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\nt = temp.get_robot_floor_pose_xya(floor_frame='odom')\nprint(t)\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#mainnode_name-node_topic_namespace-wait_for_first_pointcloudtrue","title":"<code>main(node_name, node_topic_namespace, wait_for_first_pointcloud=True)</code>","text":"<p>When extending the <code>HelloNode</code> class, call this method at the very beginning of your <code>main()</code> method. This method handles setting up a few ROS components, including registering the node with the ROS server, creating a TF listener, creating a FollowJointTrajectory client for the <code>move_to_pose()</code> method, subscribing to depth camera point cloud topic, and connecting to the quick-stop service.</p> <p>Since it takes up to 30 seconds for the head camera to start streaming data, the <code>wait_for_first_pointcloud</code> argument will get the node to wait until it has seen camera data, which is helpful if your node is processing camera data.</p>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#quick_createname-wait_for_first_pointcloudfalse","title":"<code>quick_create(name, wait_for_first_pointcloud=False)</code>","text":"<p>A class level method for quick testing. This allows you to avoid having to extend <code>HelloNode</code> to use it.</p> <pre><code># launch the stretch driver launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.move_to_pose({'joint_lift': 0.4})\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#subscribed-topics","title":"Subscribed Topics","text":""},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#cameradepthcolorpoints-sensor_msgspointcloud2","title":"/camera/depth/color/points (sensor_msgs/PointCloud2)","text":"<p>Provides a point cloud as currently seen by the Realsense depth camera in Stretch's head. Accessible from the <code>self.point_cloud</code> attribute.</p> <pre><code># launch the stretch driver launch file beforehand\n\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp', wait_for_first_pointcloud=True)\nprint(temp.point_cloud)\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#stretchjoint_states-sensor_msgsjointstate","title":"/stretch/joint_states (sensor_msgs/JointState)","text":"<p>Provides the current state of robot joints that includes joint names, positions, velocities, efforts. Accessible from the <code>self.joint_state</code> attribute. <pre><code>print(temp.joint_state)\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#mode-std_msgsstring","title":"/mode (std_msgs/String)","text":"<p>Provides the mode the stretch driver is currently in. Possible values include <code>position</code>, <code>trajectory</code>, <code>navigation</code>, <code>homing</code>, <code>stowing</code>. <pre><code>print(temp.mode)\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#tool-std_msgsstring","title":"/tool (std_msgs/String)","text":"<p>Provides the end of arm tool attached to the robot. <pre><code>print(temp.tool)\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#subscribed-services","title":"Subscribed Services","text":""},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#stop_the_robot-std_srvstrigger","title":"/stop_the_robot (std_srvs/Trigger)","text":"<p>Provides a service to quickly stop any motion currently executing on the robot.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.stop_the_robot_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#stow_the_robot-std_srvstrigger","title":"/stow_the_robot (std_srvs/Trigger)","text":"<p>Provides a service to stow the robot arm.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.stow_the_robot_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#home_the_robot-std_srvstrigger","title":"/home_the_robot (std_srvs/Trigger)","text":"<p>Provides a service to home the robot joints.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.home_the_robot_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#switch_to_trajectory_mode-std_srvstrigger","title":"/switch_to_trajectory_mode (std_srvs/Trigger)","text":"<p>Provides a service to quickly stop any motion currently executing on the robot.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.switch_to_trajectory_mode_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#switch_to_position_mode-std_srvstrigger","title":"/switch_to_position_mode (std_srvs/Trigger)","text":"<p>Provides a service to quickly stop any motion currently executing on the robot.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.switch_to_position_mode_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_hellonode/#switch_to_navigation_mode-std_srvstrigger","title":"/switch_to_navigation_mode (std_srvs/Trigger)","text":"<p>Provides a service to quickly stop any motion currently executing on the robot.</p> <pre><code># launch the stretch driver launch file beforehand\n\nfrom std_srvs.srv import Trigger\nimport hello_helpers.hello_misc as hm\ntemp = hm.HelloNode.quick_create('temp')\ntemp.switch_to_navigation_mode_service.call_async(Trigger.Request())\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/","title":"Introduction to ROS 2","text":"<p>In this tutorial we will explore rclpy, the client library for interacting with ROS 2 using the Python API. The rclpy library forms the base of ROS 2 and you will notice that all tutorials in the following sections will use it. In this section we will focus on a few common constructs of rclpy and then follow some examples using the IPython interpreter to get familiar with them.</p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#ipython","title":"IPython","text":"<p>It is not always necessary to write a functional Python script while prototyping or exploring a new library. It's instructive and helpful to use what\u2019s called an REPL (Read-Eval-Print Loop), which quickly allows us to execute Python instructions and see their output immediately. This allows better understanding of what each instruction does and is often a great way to debug unexpected behavior. IPython is a command line based interactive interpreter for Python that uses REPL and can be used to run Python snippets for quick prototyping.</p> <p>To run IPython in a terminal, simply execute: <pre><code>python3 -m IPython\n</code></pre></p> <p>Try out the following snippets for a ROS 2 quickstart:</p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#initialization-and-shutdown","title":"Initialization and Shutdown","text":""},{"location":"stretch-tutorials/ros2/intro_to_ros2/#rclpyinit","title":"rclpy.init()","text":"<p>All rclpy functionality can be exposed after initialization: <pre><code>import rclpy\n\nrclpy.init()\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#rclpycreate_node","title":"rclpy.create_node()","text":"<p>To create a new ROS 2 node, one can use the create_node method with the node name as the argument: <pre><code>node = rclpy.create_node('temp')\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#rclpyloggingget_logger","title":"rclpy.logging.get_logger()","text":"<p>The rclpy library also provides a logger to print messages with different severity levels to stdout. Here\u2019s how you can use it: <pre><code>import rclpy.logging\nlogger = rclpy.logging.get_logger('temp')\nlogger.info(\"Hello\")\nlogger.warn(\"Robot\")\nlogger.error(\"Stretch\")\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#rclpyok","title":"rclpy.ok()","text":"<p>If you want to check whether rclpy has been initialized, you can run the following snippet. This is especially useful to simulate an infinite loop based on whether rclpy has been shutdown. <pre><code>import time\n\nwhile rclpy.ok():\n    print(\"Hello\")\n    time.sleep(1.0)\n</code></pre></p> <p>Press ctrl+c to get out of the infinite loop.</p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#rclpyshutdown","title":"rclpy.shutdown()","text":"<p>Finally, to destroy a node safely and shutdown the instance of rclpy you can run:</p> <pre><code>node.destroy_node()\nrclpy.shutdown()\n</code></pre>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#publishing-and-subscribing","title":"Publishing and Subscribing","text":""},{"location":"stretch-tutorials/ros2/intro_to_ros2/#create_publisher","title":"create_publisher()","text":"<p>ROS 2 is a distributed communication system and one way to send data is through a publisher. It takes the following arguments: msg_type, msg_topic and a history depth (formerly queue_size): <pre><code>from std_msgs.msg import String\nimport rclpy\n\nrclpy.init()\nnode = rclpy.create_node('temp')\npub = node.create_publisher(String, 'hello', 10)\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#create_subscription","title":"create_subscription()","text":"<p>To receive a message, we need to create a subscriber with a callback function that listens to the arriving messages. Let's create a subscriber and define a callback called hello_callback() that logs the a message as soon as one is received: <pre><code>def hello_callback(msg):\n    print(\"Received message: {}\".format(msg.data))\n\nsub = node.create_subscription(String, 'hello', hello_callback, 10)\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#publish","title":"publish()","text":"<p>Now that you have defined a publisher and a subscriber, let\u2019s send a message and see if it gets printed to the console: <pre><code>msg = String()\nmsg.data = \"Hello\"\npub.publish(msg)\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#rclpyspin_once","title":"rclpy.spin_once()","text":"<p>That didn\u2019t do it! Although the message was sent, it didn't get printed to the console. Why? Because the hello_callback() method was never called to print the message. In ROS, we don\u2019t call this method manually, but rather leave it to what\u2019s called the executor. The executor can be invoked by calling the spin_once() method. We pass the node object and a timeout of 2 seconds as the arguments. The timeout is important because the spin_once() method is blocking and it will wait for a message to arrive indefinitely if a timeout is not defined. It returns immediately once a message is received. <pre><code>rclpy.spin_once(node, timeout_sec=2.0)\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#rclpyspin","title":"rclpy.spin()","text":"<p>The spin_once() method only does work equivalent to a single message callback. What if you want the executor to process callbacks continuously? This can be achieved using the spin() method. While retaining the current interpreter instance, let\u2019s open a new terminal window with a new instance of IPython and execute the following:</p> <p>Terminal 2: <pre><code>import rclpy\nfrom std_msgs.msg import String\nrclpy.init()\nnode = rclpy.create_node('temp2')\ndef hello_callback(msg):\n    print(\"I heard: {}\".format(msg.data))\nsub = node.create_subscription(String, 'hello', hello_callback, 10)\nrclpy.spin(node)\n</code></pre></p> <p>Now, from the first IPython instance, send a series of messages and see what happens:</p> <p>Terminal 1: <pre><code>for i in range(10):\n    msg.data = \"Hello {}\".format(i)\n    pub.publish(msg)\n</code></pre></p> <p>Voila! Finally, close both the terminals to end the session.</p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#service-server-and-client","title":"Service Server and Client","text":""},{"location":"stretch-tutorials/ros2/intro_to_ros2/#create_service","title":"create_service()","text":"<p>Let\u2019s explore another common way of using ROS 2. Imagine a case where you need to request some information from a node and you expect to receive a response. This can be achieved using the service client paradigm in ROS 2. Let\u2019s fire up IPython again and create a quick service: <pre><code>import rclpy\nfrom example_interfaces.srv import AddTwoInts\nrclpy.init()\n\ndef add_ints(req, res):\n    print(\"Received request\")\n    res.sum = req.a + req.b\n    return res\n\nnode = rclpy.create_node('temp')\nsrv = node.create_service(AddTwoInts, 'add_ints', add_ints)\n\n# you need to spin to receive the request\nrclpy.spin_once(node, timeout_sec=60.0)\n</code></pre></p> <p>Note</p> <p>You need to execute the next section of this tutorial within 60 seconds as the timeout defined for the spin_once() method to receive incoming requests is defined as 60 seconds. If the time elapses, you can execute the spin_once() method again before issuing a service request in the next section. Alternatively, you can call the spin() method to listen for incoming requests indefinitely.</p> <p>The add_ints() method is the callback method for the service server. Once a service request is received, this method will act on it to generate the response. Since a service request is a ROS message, we need to invoke the executor with a spin method to receive the message.</p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#create_client","title":"create_client()","text":"<p>Now, while retaining the current IPython session, open another session of the IPython interpreter in another terminal to write the service client: <pre><code>import rclpy\nfrom example_interfaces.srv import AddTwoInts\nrclpy.init()\nnode = rclpy.create_node('temp2')\n\ncli = node.create_client(AddTwoInts, 'add_ints')\n\nreq = AddTwoInts.Request()\nreq.a = 10\nreq.b = 20\n\nreq_future = cli.call_async(req)\nrclpy.spin_until_future_complete(node, req_future, timeout_sec=2.0)\n\nprint(\"Received response: {}\".format(req_future.result().sum))\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/intro_to_ros2/#rclpyspin_until_future_complete","title":"rclpy.spin_until_future_complete()","text":"<p>Notice that the spin method manifests itself as the spin_until_future_complete() method which takes the node, future and timeout_sec as the arguments. The future is an object in ROS 2 that\u2019s returned immediately after an async service call has been made. We can then wait on the result of this future. This way the call to the service is not blocking and the code execution can continue as soon as the service call is issued.</p>"},{"location":"stretch-tutorials/ros2/modes/","title":"Stretch Modes","text":"<p>Stretch Driver allows the robot to be commanded in several modes of operation. Understanding what these modes are and what behaviors they enable is central to using the robot effectively.</p>"},{"location":"stretch-tutorials/ros2/modes/#position-mode","title":"Position mode","text":"<p>Position mode enables position control of the arm, head and mobile base with sequential incremental positions achieved using the move_by() method in the underlying Python interface to the robot (Stretch Body). It disables velocity control of the mobile base through the /cmd_vel topic. The position commands to various joints are honored by the Joint Trajectory Server through the /stretch_controller/follow_joint_trajectory action interface.</p> <p>To understand how to command joints using the Joint Trajectory Server, refer to the Follow Joint Trajectory Commands tutorial.</p>"},{"location":"stretch-tutorials/ros2/modes/#trajectory-mode","title":"Trajectory mode","text":"<p>Trajectory mode is able to execute plans from high level planners like MoveIt2. These planners are able to generate waypoint trajectories for each joint for smooth motion profiles in velocity and acceleration space. The joint trajectory action server, and the underlying Python interface to the robot (Stretch Body) execut the trajectory respecting each waypoints' time_from_start attribute of the trajectory_msgs/JointTrajectoryPoint message. This allows coordinated motion of the base + arm. To understand better how this is achieved, it might be instructive to look at the Trajectory API tutorial in Stretch Body.</p> <p>To understand how to command joints using the Joint Trajectory Server, refer to the Follow Joint Trajectory Commands tutorial.</p>"},{"location":"stretch-tutorials/ros2/modes/#navigation-mode","title":"Navigation mode","text":"<p>Navigation mode enables mobile base velocity control via the /cmd_vel topic, and disables position-based control of the mobile base as in position mode. The arm and head joints remain commandable in position mode.</p> <p>To understand how to command the mobile base using navigation mode, refer to the Mobile Base Velocity Control tutorial.</p>"},{"location":"stretch-tutorials/ros2/moveit_basics/","title":"MoveIt! Basics","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p>"},{"location":"stretch-tutorials/ros2/moveit_basics/#overview","title":"Overview","text":"<p>MoveIt 2 is a whole-body motion planning framework for mobile manipulators that allows planning pose and joint goals in environments with and without obstacles. Stretch being a mobile manipulator is uniquely well-suited to utilize the planning capabilities of MoveIt 2 in different scenarios.</p>"},{"location":"stretch-tutorials/ros2/moveit_basics/#motivation","title":"Motivation","text":"<p>Stretch has a kinematically simple 3 DoF arm (+2 with DexWrist) that is suitable for pick and place tasks of varied objects. Its mobile base provides it with 2 additional degrees of freedom that afford it more manipulability and also the ability to move around freely in its environment. To fully utilize these capabilities, we need a planner that can plan for both the arm and the mobile base at the same time. With MoveIt 2 and ROS 2, it is now possible to achieve this, empowering users to plan more complicated robot trajectories in difficult and uncertain environments.</p>"},{"location":"stretch-tutorials/ros2/moveit_basics/#demo-with-stretch-robot","title":"Demo with Stretch Robot","text":"<p>Before we proceed, it's always a good idea to home the robot first by running the following script so that we have the correct joint positions being published on the /joint_states topic. This is necessary for planning trajectories on Stretch with MoveIt.</p> <pre><code>stretch_robot_home.py\n</code></pre>"},{"location":"stretch-tutorials/ros2/moveit_basics/#planning-with-moveit-2-using-rviz","title":"Planning with MoveIt 2 Using RViz","text":"<p>The easiest way to run MoveIt 2 on your robot is through RViz. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To launch RViz with MoveIt 2, run the following command. (Press Ctrl+C in the terminal to terminate)</p> <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre> <p>Follow the instructions in this tutorial to plan and execute trajectories using the interactive markers in RViz.</p> <p>Use the interactive markers to drag joints to desired positions or go to the manipulation tab in the Motion Planning pane to fine-tune joint values using the sliders. Next, click the 'Plan' button to plan the trajectory. If the plan is valid, you should be able to execute the trajectory by clicking the 'Execute' button. Below we see Stretch raising its arm without any obstacle in the way.</p> <p></p> <p>To plan with obstacles, you can insert objects like a box, cyclinder or sphere, in the planning scene to plan trajectories around the object. This can be done by adding an object using the Scene Objects tab in the Motion Planning pane. Below we see Stretch raising its arm with a flat cuboid obstacle in the way. The mobile base allows Stretch to move forward and then back again while raising the arm to avoid the obstacle.</p> <p></p>"},{"location":"stretch-tutorials/ros2/moveit_basics/#planning-with-moveit-2-using-the-movegroup-c-api","title":"Planning with MoveIt 2 Using the MoveGroup C++ API","text":"<p>If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. Execute the launch file again and go through the comments in the code to understand what's going on. (Press Ctrl+C in the terminal to terminate)</p> <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre> <p>Follow the instructions in this tutorial to plan and execute trajectories using the MoveGroup C++ API.</p> <p></p>"},{"location":"stretch-tutorials/ros2/moveit_movegroup_demo/","title":"Moveit movegroup demo","text":""},{"location":"stretch-tutorials/ros2/moveit_movegroup_demo/#planning-with-moveit-2-using-the-movegroup-c-api","title":"Planning with MoveIt 2 Using the MoveGroup C++ API","text":"<p>If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. For this tutorial, we are going to use the RViz Visual Tools plugin to execute the C++ source code part by part to explore more sophisticated functionalities.</p> <p>Execute the launch file again to begin the tutorial. You can follow along in the C++ code to inspect finer details. (Press Ctrl+C in the terminal to terminate) (Ensure you have enough room around the robot before running the script)</p> <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre> <p>To execute the script and interact with the robot, all you need to do is press the Next button in the RViz Visual Tools window at the bottom left. Follow the prompts on the terminal to run through the tutorial. While executing the script, it's also a good idea to study and understand the script that is being executed. Find it here.</p> <p></p> <ol> <li>Let's begin by jogging the camera pan and tilt joints. For having a complete 3D representation of its environment, Stretch needs to point its head in all directions, up, down, left, right, you name it! Luckily, we have a planning group that allows you to do just that - the stretch_head planning group. Go ahead and press the Next button to jog the camera.</li> </ol> <p></p> <ol> <li>What good is a robot that can't hold your hand on your worst days. We gave Stretch a gripper to do just that and more! Let's exercise it using the stretch_gripper planning group. All you have to do is press Next.</li> </ol> <p></p> <ol> <li>What about the good days you ask? Stretch always wants to reach out to you, no matter what. Speaking of reaching out, let's make Stretch exercise its arm for the next time you need it. Press Next.</li> </ol> <p></p> <ol> <li>Stretch doesn't like sitting in a corner fretting about the future. It is the future. Stretch wants to explore and in style. What better way to do it than by rolling around? Press Next and you'll see. That's the mobile_base planning group.</li> </ol> <p></p> <ol> <li>All that exploring does get tiring and sometimes Stretch just wants to relax and dream about its next adventure. Stretch prefers to relax with its arm down, lest someone trips over it and disturb Stretch's peaceful slumber. Press Next to see the mobile_base_arm planning group.</li> </ol> <p></p> <ol> <li>Did someone say adventure? How about dodging some pesky obstacles? They're everywhere, but they don't bother Stretch a lot. It can go around them. Nothing stops Stretch! You know what to do.</li> </ol> <p></p> <ol> <li>Stretch is smart, you don't need to tell it which joint goes where. Just say what you want done and it does it. How about planning a pose goal to see it in action? Press Next.</li> </ol> <p></p> <p>Press Ctrl+C to end this demo.</p> <ol> <li>To wrap it up, the final act! This one is a surprise that's only a click away. Go on and execute the following command: <pre><code>ros2 launch stretch_moveit2 moveit_draw.launch.py\n</code></pre></li> </ol> <p></p>"},{"location":"stretch-tutorials/ros2/moveit_rviz_demo/","title":"Moveit rviz demo","text":""},{"location":"stretch-tutorials/ros2/moveit_rviz_demo/#planning-with-moveit-2-using-rviz","title":"Planning with MoveIt 2 Using RViz","text":"<ol> <li>The easiest way to run MoveIt 2 on your robot is through the RViz plugin. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To get started using MoveIt 2 with RViz, execute the following command in a terminal. (Press Ctrl+C in the terminal to terminate)</li> </ol> <pre><code>ros2 launch stretch_moveit2 movegroup_moveit2.launch.py\n</code></pre> <ol> <li>You should see Stretch visualized in RViz with joint positions exactly as they appear on the actual robot (If not, home the robot and start from step 1!). You can select a Planning Group from the drop down menu that allows you to choose a group of joints to plan for and control using MoveIt. When you select a Planning Group the joints that can be controlled are highlighted with interactive markers. Let\u2019s go ahead and select the stretch_arm planning group.</li> </ol> <ol> <li>Now, click and drag the arrow down to slide the arm lift downwards and then use the wheel to turn the gripper inwards so that it fits squarely over the robot base. At this point if the robot base glows red in RViz, it means the robot arm is in collision with the base. You should move the lift upwards slightly until the red highlight disappears.</li> </ol> <ol> <li>Now click on the Plan button to see the simulated motion of the robot in RViz</li> </ol> <ol> <li>Before proceeding, ensure that the robot is in an open space without obstalces. Click the Execute button to execute the plan on the actual robot. Congratulations, you just stowed the robot arm using MoveIt! (Alternatively, if you do not want to review the simulated plan, you can click \u2018Plan and Execute\u2019 to execute the planned trajectory directly)</li> </ol> <ol> <li>Now, let\u2019s move Stretch\u2019s mobile base! Select the mobile_base_arm planning_group from the drop down menu. You should see the base interactive marker appear in RViz. Use the arrow to drag the base forward or backward for about 1m. Click Plan and Execute when you are done. Voila!</li> </ol> <p>The mobile_base_arm planning group also allows you to execute a coordinated base and arm motion plan. Go ahead and move the markers around to plan some fun trajectories, maybe make Stretch do a Pirouette! Similarly, the stretch_gripper and stretch_head planning groups allow opening/closing the gripper and panning/tilting the camera.</p> <ol> <li>The interactive markers are just one way to control the joints. If you want a finer control, you can switch to the Joints tab of the plugin and use the sliders to adjust the desired end state of the joints.</li> </ol> <p></p> <ol> <li>MoveIt allows you to plan not just simple trajectories but also avoid obstacles. Let\u2019s add an obstacle to the planning scene. Click on the Scene Objects tab and select the Box object. Define a cube of dimensions 0.1x0.1x0.1m and add it to the scene using the green + button next to it. Now, place it just in front of the mobile base using the fine controls in the Change object pose/scale buttons to the right. Click on the Publish button for MoveIt to account for the object while planning.</li> </ol> <p></p> <ol> <li>Now return back to the Planning tab and define an end state such that the Box is in between the robot start and end states. Again, ensure that the robot has enough space around it. Plan and Execute!</li> </ol> <p></p> <p>With a fully functional perception pipeline, the planning scene can represent Stretch\u2019s surroundings accurately and allow Stretch to manipulate and navigate in a cluttered environment</p> <ol> <li>Feel free to explore more sophisticated planners shipped along with MoveIt 2 in the Context tab. End!</li> </ol>"},{"location":"stretch-tutorials/ros2/navigation_overview/","title":"Nav2 Overview","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p>"},{"location":"stretch-tutorials/ros2/navigation_overview/#overview","title":"Overview","text":"<p>The ROS 2 Navigation Stack or Nav2 is a motion planning and control framework for the mobile base. It is a stack because it encompasses several ROS packages that help with mapping, localization, planning and navigation. Stretch's mobile base has been integrated and works well right out of the box with Nav2.</p>"},{"location":"stretch-tutorials/ros2/navigation_overview/#motivation","title":"Motivation","text":"<p>Stretch has a differential drive mobile base that enables navigation. However, before Stretch can navigate, it must be able to generate a map, localize itself in the environment and plan a path between points. This is challenging when the environment could present complexities such as static or dynamic obstacles that can get in the way of the robot, or floor surfaces with different frictional properties that can trip a controller.</p> <p>Fortunately, the Nav2 stack enables these capabilities through various packages and provides a simple Python API to interact with them. Let\u2019s take a quick tour!</p>"},{"location":"stretch-tutorials/ros2/navigation_overview/#demo-with-stretch","title":"Demo with Stretch","text":""},{"location":"stretch-tutorials/ros2/navigation_overview/#mapping","title":"Mapping","text":"<p>It is possible to generate a high-fidelity 2D map of your environment using the slam_toolbox package. For you, it is as easy as teleoperating Stretch in your home, laboratory or office. Stretch does the rest.</p> <p> </p>"},{"location":"stretch-tutorials/ros2/navigation_overview/#navigation","title":"Navigation","text":"<p>Once a map has been generated, Stretch can localize itself and find its way in its environment. Just point on the map and Stretch will get there, without your help.</p> <p> </p>"},{"location":"stretch-tutorials/ros2/navigation_overview/#simple-commander-python-api","title":"Simple Commander Python API","text":"<p>It is as simple to interact with Nav2 programmatically, thanks to the Python API. For example, have a look at this neat patrol demo.</p> <p> </p> <p>Have a look at the following tutorials to explore the above capabilities on your own Stretch! If you stumble upon something unexpected, please let us know.</p>"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/","title":"Nav2 Stack Using Simple Commander Python API","text":"<p>In this tutorial, we will work with Stretch to explore the Simple Commander Python API to enable autonomous navigation programmatically. We will also demonstrate a security patrol routine for Stretch developed using this API. If you just landed here, it might be a good idea to first review the previous tutorial which covered mapping and navigation using RViz as an interface.</p>"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/#the-simple-commander-python-api","title":"The Simple Commander Python API","text":"<p>To develop complex behaviors with Stretch where navigation is just one aspect of the autonomy stack, we need to be able to plan and execute navigation routines as part of a bigger program. Luckily, the Nav2 stack exposes a Python API that abstracts the ROS layer and the Behavior Tree framework (more on that later!) from the user through a pre-configured library called the robot navigator. This library defines a class called BasicNavigator which wraps the planner, controller and recovery action servers and exposes methods such as <code>goToPose()</code>, <code>goToPoses()</code> and <code>followWaypoints()</code> to execute navigation behaviors.</p> <p>Let's first see the demo in action and then explore the code to understand how this works!</p> <p>Warning</p> <p>We will not be using the arm for this demo. We recommend stowing the arm to avoid inadvertently bumping it into walls while the robot is navigating. </p> <p>Execute:</p> <pre><code>stretch_robot_stow.py\n</code></pre>"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/#setup","title":"Setup","text":"<p>Let's set the patrol route up before you can execute this demo in your map. This requires reading the position of the robot at various locations in the map and entering the co-ordinates in the array called <code>security_route</code> in the simple_commander_demo.py file. </p> <p>First, execute the following command while passing the correct map YAML. Then, press the 'Startup' button:</p> <pre><code>ros2 launch stretch_nav2 navigation.launch.py map:=${HELLO_ROBOT_FLEET}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p>Since we expect the first point in the patrol route to be at the origin of the map, the first coordinates should be (0.0, 0.0). Next, to define the route, the easiest way to define the waypoints in the <code>security_route</code> array is by setting the robot at random locations in the map using the '2D Pose Estimate' button in RViz as shown below. For each location, note the x, and y coordinates in the position field of the base_footprint frame and add it to the <code>security_route</code> array in simple_commander_demo.py.</p> <p> </p> <p>Finally, Press Ctrl+C to exit out of navigation and save the simple_commander_demo.py file. Now, build the workspace to make the updated file available for the next launch command. </p> <pre><code>cd ~/ament_ws/\ncolcon build\n</code></pre>"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/#see-it-in-action","title":"See It In Action","text":"<p>Go ahead and execute the following command to run the demo and visualize the result in RViz. Be sure to pass the correct path to the map YAML: Terminal 1:</p> <pre><code>ros2 launch stretch_nav2 demo_security.launch.py map:=${HELLO_ROBOT_FLEET}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p> </p>"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/#code-breakdown","title":"Code Breakdown","text":"<p>Now, let's jump into the code to see how things work under the hood. Follow along in the code to have a look at the entire script.</p> <p>First, we import the <code>BasicNavigator</code> class from the robot_navigator library which comes standard with the Nav2 stack. This class wraps around the planner, controller and recovery action servers.</p> <pre><code>from stretch_nav2.robot_navigator import BasicNavigator, TaskResult\n</code></pre> <p>In the main method, we initialize the node and create an instance of the BasicNavigator class called navigator.</p> <pre><code>def main():\n    rclpy.init()\n\n    navigator = BasicNavigator()\n</code></pre> <p>Then, we set up a path for Stretch to patrol consisting of the coordinates in the map reference frame. These coordinates are specific to the map generated for this tutorial and would not be suitable for your robot. To define coordinates that work with your robot, first command the robot to at least three random locations in the map you have generated of your environment, then read the base_link x and y coordinates for each of them from the RViz TF plugin. Plug them in the <code>security_route</code> list. Keep in mind that for this demo, the robot is starting from [0.0, 0.0] which is the origin of the map. This might not be the case for you.</p> <pre><code>    security_route = [\n        [0.0, 0.0],\n        [1.057, 1.3551],\n        [1.5828, 5.0823],\n        [-0.5390, 5.6623],\n        [0.8975, 9.7033]]\n</code></pre> <p> </p> <p>Next, we set an initial pose for the robot which would help AMCL localize the robot by providing an initial estimate of the robot's location. For this, we pass a PoseStamped message in the map reference frame with the robot's pose to the <code>setInitialPose()</code> method. The Nav2 stack recommends this before starting the lifecycle nodes using the \"Startup\" button in RViz. The <code>waitUntilNav2Active()</code> method waits until precisely this event.</p> <pre><code>    initial_pose = PoseStamped()\n    initial_pose.header.frame_id = 'map'\n    initial_pose.header.stamp = navigator.get_clock().now().to_msg()\n    initial_pose.pose.position.x = 0.0\n    initial_pose.pose.position.y = 0.0\n    initial_pose.pose.orientation.z = 0.0\n    initial_pose.pose.orientation.w = 1.0\n    navigator.setInitialPose(initial_pose)\n\n    navigator.waitUntilNav2Active()\n</code></pre> <p>Once the nodes are active, the navigator is ready to receive pose goals either through the <code>goToPose()</code>, <code>goToPoses()</code> or <code>followWaypoints()</code> methods. For this demo, we will be using the <code>followWaypoints()</code> method which takes a list of poses as an argument. Since we intend for the robot to patrol the route indefinitely or until the node is killed (or the robot runs out of battery!), we wrap the method in an infinite while loop with <code>rclpy.ok()</code>. Then, we generate pose goals with the <code>security_route</code> list and append them to a new list called <code>route_poses</code> which is passed to the <code>followWaypoints()</code> method.</p> <pre><code>    while rclpy.ok():\n\n        route_poses = []\n        pose = PoseStamped()\n        pose.header.frame_id = 'map'\n        pose.header.stamp = navigator.get_clock().now().to_msg()\n        pose.pose.orientation.w = 1.0\n        for pt in security_route[1:]:\n            pose.pose.position.x = pt[0]\n            pose.pose.position.y = pt[1]\n            route_poses.append(deepcopy(pose))\n\n        nav_start = navigator.get_clock().now()\n        navigator.followWaypoints(route_poses)\n</code></pre> <p>Since we are utilizing an action server built into Nav2, it's possible to seek feedback on this long running task through the action interface. The <code>isTaskComplete()</code> method returns a boolean depending on whether the patrolling task is complete. For the follow waypoints action server, the feedback message tells us which waypoint is currently being executed through the <code>feedback.current_waypoint</code> attribute. It is possible to cancel a goal using the <code>cancelTask()</code> method if the robot gets stuck. For this demo, we have set the timeout at 600 seconds to allow sufficient time for the robot to succeed. However, if you wish to see it in action, you can reduce the timeout to 30 seconds.</p> <pre><code>        i = 0\n        while not navigator.isTaskComplete():\n            i += 1\n            feedback = navigator.getFeedback()\n            if feedback and i % 5 == 0:\n                navigator.get_logger().info('Executing current waypoint: ' +\n                    str(feedback.current_waypoint + 1) + '/' + str(len(route_poses)))\n                now = navigator.get_clock().now()\n\n                if now - nav_start &gt; Duration(seconds=600.0):\n                    navigator.cancelTask()\n</code></pre> <p>Once the robot reaches the end of the route, we reverse the <code>security_route</code> list to generate the goal pose list that would be used by the <code>followWaypoints()</code> method in the next iteration of this loop.</p> <pre><code>        security_route.reverse()\n</code></pre> <p>Finally, after a leg of the patrol route is executed, we call the <code>getResult()</code> method to know whether the task succeeded, canceled or failed to log a message.</p> <pre><code>        result = navigator.getResult()\n        if result == TaskResult.SUCCEEDED:\n            navigator.get_logger().info('Route complete! Restarting...')\n        elif result == TaskResult.CANCELED:\n            navigator.get_logger().info('Security route was canceled, exiting.')\n            rclpy.shutdown()\n        elif result == TaskResult.FAILED:\n            navigator.get_logger().info('Security route failed! Restarting from other side...')\n</code></pre> <p>That's it! Using the Simple Commander API is as simple as that. Be sure to follow more examples in the nav2_simple_commander package if you wish to work with other useful methods exposed by the library.</p>"},{"location":"stretch-tutorials/ros2/navigation_stack/","title":"Nav2 Stack Using RViz","text":"<p>In this tutorial, we will explore the ROS 2 navigation stack using slam_toolbox for mapping an environment and the core Nav2 packages to navigate in the mapped environment. If you want to know more about teleoperating the mobile base or working with the RPlidar 2D scanner on Stretch, we recommend visiting the previous tutorials on Teleoperating stretch and Filtering Laser Scans. These topics are a vital part of how Stretch's mobile base can be velocity controlled using Twist messages, and how the RPlidar's LaserScan messages enable Obstacle Avoidance for autonomous navigation.</p> <p>Navigation is a key aspect of an autonomous agent because, often, to do anything meaningful, the agent needs to traverse an environment to reach a specific spot to perform a specific task. With a robot like Stretch, the task could be anything from delivering water or medicines for the elderly to performing a routine patrol of an establishment for security.</p> <p>Stretch's mobile base enables this capability and this tutorial will explore how we can autonomously plan and execute mobile base trajectories. Running this tutorial will require the robot to be untethered, so please ensure that the robot is adequately charged.</p>"},{"location":"stretch-tutorials/ros2/navigation_stack/#mapping","title":"Mapping","text":"<p>The first step is to map the space that the robot will navigate in. The <code>offline_mapping.launch.py</code> file will enable you to do this. First, run:</p> <pre><code>ros2 launch stretch_nav2 offline_mapping.launch.py\n</code></pre> <p>Rviz will show the robot and the map that is being constructed. Now, use the Xbox controller (see instructions below for using a keyboard) to teleoperate the robot around. To teleoperate the robot using the Xbox controller, keep the front left (LB) button pressed while using the right joystick for translation and rotation.</p> <p>Avoid sharp turns and revisit previously visited spots to form loop closures.</p> <p> </p> <p> </p> <p>In Rviz, once you see a map that has reconstructed the space well enough, open a new terminal and run the following commands to save the map to the <code>stretch_user/</code> directory.</p> <pre><code>mkdir ${HELLO_FLEET_PATH}/maps\nros2 run nav2_map_server map_saver_cli -f ${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;\n</code></pre> <p>Note</p> <p>The <code>&lt;map_name&gt;</code> does not include an extension. The map_saver node will save two files as <code>&lt;map_name&gt;.pgm</code> and <code>&lt;map_name&gt;.yaml</code>.</p> <p>Tip</p> <p>For a quick sanity check, you can inspect the saved map using a pre-installed tool called Eye of Gnome (eog) by running the following command:</p> <pre><code>eog ${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.pgm\n</code></pre>"},{"location":"stretch-tutorials/ros2/navigation_stack/#navigation","title":"Navigation","text":"<p>Next, with <code>&lt;map_name&gt;.yaml</code>, we can navigate the robot around the mapped space. Run:</p> <pre><code>ros2 launch stretch_nav2 navigation.launch.py map:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre> <p>A new RViz window should pop up with a <code>Startup</code> button in a menu at the bottom left of the window. Press the <code>Startup</code> button to kick-start all navigation related lifecycle nodes. Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location on the map does not match the robot's location in the real space. To correct this, from the top bar of Rviz, use <code>2D Pose Estimate</code> to lay an arrow down roughly where the robot is located in real space. This gives an initial estimate of the robot's location to AMCL, the localization package. AMCL will better localize the robot once we pass the robot a <code>2D Nav Goal</code>. </p> <p>In the top bar of Rviz, use <code>2D Nav Goal</code> to lay down an arrow where you'd like the robot to navigate. In the terminal, you'll see Nav2 go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior - spinning around 180 degrees in place or backing up.</p> <p> </p> <p>Tip</p> <p>If navigation fails or the robot becomes unresponsive to subsequent goals through RViz, you can still teleoperate the robot using an Xbox controller.</p>"},{"location":"stretch-tutorials/ros2/navigation_stack/#note","title":"Note","text":"<p>The launch files expose the launch argument \"teleop_type\". By default, this argument is set to \"joystick\", which launches joystick teleop in the terminal with the Xbox controller that ships with Stretch. The Xbox controller utilizes a dead man's switch safety feature to avoid unintended movement of the robot. This is the switch located on the front left side of the controller marked \"LB\". Keep this switch pressed while translating or rotating the base using the joystick located on the right side of the Xbox controller.</p> <p>If the Xbox controller is not available, the following commands will launch mapping or navigation, respectively, with keyboard teleop:</p> <pre><code>ros2 launch stretch_nav2 offline_mapping.launch.py teleop_type:=keyboard\n</code></pre> <p>or</p> <pre><code>ros2 launch stretch_nav2 navigation.launch.py teleop_type:=keyboard map:=${HELLO_FLEET_PATH}/maps/&lt;map_name&gt;.yaml\n</code></pre>"},{"location":"stretch-tutorials/ros2/navigation_stack/#simple-commander-api","title":"Simple Commander API","text":"<p>It is also possible to send 2D Pose Estimates and Nav Goals programmatically. In your own launch file, you may include <code>navigation.launch</code> to bring up the navigation stack. Then, you can send pose goals using the Nav2 simple commander API to navigate the robot programatically. We will explore this in the next tutorial.</p>"},{"location":"stretch-tutorials/ros2/obstacle_avoider/","title":"Obstacle Avoider","text":"<p>In this tutorial, we will work with Stretch to detect and avoid obstacles using the onboard RPlidar A1 laser scanner and learn how to filter laser scan data. If you want to know more about the laser scanner setup on Stretch and how to get it up and running, we recommend visiting the previous tutorials on Filtering Laser Scans and Mobile Base Collision Avoidance.</p> <p>A major drawback of using any ToF (Time of Flight) sensor is the inherent inaccuracies as a result of occlusions and weird reflection and diffraction phenomena the light pulses are subject to in an unstructured environment. This results in unexpected and undesired noise that can get in the way of an otherwise extremely useful sensor. Fortunately, it is easy to account for and eliminate these inaccuracies to a great extent by filering out the noise. We will do this with a ROS package called laser_filters that comes prebuilt with some pretty handy laser scan message filters.</p> <p>By the end of this tutorial, you will be able to tweak them for your particular use case and publish and visualize them on the /scan_filtered topic using RViz. So let\u2019s jump in! We will look at three filters from this package that have been tuned to work well with Stretch in an array of scenarios.</p>"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#laserscan-filtering","title":"LaserScan Filtering","text":"<p>LaserScanAngularBoundsFilterInPlace - This filter removes laser scans belonging to an angular range. For Stretch, we use this filter to discount points that are occluded by the mast because it is a part of Stretch\u2019s body and not really an object we need to account for as an obstacle while navigating the mobile base.</p> <p>LaserScanSpeckleFilter - We use this filter to remove phantom detections in the middle of empty space that are a result of reflections around corners. These disjoint speckles can be detected as false positives and result in jerky motion of the base through empty space. Removing them returns a relatively noise-free scan.</p> <p>LaserScanBoxFilter - Stretch is prone to returning false detections right over the mobile base. While navigating, since it\u2019s safe to assume that Stretch is not standing right above an obstacle, we filter out any detections that are in a box shape over the mobile base.</p> <p>Beware that filtering laser scans comes at the cost of a sparser scan that might not be ideal for all applications. If you want to tweak the values for your end application, you could do so by changing the values in the laser_filter_params.yaml file and by following the laser_filters package wiki. Also, if you are feeling zany and want to use the raw unfiltered scans from the laser scanner, simply subscribe to the /scan topic instead of the /scan_filtered topic.</p> <p></p>"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#avoidance-logic","title":"Avoidance logic","text":"<p>Now, let\u2019s use what we have learned so far to upgrade the collision avoidance demo in a way that Stretch is able to scan an entire room autonomously without bumping into things or people. To account for dynamic obstacles getting too close to the robot, we will define a keepout distance of 0.4 m - detections below this value stop the robot. To keep Stretch from getting too close to static obstacles, we will define another variable called turning distance of 0.75 m - frontal detections below this value make Stretch turn to the left until it sees a clear path ahead.</p> <p>Building up on the teleoperation using velocity commands tutorial, let's implement a simple logic for obstacle avoidance. The logic can be broken down into three steps:</p> <ol> <li>If the minimum value from the frontal scans is greater than 0.75 m, then continue to move forward</li> <li>If the minimum value from the frontal scans is less than 0.75 m, then turn to the right until this is no longer true</li> <li>If the minimum value from the overall scans is less than 0.4 m, then stop the robot</li> </ol> <p>Warning</p> <pre><code>If you see Stretch try to run over your lazy cat or headbutt a wall, just press the bright runstop button on Stretch's head to calm it down. For pure navigation tasks, it's also safer to stow Stretch's arm in.\n</code></pre> <p>Execute the command: <pre><code>stretch_robot_stow.py\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#see-it-in-action","title":"See It In Action","text":"<p>Alright, let's see it in action! Execute the following command to run the scripts: <pre><code>ros2 launch stretch_core rplidar_keepout.launch.py\n</code></pre></p> <p></p>"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#code-breakdown","title":"Code Breakdown:","text":"<p>Let's jump into the code to see how things work under the hood. Follow along here to have a look at the entire script.</p> <p>The turning distance is defined by the distance attribute and the keepout distance is defined by the keepout attribute.</p> <pre><code>        self.distance = 0.75 # robot turns at this distance\n        self.keepout = 0.4 # robot stops at this distance\n</code></pre> <p>To pass velocity commands to the mobile base, we publish the translational and rotational velocities to the /stretch/cmd_vel topic. To subscribe to the filtered laser scans from the laser scanner, we subscribe to the /scan_filtered topic. While you are at it, go ahead and check the behavior by switching to the /scan topic instead. See why filtering is necessary?</p> <pre><code>        self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 1) #/stretch_diff_drive_controller/cmd_vel for gazebo\n        self.subscriber_ = self.create_subscription(LaserScan, '/scan_filtered', self.lidar_callback, 10)\n</code></pre> <p>lidar_callback() is the callback function for the laser scanner that gets called every time a new message is received.</p> <pre><code>def lidar_callback(self, msg):\n</code></pre> <p>When the scan message is filtered, all the ranges that are filtered out are assigned the nan (not a number) value. This can get in the way of computing the minimum. Therefore, we reassign these values to inf (infinity).</p> <pre><code>        all_points = [r if (not isnan(r)) else inf for r in msg.ranges]\n</code></pre> <p>Next, we compute the two minimums that are necessary for the avoidance logic to work - the overall minimum and the frontal minimum named min_all and min_front respectively.</p> <pre><code>        front_points = [r * sin(theta) if (theta &lt; -2.5 or theta &gt; 2.5) else inf for r,theta in zip(msg.ranges, angles)]\n        front_ranges = [r if abs(y) &lt; self.extent else inf for r,y in zip(msg.ranges, front_points)]\n\n        min_front = min(front_ranges)\n        min_all = min(all_points)\n</code></pre> <p>Finally, we check the minimum values against the distance and keepout attributes to set the rotational and linear velocities of the mobile base with the set_speed() method.</p> <pre><code>        if(min_all &lt; self.keepout):\n            lin_vel = 0.0\n            rot_vel = 0.0\n        elif(min_front &lt; self.distance):\n            lin_vel = 0.0\n            rot_vel = 0.25\n        else:\n            lin_vel = 0.5\n            rot_vel = 0.0\n\n        self.set_speed(lin_vel, rot_vel)\n</code></pre> <p>That wasn't too hard, was it? Now, feel free to play with this code and change the attributes to see how it affects Stretch's behavior.</p>"},{"location":"stretch-tutorials/ros2/perception/","title":"Perception","text":""},{"location":"stretch-tutorials/ros2/perception/#perception-introduction","title":"Perception Introduction","text":"<p>The Stretch robot is equipped with the Intel RealSense D435i camera, an essential component that allows the robot to measure and analyze the world around it. In this tutorial, we are going to showcase how to visualize the various topics published by the camera.</p> <p>Begin by running the stretch <code>driver.launch.py</code> file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal.</p> <pre><code>ros2 launch stretch_core d435i_low_resolution.launch.py\n</code></pre> <p>Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal.</p> <pre><code>ros2 run rviz2 rviz2 -d /home/hello-robot/ament_ws/src/stretch_tutorials/rviz/perception_example.rviz\n</code></pre>"},{"location":"stretch-tutorials/ros2/perception/#pointcloud2-display","title":"PointCloud2 Display","text":"<p>A list of displays on the left side of the interface can visualize the camera data. Each display has its properties and status that notify a user if topic messages are received.</p> <p>For the <code>PointCloud2</code> display, a sensor_msgs/pointCloud2 message named <code>/camera/depth/color/points</code> is received and the GIF below demonstrates the various display properties when visualizing the data.</p> <p> </p>"},{"location":"stretch-tutorials/ros2/perception/#image-display","title":"Image Display","text":"<p>The <code>Image</code> display when toggled creates a new rendering window that visualizes a sensor_msgs/Image messaged, /camera/color/image_raw. This feature shows the image data from the camera; however, the image comes out sideways.</p> <p> </p>"},{"location":"stretch-tutorials/ros2/perception/#depthcloud-display","title":"DepthCloud Display","text":"<p>The <code>DepthCloud</code> display is visualized in the main RViz window. This display takes in the depth image and RGB image provided by RealSense to visualize and register a point cloud.</p> <p> </p>"},{"location":"stretch-tutorials/ros2/perception/#deep-perception","title":"Deep Perception","text":"<p>Hello Robot also has a ROS package that uses deep learning models for various detection demos. A link to the tutorials is provided: stretch_deep_perception.</p>"},{"location":"stretch-tutorials/ros2/remote_compute/","title":"Offloading Heavy Robot Compute to Remote Workstation","text":"<p>In this tutorial, we will explore the method for offloading computationally intensive processes, such as running computer vision models, to a remote workstation computer. This approach offers several advantages such as: - Saving robot's processing power. - Increasing robot's efficiency by offloading high-power consuming processes.  - Utilizing available GPU hardware on powerful workstations to run large deep learning models.</p> <p>We will delve into the process of offloading Stretch Deep Perception ROS2 nodes. These nodes are known for their demanding computational requirements and are frequently used in Stretch Demos. </p> <p>NOTE: All Stretch ROS2 packages are developed with Humble distro.</p>"},{"location":"stretch-tutorials/ros2/remote_compute/#1-setting-a-ros_domain_id","title":"1. Setting a ROS_DOMAIN_ID","text":"<p>ROS2 utilizes DDS as the default middleware for communication. DDS enables nodes within the same physical network to seamlessly discover one another and establish communication, provided they share the same ROS_DOMAIN_ID. This powerful mechanism ensures secure message passing between remote nodes as intended.</p> <p>By default, all ROS 2 nodes are configured with domain ID 0. To avoid conflicts, select a domain ID from the range of 0 to 101, and then set this chosen domain ID as the value for the <code>ROS_DOMAIN_ID</code> environment variable in both the Workstation and the Robot. <pre><code>export ROS_DOMAIN_ID=&lt;ID&gt;\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/remote_compute/#2-setup-the-workstation-to-work-with-stretch","title":"2. Setup the Workstation to work with Stretch","text":"<p>The workstation needs to be installed with the stretch related ros2 packages to have access to robot meshes for Visualization in Rviz, custom interfaces dependencies and essential perception packages.</p> <p>This section assumes the workstation already has an active ROS2 distro and colcon dependencies pre-installed. You can find ROS2 Installation step for Ubuntu here.</p>"},{"location":"stretch-tutorials/ros2/remote_compute/#setup-essential-stretch_ros2-packages","title":"Setup Essential stretch_ros2 Packages","text":"<p>Make sure the ROS2 distro is sourced. <pre><code>source /opt/ros/humble/setup.bash\n</code></pre></p> <p>Create workspace directory and clone stretch_ros2 packages along with it's dependency packages to <code>src</code> folder. <pre><code>mkdir -p ~/ament_ws/src\ncd ~/ament_ws/src/\ngit clone https://github.com/hello-robot/stretch_ros2\ngit clone https://github.com/hello-binit/ros2_numpy -b humble\ngit clone https://github.com/IntelRealSense/realsense-ros.git -b ros2-development\ngit clone https://github.com/Slamtec/sllidar_ros2.git -b main\ngit clone https://github.com/hello-binit/respeaker_ros2.git -b humble\ngit clone https://github.com/hello-binit/audio_common.git -b humble\n</code></pre></p> <p>Build and install all the packages present in source folder.  <pre><code>cd ~/ament_ws\nrosdep install --rosdistro=humble -iyr --skip-keys=\"librealsense2\" --from-paths src\ncolcon build --cmake-args -DCMAKE_BUILD_TYPE=Release\n</code></pre></p> <p>Make sure to source the workspace to discover the packages in it. <pre><code>source ~/ament_ws/install/setup.bash\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/remote_compute/#setup-robot-urdf-and-meshes","title":"Setup Robot URDF and Meshes","text":"<p>All the robots will have calibrated URDF with pre-configured mesh files in the stretch_description package directory that is specific to your actual robot. So we recommend you to copy the <code>stretch_description</code> directory that exists inside your robot and replace it with the one existing in the workstation. The Stretch Description directory exists in the path <code>~/ament_ws/src/stretch_ros2/stretch_description</code>.</p> <p>If you dont want to use the URDFs from the robot, you can manually generate the uncalibrated URDF w.r.t your robot configuration using the following commands: <pre><code>cd ~/ament_ws/src/stretch_ros2/stretch_description/urdf/\n\n#if Dex-Wrist Installed\ncp stretch_description_dex.xacro stretch_description.xacro\n\n#if Standard Gripper\ncp stretch_description_standard.xacro stretch_description.xacro\n\nros2 run stretch_calibration update_uncalibrated_urdf\ncp stretch_uncalibrated.urdf stretch.urdf\n</code></pre></p> <p>After setting up the stretch_description folder, re-build the workspace to update the package with latest changes. <pre><code>cd ~/ament_ws\ncolcon build\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/remote_compute/#download-stretch-deep-perception-models","title":"Download Stretch Deep Perception Models","text":"<p>stretch_deep_perception_models provides open deep learning models from third parties for use. We are cloning this directory to the home folder in the workstation. <pre><code>cd ~/ \ngit clone https://github.com/hello-robot/stretch_deep_perception_models\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/remote_compute/#3-start-core-nodes-on-the-robot-compute","title":"3. Start core Nodes on the Robot Compute","text":"<p>Start the core driver nodes for controlling the robot, streaming the Lidar and realsense depth camera/s data using the stretch_core package.</p> <pre><code># Terminal 1: Start the Stretch Driver Node\nros2 launch stretch_core stretch_driver.launch.py\n# Terminal 2: Start the realsense D435i stream.\nros2 launch stretch_core d435i_high_resolution.launch.py\n# Terminal 3: Start lidar.\nros2 launch stretch_core rplidar.launch.py\n</code></pre>"},{"location":"stretch-tutorials/ros2/remote_compute/#4-verify-remote-workstation-is-able-to-discover-stretch-nodes","title":"4. Verify Remote Workstation is able to discover Stretch Nodes","text":"<p>After launching the above core nodes, all the robot control interfaces and sensor data streams should be exposed to all the other nodes in the same physical network with common ROS_DOMAIN_ID set.</p> <p>From the remote workstation try the following test commands: <pre><code># Check if all robot topics are visible.\nros2 topic list\n\n# Check if able to receive a sensor data by printing from Joint States topic.\nros2 topic echo /joint_states\n\n# Check if able to send commands to robot by triggering stow_the_robot service\nros2 service call /stow_the_robot std_srvs/srv/Trigger\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/remote_compute/#5-offload-object-detection-node-to-remote-workstation","title":"5. Offload Object Detection Node to Remote Workstation","text":"<p>From the workstation, run the object detection node which runs a YoloV5 model. <pre><code>ros2 run stretch_deep_perception detect_objects\n</code></pre> The node would start printing out the detected objects. <pre><code>Fusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\nAdding AutoShape... \n[INFO] [1698379209.925727618] [DetectObjectsNode]: DetectObjectsNode started\ntv  detected\nkeyboard  detected\nchair  detected\nmouse  detected\nmouse  detected\ntv  detected\nkeyboard  detected\nchair  detected\nmouse  detected\nmouse  detected\nbowl  detected\ntv  detected\nkeyboard  detected\nchair  detected\nmouse  detected\nmouse  detected\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/remote_compute/#visualize-in-rviz","title":"Visualize in Rviz","text":"<p><pre><code>rviz2 -d ~/ament_ws/install/stretch_deep_perception/share/stretch_deep_perception/rviz/object_detection.rviz\n</code></pre> </p>"},{"location":"stretch-tutorials/ros2/remote_compute/#6-offload-face-detection-node-to-remote-workstation","title":"6. Offload Face  detection Node to Remote Workstation","text":"<p>From the workstation, run the face detection node. The face-detection node uses model parameters loaded from the stretch_deep_perception_models directory, whose path is pulled from HELLO_FLEET_PATH environment variable. In our case, we will set the HELLO_FLEET_PATH environment variable to point to the home folder where the stretch_deep_perception_models directory was cloned. <pre><code>export HELLO_FLEET_PATH=~/\nros2 run stretch_deep_perception detect_faces\n</code></pre> The node will load the face detection model network and start poblishing the detection. <pre><code>head_detection_model.getUnconnectedOutLayers() = [112]\nhead_detection_model output layer names = ['detection_out']\nhead_detection_model output layer names = ('detection_out',)\nhead_detection_model input layer = &lt;dnn_Layer 0x7f7d1e695cd0&gt;\n.\n.\n.\nlandmarks_model input layer name = align_fc3\nlandmarks_model out_layer = &lt;dnn_Layer 0x7f7d1e695d30&gt;\n[INFO] [1698383830.671699923] [DetectFacesNode]: DetectFacesNode started\n</code></pre></p>"},{"location":"stretch-tutorials/ros2/remote_compute/#visualize-in-rviz_1","title":"Visualize in Rviz","text":"<p><pre><code>rviz2 -d ~/ament_ws/install/stretch_deep_perception/share/stretch_deep_perception/rviz/face_detection.rviz\n</code></pre> </p>"},{"location":"stretch-tutorials/ros2/remote_compute/#troubleshooting-notes","title":"Troubleshooting Notes","text":"<ul> <li>Using a dedicated Wi-Fi router would increase the data transmission speeds significantly.</li> <li>Realtime PointCloud visualization in Rviz commonly lags because of subscribing to a large message data stream. We recommend turning off the point-cloud visualization in remote workstations when possible to decrease network overhead.</li> <li>If the nodes in the remote network are unable to discover robot running nodes, here are two debug steps:</li> <li>Check if you can ping between the robot and remote workstation computer.</li> <li>Use <code>ifconfig</code> command and compare the Network assigned IP addresses of both the robot and workstation. The first two parts of the IP address should normally match for both computers to discover each other in the network.</li> </ul>"},{"location":"stretch-tutorials/ros2/respeaker_mic_array/","title":"ReSpeaker Microphone Array","text":"<p>For this tutorial, we will get a high-level view of how to use Stretch's ReSpeaker Mic Array v2.0.  </p> <p> </p>"},{"location":"stretch-tutorials/ros2/respeaker_mic_array/#stretch-body-package","title":"Stretch Body Package","text":"<p>In this section we will use command line tools in the Stretch_Body package, a low-level Python API for Stretch's hardware, to directly interact with the ReSpeaker.</p> <p>Begin by typing the following command in a new terminal.</p> <pre><code>stretch_respeaker_test.py\n</code></pre> <p>The following will be displayed in your terminal:</p> <pre><code>For use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\n* waiting for audio...\n* recording 3 seconds\n* done\n* playing audio\n* done\n</code></pre> <p>The ReSpeaker Mico Array will wait until it hears audio loud enough to trigger its recording feature. Stretch will record audio for 3 seconds and then replay it through its speakers. This command line is a good method to see if the hardware is working correctly.</p> <p>To stop the python script, type <code>Ctrl</code> + <code>c</code> in the terminal.</p>"},{"location":"stretch-tutorials/ros2/respeaker_mic_array/#respeaker_ros-package","title":"ReSpeaker_ROS Package","text":"<p>A ROS package for the ReSpeaker is utilized for this section.</p> <p>Begin by running the <code>sample_respeaker.launch.py</code> file in a terminal.</p> <pre><code>ros2 launch respeaker_ros2 respeaker.launch.py\n</code></pre> <p>This will bring up the necessary nodes that will allow the ReSpeaker to implement a voice and sound interface with the robot.</p>"},{"location":"stretch-tutorials/ros2/respeaker_mic_array/#respeaker-topics","title":"ReSpeaker Topics","text":"<p>Below are executables you can run to see the ReSpeaker results.</p> <pre><code>ros2 topic echo /sound_direction\nros2 topic echo /sound_localization\nros2 topic echo /is_speeching\nros2 topic echo /audio\nros2 topic echo /speech_audio\nros2 topic echo /speech_to_text\n</code></pre> <p>There's also another topic called <code>/status_led</code>, with this topic you can change the color of the LEDs in the ReSpeaker, you need to publish the desired color in the terminal using <code>ros2 topic pub</code>. We will explore this topics in the next tutorial.</p> <p>You can also set various parameters via <code>dynamic_reconfigure</code> by running the following command in a new terminal.</p> <pre><code>ros2 run rqt_reconfigure rqt_reconfigure\n</code></pre>"},{"location":"stretch-tutorials/ros2/respeaker_topics/","title":"ReSpeaker Microphone Array Topics","text":"<p>In this tutorial we will see the topics more in detail and have an idea of what the ReSpeaker can do. If you just landed here, it might be a good idea to first review the previous tutorial which covered the basics of the ReSpeaker and the information about the package used</p>"},{"location":"stretch-tutorials/ros2/respeaker_topics/#respeaker-topics","title":"ReSpeaker Topics","text":"<p>Begin by running the <code>sample_respeaker.launch.py</code> file in a terminal.</p> <pre><code>ros2 launch respeaker_ros respeaker.launch.py\n</code></pre> <p>This will bring up the necessary nodes that will allow the ReSpeaker to implement a voice and sound interface with the robot. To see the topics that are available for us you can run the command <code>ros2 topic list -t</code> and search the topics that we are looking for. Don't worry these are the executables you can run to see the ReSpeaker results.</p> <p><pre><code>ros2 topic echo /sound_direction    # Result of Direction (in Radians) of Audio\nros2 topic echo /sound_localization # Result of Direction as Pose (Quaternion values)\nros2 topic echo /is_speeching       # Result of Voice Activity Detector\nros2 topic echo /audio              # Raw audio data\nros2 topic echo /speech_audio       # Raw audio data when there is speech\nros2 topic echo /speech_to_text     # Voice recognition\nros2 topic pub  /status_led ...     # Modify LED color\n</code></pre> Let's go one by one and see what we can expect of each topic, the first is the <code>sound_direction</code> topic, in the terminal execute the command that you learned earlier:</p> <p><pre><code>ros2 topic echo /sound_direction    # Result of Direction (in Radians) of Audio\n</code></pre> This will give you the direction of the sound detected by the ReSpeaker in radians</p> <p><pre><code>data: 21\n---\ndata: 138\n---\ndata: -114\n---\ndata: -65\n---\n</code></pre> The Direction of Arrival (DOA) for the ReSpeaker goes from -180 to 180, to know more about how is it in Stretch watch this DOA diagram:</p> <p> </p> <p>The next topic is the <code>sound_localization</code>, this is similar to the <code>sound_direction</code> topic but now the result it's as pose (Quaternion Values), try it out, execute the command:</p> <pre><code>ros2 topic echo /sound_localization # Result of Direction as Pose (Quaternion values)\n</code></pre> <p>With this you will have in your terminal this:</p> <pre><code>---\nheader:\n  stamp:\n    sec: 1695325677\n    nanosec: 882383094\n  frame_id: respeaker_base\npose:\n  position:\n    x: -0.0\n    y: 0.0\n    z: 0.0\n  orientation:\n    x: 0.0\n    y: 0.0\n    z: 0.43051109680829525\n    w: 0.9025852843498605\n---\n</code></pre> <p>The next one on the list is the <code>is_speeching</code> topic, with this you will have the result of Voice Activity Detector, let's try it out:</p> <pre><code>ros2 topic echo /is_speeching   # Result of Voice Activity Detector\n</code></pre> <p>The result will be a true or false in the data but it can detect sounds as true so be careful with this <pre><code>data: false\n---\ndata: true\n---\ndata: false\n---\n</code></pre></p> <p>The <code>audio</code> topic is goint to output all the Raw audio data, if you want to see what this does execute the command:</p> <p><pre><code>ros2 topic echo /audio  # Raw audio data\n</code></pre> You will expect a lot of data from this, you will see this output: <pre><code>---\ndata:\n- 229\n- 0\n- 135\n- 0\n- 225\n- 0\n- 149\n- 0\n- 94\n- 0\n- 15\n</code></pre></p> <p>For the <code>speech_audio</code> topic you can expect the same result as the <code>audio</code> topic but this time you are going to have the raw data when there is a speech, execute the next command and speak near the microphone array: <pre><code>ros2 topic echo /speech_audio   # Raw audio data when there is speech\n</code></pre> So if it's almost the same topic but now is going to ouput the data when you are talking then you guessed right, the result will look like the same as before. <pre><code>---\ndata:\n- 17\n- 254\n- 70\n- 254\n</code></pre></p> <p>Passing to the <code>speech_to_text</code> topic, with this you can say a small sentence and it will output what you said. In the terminal, execute the next command and speak near the microphone array again:</p> <pre><code>ros2 topic echo /speech_to_text   # Voice recognition\n</code></pre> <p>In this instance, \"hello robot\" was said. The following will be displayed in your terminal:</p> <p><pre><code>transcript:\n  - hello robot\nconfidence:\n- ######\n---\n</code></pre> And for the final topic, the <code>status_led</code>, with this you can setup custom LED patterns and effects. There are 3 ways to do it, the first one is using <code>rqt_publisher</code>, in the terminal input:</p> <p><pre><code>ros2 run rqt_publisher rqt_publisher\n</code></pre> With this the rqt_publisher window will open, there you need to add the topic manually, search for the <code>/status_led</code> topic, then click in the plus button, this is the add new publisher button and the topic will be added, then you can start moving the RGBA values between 0 to 1 and that's it, you can try it with the next example:</p> <p> </p> <p>You will see that there's a purple light coming out from the ReSpeaker, you can change the rate and color if you want.</p> <p>Now for the next way you can do it in the terminal, let's try again with the same values that we had so input this command in the terminal: <pre><code>ros2 topic pub /status_led std_msgs/msg/ColorRGBA \"r: 1.0\ng: 0.0\nb: 1.0\na: 1.0\"\n</code></pre> And you can see that we have the same result as earlier, good job!</p> <p>And for the final way it's going to be with a python code, here you can modify the lights just as we did before but now you have color patterns that you can create, let's try it so that you can see yourself, input in the terminal: <pre><code>cd ament_ws/src/stretch_tutorials/stretch_ros_tutorials/\npython3 led_color_change.py\n</code></pre> With this we can change the colors as well but the difference is that we are able to create our own patterns, in the ReSpeaker Documentation there are more options to customize and control de LEDs.</p>"},{"location":"stretch-tutorials/ros2/rviz_basics/","title":"RViz Basics","text":""},{"location":"stretch-tutorials/ros2/rviz_basics/#visualizing-with-rviz","title":"Visualizing with RViz","text":"<p>Note</p> <p>ROS 2 tutorials are still under active development. </p> <p>You can utilize RViz to visualize Stretch's sensor information. To begin, run the stretch driver launch file.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then run the following command to bring up a simple RViz configuration of the Stretch robot.</p> <pre><code>ros2 run rviz2 rviz2 -d `ros2 pkg prefix --share stretch_calibration`/rviz/stretch_simple_test.rviz\n</code></pre> <p>An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window.</p> <p></p> <p>If you want to visualize Stretch's tf transform tree, you need to add the display type to the RViz window. First, click on the Add button and include the TF  type to the display. You will then see all of the transform frames of the Stretch robot and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference.</p> <p></p> <p>There are further tutorials for RViz that can be found here.</p>"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/","title":"Teleoperating Stretch","text":""},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#teleoperating-stretch","title":"Teleoperating Stretch","text":"<p>Note</p> <p>Teleoperation support for Stretch in ROS 2 is under active development. Please reach out to us if you want to teleoperate Stretch in ROS 2.</p>"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#xbox-controller-teleoperating","title":"Xbox Controller Teleoperating","text":"<p>If you have not already had a look at the Xbox Controller Teleoperation section in the Quick Start guide, now might be a good time to try it.</p>"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#keyboard-teleoperating-full-body","title":"Keyboard Teleoperating: Full Body","text":"<p>For full-body teleoperation with the keyboard, you first need to run the <code>stretch_driver.launch.py</code> in a terminal.</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py\n</code></pre> <p>Then in a new terminal, type the following command</p> <pre><code>ros2 run stretch_core keyboard_teleop\n</code></pre> <p>Below are the keyboard commands that allow a user to control all of Stretch's joints.</p> <pre><code>---------- KEYBOARD TELEOP MENU -----------\n\n              i HEAD UP                    \n j HEAD LEFT            l HEAD RIGHT       \n              , HEAD DOWN                  \n\n\n 7 BASE ROTATE LEFT     9 BASE ROTATE RIGHT\n home                   page-up            \n\n\n              8 LIFT UP                    \n              up-arrow                     \n 4 BASE FORWARD         6 BASE BACK        \n left-arrow             right-arrow        \n              2 LIFT DOWN                  \n              down-arrow                   \n\n\n              w ARM OUT                    \n a WRIST FORWARD        d WRIST BACK       \n              x ARM IN                     \n\n\n              5 GRIPPER CLOSE              \n              0 GRIPPER OPEN               \n\n  step size:  b BIG, m MEDIUM, s SMALL     \n\n              q QUIT                       \n\n-------------------------------------------\n</code></pre> <p>To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#keyboard-teleoperating-mobile-base","title":"Keyboard Teleoperating: Mobile Base","text":"<p>Begin by running the following command in your terminal:</p> <pre><code>ros2 launch stretch_core stretch_driver.launch.py mode:=navigation\n</code></pre> <p>To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to nagivation for the robot to receive Twist messages. In comparison with ROS1 that we needed to use the rosservice command, we can do it in the same driver launch as you can see in the command you just input! Now in other terminal run the teleop_twist_keyboard node with the argument remapping the cmd_vel topic name to stretch/cmd_vel.</p> <pre><code>ros2 run teleop_twist_keyboard teleop_twist_keyboard cmd_vel:=stretch/cmd_vel\n</code></pre> <p>Below are the keyboard commands that allow a user to move Stretch's base.  </p> <pre><code>Reading from the keyboard  and Publishing to Twist!\n---------------------------\nMoving around:\n   u    i    o\n   j    k    l\n   m    ,    .\n\nFor Holonomic mode (strafing), hold down the shift key:\n---------------------------\n   U    I    O\n   J    K    L\n   M    &lt;    &gt;\n\nt : up (+z)\nb : down (-z)\n\nanything else : stop\n\nq/z : increase/decrease max speeds by 10%\nw/x : increase/decrease only linear speed by 10%\ne/c : increase/decrease only angular speed by 10%\n\nCTRL-C to quit\n\ncurrently:  speed 0.5   turn 1.0\n</code></pre> <p>To stop the node from sending twist messages, type Ctrl + c.</p>"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#create-a-node-for-mobile-base-teleoperating","title":"Create a node for Mobile Base Teleoperating","text":"<p>To move Stretch's mobile base using a python script, please look at example 1 for reference.</p>"},{"location":"stretch-tutorials/stretch_body/","title":"Overview","text":""},{"location":"stretch-tutorials/stretch_body/#tutorial-track-stretch-body","title":"Tutorial Track: Stretch Body","text":"<p>Stretch Body is a set of Python packages that allow a developer to directly program the hardware of the Stretch robots. The Stretch Body interface is intended for users who are looking for an alternative to ROS.</p> <p>Stretch Body currently supports both Python2 and Python3. These tutorials assume a general familiarity with Python as well as basic robot motion control.</p>"},{"location":"stretch-tutorials/stretch_body/#basics","title":"Basics","text":"Tutorial Description 1 Introduction Introduction to the Stretch Body package 2 Command line Tools Using the Stretch Body command line tools 3 Stretch Body API Walk through of the Stretch Body API 4 Robot Motion How to command robot motion 4 Robot Sensors How to read robot sensors"},{"location":"stretch-tutorials/stretch_body/#advanced","title":"Advanced","text":"Tutorial Description 1 Dynamixel Servos How to configure and work with the Dynamixel servos 2 Parameter Management How to work with parameter system 3 Splined Trajectories How to generated coordinated, smooth, and full-body motion 4 Collision Avoidance How to work with the collision avoidance system 5 Contact Models How to work with the contact detection system 6 Changing Tools How to configure Stretch to work with a different tool 7 Custom Wrist DOF How to integrate custom DOF onto the wrist 8 Safety Features Learn about Stretch Body features that keep the robot safe"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/","title":"Tutorial: Collision Avoidance","text":"<p>In this tutorial, we will discuss the simple collision avoidance system that runs as a part of Stretch Body.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#overview","title":"Overview","text":"<p>Stretch Body includes a system to prevent inadvertent self-collisions.  It will dynamically limit the range of motion of each joint to prevent self-collisions. </p> <p>Warning</p> <p>Self collisions are still possible while using the collision-avoidance system. The factory default collision models are coarse and not necessarily complete.</p> <p>This system is turned off by default starting with Stretch 2. It may be turned off by default on many RE1 systems. First check if the collision detection system is turned on:</p> <pre><code>stretch_params.py | grep use_collision_manager\n</code></pre> <p>Output: <pre><code>stretch_body.robot_params.nominal_params       param.robot.use_collision_manager         1\n</code></pre></p> <p>If it is turned off you can enable it by adding the following to your stretch_user_yaml.py:</p> <pre><code>robot:\n  use_collision_manager: 1\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#common-self-collisions","title":"Common Self Collisions","text":"<p>Fortunately, the simple kinematics of Stretch make self-collisions fairly uncommon and simple to predict. The primary places where self-collisions may occur are</p> <ul> <li>The lift lowering the wrist or tool into the base</li> <li>The arm retracting the wrist or tool into the base</li> <li>The head_pan at <code>pos==0</code> and head_tilt at <code>pos=-90 deg</code> and the lift raising the arm into the camera (minor collision)</li> <li>The Dex Wrist (if installed) colliding with itself</li> <li>The Dex Wrist (if installed) colliding with the base</li> </ul>"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#joint-limits","title":"Joint Limits","text":"<p>The collision avoidance system works by dynamically modifying the acceptable range of motion for each joint. By default, a joint's range is set to the physical hard stop limits. For example, the lift has a mechanical throw of 1.1m:</p> <pre><code>stretch_params.py | grep range | grep lift\n</code></pre> <p>Output: <pre><code>stretch_body.robot_params.factory_params       param.lift.range_m      [0.0, 1.1]                \n</code></pre></p> <p>A reduced range of motion can be set at run-time by setting the Soft Motion Limit. For example, to limit the lift range of motion to 0.3 meters off the base:</p> <pre><code>import stretch_body.robot as robot\nr=robot.Robot()\nr.startup()\nr.lift.set_soft_motion_limit_min(0.3)\n</code></pre> <p>We see in the API, the value of <code>None</code> is used to designate no soft limit.</p> <p>It is possible that when setting the Soft Motion Limit the joint's current position is outside of the specified range. In this case, the joint will move to the nearest soft limit to comply with the limits. This can be demonstrated by:</p> <pre><code>import stretch_body.robot as robot\nimport time\n\nr=robot.Robot()\nr.startup()\n\n#Move to 0.2\nr.lift.move_to(0.2)\nr.push_command()\ntime.sleep(5.0) \n\n#Will move to 0.3\nr.lift.set_soft_motion_limit_min(0.3)\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#collision-models","title":"Collision Models","text":"<p>The RobotCollision class manages a set of RobotCollisionModels. Each RobotCollisionModel computes the soft limits for a subset of joints based on a simple geometric model. This geometric model captures the enumerated set of potential collisions listed above.</p> <p>We can see which collision models will execute when <code>use_collision_manager</code> is set to 1:</p> <pre><code>stretch_params.py | grep collision | grep enabled\n</code></pre> <p>Output: <pre><code>stretch_body.robot_params.nominal_params    param.collision_arm_camera.enabled           1                             \nstretch_body.robot_params.nominal_params    param.collision_stretch_gripper.enabled      1  \n</code></pre></p> <p>We see two models. One that protects the camera from the arm, and one that protects the base from the gripper. Each model is registered with the RobotCollision instance as a loadable plug-in. The Robot class calls the <code>RobotCollision.step</code> method periodically at approximately 10hz. </p> <p><code>RobotCollision.step</code> computes the 'AND' of the limits specified across each Collision Model such that the most restrictive joint limits are set for each joint using the <code>set_soft_motion_limit_min</code> and <code>set_soft_motion_limt_max</code> methods. </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#default-collision-models","title":"Default Collision Models","text":"<p>The default collision models for Stretch Body are found in robot_collision_models.py. As of this writing, the provided models are:</p> <ul> <li>CollisionArmCamera: Avoid collision of the head camera with the arm</li> <li>CollisionStretchGripper: Avoid collision of the wrist-yaw and gripper with the base and ground</li> </ul> <p>Warning</p> <p>The provided collision models are coarse and are provided to avoid common potentially harmful collisions only. Using these models it is still possible to collide the robot with itself in some cases.</p> <p>Info</p> <p>Additional collision models are provided for the DexWrist</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#working-with-models","title":"Working with Models","text":"<p>The collision models to be used by Stretch Body are defined with the <code>robot_collision</code> parameter. For example, we see in <code>robot_params.py</code>  that the CollisionArmCamera is loaded by default:</p> <pre><code>\"robot_collision\": {'models': ['collision_arm_camera']},\n</code></pre> <p>We also see that model <code>collision_arm_camera</code> is defined as:</p> <pre><code>  \"collision_arm_camera\": {\n        'enabled': 1,\n        'py_class_name': 'CollisionArmCamera',\n        'py_module_name': 'stretch_body.robot_collision_models'\n    }\n</code></pre> <p>This instructs RobotCollision to construct a model of type <code>CollisionArmCamera</code> and enable it by default. One can disable this model by default by specifying the following <code>stretch_re1_user_params.yaml</code>:</p> <pre><code>collision_arm_camera:\n  enabled: 0\n</code></pre> <p>The  entire collision avoidance system can be disabled in <code>stretch_re1_user_params.yaml</code> by:</p> <pre><code>robot:\n  use_collision_manager: 0\n</code></pre> <p>A specific collision model can be enabled or disabled during runtime by:</p> <pre><code>import stretch_body.robot\nr=stretch_body.robot.Robot()\nr.startup() \n... #Do some work\nr.collision.disable_model('collsion_arm_camera')\n... #Do some work\nr.collision.enable_model('collsion_arm_camera')\n</code></pre> <p>Finally, if we want to also use the CollisionStretchGripper model, we can add to <code>stretch_re1_user_params.py</code>:</p> <pre><code>robot_collision:\n  models:\n  - collision_arm_camera\n  - collision_stretch_gripper\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#creating-custom-collision-models","title":"Creating Custom Collision Models","text":"<p>The <code>step</code> method of a RobotCollisionModel returns the desired joint limits given that model. For example, the base class is simply:</p> <pre><code>    class RobotCollisionModel(Device):\n        def step(self, status):\n            return {'head_pan': [None, None],'head_tilt': [None, None], \n                    'lift': [None, None],'arm': [None, None],'wrist_yaw': [None, None]}\n</code></pre> <p>where the value of <code>None</code> specifies that no limit is specified and the full range of motion for the joint is acceptable.  </p> <p>We could define a new collision model that simply limits the lift range of motion to 1 meter by:</p> <pre><code>    class MyCollisionModel(Device):\n        def step(self, status):\n            return {'head_pan': [None, None],'head_tilt': [None, None], \n                    'lift': [None, 1.0],'arm': [None, None],'wrist_yaw': [None, None]}\n</code></pre> <p>It is straightforward to create a custom collision model. As an example, we will create a model that avoids collision of the arm with a tabletop by</p> <ul> <li>Preventing the lift from descending below the table top when the arm is extended </li> <li>Allowing the lift to descend below the tabletop so long as the arm retracted</li> </ul> <p>This assumes the arm is initially above the tabletop. To start, in a file <code>collision_arm_table.py</code> we add:</p> <p><pre><code>from stretch_body.robot_collision import *\nfrom stretch_body.hello_utils import *\n\nclass CollisionArmTable(RobotCollisionModel):\n    def __init__(self, collision_manager):\n        RobotCollisionModel.__init__(self, collision_manager, 'collision_arm_table')\n\n    def step(self, status):\n        limits = {'lift': [None, None],'arm': [None, None]}\n        table_height = 0.5 #m\n        arm_safe_retract = 0.1 #m\n        safety_margin=.05#m\n\n        x_arm = status['arm']['pos']\n        x_lift = status['lift']['pos']\n\n        #Force arm to stay retracted if below table\n        if x_lift&lt;table_height:\n            limits['arm'] = [None, arm_safe_retract-safety_margin]\n        else:\n            limits['arm'] = [None, None]\n\n        #Force lift to stay above table unless arm is retracted\n        if x_arm&lt;arm_safe_retract:\n            limits['lift'] =[None,None]\n        else:\n            limits['lift']=[table_height+safety_margin,None]\n        return limits\n</code></pre> In this example, we include the <code>safety_margin</code> as a way to introduce some hysteresis around state changes to avoid toggling between the soft limits.</p> <p>The following command should be run to add the working directory to the PYTHONPATH env. This can also be added to our <code>.bashrc</code> to permanently edit the path:</p> <pre><code>export PYTHONPATH=$PYTHONPATH:/&lt;path_to_modules&gt;\n</code></pre> <p>Next, we configure RobotCollision to use our CollisionArmTable model in <code>stretch_re1_user_yaml</code>:</p> <pre><code>robot_collision:\n  models:\n  - collision_arm_table\n\ncollision_arm_table:\n  enabled: 1\n  py_class_name: 'CollisionArmTable'\n  py_module_name: 'collision_arm_table'\n</code></pre> <p>Finally, test out the model by driving the arm and lift around using the Xbox teleoperation tool:</p> <pre><code>stretch_xbox_controller_teleop.py\n</code></pre>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_command_line_tools/","title":"Tutorial: Stretch Body Command Line Tools","text":"<p>Stretch Body includes the package hello-robot-stretch-body-tools - a suite of command line tools that allow direct interaction with hardware subsystems. </p> <p>These tools are useful when developing and debugging applications. They also serve as code examples when developing applications for Stretch_Body.</p> <p>These tools can be found by tab completion of 'stretch_' from a terminal.</p> <pre><code>stretch_\n\nstretch_about.py\nstretch_about_text.py\nstretch_arm_home.py\nstretch_arm_jog.py\nstretch_audio_test.py\nstretch_base_jog.py\nstretch_gripper_home.py\nstretch_gripper_jog.py\nstretch_hardware_echo.py\nstretch_head_jog.py\nstretch_lift_home.py\nstretch_lift_jog.py\nstretch_params.py\nstretch_pimu_jog.py\nstretch_pimu_scope.py\nstretch_realsense_visualizer.py\nstretch_respeaker_test.py\nstretch_robot_battery_check.py\nstretch_robot_dynamixel_reboot.py\nstretch_robot_home.py\nstretch_robot_jog.py\nstretch_robot_keyboard_teleop.py\nstretch_robot_monitor.py\nstretch_robot_stow.py\nstretch_robot_system_check.py\nstretch_robot_urdf_visualizer.py\nstretch_rp_lidar_jog.py\nstretch_trajectory_jog.py\nstretch_version.sh\nstretch_wacc_jog.py\nstretch_wacc_scope.py\nstretch_wrist_yaw_home.py\nstretch_wrist_yaw_jog.py\nstretch_xbox_controller_teleop.py\n</code></pre> <p>All tools accept the '--help' flag as a command line argument to describe its function. For example:</p> <pre><code>stretch_pimu_scope.py --help\n</code></pre> <p>Output: <pre><code>For use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\nusage: stretch_pimu_scope.py [-h] [--cliff] [--at_cliff] [--voltage] [--current] [--temp] [--ax] [--ay] [--az] [--mx] [--my] [--mz] [--gx] [--gy] [--gz] [--roll] [--pitch] [--heading] [--bump]\n\nVisualize Pimu (Power+IMU) board data with an oscilloscope\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --cliff     Scope base cliff sensors\n  --at_cliff  Scope base at_cliff signal\n  --voltage   Scope bus voltage (V)\n  --current   Scope bus current (A)\n  --temp      Scope base internal temperature (C)\n  --ax        Scope base accelerometer AX\n  --ay        Scope base accelerometer AY\n  --az        Scope base accelerometer AZ\n  --mx        Scope base magnetometer MX\n  --my        Scope base magnetometer MY\n  --mz        Scope base magnetometer MZ\n  --gx        Scope base gyro GX\n  --gy        Scope base gyro GY\n  --gz        Scope base gyro GZ\n  --roll      Scope base imu Roll\n  --pitch     Scope base imu Pitch\n  --heading   Scope base imu Heading\n  --bump      Scope base imu bump level\n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_command_line_tools/#commonly-used-tools","title":"Commonly Used Tools","text":"<p>These are the tools a typical user is expected to interact with regularly and would benefit from becoming familiar with.</p> Tool Utility stretch_robot_home.py Commonly run after booting up the robot in-order to calibrate the joints stretch_robot_system_check.py Scans for all hardware devices and ensures they are present on the bus and reporting valid values. Useful to verify that the robot is in good working order prior to commanding motion. It will report all success in green, failures in red. stretch_robot_stow.py Useful to return the robot arm and tool to a safe position within the base footprint. It can also be useful if a program fails to exit cleanly and the robot joints are not backdriveable. It will restore them to their 'Safety' state. stretch_robot_battery_check.py Quick way to check the battery voltage / current consumption stretch_xbox_controller_teleop.py Useful to quickly test if a robot can achieve a task by manually teleoperating the robot stretch_robot_dynamixel_reboot.py Resets all Dynamixels in the robot, which might be necessary if a servo overheats during use and enters an error state. <p>Take a minute to explore each of these tools from the console.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/","title":"Tutorial: Contact Models","text":"<p>This tutorial introduces the Stretch Body contact detection system and explains how to configure it for your application.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#what-is-guarded-contact","title":"What is Guarded Contact?","text":"<p>Guarded contact is our term for the Stretch contact sensitive behaviors. The guarded contact behavior is simply:</p> <ol> <li>Detect when the actuator effort exceeds a user-specified threshold during joint motion.</li> <li>If the threshold is exceeded:<ol> <li>Enable the default safety controller for the joint</li> <li>Remain in safety mode until a subsequent joint command is received</li> </ol> </li> </ol> <p>Practically this enables the arm, for example, to move out yet stop upon collision. Let's test this out with the following script:</p> <pre><code>#!/usr/bin/env python\n\nimport time\nimport stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move to full retraction\nrobot.arm.move_to(0.0)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint()\n\ninput('Arm will extend and respond to contact. Manually attempt to stop it. Hit enter when ready')\nrobot.arm.move_to(0.3)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint(timeout=5.0)\n\n#Now turn off guarded contacts\ninput('Arm will retract but will not respond to contact. Manually attempt to stop it. Hit enter when ready')\nrobot.arm.motor.disable_guarded_mode()\nrobot.arm.move_to(0.0)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint(timeout=5.0)\n\nrobot.stop()\n</code></pre> <p>You should see that the arm stops on contact when it extends, however, it doesn't stop on contact when it then retracts. This is the guarded contact behavior in action.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#specifying-guarded-contacts","title":"Specifying Guarded Contacts","text":"<p>The four stepper joints (base, arm, and lift) all support guarded contact settings when executing motion. This is evident in their <code>move_to</code> and <code>move_by</code> methods. For example, we see in the Arm's base class of PrismaticJoint:</p> <pre><code>  def move_by(self,x_m,v_m=None, a_m=None, stiffness=None, contact_thresh_pos_N=None,contact_thresh_neg_N=None, req_calibration=True,contact_thresh_pos=None,contact_thresh_neg=None)\n</code></pre> <p>In this method, you can optionally specify a contact threshold in the positive and negative direction with <code>contact_thresh_pos</code> and <code>contact_thresh_neg</code> respectively. </p> <p>Note</p> <p>These optional parameters will default to <code>None</code>, in which case the motion will adopt the default settings as defined by the robot's parameters.</p> <p>Warning</p> <p>The parameters <code>contact_thresh_pos_N</code> and <code>contact_thresh_neg_N</code> are deprecated and no longer supported.</p> <p><pre><code>stretch_params.py | grep arm | grep contact\n</code></pre> <pre><code>...                                              \nstretch_configuration_params.yaml            param.arm.contact_models.effort_pct.contact_thresh_default    [-45.0, 45.0]    \n...\n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#contact-models","title":"Contact Models","text":"<p>A contact model is simply a function that, given a user-specified contact threshold, computes the motor current at which the motor controller will trigger a guarded contact. The following contact models are currently implemented:</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#the-effort-pct-contact-model","title":"The Effort-Pct Contact Model","text":"<p>Effort-Pct is the default contact model for Stretch 2. It simply scales the maximum range of motor currents into the range of [-100,100]. Thus, if you desire to have the robot arm extend but stop at 50% of its maximum current, you would write:</p> <pre><code>robot.arm.move_by(0.1,contact_thresh_pos=50.0)\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#adjusting-contact-behaviors","title":"Adjusting Contact Behaviors","text":"<p>The default factory settings for contact thresholds are tuned to allow Stretch to move throughout its workspace without triggering false-positive guarded contact events. These settings are the worst-case tuning as they account for the internal disturbances of the Stretch drive-train across its entire workspace. </p> <p>It is possible to obtain greater contact sensitivity in carefully selected portions of the arm and lift workspace. Users who wish to programmatically adjust contact behaviors can create a simple test script and experiment with different values. For example:</p> <pre><code>#!/usr/bin/env python\n\nimport time\nimport stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\ncpos = 30.0\ncneg = -30.0\n\nrobot.arm.move_to(0.0, contact_thresh_pos=cpos, contact_thresh_neg=cneg)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint()\n\nrobot.arm.move_to(0.5,contact_thresh_pos=cpos, contact_thresh_neg=cneg)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint(timeout=5.0)\n\nrobot.stop()\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#guarded-contact-with-the-base","title":"Guarded Contact with the Base","text":"<p>Guarded contacts are enabled by default for the arm and lift as they typically require safe and contact-sensitive motion. They are turned off on the base by default as varying surface terrain can produce undesired false-positive events.</p> <p>That being said, guarded contacts can be enabled on the base. They may be useful as a simple bump detector such that the base will stop when it runs into a wall. </p> <pre><code>#!/usr/bin/env python\n\nimport time\nimport stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.base.left_wheel.enable_guarded_mode()\nrobot.base.right_wheel.enable_guarded_mode()\n\nrobot.base.move_by(0.025)\nrobot.push_command()\nrobot.base.wait_until_at_setpoint()\n\nrobot.base.move_by(-0.025)\nrobot.push_command()\nrobot.base.wait_until_at_setpoint()\n\nrobot.stop()\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#advanced-calibrating-contact-thresholds","title":"Advanced: Calibrating Contact Thresholds","text":"<p>The Stretch Factory package provides a tool to allow advanced users to recalibrate the default guarded contact thresholds. This tool can be useful if you've added additional payload to the arm and are experiencing false-positive guarded contact detections.</p> <p>The tool sweeps the joint through its range of motion for <code>n-cycle</code> iterations. It computes the maximum contact forces in both directions, adds padding, <code>contact_thresh_calibration_margin</code>, to this value, and stores it to the robot's configuration YAML.</p> <p><pre><code>REx_calibrate_guarded_contact.py -h\n</code></pre> <pre><code>For use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\nusage: REx_calibrate_guarded_contact.py [-h] (--lift | --arm) [--ncycle NCYCLE]\n\nCalibrate the default guarded contacts for a joint.\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --lift           Calibrate the lift joint\n  --arm            Calibrate the arm joint  \n  --ncycle NCYCLE  Number of sweeps to run [4]  \n</code></pre></p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_custom_wrist_dof/","title":"Tutorial: Custom Wrist DOF","text":"<p>In this tutorial, we explore how to add additional degrees of freedom to the Stretch wrist. </p> <p>Stretch exposes a Dynamixel X-Series TTL control bus at the end of its arm. It uses the Dynamixel XL430-W250 for the Wrist Yaw and the Stretch Gripper that comes standard with the robot. </p> <p>See the Hardware User Guide to learn how to mechanically attach additional DOFs to the robot.</p> <p>Note</p> <p>Stretch is compatible with any Dynamixel X Series servo that utilizes the TTL level Multidrop Bus.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_custom_wrist_dof/#adding-a-custom-dof","title":"Adding a Custom DOF","text":"<p>Adding one or more custom Dynamixel X Series servos to Stretch wrist involves:</p> <ul> <li>Creating a new class that derives from DynamixelHelloXL430</li> <li>Adding YAML parameters to <code>stretch_user_params.yaml</code> that configure the servo as desired</li> <li>Adding YAML parameters to <code>stretch_user_params.yaml</code> that tell Stretch to include this class in its EndOfArm list of servos</li> </ul> <p>Let's create a new DOF called MyWristPitch in a file named my_wrist_pitch.py. Place the file somewhere on the $PYTHONPATH.</p> <pre><code>from stretch_body.dynamixel_hello_XL430 import DynamixelHelloXL430\nfrom stretch_body.hello_utils import *\n\nclass MyWristPitch(DynamixelHelloXL430):\n    def __init__(self, chain=None):\n        DynamixelHelloXL430.__init__(self, 'my_wrist_pitch', chain)\n        self.poses = {'tool_up': deg_to_rad(45),\n                      'tool_down': deg_to_rad(-45)}\n\n    def pose(self,p,v_r=None,a_r=None):\n        self.move_to(self.poses[p],v_r,a_r)\n</code></pre> <p>Now let's add the tools' parameters to your <code>stretch_user_params.yaml</code> to configure this servo. You may want to adapt these parameters to your application but the nominal values found here usually work well. Below we highlight some of the more useful parameters.</p> <pre><code>my_wrist_pitch:\n  id: 1                         #ID on the Dynamixel bus\n  range_t:                      #Range of servo, in ticks\n  - 0\n  - 4096\n  req_calibration: 0            #Does the joint require homing after startup\n  use_multiturn: 0              #Single turn or multi-turn mode of rotation\n  zero_t: 2048                  #Position in ticks that corresponds to zero radians\n</code></pre> <p>For this example, we are assuming a single-turn joint that doesn't require hard stop-based homing. We also assume the servo has the Robotis default ID of 1.</p> <p>At this point, your MyWristPitch class is ready to use. Plug the servo into the cable leaving the Stretch WristYaw joint. Experiment with the API from iPython</p> <pre><code>In [1]: import my_wrist_pitch\n\nIn [2]: w=wrist_pitch.WristPitch()\n\nIn [3]: w.startup()\n\nIn [4]: w.move_by(0.1)\n\nIn [5]: w.pose('tool_up')\n\nIn [6]: w.pose('tool_down')\n</code></pre> <p>Finally, you'll want to make your WristPitch available from <code>stretch_body.robot</code>. Add the following YAML to your <code>stretch_user_params.yaml</code></p> <pre><code>end_of_arm:\n  devices:\n    wrist_pitch:\n      py_class_name: WristPitch\n      py_module_name: wrist_pitch\n</code></pre> <p>This tells <code>stretch_body.robot</code> to manage a <code>wrist_pitch.WristPitch</code> instance and add it to the EndOfArm list of tools. Try it from iPython:</p> <pre><code>In [1]: import stretch_body.robot as robot\n\nIn [2]: r=robot.Robot()\n\nIn [3]: r.startup()\n\nIn [4]: r.end_of_arm.move_by('wrist_pitch',0.1)\n</code></pre>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/","title":"Tutorial: Working with Dynamixel Servos","text":"<p>In this tutorial, we will go into the details with Dynamixel servos and Stretch.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#overview","title":"Overview","text":"<p>Stretch comes with two Dynamixel buses - one for the head and one for the end-of-arm:</p> <pre><code>ls  /dev/hello-dynamixel-*\n</code></pre> <p>Output: <pre><code>/dev/hello-dynamixel-head  /dev/hello-dynamixel-wrist\n</code></pre></p> <p>Typically, users will interact with these devices through either the Head or the EndOfArm interfaces.  This tutorial is for users looking to work directly with the servos from the provided servo tools or through Stretch Body's low-level Dynamixel API. </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#servo-tools","title":"Servo Tools","text":"<p>Note</p> <p>The servo tools here are part of the Stretch Factory package which is installed as a part of Stretch Body.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#jogging-the-servos","title":"Jogging the Servos","text":"<p>You can directly command each servo using the command line tool <code>REx_dynamixel_servo_jog.py</code>. This can be useful for debugging new servos added to the end-of-arm tool during system bring-up. For example, to command the head pan servo:</p> <pre><code>REx_dynamixel_jog.py /dev/hello-dynamixel-head 11\n</code></pre> <p>Output: <pre><code>[Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1080. Baud 115200\n------ MENU -------\nm: menu\na: increment position 50 tick\nb: decrement position 50 tick\nA: increment position 500 ticks\nB: decrement position 500 ticks\nv: set profile velocity\nu: set profile acceleration\nz: zero position\nh: show homing offset\no: zero homing offset\nq: got to position\np: ping\nr: reboot\nw: set max pwm\nt: set max temp\ni: set id\nd: disable torque\ne: enable torque\nx: put in multi-turn mode\ny: put in position mode\nw: put in pwm mode\nf: put in vel mode\n-------------------\n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#rebooting-the-servos","title":"Rebooting the Servos","text":"<p>Under high-load conditions, the servos may enter an error state to protect themselves from thermal overload. In this case, the red LED on the servo will flash (if visible). In addition, the servo will be unresponsive to motion commands. In this case, allow the overheating servo to cool down and reboot the servos using the <code>stretch_robot_dynamixel_reboot.py</code> tool: </p> <pre><code>stretch_robot_dynamixel_reboot.py\n</code></pre> <p>Output: <pre><code>For use with S T R E T C H (TM) RESEARCH EDITION from Hello Robot Inc.\n\nRebooting: head_pan\n[Dynamixel ID:011] Reboot Succeeded.\nRebooting: head_tilt\n[Dynamixel ID:012] Reboot Succeeded.\nRebooting: stretch_gripper\n[Dynamixel ID:014] Reboot Succeeded.\nRebooting: wrist_yaw\n[Dynamixel ID:013] Reboot Succeeded.\n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#identify-servos-on-the-bus","title":"Identify Servos on the Bus","text":"<p>If it is unclear which servos are on the bus, and at what baud rate, you can use the <code>REx_dynamixel_id_scan.py</code> tool. Here we see that the two head servos are at ID <code>11</code> and <code>12</code> at baud <code>57600</code>.</p> <pre><code>REx_dynamixel_id_scan.py /dev/hello-dynamixel-head\n</code></pre> <p>Output: <pre><code>Scanning bus /dev/hello-dynamixel-head at baud rate 57600\n----------------------------------------------------------\nScanning bus /dev/hello-dynamixel-head\nChecking ID 0\nChecking ID 1\nChecking ID 2\nChecking ID 3\nChecking ID 4\nChecking ID 5\nChecking ID 6\nChecking ID 7\nChecking ID 8\nChecking ID 9\nChecking ID 10\nChecking ID 11\n[Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1080. Baud 115200\nChecking ID 12\n[Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060. Baud 115200\nChecking ID 13\nChecking ID 14\nChecking ID 15\nChecking ID 16\nChecking ID 17\nChecking ID 18\nChecking ID 19\nChecking ID 20\nChecking ID 21\nChecking ID 22\nChecking ID 23\nChecking ID 24\n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#setting-the-servo-baud-rate","title":"Setting the Servo Baud Rate","text":"<p>Stretch ships with its Dynamixel servos configured to <code>baudrate=115200</code>.  When adding your servos to the end-of-arm tool, you may want to set the servo baud using the <code>REx_dynamixel_set_baud.py</code> tool. For example:</p> <pre><code>REx_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200\n</code></pre> <p>Output: <pre><code>---------------------\n\nSuccess at changing baud. Current baud is 115200 for servo 13 on bus /dev/hello-dynamixel-wrist\n</code></pre></p> <p>Note</p> <p>Earlier units of Stretch RE1 may be running Dynamixel servos at baud 57600.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#setting-the-servo-id","title":"Setting the Servo ID","text":"<p>Dynamixel servos come with <code>ID=1</code> from the factory. When adding your servos to the end-of-arm tool, you may want to set the servo ID using the <code>REx_dynamixel_id_change.py</code> tool. For example:</p> <pre><code>REx_dynamixel_id_change.py /dev/hello-dynamixel-wrist 1 13\n</code></pre> <p>Output: <pre><code>[Dynamixel ID:001] ping Succeeded. Dynamixel model number : 1080. Baud 115200\nReady to change ID 1 to 13. Hit enter to continue:\n\n[Dynamixel ID:013] ping Succeeded. Dynamixel model number : 1080. Baud 115200\nSuccess at setting ID to 13\n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#stretch-body-dynamixel-api","title":"Stretch Body Dynamixel API","text":"<p>Stretch Body's low-level Dynamixel API includes a hierarchy of three classes</p> Class DynamixelXChain DynamixelHelloXL430 DynamixelXL430 <p>Note</p> <p>The naming of XL430 is for legacy reasons. These classes will work with all X Series servos. </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#dynamixelxchain","title":"DynamixelXChain","text":"<p>DynamixelXChain manages a set of daisy-chained servos on a single bus (for example the head_pan and head_tilt servos). It allows for greater communication bandwidth by doing a group read/write over USB. </p> <p>The EndOfArm class derives from DynamixelXChain to provide an extensible interface that supports a user in integrating additional degrees of freedom to the robot. The tutorial Adding Custom Wrist DoF explains how to do this.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#dynamixelhelloxl430","title":"DynamixelHelloXL430","text":"<p>DynamixelHelloXL430 provides an interface to servo motion that is consistent with the Stretch Body lift, arm, and base joints. It also manages the servo parameters and calibration. Let's explore this interface further. From iPython, let's look at the status message for DynamixelHelloXL430</p> <pre><code>import stretch_body.dynamixel_hello_XL430 \n\nm = stretch_body.dynamixel_hello_XL430.DynamixelHelloXL430('head_pan')\nm.startup()\n\nm.pretty_print()\n</code></pre> <p>Output: <pre><code>----- HelloXL430 ------ \nName head_pan\nPosition (rad) -0.0\nPosition (deg) -0.0\nPosition (ticks) 1250\nVelocity (rad/s) -0.0\nVelocity (ticks/s) 0\nEffort (%) 0.0\nEffort (ticks) 0\nTemp 34.0\nComm Errors 0\nHardware Error 0\nHardware Error: Input Voltage Error:  False\nHardware Error: Overheating Error:  False\nHardware Error: Motor Encoder Error:  False\nHardware Error: Electrical Shock Error:  False\nHardware Error: Overload Error:  False\nWatchdog Errors:  0\nTimestamp PC 1661552966.7202659\nRange (ticks) [0, 3827]\nRange (rad) [ 1.9174759848570513  ,  -3.953068490381297 ]\nStalled True\nStall Overload False\nIs Calibrated 0\n</code></pre></p> <p>We see that it reports the position in both radians (with respect to the joint frame) and ticks (with respect to the servo encoder). DynamixelHelloXL430 handles the calibration between the two using its method <code>ticks_to_world_rad</code> through the following params:</p> <pre><code>stretch_params.py | grep head_pan | grep '_t '\n</code></pre> <p>Output: <pre><code>stretch_configuration_params.yaml         param.head_pan.range_t           [0, 3827]                     \nstretch_configuration_params.yaml         param.head_pan.zero_t            1250 \n</code></pre></p> <p>In addition to <code>move_to</code> and <code>move_by</code>, the class also implements a splined trajectory interface as discussed in the Splined Trajectory Tutorial. </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#dynamixelxl430","title":"DynamixelXL430","text":"<p>DynamixelXL430 provides a thin wrapper to the Robotis Dynamixel SDK. You may choose to interact with the servo at this level as well. For example, to jog the head_pan 200 ticks: <pre><code>import stretch_body.dynamixel_XL430\nimport time\n\nm = stretch_body.dynamixel_XL430.DynamixelXL430(11, '/dev/hello-dynamixel-head',baud=115200)\nm.startup()\n\nx=m.get_pos() #In encoder ticks\nm.go_to_pos(x+200) #Move 200 ticks incremental\n\ntime.sleep(2.0)\nm.stop()\n</code></pre></p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/","title":"Tutorial: Introduction to Stretch Body","text":"<p>The Stretch_Body package provides a low-level Python API to the Stretch hardware.  The Stretch_Body package is intended for advanced users who prefer to not use ROS to control the robot. It assumes a moderate level of experience programming robot sensors and actuators.</p> <p>The package is available on Git and installable via Pip.</p> <p>It encapsulates the:</p> <ul> <li>Mobile base </li> <li>Arm </li> <li>Lift </li> <li>Head actuators</li> <li>End-of-arm-actuators</li> <li>Wrist board with accelerometer (Wacc)</li> <li>Base power and IMU board (Pimu)</li> </ul> <p>As shown below, the primary programming interface to Stretch Body is the Robot class. This class encapsulates the various hardware module classes  (e.g. Lift, Arm, etc). Each of these modules then communicates with the robot firmware over USB using various utility classes.</p> <p></p> <p>Stretch also includes 3rd party hardware devices that are not accessible through Stretch_Body. However, it is possible to directly access this hardware through open-source Python packages:</p> <ul> <li>Laser range finder:  rplidar</li> <li>Respeaker: respeaker_python_library</li> <li>D435i: pyrealsense2</li> </ul>"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/#robot-interface","title":"Robot Interface","text":"<p>The primary developer interface to  Stretch_Body is the Robot class.  Let's write some code to explore the interface. Launch an interactive Python terminal:</p> <pre><code>ipython\n</code></pre> <p>Then type in the following:</p> <pre><code>import time\nimport stretch_body.robot\n\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nfor i in range(10):\n    robot.pretty_print()\n    time.sleep(0.25)\n\nrobot.stop()\n</code></pre> <p>As you can see, this prints all robot sensors and state data to the console every 250ms. </p> <p>Looking at this in detail:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n</code></pre> <p>Here we instantiated an instance of our robot through the Robot class. The call to <code>startup()</code> opens the serial ports to the various devices, loads the robot YAML parameters, and launches a few helper threads.</p> <pre><code>for i in range(10):\n    robot.pretty_print()\n    time.sleep(0.25)\n</code></pre> <p>The call to <code>pretty_print()</code> prints to console all of the robot's sensor and state data. The sensor data is automatically updated in the background by a helper thread.</p> <pre><code>robot.stop()\n</code></pre> <p>Finally, the <code>stop()</code> method shuts down the threads and cleanly closes the open serial ports.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/#units","title":"Units","text":"<p>The Robot API uses SI units:</p> <ul> <li>meters</li> <li>radians</li> <li>seconds</li> <li>Newtons</li> <li>Amps</li> <li>Volts</li> </ul> <p>Parameters may be named with a suffix to help describe the unit type. For example:</p> <ul> <li>pos_m : meters</li> <li>pos_r: radians</li> </ul>"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/#the-robot-status","title":"The Robot Status","text":"<p>The Robot derives from the Device class and we have subclasses that derives from this Device class such as the Prismatic Joint and the Dynamixel XL460. It also encapsulates several other Devices:</p> <p>Device * robot.base * robot.wacc * robot.pimu</p> <p>Prismatic Joint * robot.arm * robot.lift</p> <p>Dynamixel XL460 * robot.head * robot.end_of_arm</p> <p>All devices contain a Status dictionary. The Status contains the most recent sensor and state data of that device. For example, looking at the Arm class we see:</p> <p><pre><code>class Arm(PrismaticJoint):\n    def __init__(self,usb=None):\n</code></pre> As we can see the arm class is part of the PrismaticJoint class but this is also part of the Device class as we can see here:</p> <pre><code>class PrismaticJoint(Device):\n    def __init__(self,name,usb=None):\n        ...\n        self.status = {'timestamp_pc':0,'pos':0.0, 'vel':0.0, \\\n                       'force':0.0,'motor':self.motor.status}\n</code></pre> <p>The Status dictionaries are automatically updated by a background thread of the Robot class at around 25Hz. The Status data can be accessed via the Robot class as below:</p> <pre><code>if robot.arm.status['pos']&gt;0.25:\n    print('Arm extension greater than 0.25m')\n</code></pre> <p>If an instantaneous snapshot of the entire Robot Status is needed, the <code>get_status()</code> method can be used instead:</p> <pre><code>status=robot.get_status()\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/#the-robot-command","title":"The Robot Command","text":"<p>In contrast to the Robot Status which pulls data from the Devices, the Robot Command pushes data to the Devices.</p> <p>Consider the following example which extends and then retracts the arm by 0.1 meters:</p> <pre><code>import time\nimport stretch_body.robot\n\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.arm.move_by(0.1)\nrobot.push_command()\ntime.sleep(2.0) \n\nrobot.arm.move_by(-0.1)\nrobot.push_command()\ntime.sleep(2.0)\n\nrobot.stop()\n</code></pre> <p>A few important things are going on:</p> <pre><code>robot.arm.move_by(0.1)\n</code></pre> <p>The <code>move_by()</code> method queues up the command to the stepper motor controller. However, the command does not yet execute.</p> <pre><code>robot.push_command()\n</code></pre> <p>The <code>push_command()</code> causes all queued-up commands to be executed at once. This allows for the synchronization of motion across joints. For example, the following code will cause the base, arm, and lift to initiate motion simultaneously:</p> <pre><code>import time\nimport stretch_body.robot\n\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.arm.move_by(0.1)\nrobot.lift.move_by(0.1)\nrobot.base.translate_by(0.1)\nrobot.push_command()\ntime.sleep(2.0)\n\nrobot.stop()\n</code></pre> <p>Note</p> <p>In this example we call <code>sleep()</code> to allow time for the motion to complete before initiating a new motion.</p> <p>Note</p> <p>The Dynamixel servos do not use the Hello Robot communication protocol. As such, the head, wrist, and gripper will move immediately upon issuing a motion command. </p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/","title":"Tutorial: Parameter Management","text":"<p>In this tutorial, we will discuss how parameters are managed in Stretch Body and show examples of how to customize your robot by overriding parameters.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/#overview","title":"Overview","text":"<p>Stretch Body shares a global set of parameters across all of the hardware it manages. All members of the Device class have an instance of RobotParams. This class constructs a dictionary of the device parameters as well as the global parameters for each device. For example, from iPython try:</p> <pre><code>import stretch_body.arm\na=stretch_body.arm.Arm()\n\na.params\n</code></pre> <pre><code>Out[7]: \n{'usb_name': '/dev/hello-motor-arm',\n 'force_N_per_A': 55.9\n 'chain_pitch': 0.0167,\n 'chain_sprocket_teeth': 10,\n 'gr_spur': 3.875,\n 'i_feedforward': 0,\n 'calibration_range_bounds': [0.515, 0.525],\n 'contact_model': 'effort_pct',\n 'contact_model_homing': 'effort_pct',\n 'contact_models': {'effort_pct': {'contact_thresh_calibration_margin': 10.0,\n   'contact_thresh_max': [-90.0, 90.0],\n   'contact_thresh_default': [-45.0, 45.0],\n   'contact_thresh_homing': [-45.0, 45.0]}},\n 'motion': {'default': {'accel_m': 0.14, 'vel_m': 0.14},\n  'fast': {'accel_m': 0.3, 'vel_m': 0.3},\n  'max': {'accel_m': 0.4, 'vel_m': 0.4},\n  'slow': {'accel_m': 0.05, 'vel_m': 0.05},\n  'trajectory_max': {'vel_m': 0.4, 'accel_m': 0.4}},\n 'range_m': [0.0, 0.52]}\n</code></pre> <p>or to access another device params:</p> <pre><code>a.robot_params['lift']\n</code></pre> <pre><code>Out[9]: \n{'usb_name': '/dev/hello-motor-lift',\n 'force_N_per_A': 75.0\n 'calibration_range_bounds': [1.094, 1.106],\n 'contact_model': 'effort_pct',\n 'contact_model_homing': 'effort_pct',\n 'contact_models': {'effort_pct': {'contact_thresh_calibration_margin': 10.0,\n   'contact_thresh_max': [-100, 100],\n   'contact_thresh_default': [-69.0, 69.0],\n   'contact_thresh_homing': [-69.0, 69.0]}},\n 'belt_pitch_m': 0.005,\n 'motion': {'default': {'accel_m': 0.2, 'vel_m': 0.11},\n  'fast': {'accel_m': 0.25, 'vel_m': 0.13},\n  'max': {'accel_m': 0.3, 'vel_m': 0.15},\n  'slow': {'accel_m': 0.05, 'vel_m': 0.05},\n  'trajectory_max': {'accel_m': 0.3, 'vel_m': 0.15}},\n 'pinion_t': 12,\n 'i_feedforward': 1.2,\n 'range_m': [0.0, 1.1]}\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/#parameter-organization","title":"Parameter Organization","text":"<p>Stretch Body utilizes a prioritized parameter organization such that default settings can be easily overridden </p> Priority Name Location Description 1 user_params $HELLO_FLEET_PATH/$HELLO_FLEET_ID/ stretch_user_params.yaml Yaml file for users to override default settings and to define custom configurations. 2 configuration_params $HELLO_FLEET_PATH/$HELLO_FLEET_ID/  stretch_configuration_params.yaml Robot specific data  (eg, serial numbers and calibrations). Calibration tools may update these. 3 external_params Imported via a list defined as <code>params</code> in stretch_user_params.yaml External Python parameter dictionaries for 3rd party devices and peripherals. 4 nominal_params stretch_body.robot_params_RE2V0.py Generic systems settings (common across all robots of a given model. 5 nominal_system_params stretch_body.robot_params.py Generic systems settings (common across all robot models). <p>This allows the user to override any of the parameters by defining it in their <code>stretch_user_params.yaml</code>. It also allows Hello Robot to periodically update parameters defined in the Python files via Pip updates.</p> <p>The tool <code>stretch_params.py</code> will print out all of the robot parameters as well as their origin. For example:</p> <p><pre><code>stretch_params.py \n</code></pre> <pre><code>############################################################ Parameters for stretch-re2-xxxx \nOrigin          Parameter                                                              Value                         \n--------------------------------------------------------------------------------------------------------------------------------- ...         \nstretch_body.robot_params.nominal_params         param.arm.chain_pitch          0.0167                        \nstretch_body.robot_params.nominal_params         param.arm.chain_sprocket_teeth         10                                     ...               \nstretch_configuration_params.yaml       param.arm.contact_models.effort_pct.contact_thresh_default     [-45.0, 45.0]         \n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/#manually-querying-and-modifying-parameters","title":"Manually Querying and Modifying Parameters","text":"<p>A quick way to query parameters is with the <code>stretch_params.py</code> tool. For example, to look at parameters relating to the arm motion:</p> <p><pre><code>stretch_params.py | grep arm | grep motion\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params    param.arm.motion.default.accel_m    0.14                          \nstretch_body.robot_params.nominal_params    param.arm.motion.default.vel_m      0.14                          \nstretch_body.robot_params.nominal_params    param.arm.motion.fast.accel_m       0.3                           \nstretch_body.robot_params.nominal_params    param.arm.motion.fast.vel_m         0.3                           \nstretch_body.robot_params.nominal_params    param.arm.motion.max.accel_m        0.4                           \nstretch_body.robot_params.nominal_params    param.arm.motion.max.vel_m          0.4                           \nstretch_body.robot_params.nominal_params    param.arm.motion.slow.accel_m       0.05                          \nstretch_body.robot_params.nominal_params    param.arm.motion.slow.vel_m         0.05                          \n...               \n</code></pre></p> <p>The tool displays each parameter's value as well as which parameter file it was loaded from.</p> <p>For example, if you want to override the default motion settings for the arm, you could add the following to your <code>stretch_user_params.yaml</code>:</p> <pre><code>arm:\n  motion:\n    default:\n      vel_m: 0.1\n      accel_m: 0.1\n</code></pre> <p>Run the tool again and we see:</p> <p><pre><code>stretch_params.py | grep arm | grep motion | grep default\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params    param.arm.motion.default.accel_m    0.1                          \nstretch_body.robot_params.nominal_params    param.arm.motion.default.vel_m      0.1  \n</code></pre></p> <p>Note</p> <p>The factory parameter settings should suffice for most use cases. </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/#programmatically-modifying-and-storing-parameters","title":"Programmatically Modifying and Storing Parameters","text":"<p>A user can compute the value of a parameter programmatically and modify the robot settings accordingly. For example, in the Stretch Factory tool REx_base_calibrate_wheel_separation.py we see that the parameter <code>wheel_separation_m</code> is recomputed as the variable <code>d_avg</code>. This new value could be used during the robot execution by simply:</p> <pre><code>robot.base.params['wheel_separation_m']=d_vag\n</code></pre> <p>or it could be saved as a user override:</p> <pre><code>robot.write_user_param_to_YAML('base.wheel_separation_m', d_avg)\n</code></pre> <p>This will update the file <code>stretch_user_params.yaml</code>.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/","title":"Tutorial: Robot Motion","text":"<p>As we've seen in previous tutorials, commanding robot motion is simple and straightforward. For example, the incremental motion of the arm can be commanded by:</p> <pre><code>import stretch_body.robot\nimport time\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.arm.move_by(0.1)\nrobot.push_command()\ntime.sleep(2.0)\n\nrobot.stop()\n</code></pre> <p>The absolute motion can be commanded by:</p> <pre><code>import stretch_body.robot\nimport time\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.arm.move_to(0.1)\nrobot.push_command()\ntime.sleep(2.0)\n\nrobot.stop()\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#waiting-on-motion","title":"Waiting on Motion","text":"<p>In the above examples, we executed a <code>time.sleep()</code> after <code>robot.push_command()</code>. This allows the joint time to complete its motion. As an alternative, we can use the <code>wait_until_at_setpoint()</code> method that polls the joint position versus the target position. We can also interrupt a motion by sending a new motion command at any time. For example, try the following script:</p> <pre><code>import stretch_body.robot\nimport time\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move to full retraction\nrobot.arm.move_to(0.0)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint()\n\n#Move to full extension\nrobot.arm.move_to(0.5)\nrobot.push_command()\n\n#Interrupt motion midway and retract again\ntime.sleep(2.0)\nrobot.arm.move_to(0.0)\nrobot.arm.wait_until_at_setpoint()\n\nrobot.stop()\n</code></pre> <p>You will see the arm fully retract, begin to extend, and then fully retract again.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#motion-profiles","title":"Motion Profiles","text":"<p>All joints support trapezoidal motion profile generation. Other types of controllers are available (splined trajectory, PID, velocity, etc) but they are not covered here. The trapezoidal motion controllers require three values:</p> <ul> <li>x: target position of joint</li> <li>v: maximum velocity of motion</li> <li>a: acceleration of motion</li> </ul> <p>We provide 'defaults' for the velocity and acceleration settings, as well as 'fast', and 'slow' settings. These values have been tuned to be appropriate for the safe movement of the robot. These values can be queried using the <code>stretch_params.py</code> tool:</p> <p><pre><code>stretch_params.py | grep arm | grep motion | grep default\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params      param.arm.motion.fast.accel_m     0.14                           \nstretch_body.robot_params.nominal_params      param.arm.motion.fast.vel_m       0.14\n</code></pre></p> <p>We see that the arm motion in 'default' mode will move with a velocity of 0.14 m/s and an acceleration of 0.14 m/s^2. </p> <p>The <code>move_by</code> and <code>move_to</code> commands support optional motion profile parameters. For example, for a fast motion: </p> <pre><code>v = robot.arm.params['motion']['fast']['vel_m']\na = robot.arm.params['motion']['fast']['accel_m']\n\nrobot.arm.move_by(x_m=0.1, v_m=v, a_m=a)\nrobot.push_command()\n</code></pre> <p>The motion will use the 'default' motion profile settings if no values are specified.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#motion-limits","title":"Motion Limits","text":"<p>All joints obey motion limits which are specified in the robot parameters. </p> <p><pre><code>stretch_params.py | grep arm | grep range_m\n</code></pre> <pre><code>stretch_user_params.yaml          param.arm.range_m     [0.0, 0.515] \n</code></pre></p> <p>These are the mechanical limits of the joints and have been set at the factory to prevent damage to the hardware. It is not recommended to set them to be greater than the factory-specified values. However, they can be further limited if desired by setting soft motion limits:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.arm.set_soft_motion_limit_min(0.2)\nrobot.arm.set_soft_motion_limit_max(0.4)\n\n#Will move to position 0.2\nrobot.arm.move_to(0.0)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint()\n\n#Will move to position 0.4\nrobot.arm.move_to(0.5)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint()\n\nrobot.stop()\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#controlling-dynamixel-motion","title":"Controlling Dynamixel Motion","text":"<p>The above examples have focused on the motion of the arm. Like the lift and the base, the arm utilizes Hello Robot's custom stepper motor controller. Control of the Dynamixels of the head and the end-of-arm is very similar to that of the arm, though not identical.</p> <p>As we see here, the <code>robot.push_command()</code> call is not required as the motion begins instantaneously and is not queued. In addition, the Dynamixel servos are managed as a chain of devices, so we must pass in the joint name along with the command.</p> <pre><code>import stretch_body.robot\nimport time\nfrom stretch_body.hello_utils import deg_to_rad\n\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\nrobot.head.move_to('head_pan',0)\nrobot.head.move_to('head_tilt',0)\n\ntime.sleep(3.0)\n\nrobot.head.move_to('head_pan',deg_to_rad(90.0))\nrobot.head.move_to('head_tilt',deg_to_rad(45.0))\n\ntime.sleep(3.0)\n\nrobot.stop()\n</code></pre> <p>Similar to the stepper joints, the Dynamixel joints accept motion profile and motion limit commands. For example, here we restrict the head pan range of motion while executing both a fast and slow move:</p> <pre><code>import stretch_body.robot\nimport time\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Limit range of motion of head_pan\nrobot.head.get_motor('head_pan').set_soft_motion_limit_min(deg_to_rad(-45.0))\nrobot.head.get_motor('head_pan').set_soft_motion_limit_max(deg_to_rad(45.0))\n\n#Do a fast motion\nv = robot.params['head_pan']['motion']['fast']['vel']\na = robot.params['head_pan']['motion']['fast']['accel']\nrobot.head.move_to('head_pan',deg_to_rad(-90.0),v_r=v, a_r=a)\n\ntime.sleep(3.0)\n\n#Do a slow motion\nv = robot.params['head_pan']['motion']['slow']['vel']\na = robot.params['head_pan']['motion']['slow']['accel']\nrobot.head.move_to('head_pan',deg_to_rad(90.0),v_r=v, a_r=a)\n\ntime.sleep(3.0)\n\nrobot.stop()\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#base-velocity-control","title":"Base Velocity Control","text":"<p>The Base also supports a velocity control mode which can be useful with navigation planners. The Base controllers will automatically switch between velocity and position control. For example:</p> <pre><code>robot.base.translate_by(x_m=0.5)\nrobot.push_command()\ntime.sleep(4.0) #wait\n\nrobot.base.set_rotational_velocity(v_r=0.1) #switch to velocity controller\nrobot.push_command()\ntime.sleep(4.0) #wait\n\nrobot.base.set_rotational_velocity(v_r=0.0) #stop motion\nrobot.push_command()\n</code></pre> <p>Warning</p> <p>As shown, care should be taken to set commanded velocities to zero on exit to avoid runaway.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#advanced-topics","title":"Advanced Topics","text":""},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#stepper-control-modes","title":"Stepper Control Modes","text":"<p>Most users will control robot motion using the <code>move_to</code> and <code>move_by</code> commands as described above. However, there are numerous other low-level controller modes available. While this is a topic for advanced users, it is worth noting that each joint has a default safety mode and a default position control mode. These are:</p> Joint Default Safety Mode Default Position Control Mode left_wheel Freewheel Trapezoidal position control right_wheel Freewheel Trapezoidal position control lift Gravity compensated 'float' Trapezoidal position control arm Freewheel Trapezoidal position control head_pan Torque disabled Trapezoidal position control head_tilt Torque disabled Trapezoidal position control wrist_yaw Torque disabled Trapezoidal position control stretch_gripper Torque disabled Trapezoidal position control <p>Each joint remains in its <code>Safety Mode</code> when no program is running. When the <code>&lt;device&gt;.startup()</code> function is called, the joint controller transitions from <code>Safety Mode</code> to its <code>Default Position Control Mode</code>. It is then placed back in <code>Safety Mode</code> when <code>&lt;device&gt;.stop()</code> is called.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#motion-runstop","title":"Motion Runstop","text":"<p>Runstop activation will cause the Base, Arm, and Lift to switch to <code>Safety Mode</code> and subsequent motion commands will be ignored. The motion commands will resume smoothly when the Runstop is deactivated.  This is usually done via the Runstop button. However, it can also be done via the Pimu interface. For example:</p> <pre><code>import stretch_body.robot\nrobot=stretch_body.robot.Robot()\nrobot.startup()\n\n#Move to full retraction\nrobot.arm.move_to(0.0)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint()\n\n#Move to full extension\nrobot.arm.move_to(0.5)\nrobot.push_command()\n\n#Runstop motion midway through the motion\ninput('Hit enter to runstop motion')\nrobot.pimu.runstop_event_trigger()\nrobot.push_command()\n\ninput('Hit enter to restart motion')    \nrobot.pimu.runstop_event_reset()\nrobot.push_command()\n\nrobot.arm.move_to(0.5)\nrobot.push_command()\nrobot.arm.wait_until_at_setpoint()\n\nrobot.stop()\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#guarded-motion","title":"Guarded Motion","text":"<p>The Arm, Lift, and Base support a guarded motion function. It will automatically transition the actuator from Control mode to Safety mode when the exerted motor torque exceeds a threshold. </p> <p>This functionality is most useful for the Lift and the Arm. It allows these joints to safely stop upon contact. It can be used to:</p> <ul> <li>Safely stop when contacting an actuator hard stop</li> <li>Safely stop when making unexpected contact with the environment or a person</li> <li>Make a guarded motion where the robot reaches a surface and then stops</li> </ul> <p>For more information on guarded motion, see the Contact Models Tutorial</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#synchronized-motion","title":"Synchronized Motion","text":"<p>The Arm, Lift, and Base actuators have a hardware synchronization mechanism. This allows for stepper controller commands to be time synchronized across joints.  This behavior can be disabled via the user YAML. By default the settings are:</p> <p><pre><code>stretch_params.py | grep enable_sync_mode\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params   param.hello-motor-arm.gains.enable_sync_mode         1                             \nstretch_body.robot_params.nominal_params   param.hello-motor-left-wheel.gains.enable_sync_mode  1                             \nstretch_body.robot_params.nominal_params   param.hello-motor-lift.gains.enable_sync_mode        1                             \nstretch_body.robot_params.nominal_params   param.hello-motor-right-wheel.gains.enable_sync_mode 1  \n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#motion-status","title":"Motion Status","text":"<p>It can be useful to poll the status of a joint during a motion to modify the robot's behavior, etc. The useful status values include:</p> <pre><code>robot.arm.status['pos']                     #Joint position\nrobot.arm.status['vel']                     #Joint velocity\nrobot.arm.motor.status['effort_pct']        #Joint effort (-100 to 100) (derived from motor current)    \nrobot.arm.motor.status['near_pos_setpoint'] #Is sensed position near commanded position\nrobot.arm.motor.status['near_vel_setpoint'] #Is sensed velocity near commanded velocity\nrobot.arm.motor.status['is_moving']         #Is the joint in motion\nrobot.arm.motor.status['in_guarded_event']  #Has a guarded event occured\nrobot.arm.motor.status['in_safety_event']   #Has a safety event occured\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#update-rates","title":"Update Rates","text":"<p>The following update rates apply to Stretch:</p> Item Rate Notes Status data for Arm, Lift, Base, Wacc, and Pimu 25Hz Polled automatically by Robot thread Status data for End of Arm and Head servos 15Hz Polled automatically by Robot thread Command data for Arm, Lift, Base, Wacc, Pimu N/A Commands are queued and executed upon calling robot.push_command( ) Command data for End of Arm and Head servos N/A Commands execute immediately <p>Note</p> <p>Motion commands are non-blocking and it is the responsibility of the user code to poll the Robot Status to determine when and if a motion target has been achieved.</p> <p>Info</p> <p>The Stretch Body interface is not designed to support high bandwidth control applications. The natural dynamics of the robot actuators do not support high bandwidth control, and the USB-based interface limits high-rate communication.</p> <p>Tip</p> <p>In practice, a Python-based control loop that calls push_command() at 1Hz to 10Hz is sufficiently matched to the robot's natural dynamics.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/","title":"Tutorial: Robot Sensors","text":""},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#introduction","title":"Introduction","text":"<p>Stretch Body exposes a host of sensor data through the status dictionaries of its devices. In this tutorial, we'll cover how to access, view, and configure this sensor data. </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#tools-to-view-sensor-data","title":"Tools to View Sensor Data","text":"<p>There are two useful tools for scoping Pimu and Wacc sensor data in real-time:</p> <p><pre><code>stretch_pimu_scope.py --help\n</code></pre> <pre><code>For use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\nusage: stretch_pimu_scope.py [-h] (--cliff | --at_cliff | --voltage | --current | --temp | --ax | --ay | --az | --mx | --my | --mz | --gx | --gy | --gz | --roll | --pitch | --heading | --bump)\n\nVisualize Pimu (Power+IMU) board data with an oscilloscope\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --cliff     Scope base cliff sensors\n  --at_cliff  Scope base at_cliff signal\n  --voltage   Scope bus voltage (V)\n  --current   Scope bus current (A)\n  --temp      Scope base internal temperature (C)\n  --ax        Scope base accelerometer AX\n  --ay        Scope base accelerometer AY\n  --az        Scope base accelerometer AZ\n  --mx        Scope base magnetometer MX\n  --my        Scope base magnetometer MY\n  --mz        Scope base magnetometer MZ\n  --gx        Scope base gyro GX\n  --gy        Scope base gyro GY\n  --gz        Scope base gyro GZ\n  --roll      Scope base imu Roll\n  --pitch     Scope base imu Pitch\n  --heading   Scope base imu Heading\n  --bump      Scope base imu bump level\n</code></pre></p> <p>and,</p> <p><pre><code>stretch_wacc_scope.py --help\n</code></pre> <pre><code>For use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\nusage: stretch_wacc_scope.py [-h] [--ax] [--ay] [--az] [--a0] [--d0] [--d1] [--tap]\n\nVisualize Wacc (Wrist+Accel) board data with an oscilloscope\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --ax        Scope accelerometer AX\n  --ay        Scope accelerometer AY\n  --az        Scope accelerometer AZ\n  --a0        Scope analog-in-0\n  --d0        Scope digital-in-0\n  --d1        Scope digital-in-1\n  --tap       Scope single tap\n</code></pre></p> <p>Each motor also has associated sensor data available in its status dictionaries. The corresponding 'jog' tool for each joint will pretty-print the sensor data for that motor to the console. For example:</p> <p><pre><code>stretch_arm_jog.py \n</code></pre> <pre><code>For use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n...\n----- Arm ------ \nPos (m):  0.0032848120914969895\nVel (m/s):  0.0002031017627742426\nSoft motion limits (m) [0.0, 0.52]\nTimestamp PC (s): 1661797443.1212385\n-----------\nMode MODE_SAFETY\nx_des (rad) 0 (deg) 0.0\nv_des (rad) 25 (deg) 1432.3944878270581\na_des (rad) 15 (deg) 859.4366926962349\nStiffness 1\nFeedforward 0\nPos (rad) 0.47890087962150574 (deg) 27.438999207414973\nVel (rad/s) 0.029610708355903625 (deg) 1.6965686171860386\nEffort (Ticks) 0.0\nEffort (Pct) 0.0\nCurrent (A) 0.0\nError (deg) 0.0\nDebug 0.0\nGuarded Events: 0\nDiag 00000000000000000000000100000000\n       Position Calibrated: False\n       Runstop on: False\n       Near Pos Setpoint: False\n       Near Vel Setpoint: False\n       Is Moving: False\n       Is Moving Filtered: 0\n       At Current Limit: False\n       Is MG Accelerating: False\n       Is MG Moving: False\n       Encoder Calibration in Flash: True\n       In Guarded Event: False\n       In Safety Event: False\n       Waiting on Sync: False\nWaypoint Trajectory\n       State: idle\n       Setpoint: (rad) 0.0 | (deg) 0.0\n       Segment ID: 0\nTimestamp (s) 1661797443.110996\nRead error 0\nBoard variant: Stepper.1\nFirmware version: Stepper.v0.2.0p1\n...\n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#accessing-the-status-dictionaries","title":"Accessing the Status Dictionaries","text":"<p>Each Robot device has a status dictionary that is automatically updated with the latest sensor data. The primary dictionaries are:</p> <ul> <li>Stepper Status</li> <li>Wacc Status</li> <li>Pimu Status</li> <li>Dynamixel Status</li> </ul> <p>Each of these dictionaries can be accessed through the Robot instance. For example, try in iPython:</p> <pre><code>import stretch_body.robot\nimport time\n\nr=stretch_body.robot.Robot()\nr.startup()\nfor i in range(10):\n    print('Arm position (m)%f'%r.arm.status['pos'])\n    time.sleep(0.1) \n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#base-imu","title":"Base IMU","text":"<p>The base has a 9-DoF IMU using the 9-DoF FXOS8700 + FXAS21002 chipset. This is the same chipset used on the Adafruit NXP IMU board. </p> <p>The Pimu reports back the IMU sensor readings in its IMU status dictionary. For example, from iPython try:</p> <pre><code>import stretch_body.robot\nr=stretch_body.robot.Robot()\nr.startup()\n\nr.pimu.status['imu']\nOut[6]: \n{'ax': 0.30007487535476685,\n 'ay': -0.355493426322937,\n 'az': -9.736297607421875,\n 'gx': 0.0009544769418425858,\n 'gy': 0.00013635384675581008,\n 'gz': 0.00027270769351162016,\n 'mx': -10.699999809265137,\n 'my': -42.900001525878906,\n 'mz': -51.0,\n 'roll': 0.03657745230780231,\n 'pitch': -0.02960890640560868,\n 'heading': 1.2786458241584955,\n 'timestamp': 1661788669.3042662,\n 'qw': 0.0009681061492301524,\n 'qx': 0.59670090675354,\n 'qy': -0.8021157383918762,\n 'qz': 0.023505505174398422,\n 'bump': -0.9188174605369568}\n\nr.pimu.pretty_print()\n----------IMU -------------\nAX (m/s^2) 0.30007487535476685\nAY (m/s^2) -0.355493426322937\nAZ (m/s^2) -9.736297607421875\nGX (rad/s) 0.0009544769418425858\nGY (rad/s) 0.00013635384675581008\nGZ (rad/s) 0.00027270769351162016\nMX (uTesla) -10.699999809265137\nMY (uTesla) -42.900001525878906\nMZ (uTesla) -51.0\nQW 0.0009681061492301524\nQX 0.59670090675354\nQY -0.8021157383918762\nQZ 0.023505505174398422\nRoll (deg) 2.095733642578125\nPitch (deg) -1.6964653730392456\nHeading (deg) 73.26100921630858\n</code></pre> <p>It reports:</p> <ul> <li>Acceleration (AX, AY, AZ)</li> <li>Gravity (GX, GY GZ)</li> <li>Magnetic field (MX, MY, MZ)</li> <li>Quaternion orientation (QW, QX, QY, QZ)</li> <li>Euler angle orientation (Roll, Pitch, Heading)</li> </ul> <p>These values are computed on the Pimu. As we can see in its firmware code, a 100Hz Madgwick filter is used to compute the orientation. </p> <p>Stretch Body also implements a bump detector using the IMU accelerometers. This detector simply computes the sum of squares of AX, AY, and AZ. This value is then compared to the following threshold to determine if a bump is detected:</p> <p><pre><code>stretch_params.py | grep pimu | grep bump\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params   param.pimu.config.bump_thresh       20.0 \n</code></pre></p> <p>You can experiment with the bump detector with the following code:</p> <pre><code>import stretch_body.robot\nr=stretch_body.robot.Robot()\nr.startup()\n\nr.pimu.config['bump_thresh']= 20.0 #Experiment with values\nr.pimu.set_config(p.config)\nr.push_command()\n\nfor i in range(100):\n    time.sleep(0.1)\n    print('Bump %f'%r.pimu.status['bump'])\n    print('Bump event count %d'%r.pimu.status['bump_event_cnt'])\n</code></pre> <p>Note</p> <p>The IMU is calibrated by Hello Robot at the factory. Please contact Hello Robot support for details on recalibrating your IMU.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#wrist-accelerometer","title":"Wrist Accelerometer","text":"<p>The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The Wacc reports back AX, AY, and AZ in its status dictionary. For example, from iPython try:</p> <pre><code>import stretch_body.robot\nr=stretch_body.robot.Robot()\nr.startup()\nr.wacc.status\n\nOut[5]: \n{'ax': 10.093315124511719,\n 'ay': 0.10557432472705841,\n 'az': -0.45386940240859985,\n 'a0': 155,\n 'd0': 1,\n 'd1': 1,\n 'd2': 0,\n 'd3': 0,\n 'single_tap_count': 15,\n 'state': 0,\n 'debug': 0,\n 'timestamp': 1661795676.203578,\n 'transport': {'rate': 0.4572487091345871,\n  'read_error': 0,\n  'write_error': 0,\n  'itr': 3,\n  'transaction_time_avg': 0,\n  'transaction_time_max': 0,\n  'timestamp_pc': 0}}\n\nr.wacc.pretty_print()\n------------------------------\nAx (m/s^2) 10.093315124511719\nAy (m/s^2) 0.10557432472705841\nAz (m/s^2) -0.45386940240859985\nA0 155\nD0 (In) 1\nD1 (In) 1\nD2 (Out) 0\nD3 (Out) 0\nSingle Tap Count 15\nState  0\nDebug 0\nTimestamp (s) 1661795676.203578\nBoard variant: Wacc.1\nFirmware version: Wacc.v0.2.0p1\n</code></pre> <p>In addition to AX, AY, and AZ we also see the <code>single_tap_count</code> value which reports back a count of the number of single-tap contacts the accelerometer has experienced since power-up. </p> <p>The following Wacc parameters configure the accelerometer low-pass filter and single-tap settings. See the ADXL343 datasheet for more details.</p> <p><pre><code>stretch_params.py | grep wacc\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params      param.wacc.config.accel_LPF         10.0                          \nstretch_body.robot_params.nominal_params      param.wacc.config.accel_range_g        4                             \nstretch_body.robot_params.nominal_params      param.wacc.config.accel_single_tap_dur       70                            \nstretch_body.robot_params.nominal_params      param.wacc.config.accel_single_tap_thresh    50                                     \nstretch_configuration_params.yaml              param.wacc.config.accel_gravity_scale      1.0 \n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#cliff-sensors","title":"Cliff Sensors","text":"<p>Stretch has four Sharp GP2Y0A51SK0F IR cliff sensors pointed toward the floor. These report the distance to the floor, allowing for the detection of thresholds, stair edges, etc. </p> <p>Relevant parameters for the cliff sensors are:</p> <p><pre><code>stretch_params.py | grep cliff\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params  param.pimu.config.cliff_LPF           10.0                          \nstretch_body.robot_params.nominal_params  param.pimu.config.cliff_thresh        -50                           \nstretch_body.robot_params.nominal_params  param.pimu.config.stop_at_cliff        0                           \nstretch_configuration_params.yaml         param.pimu.config.cliff_zero          [518.6307312011719, 530.9835095214844, 500.7268048095703, 509.92264434814456]         \nstretch_body.robot_params.nominal_params  param.robot_monitor.monitor_base_cliff_event   1  \n</code></pre></p> <p>The sensors are calibrated such that a zero value (as defined by <code>cliff_zero</code>) indicates the sensor is at the correct height from the floor surface. A negative value indicates a drop off such as a stair ledge while a positive value indicates an obstacle like a threshold or high pile carpet.  You may want to recalibrate this zero based on the surface the robot is on (eg, carpet, tile, etc). To do this:</p> <p><pre><code>REx_cliff_sensor_calibrate.py \n</code></pre> <pre><code>For use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\nEnsure cliff sensors are not obstructed and base is on a flat surface\nHit enter when ready\nItr 0 Val [518.630126953125, 530.4168701171875, 500.863037109375, 510.0032958984375]\n...\nItr 99 Val [518.8374633789062, 530.858154296875, 500.5805969238281, 509.9013671875]\nGot cliff zeros of:  [518.6307312011719, 530.9835095214844, 500.7268048095703, 509.92264434814456]\nCalibration passed. Storing to YAML...\n</code></pre></p> <p>The  <code>stop_at_cliff</code> field causes the robot to execute a Runstop when the cliff sensor readings exceed the value <code>cliff_thresh</code>. The parameter <code>cliff_LPF</code> defines the low-pass-filter rate (Hz) on the analog sensor readings.</p> <p>Note</p> <p>As configured at the factory,  <code>stop_at_cliff</code> is set to zero and Stretch does not stop its motion based on the cliff sensor readings. Hello Robot makes no guarantees as to the reliability of Stretch's ability to avoid driving over ledges and stairs when this flag is enabled.</p> <p>The range values from the sensors can be read from the <code>robot.pimu.status</code> message. The relevant fields are:</p> <pre><code>import stretch_body.robot\nr=stretch_body.robot.Robot()\nr.startup()\n\nr.pimu.status['cliff_range']\nOut[4]: [0.39227294921875, -0.2047119140625, -0.26422119140625, 0.006134033203125]\n\nr.pimu.status['at_cliff']\nOut[5]: [False, False, False, False]\n\nr.pimu.status['cliff_event']\nOut[5]: False\n</code></pre> <p>The <code>cliff_event</code> flag is set when any of the four sensor readings exceed <code>cliff_thresh</code> and <code>stop_at_cliff</code> is enabled. In the event of a Cliff Event, it must be reset by <code>robot.pimu.cliff_event_reset()</code> to reset the generated Runstop.</p> <p>The cliff detection logic can be found in the Pimu firmware.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/","title":"Tutorial: Safety Features","text":"<p>Stretch includes several built-in functions that help it maintain safe operating conditions. These functions can be disabled and enabled via the robot user parameters.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#logging","title":"Logging","text":"<p>Upon instantiation, the Robot class opens a new log file for warning and informational messages to be written to. These timestamped logs are found under $HELLO_FLEET_DIRECTORY/log.</p> <p>The logging messages can additionally be echoed to the console by setting:</p> <pre><code>robot:\n  log_to_console: 1\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#runstop-functions","title":"Runstop Functions","text":"<p>The Runstop deactivates all robot motion. It can be triggered by the physical button on the robot's head. It can also be triggered by internal monitors of the system state. The default configuration of these parameters is:</p> <p><pre><code>stretch_params.py | grep stop_at\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params param.pimu.config.stop_at_cliff       0                             \nstretch_body.robot_params.nominal_params param.pimu.config.stop_at_high_current   0                             \nstretch_body.robot_params.nominal_params param.pimu.config.stop_at_low_voltage  1                             \nstretch_body.robot_params.nominal_params param.pimu.config.stop_at_runstop 1                             \nstretch_body.robot_params.nominal_params param.pimu.config.stop_at_tilt   0 \n</code></pre></p> Parameter Function stop_at_low_voltage Trigger runstop / beep when voltage too low stop_at_high_current Trigger runstop when bus current too high stop_at_cliff Trigger runstop when a cliff sensor is outside of range stop_at_runstop Allow runstop to disable motors stop_at_tilt Trigger runstop when robot tilts too far <p>The Pimu firmware details the implementation of these functions.</p> <p>Warning</p> <p>The <code>stop_at_cliff</code> and <code>stop_at_tilt</code> functions are disabled by default as they are not robust to the normal operating conditions of the robot. Therefore do not rely on these functions for robot safety.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#robot-monitor","title":"Robot Monitor","text":"<p>The Robot Monitor is a thread that monitors the Robot Status data for significant events. For example, it can monitor the error flags from the Dynamixel servos and notify when a thermal overload occurs. The Robot Monitor logs warnings to a log file by default.</p> <p>The default parameters associated with RobotMonitor are:</p> <p><pre><code>stretch_params.py | grep monitor\n</code></pre> <pre><code>...             \nstretch_body.robot_params.nominal_params   param.robot.use_monitor            1               \nstretch_body.robot_params.nominal_params  param.robot_monitor.monitor_base_bump_event    1                        \nstretch_body.robot_params.nominal_params   param.robot_monitor.monitor_base_cliff_event          1                             \nstretch_body.robot_params.nominal_params    param.robot_monitor.monitor_current       1                             \nstretch_body.robot_params.nominal_params   param.robot_monitor.monitor_dynamixel_flags              1                             \nstretch_body.robot_params.nominal_params   param.robot_monitor.monitor_guarded_contact               1      \nstretch_body.robot_params.nominal_params   param.robot_monitor.monitor_over_tilt_alert     1                             \nstretch_body.robot_params.nominal_params     param.robot_monitor.monitor_runstop        1                     \nstretch_body.robot_params.nominal_params     param.robot_monitor.monitor_voltage                 1             \nstretch_body.robot_params.nominal_params      param.robot_monitor.monitor_wrist_single_tap          1\n</code></pre></p> YAML Function monitor_base_bump_event Report when the accelerometer detects a bump event monitor_base_cliff_event Report when a cliff sensor event occurs monitor_current Report when the battery current exceeds desired range monitor_dynamixel_flags Report when a Dynamixel servo enters an error state monitor_guarded_contact Report when a guarded contact event occurs monitor_over_tilt_alert Report when an over-tilt event occurs monitor_runstop Report when the runstop is activated / deactivated monitor_voltage Report when the battery voltage is out of range monitor_wrist_single_tap Report when the wrist accelerometer reports a single tap event <p>Test out the RobotMonitor system by first enabling the console logging in stretch_user_params.yaml:</p> <pre><code>robot:\n  log_to_console: 1\n</code></pre> <p>Then run the tool and hit the Runstop button, and then hold it down for 2 seconds:</p> <p><pre><code>stretch_robot_monitor.py\n</code></pre> <pre><code>For use with S T R E T C H (R) RESEARCH EDITION from Hello Robot Inc.\n---------------------------------------------------------------------\n\nStarting Robot Monitor. Ctrl-C to exit\n[INFO] [robot_monitor]: Runstop activated\n[INFO] [robot_monitor]: Runstop deactivated\n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#robot-sentry","title":"Robot Sentry","text":"<p>The Robot Sentry is a thread that can override and also generate commands to the robot hardware. Its purpose is to keep the robot operating within a safe regime. For example, the Robot Sentry monitors the position of the Lift and Arm and limits the maximum base velocity and acceleration to reduce the chance of toppling. The Robot Sentry reports events to the log file as well. </p> YAML Function base_fan_control Turn the fan on when CPU temp exceeds range base_max_velocity Limit the base velocity when robot CG is high stretch_gripper_overload Reset commanded position to prevent thermal overload during grasp wrist_yaw_overload Reset commanded position to prevent thermal overload during pushing"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#collision-avoidance","title":"Collision Avoidance","text":"<p>See the Collision Avoidance Tutorial for more information on the Stretch collision avoidance system.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/","title":"Tutorial: Splined Trajectories","text":"<p>Stretch Body supports splined trajectory controllers across all of its joints. This enables Stretch to achieve smooth and coordinated full-body control of the robot. </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/#what-are-splined-trajectories","title":"What are Splined Trajectories?","text":"<p>A splined trajectory is a smooth path that a robot joint follows over a specific period of time. Cubic or quintic splines are used to represent the trajectory. As shown below, the splines (blue) are defined by a series of user-provided waypoints (black dot). A waypoint is simply a target position, velocity, and optional acceleration at a given time. The spline ensures continuity and smoothness when interpolating between the waypoint targets.</p> <p>During execution, the trajectory controller uses this splined representation to compute the instantaneous desired position, velocity, and acceleration of the joint (red). On Stretch, this instantaneous target is then passed to a lower-level position or velocity controller. </p> <p>Splined trajectories are particularly useful when you want to coordinate motion across several joints. Because the trajectory representation is time-based, it is straightforward to encode multi-joint coordination. Stretch Body supports both cubic and quintic splines. A quintic spline waypoint includes acceleration in the waypoint target, while a cubic spline does not.</p> <p></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/#the-splined-trajectory-tool","title":"The Splined Trajectory Tool","text":"<p>Stretch Body includes a graphical tool for exploring splined trajectory control on the robot:</p> <p><pre><code>stretch_trajectory_jog.py -h\n</code></pre> <pre><code>usage: stretch_trajectory_jog.py [-h] [--text] [--preloaded_traj {1,2,3}] (--head_pan | --head_tilt | --wrist_yaw | --gripper | --arm | --lift | --base_translate | --base_rotate | --full_body)\n\nTest out splined trajectories on the various joint from a GUI or text menu.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --text, -t            Use text options instead of GUI\n  --preloaded_traj {1,2,3}, -p {1,2,3}\n                        Load one of three predefined trajectories\n  --head_pan            Test trajectories on the head_pan joint\n  --head_tilt           Test trajectories on the head_tilt joint\n  --wrist_yaw           Test trajectories on the wrist_yaw joint\n  --gripper             Test trajectories on the stretch_gripper joint\n  --arm                 Test trajectories on the arm joint\n  --lift                Test trajectories on the lift joint\n  --base_translate      Test translational trajectories on diff-drive base\n  --base_rotate         Test rotational trajectories on diff-drive base\n  --full_body           Test trajectories on all joints at once\n</code></pre></p> <p>The tool GUI allows you to interactively construct a splined trajectory and then execute it on the robot. For example, on the arm:</p> <p></p> <p>Note</p> <p>Use caution when commanding the base. Ensure that the attached cables are long enough to support the base motion. Alternatively, you may want to put the base on top of a book so the wheels don't touch the ground.</p> <p>Finally, you can explore a full-body trajectory using the non-GUI version of the tool:</p> <pre><code>stretch_trajectory_jog.py --full_body\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/#programming-trajectories","title":"Programming Trajectories","text":"<p>Programming a splined trajectory is straightforward. For example, try the following from iPython:</p> <pre><code>import stretch_body.robot\nr=stretch_body.robot.Robot()\nr.startup()\n#r.arm.motor.disable_sync_mode() **If you want to try running the code with this command you'll need to coment the r.push_command() and it will work as well\n\n#Define the waypoints\ntimes = [0.0, 10.0, 20.0]\npositions = [r.arm.status['pos'], 0.45, 0.0]\nvelocities = [r.arm.status['vel'], 0.0, 0.0]\n\n#Create the spline trajectory\nfor waypoint in zip(times, positions, velocities):\n    r.arm.trajectory.add(waypoint[0], waypoint[1], waypoint[2])\n\n#Begin execution\nr.arm.follow_trajectory()\nr.push_command()\ntime.sleep(0.1)\n\n#Wait unti completion\nwhile r.arm.is_trajectory_active():\n    print('Execution time: %f'%r.arm.get_trajectory_time_remaining())\n    time.sleep(0.1)\n\nr.stop()\n</code></pre> <p>This will cause the arm to move from its current position to 0.45m, then back to fully retracted. A few things to note:</p> <ul> <li>This will execute a Cubic spline as we did not pass in accelerations to in <code>r.arm.trajectory.add</code></li> <li>The call to <code>r.arm.follow_trajectory</code> is non-blocking and the trajectory generation is handled by a background thread of the Robot class</li> </ul> <p>If you're interested in exploring the trajectory API further. the code for the <code>stretch_trajectory_jog.py</code> is a great reference to get started.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/#advanced-controller-parameters","title":"Advanced: Controller Parameters","text":"<p>Sometimes the robot's motion isn't quite what is expected when executing a splined trajectory. It is important that the trajectory be well-formed, meaning that it:</p> <ul> <li>Respects the maximum velocity and accelerations limits of the joint</li> <li>Doesn't create a large 'excursion' outside of the acceptable range of motion to hit a target waypoint</li> <li>Doesn't have waypoints so closely spaced together that it exceeds the nominal control rates of Stretch (~10-20 Hz)</li> </ul> <p>For example, the arm trajectory below has a large excursion outside of the joint's range of motion (white). This is because the second waypoint expects a non-zero velocity when the arm reaches full extension.</p> <p></p> <p>Often the trajectory waypoints will be generated from a motion planner. The planner needs to incorporate the position, velocity, and acceleration constraints of the joint. These can be found by, for example:</p> <p><pre><code>stretch_params.py | grep arm | grep motion | grep trajectory\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params  param.arm.motion.trajectory_max.vel_m    0.4             \nstretch_body.robot_params.nominal_params param.arm.motion.trajectory_max.accel_m   0.4\n</code></pre></p> <p><pre><code>stretch_params.py | grep arm | grep range_m\n</code></pre> <pre><code>stretch_user_params.yaml       param.arm.range_m      [0.0, 0.515] \n</code></pre></p> <p>Fortunately, the Stretch Body Trajectory classes do some preliminary feasibility checking of trajectories using the is_segment_feasible function. This checks if the generated motions lie within the constraints of the <code>trajectory_max</code> parameters.</p> <p>It is generally important for the waypoints to be spaced far apart. Stretch isn't a dynamic and fast-moving robot, so there isn't a practical advantage to closely spaced waypoints at any rate.</p> <p>The stepper controllers (arm, lift, and base) can be updated at approximately 20 Hz maximum. Therefore, if your waypoints are spaced 50 ms apart, you run the risk of overflowing the stepper controller. Likewise, the Dynamixel joints can be updated at approximately 12 Hz. </p> <p>Tip</p> <p>As a rule of thumb, spacing the waypoints over 100 ms apart is a good idea.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_chat/","title":"Tutorial Stretch OpenAI Chat","text":"<p>This tutorial introduces the API from OpenAI and explains how to implement it in Stretch to make basic movement.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_chat/#explore-the-api","title":"Explore the API","text":"<p>The OpenAI API is a sophisticated language model, design to assist users with a range of models with different capabilities and price points, as well as the ability to fine-tune custom models. There are some key concepts to understand better what we can do, lets focus only in the text generation models and the tokens. </p> <p>OpenAI's text generation models, such as GPT-4 and GPT-3.5, have undergone extensive training to comprehend both natural and formal language. These models, like GPT-4, are capable of generating text based on given inputs, often referred to as \"prompts\". To effectively utilize a model like GPT-4, the process involves designing prompts which essentially serve as instructions or examples to guide the model in successfully completing a given task. GPT-4 can be applied to a wide range of tasks including content or code generation, summarization, conversation, creative writing, and more.</p> <p>This text generation, process text in chunks called tokens. Tokens represent commonly ocurring sequences of characters. You can checkout the OpenAI's tokenizer tool to test specific strings and see how they are translated into tokens. Why are tokens so important? It's simple, because depending in the number of tokens you use, as well as the model (text generation, image or audio models), it will cost money. The good news, it's not that expensive and it's really useful, just be careful when dealing with image generation, as the cost is calculated per image, in contrast to text, which is priced per 1,000 tokens, or audio, which is billed per minute. You can take a look at the pricing page from OpenAI for more information.</p> <p>In this tutorial, we are using the GPT-3.5-turbo model, one of the newer text generation models alongside GPT-4 and GPT-4-turbo, we will use this model alongside the Chat Completion API, this will help us with our model,so that we can \"chat\" with Stretch and command some basic movements using natural language. If you want to know more about this and maybe create some applications, take a look at this examples.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_chat/#chat-completion-api","title":"Chat Completion API","text":"<p>Before jumping into the Stretch Chat code, there are some things to know well about the Chat Completion API, take a look at this example from the documentation: <pre><code>from openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n  ]\n)\n</code></pre> As you can see, there are different roles in the chat completion, the system, the user and the assistant, each one with it's own content. This is the base for the Chat Completion and even the base to create your own chatbot, take a look at the roles: 1. The system: You will write direct instructions, sometimes the shorter and clearer is better, but if you have a long context for the AI to understand you need to be more specific (you'll see it in the tutorial) 2. The user: This will be your input text for the model to do something, it can be questions or even normal conversations, it depends on the context of the system as well, if you want the model to know everything about robotics and you ask something about chemistry or biotechnology it will output a message that it cannot process your request. 3. The assistant: Here you can help the model to understand what you are going to do, this can also be a pre crafted bot response, take a look at the tutorial to understand this better.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_chat/#stretch-mobility-with-openai","title":"Stretch Mobility with OpenAI","text":"<p>Note</p> <p>For your safety, put stretch in an open area when you try this tutorial.</p> <p>For this tutorial, we'll guide Stretch to move around and perform actions by writing our instructions in natural language within the terminal, copy the next python code and paste it in your own folder, we are only going to use Stretch body and the OpenAI python library, if you haven't installed it yet don't worry, follow this link and read the quickstart guide, there you can create an OpenAI account and setup your API key as well, this is important and it's only yours so be careful where you save it! To install the library just write down in your terminal: <pre><code>pip3 install --upgrade openai\n</code></pre></p> <p>Now going to the code: <pre><code>from openai import OpenAI\nfrom stretch_body import robot\nimport time\n\nclient = OpenAI(api_key=(\"OPEN_AI_KEY\")) # &lt;---------- USE YOUR API KEY HERE\n\ndef move_forward(robot):\n    robot.base.translate_by(0.2)\n    robot.push_command()\n    robot.base.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef move_backward(robot):\n    robot.base.translate_by(-0.2)\n    robot.push_command()\n    robot.base.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef turn_right(robot):\n    robot.base.rotate_by(-1.57)\n    robot.push_command()\n    robot.base.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef turn_left(robot):\n    robot.base.rotate_by(1.57)\n    robot.push_command()\n    robot.base.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef arm_front(robot):\n    robot.arm.move_by(0.1)\n    robot.push_command()\n    robot.arm.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef arm_back(robot):\n    robot.arm.move_by(-0.1)\n    robot.push_command()\n    robot.arm.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef lift_up(robot):\n    robot.lift.move_by(0.1)\n    robot.push_command()\n    robot.lift.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef lift_down(robot):\n    robot.lift.move_by(-0.1)\n    robot.push_command()\n    robot.lift.wait_until_at_setpoint()\n    time.sleep(0.1)\n\nstretch_actions = {\"move_forward\" : \"Move the robot forward 0.2m\",\n                   \"move_backward\": \"Move the robot backward 0.2m\",\n                   \"turn_right\": \"Turn the robot 90 degrees to the clockwise\",\n                   \"turn_left\": \"Turn the robot 90 degrees to the counter clockwise\",\n                   \"arm_front\": \"Move the arm to the front 0.1m\",\n                   \"arm_back\": \"Move the arm to the back 0.1m\",\n                   \"lift_up\": \"Move the lift up 0.1m\",\n                   \"lift_down\": \"Move the lift down 0.1m\"}\n\nstretch_actions_fn = {\"move_forward\" : move_forward,\n                   \"move_backward\": move_backward,\n                   \"turn_right\": turn_right,\n                   \"turn_left\": turn_left,\n                   \"arm_front\": arm_front,\n                   \"arm_back\": arm_back,\n                   \"lift_up\": lift_up,\n                   \"lift_down\": lift_down}\n\n\ndef chatter(input_text):\n    # Populate Assistance prompt\n    assistance_msg = \"Here is the description for each robot motion\"\n    for k in stretch_actions.keys():\n        assistance_msg = assistance_msg + f\"\\n - {k} : {stretch_actions[k]} \"\n\n    # Define System prompt (Personality of the system)\n    system_prompt = f\"Assume you are a mobile robot and you are able to receive a natural language instrunctions regarding the robot's movements. Based on undestanding the instructions return a sequence of discrete actions from this list {list(stretch_actions.keys())}. The only output must be in two parts. The first part should explain the sequence and second part should only be the list the actions seperated by comma. The first part you will explain the list of actions to follow and the reason behind it. The second one must be the list of movements that we are going to use, separate them only with the '[]'. This is not an explanation, it must be only the list of movements\"\n\n    response = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"assistant\", \"content\": assistance_msg},\n        {\"role\": \"user\", \"content\": input_text},\n    ]\n    )\n    response_text = response.choices[0].message.content.strip().lower()\n    print(f\"CHATGPT RESPONSE: {response_text}\")\n    return response_text\n\ndef extract_action_sequence(response_text):\n    # In the response text find the list that starts and ends with []\n    start_index = response_text.find(\"[\")\n    end_index = response_text.find(\"]\", start_index)\n\n    if start_index != -1 and end_index != -1:\n        movements_str = response_text[start_index + 1:end_index]\n        # Split the comma-separated movements into a list. The .strip(\"'\\x22\") is used to remove both single and double quotes from the beginning and end of each movement.\n        formated_list = [movement.strip().strip(\"'\\x22\") for movement in movements_str.split(\",\")]\n        print(formated_list)\n        return formated_list\n    else:\n        print(\"List of movements not found in the response.\")\n        return []\n\ndef execute_robot_motions(final_motion_list):\n    rb = robot.Robot()\n    rb.startup()\n    print(\"Starting to execute motions....\")\n    for motion_key in final_motion_list:\n        print(f\"Executing motion: {motion_key}\")\n        stretch_actions_fn[motion_key](rb)\n    print(\"Completed Executing motions...\")\n    rb.stop()\n\ndef stretch_chatter(input_text):\n    response = chatter(input_text)\n    motion_list = extract_action_sequence(response)\n    execute_robot_motions(motion_list)\n\n\nwhile True:\n    response = input(\"What motion can I do for you?\\n\")\n    stretch_chatter(response)\n</code></pre></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_chat/#the-code-explained","title":"The code explained","text":"<p>Now let's break the code down <pre><code>from openai import OpenAI\nfrom stretch_body import robot\nimport time\n\nclient = OpenAI(api_key=(\"OPEN_AI_KEY\")) # &lt;---------- USE YOUR API KEY HERE\n</code></pre> You need to import openai if you are going to use the API. Import robot from Stretch body for the movement and don't forget to use your secret key, if you don't use it, it will not work.</p> <p><pre><code>def move_forward(robot):\n    robot.base.translate_by(0.2)\n    robot.push_command()\n    robot.base.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef move_backward(robot):\n    robot.base.translate_by(-0.2)\n    robot.push_command()\n    robot.base.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef turn_right(robot):\n    robot.base.rotate_by(-1.57)\n    robot.push_command()\n    robot.base.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef turn_left(robot):\n    robot.base.rotate_by(1.57)\n    robot.push_command()\n    robot.base.wait_until_at_setpoint()\n    time.sleep(0.1)\n</code></pre> We will need to make methods for every movement, for the base we will need Stretch to move Forward, Backward, turn right 90 degrees or turn left 90 degrees. Keep in mind that rotations are measured in radians, if you wish to make adjustments, ensure to perform the necessary conversion.</p> <p><pre><code>def arm_front(robot):\n    robot.arm.move_by(0.1)\n    robot.push_command()\n    robot.arm.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef arm_back(robot):\n    robot.arm.move_by(-0.1)\n    robot.push_command()\n    robot.arm.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef lift_up(robot):\n    robot.lift.move_by(0.1)\n    robot.push_command()\n    robot.lift.wait_until_at_setpoint()\n    time.sleep(0.1)\n\ndef lift_down(robot):\n    robot.lift.move_by(-0.1)\n    robot.push_command()\n    robot.lift.wait_until_at_setpoint()\n    time.sleep(0.1)\n</code></pre> Now for the arm and the lift, this is different from the base, with the base we needed translations and rotations but these 2 are part from the prismatic joints so we just need the command move_by.</p> <p><pre><code>stretch_actions = {\"move_forward\" : \"Move the robot forward 0.2m\",\n                   \"move_backward\": \"Move the robot backward 0.2m\",\n                   \"turn_right\": \"Turn the robot 90 degrees to the clockwise\",\n                   \"turn_left\": \"Turn the robot 90 degrees to the counter clockwise\",\n                   \"arm_front\": \"Move the arm to the front 0.1m\",\n                   \"arm_back\": \"Move the arm to the back 0.1m\",\n                   \"lift_up\": \"Move the lift up 0.1m\",\n                   \"lift_down\": \"Move the lift down 0.1m\"}\n\nstretch_actions_fn = {\"move_forward\" : move_forward,\n                   \"move_backward\": move_backward,\n                   \"turn_right\": turn_right,\n                   \"turn_left\": turn_left,\n                   \"arm_front\": arm_front,\n                   \"arm_back\": arm_back,\n                   \"lift_up\": lift_up,\n                   \"lift_down\": lift_down}\n</code></pre> We will need the Large Language Model (LLM) to know what are the actions for each movement with the description, we want an explanation from the LLM about the movement made, that's why we want this description.</p> <p><pre><code>def chatter(input_text):\n    # Populate Assistance prompt\n    assistance_msg = \"Here is the description for each robot motion\"\n    for k in stretch_actions.keys():\n        assistance_msg = assistance_msg + f\"\\n - {k} : {stretch_actions[k]} \"\n\n    # Define System prompt (Personality of the system)\n    system_prompt = f\"Assume you are a mobile robot and you are able to receive a natural language instrunctions regarding the robot's movements. Based on undestanding the instructions return a sequence of discrete actions from this list {list(stretch_actions.keys())}. The only output must be in two parts. The first part should explain the sequence and second part should only be the list the actions seperated by comma. The first part you will explain the list of actions to follow and the reason behind it. The second one must be the list of movements that we are going to use, separate them only with the '[]'. This is not an explanation, it must be only the list of movements\"\n\n    clockwise \\n - arm_front: Move the arm to the front by 0.1m \\n - arm_back: Move the arm to the back by 0.1m \\n - lift_up: Move the lift up by 0.1m \\n - lift_down: Move the lift down by 0.1m\"\n    response = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"assistant\", \"content\": assistance_msg},\n        {\"role\": \"user\", \"content\": input_text},\n    ]\n    )\n    response_text = response.choices[0].message.content.strip().lower()\n    print(f\"CHATGPT RESPONSE: {response_text}\")\n    return response_text\n</code></pre> Let's commence with the chatter method. Here we initialize the LLM by specifying the system message and the assistant message. Precision in these initializations is crucial for the correct execution of our code. For the user role, we provide input via the terminal, to print the response, we utilize <code>response.choices[0].message.content</code> as outlined in the documentation. To ensure uniformity and ease of handling, we employ the <code>strip()</code> and <code>lower()</code> methods. The <code>strip()</code> function removes any trailing whitespaces, including spaces or tabs, simultaneously, <code>lower()</code> converts the response to lowercase, for instance, if the LLM outputs \"MOVE_FORWARD,\" we transform it into \"move_forward.\" This way we enhance consistency in handling the model's outputs.</p> <p><pre><code>def extract_action_sequence(response_text):\n    # In the response text find the list that starts and ends with []\n    start_index = response_text.find(\"[\")\n    end_index = response_text.find(\"]\", start_index)\n\n    if start_index != -1 and end_index != -1:\n        movements_str = response_text[start_index + 1:end_index]\n        # Split the comma-separated movements into a list. The .strip(\"'\\x22\") is used to remove both single and double quotes from the beginning and end of each movement.\n        formated_list = [movement.strip().strip(\"'\\x22\") for movement in movements_str.split(\",\")]\n        print(formated_list)\n        return formated_list\n    else:\n        print(\"List of movements not found in the response.\")\n        return []\n</code></pre> Going into the method of extraction, our goal is to identify the action list printed by the API. To achieve this, we search for square brackets. If the content is enclosed within these brackets at both the start and end, we recognize this as the desired list. Subsequently, we proceed to split and create a newly formatted list. If the model fails to locate the list of movements, it returns an empty list.</p> <p><pre><code>def execute_robot_motions(final_motion_list):\n    rb = robot.Robot()\n    rb.startup()\n    print(\"Starting to execute motions....\")\n    for motion_key in final_motion_list:\n        print(f\"Executing motion: {motion_key}\")\n        stretch_actions_fn[motion_key](rb)\n    print(\"Completed Executing motions...\")\n    rb.stop()\n</code></pre> To execute the robot movements we need to initialize it first, then it will execute these movements based on the provided list and then stops the robot, we need the startup and the stop for the <code>stretch_body</code> to work.</p> <p><pre><code>def stretch_chatter(input_text):\n    response = chatter(input_text)\n    motion_list = extract_action_sequence(response)\n    execute_robot_motions(motion_list)\n\n\nwhile True:\n    response = input(\"What motion can I do for you?\\n\")\n    stretch_chatter(response)\n</code></pre> The <code>stretch_chatter</code> will take the user input, this input is a natural language instruction regarding the robot movements and finaly we have the while loop, this ensures that the program keeps running, allowing us to input multiple requests without restarting the program.</p> <p>Now that you know how this works let's try it! Run your code in the terminal and try to input the next instructions: <pre><code>move forward 0.2m, turn right and move the lift up 2 times, turn left and move the arm front 1 time.\n</code></pre></p> <p> </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_chat/#potential-issues","title":"Potential Issues","text":"<ul> <li>Ocassionally, the model can skip one movement (normally the final ones), like those involving the arm or lift. For instance, if you instruct it to move up/down or front/back twice and then return to the starting position, there's a possibility it might execute only one backward movement instead of two. Unfortunately, there isn't a real solution to this, however, you can mitigate this type of error by providing more specific instructions.</li> <li>Sometimes the list of actions can appear different, what the code does is to look inside the response text and find the list that starts and ends with the square brackets \u2018[]\u2019 and then proceeds to make the movements, but it can occur a case like this: </li> </ul> <p>In this particular scenario, observe that the list of movements appears as [\u2018arm_front\u201d, if our code searches for the list of movements inside brackets it will not find anything, While this is a rare occurrence, it happened once. In such cases, the only thing to do is to stop the code and run it again. - As mentioned in the tutorial, issues may arise regarding the movement of Stretch when you want to make a geometric figure for example, it can take a wrong turn and keep moving, keep in mind this when you try moving Stretch around, try having it in an open area just in case.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/","title":"Tutorial: Tool Change","text":"<p>Many users will want to work with tools other than the default Stretch Gripper that ships with the robot. In this tutorial, you will learn how to configure the Stretch software interfaces to support other tools.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#changing-tool-interfaces-in-stretch-body","title":"Changing Tool Interfaces in Stretch Body","text":"<p>Stretch Body supports a plug-in-based architecture for tools. A tool is an extension of the EndOfArm class that supports additional degrees of freedom. </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#standard-tools","title":"Standard Tools","text":"<p>Stretch Body supports two tool interfaces by default: The ToolNone &amp; ToolStretchGripper. We will explore swapping between these default tools.</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#toolstretchgripper","title":"ToolStretchGripper","text":"<p>Stretch is configured to load the ToolStretchGripper interface by default. This tool is loaded according to the <code>robot.tool</code> parameter:</p> <p><pre><code>stretch_params.py | grep robot.tool\n</code></pre> <pre><code>stretch_body.robot_params.nominal_params     param.robot.tool               tool_stretch_gripper\n</code></pre></p> <p>We can interact with this tool from iPython <pre><code>ipython\n</code></pre></p> <pre><code>In [1]: import stretch_body.robot as robot\n\nIn [2]: r=robot.Robot()\n\nIn [3]: r.startup()\n\nIn [4]: r.end_of_arm\nOut[4]: &lt;stretch_body.end_of_arm_tools.ToolStretchGripper instance at 0x7f99109155a0&gt;\n\nIn [5]: r.end_of_arm.motors\nOut[5]: \n{'stretch_gripper': &lt;stretch_body.stretch_gripper.StretchGripper instance at 0x7f99109159b0&gt;,\n 'wrist_yaw': &lt;stretch_body.wrist_yaw.WristYaw instance at 0x7f9910915820&gt;}\n\nIn [6]: r.end_of_arm.stow()\n--------- Stowing Wrist Yaw ----\n--------- Stowing Gripper ----\nIn [7]: r.stop()\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#toolnone","title":"ToolNone","text":"<p>The ToolNone interface can be loaded when no tool is attached to the Wrist Yaw joint.  To switch to this interface, simply update the field in your <code>stretch_re1_user_params.yaml</code> to:</p> <pre><code>robot:\n  tool: tool_none\n</code></pre> <p>After updating the YAML we can interact with the ToolNone via iPython</p> <pre><code>In [1]: import stretch_body.robot as robot\n\nIn [2]: r=robot.Robot()\n\nIn [3]: r.startup()\n\nIn [4]: r.end_of_arm\nOut[4]: &lt;stretch_body.end_of_arm_tools.ToolNone instance at 0x7f245f786fa0&gt;\n\nIn [5]: r.end_of_arm.motors\nOut[5]: {'wrist_yaw': &lt;stretch_body.wrist_yaw.WristYaw instance at 0x7f245e69e410&gt;}\n\nIn [6]: r.end_of_arm.stow()\n--------- Stowing Wrist Yaw ----\nIn [7]: r.stop()\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#loading-tool-interfaces-from-stretch-tool-share","title":"Loading Tool Interfaces from Stretch Tool Share","text":"<p>The Stretch Tool Share is an open Git repository for non-standard Stretch tools. It hosts the CAD, URDF, and Python files needed to integrate these tools with your robot.</p> <p>To use Stretch Tool Share tools, first update your installation:</p> <pre><code>pip install -U hello-robot-stretch-tool-share\n</code></pre> <p>As an example, we see on the Tool Share that there is a tool, the ToolDryEraseToolHolderV1 which extends the EndOfArm class. To load this tool interface, modify your <code>stretch_user_params.yaml</code> to load the tool as before. We will also need to tell it where to find the tool's parameter file:</p> <pre><code>robot:\n  tool: tool_dry_erase_holder_v1\nparams:\n- stretch_tool_share.dry_erase_holder_v1.params\n</code></pre> <p>We can now interact with the tool in iPython:</p> <pre><code>In [1]: import stretch_body.robot as robot\n\nIn [2]: r=robot.Robot()\n\nIn [3]: r.startup()\n\nIn [4]: r.end_of_arm\nOut[4]: &lt;stretch_tool_share.dry_erase_holder_v1.tool.ToolDryEraseHolderV1 instance at 0x7f3b61c17f00&gt;\n\nIn [5]: r.end_of_arm.motors\nOut[5]: {'wrist_yaw': &lt;stretch_body.wrist_yaw.WristYaw instance at 0x7f3b61c59280&gt;}\n\nIn [6]: r.end_of_arm.stow()\n--------- Stowing Wrist Yaw ----\n</code></pre>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#changing-tool-interfaces-in-stretch-ros","title":"Changing Tool Interfaces in Stretch ROS","text":"<p>Next, we'll see how to change the ROS interface for a tool. Here we will continue with the ToolDryEraseHolderV1 example.  First, configure Stretch Body to use the tool as in the previous exercise.</p> <p>Next, ensure ROS is up to date:</p> <pre><code>cd ~/catkin_ws/src/stretch_ros/\ngit pull\n</code></pre> <p>To access the URDF data for the ToolDryEraseHolderV1 we'll need to clone the Tool Share repository:</p> <pre><code>cd ~/repos\ngit clone https://github.com/hello-robot/stretch_tool_share\n</code></pre> <p>Copy the tool's URDF data into the Stretch ROS repository:</p> <pre><code>cd ~/repos/stretch_tool_share/tool_share/dry_erase_holder_v1\ncp stretch_description/urdf/*.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf/\ncp stretch_description/meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes/\n</code></pre> <p>Now we will update the tool Xacro for Stretch. Open the file <code>~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro</code> in an editor. Comment out the current tool Xacro and include the Xacro for the dry-erase holder.</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;robot xmlns:xacro=\"http://www.ros.org/wiki/xacro\" name=\"stretch_description\"&gt;\n\n  &lt;!--&lt;xacro:include filename=\"stretch_gripper.xacro\" /&gt;--&gt;\n  &lt;xacro:include filename=\"stretch_dry_erase_marker.xacro\" /&gt;\n\n  &lt;xacro:include filename=\"stretch_main.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_aruco.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_d435i.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_laser_range_finder.xacro\" /&gt;\n  &lt;xacro:include filename=\"stretch_respeaker.xacro\" /&gt;\n&lt;/robot&gt;\n</code></pre> <p>Finally, we'll update the calibrated URDF to use this new tool:</p> <pre><code>cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf\ncp stretch.urdf stretch.urdf.bak\nrosrun stretch_calibration update_urdf_after_xacro_change.sh\n</code></pre> <p>Press Ctrl-C when the <code>rosrun</code> command terminates and you're ready to visualize the tool in RViz:</p> <pre><code>roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre> <p></p>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#advanced-topics","title":"Advanced Topics","text":""},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#understanding-how-the-tool-plug-in-works","title":"Understanding How the Tool Plug-In Works","text":"<p>For users looking to create their custom tools, it can be useful to understand how the tool plug-in architecture works. Here we will walk through the basics of the system for both Stretch Body and Stretch ROS</p>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#stretch-body","title":"Stretch Body","text":"<p>The Robot class expects an instance of the EndOfArm tool to be present. The EndOfArm tool is an extension of the DynamixelXChain class, which manages a chain of Dynamixel servos.</p> <p>A tool is defined via its parameters (either in user YAML or Python). For example, the ToolStretchGripper is defined in robot_params.py. These parameters tell the plug-in which DynamixelHelloXL430 instances to load and manage. Here we see:</p> <pre><code>\"tool_stretch_gripper\": {\n    'use_group_sync_read': 1,\n    'retry_on_comm_failure': 1,\n    'baud':115200,\n    'verbose':0,\n    'py_class_name': 'ToolStretchGripper',\n    'py_module_name': 'stretch_body.end_of_arm_tools',\n    'stow': {'stretch_gripper': 0, 'wrist_yaw': 3.4},\n    'devices': {\n        'stretch_gripper': {\n            'py_class_name': 'StretchGripper',\n            'py_module_name': 'stretch_body.stretch_gripper'\n        },\n        'wrist_yaw': {\n            'py_class_name': 'WristYaw',\n            'py_module_name': 'stretch_body.wrist_yaw'\n        }\n    }\n},\n</code></pre> <p>This dictionary defines a tool of the class ToolStretchGripper with two DynamixelHelloXL430 devices on its bus (StretchGripper and WristYaw). </p> <p>We see that the ToolStretchGripper class extends the EndOfArm class and provides its stowing behavior:</p> <pre><code>class ToolStretchGripper(EndOfArm):\n    def __init__(self, name='tool_stretch_gripper'):\n        EndOfArm.__init__(self,name)\n\n    def stow(self):\n        # Fold in wrist and gripper\n        print('--------- Stowing Wrist Yaw ----')\n        self.move_to('wrist_yaw', self.params['stow']['wrist_yaw'])\n        print('--------- Stowing Gripper ----')\n        self.move_to('stretch_gripper', self.params['stow']['stretch_gripper'])\n</code></pre> <p>For tools that are not a part of Stretch Body, such as from the Tool Share, you must include the tool parameters as well in your  <code>stretch_user_params.yaml</code>. A robot that supports many tools may have a user YAML that looks like:</p> <pre><code>params:\n- stretch_tool_share.usbcam_wrist_v1.params\n- stretch_tool_share.stretch_dex_wrist_beta.params\n- stretch_tool_share.dry_erase_holder_v1.params\nrobot:\n  tool: tool_dry_erase_holder_v1\n  #tool: tool_none\n  #tool: tool_stretch_gripper\n  #tool: tool_usbcam_wrist_v1\n  #tool: tool_stretch_dex_wrist_beta\n</code></pre> <p>Tip</p> <p>For a more complex implementation of a tool, we recommend reviewing the Stretch Dex Wrist implementation on the Stretch Tool Share. </p>"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#stretch-ros","title":"Stretch ROS","text":"<p>Stretch ROS also supports the tool plug-in architecture. Under ROS this is managed by extending the SimpleCommandGroup.</p> <p>More coming soon.</p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks."},{"location":"stretch-tutorials/stretch_body/jupyter/jupyter_getting_started/","title":"Jupyter Notebook","text":"<p>Jupyter is a free, open-source, interactive web tool known as a computational notebook, which researchers can use to combine software code, computational output, explanatory text and multimedia resources in a single document.</p>"},{"location":"stretch-tutorials/stretch_body/jupyter/jupyter_getting_started/#launch-a-jupyter-notebook-app-linux-macos","title":"Launch a Jupyter Notebook App (Linux, MacOS)","text":"<ul> <li>For Linux and MacOS systems, open a new terminal window.</li> <li>Enter the startup folder by typing <code>cd /some folder name</code></li> <li>Type <code>jupyter notebook</code> to launch the Jupyter Notebook App. The notebook interface will appear in a new browser window or tab.</li> </ul>"},{"location":"stretch-tutorials/stretch_body/jupyter/jupyter_getting_started/#launch-a-jupyter-notebook-app-windows","title":"Launch a Jupyter Notebook App (Windows)","text":"<p>Double-click on the Jupyter Notebook desktop launcher (icon shows [IPy]) to start the Jupyter Notebook App. The notebook interface will appear in a new browser window or tab. A secondary terminal window (used only for error logging and for shut down) will be also opened.</p>"},{"location":"stretch-tutorials/stretch_body/jupyter/jupyter_getting_started/#executing-a-notebook","title":"Executing a notebook","text":"<ul> <li>Launch the Jupyter Notebook App (see previous section).</li> <li>In the Notebook Dashboard navigate to find the notebook: clicking on its name will open it in a new browser tab.</li> <li>Click on the menu Help -&gt; User Interface Tour for an overview of the Jupyter Notebook App user interface.</li> <li>You can run the notebook document step-by-step (one cell a time) by pressing shift + enter.</li> <li>You can run the whole notebook in a single step by clicking on the menu Cell -&gt; Run All.</li> <li>To restart the kernel (i.e. the computational engine), click on the menu Kernel -&gt; Restart. This can be useful to start over a computation from scratch (e.g. variables are deleted, open files are closed, etc\u2026).</li> </ul>"},{"location":"stretch-tutorials/stretch_body/jupyter/jupyter_getting_started/#closing-a-notebook","title":"Closing a notebook","text":"<p>When a notebook is opened, its \u201ccomputational engine\u201d (called the kernel) is automatically started. Closing the notebook browser tab, will not shut down the kernel, instead the kernel will keep running until is explicitly shut down.</p> <p>To shut down a kernel, go to the associated notebook and click on menu File -&gt; Close and Halt. Alternatively, the Notebook Dashboard has a tab named Running that shows all the running notebooks (i.e. kernels) and allows shutting them down (by clicking on a Shutdown button).</p>"},{"location":"stretch-tutorials/stretch_body/jupyter/jupyter_getting_started/#shut-down-the-jupyter-notebook-app","title":"Shut down the Jupyter Notebook App","text":"<p>Closing the browser (or the tab) will not close the Jupyter Notebook App. To completely shut it down you need to close the associated terminal.</p>"},{"location":"stretch-tutorials/stretch_tool_share/","title":"Overview","text":"<p>We designed Stretch's hardware to be easily extended. You can make your own tool and attach it to the wrist to creatively expand what Stretch can do. Your tool can also use Dynamixel X-series servos from Robotis via the provided TTL bus.</p> <p>In this tutorial, we provide examples of some of the tools that we've created. We've released them with a permissive Apache 2.0 license, so you're free to use them as you wish. We hope they'll inspire you to create your own.</p> <p>We also include URDF and mesh files for many of the tools in their stretch_description folder. See the Stretch ROS documentation for guidance on integrating these tools into your robot model.</p> <p>We'd love it if you shared your creations with the community. We recommend you create a GitHub repository for your own tools and then post an announcement to the forum to let people know about it. </p> Tool Description Puller Attachment to pull drawers, handles or push buttons Dry Erase Holder Compliant attachment for drawing on white-boards DexWrist A 3-DoF upgrade for the standard gripper Updating URDF Updating the URDF after changing a tool"},{"location":"stretch-tutorials/stretch_tool_share/#licenses","title":"Licenses","text":"<p>The contents in this tutorial that represent parts of the Stretch robot, such as its head, arm, wrist, and default gripper, are covered by the CC BY-NC-SA 4.0 license. Please note that the Stretch robot and its default gripper are also covered by pending patents. Please see the robot license for details. </p> <p>Other contents in this tutorial created by Hello Robot Inc. that specifically pertain to the tools that attach to Stretch as accessories are covered by the Apache 2.0 license. Please see the tool license for details.</p> <p>The contents of this tutorial are intended for use with the Stretch mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/","title":"Dex to Standard Wrist","text":"<p>This tutorial will guide you through the steps of replacing a gripper with a Dex Wrist (3-DoF) for one with a Standard Wrist (1-DoF).</p>"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/#parts-and-tools-required","title":"Parts and Tools Required","text":"<p>Please note that this procedure does not require any additional parts or tools apart from the ones that were shipped with the robot:</p> <ul> <li>8 M2x6mm Torx FHCS bolts</li> <li>4 M2.5x4mm Torx FHCS bolts</li> <li>2 M2.5x8mm SHCS bolts</li> <li>T6 Torx wrench</li> <li>T8 Torx wrench</li> <li>2mm Hex key</li> </ul>"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/#removing-dex-wrist-gripper","title":"Removing Dex Wrist Gripper","text":"<p>Here we describe removing the Dex Wrist gripper. Please ensure that the robot is turned off before proceeding.</p> <p>First, inspect the parts and ensure that you have everything you need for the procedure.</p> <p></p> <p>Now, remove the cable clip by unscrewing the M2.5x8mm bolts and then unplug the Dynamixel cable out of the wrist pitch servo (pink).</p> <p></p> <p>Next, rotate the wrist yaw joint so that the wrist pitch servo body is accessible. Detach the pitch servo from the mounting bracket by unscrewing the four M2.5x4mm screws (C) with the T8 Torx wrench.</p> <p></p> <p>Slide the wrist module out horizontally so that the bearing unmates from its post.</p> <p></p> <p>Then, lower the wrist module vertically away from the mounting bracket.</p> <p></p> <p>Lastly, detach the wrist mount bracket (A) from the bottom of the tool plate by removing the M2x6mm bolts (B) using a T6 Torx wrench.</p> <p></p>"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/#attaching-standard-wrist-gripper","title":"Attaching Standard Wrist Gripper","text":"<p>Here we describe attaching the Standard Wrist gripper.</p> <p>First, note where the forward direction is on the wrist yaw tool plate. This is indicated by the additional alignment hole that is just outside the bolt pattern shown pointing down in the image.</p> <p></p> <p>Then, route the Dynamixel cable through the center of the standard gripper mounting bracket and install the bracket with the eight screws and T6 Torx wrench. Make sure the forward marking on the bracket matches the forward marking on the wrist yaw.</p> <p></p> <p>Now, affix the four screws, with the shorter two going to the servo side, to hold the gripper to the bracket. Lastly, route the Dynamixel cable through the back of the gripper and plug it securely into the servo.</p> <p></p>"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/#software-instructions","title":"Software Instructions","text":"<p>Once the hardware has been replaced, it's time to make the software changes for Stretch to recognize the Standart Wrist gripper. Turn on the robot and follow the instructions below.</p> <p>To revert the changes in stretch_configuration_params.yaml, first download the dex_to_standard_configure_params.py script using: <pre><code>cd ~/Desktop\nwget https://raw.githubusercontent.com/hello-robot/stretch_tutorials/master/stretch_tool_share/dex_to_standard_configure_params.py\n</code></pre></p> <p>Then, change execution permissions and execute it in a terminal as below: </p> <pre><code>chmod +x dex_to_standard_configure_params.py\npython3 dex_to_standard_configure_params.py\n</code></pre> <p>Ensure that the changes were written correctly to stretch_params: <pre><code>stretch_params.py | grep robot.tool\n</code></pre></p> <p>This should return the tool as <code>tool_stretch_gripper</code>.</p> <p>Next, to ensure the correct gripper is recognized by ROS, we need to update the URDF. For this, first open the stretch_description.xacro file like below.</p> <pre><code>cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf\ngedit stretch_description.xacro\n</code></pre> <p>Then, replace the contents of the file with that of the default stretch_description.xacro by copying and pasting the contents from the file in the link to the open file.</p> <p>Lastly, to generate the updated URDF, execute the following commands in a terminal.</p> <pre><code>rosrun stretch_calibration update_urdf_after_xacro_change.sh\ncd ~/catkin_ws/src/stretch_ros/stretch_description/urdf\n./export_urdf.sh\n</code></pre> <p>You can ensure that the gripper is functional by homing the Dynamixel servos with the following commands:</p> <pre><code>stretch_gripper_home.py\n</code></pre> <pre><code>stretch_wrist_yaw_home.py\n</code></pre> <p>If you encounter any issues, please contact support.</p>"},{"location":"stretch-tutorials/stretch_tool_share/dexwrist/","title":"Dex Wrist","text":""},{"location":"stretch-tutorials/stretch_tool_share/dexwrist/#stretch-dex-wrist","title":"Stretch Dex Wrist","text":"<p>Created by: Hello Robot Inc</p> <p>The Stretch Dex Wrist is commercially available from Hello Robot. The following hardware guides are available:</p> <ul> <li>Stretch RE1 DexWrist Hardware Guide</li> <li>Stretch 2 DexWrist Hardware Guide</li> </ul> <p>Additional resources available here include:</p> <ul> <li>Gazebo Support</li> <li>URDF</li> </ul>"},{"location":"stretch-tutorials/stretch_tool_share/dry_erase_holder/","title":"Dry Erase Holder","text":""},{"location":"stretch-tutorials/stretch_tool_share/dry_erase_holder/#dry-erase-holder","title":"Dry Erase Holder","text":"<p>Created by: Hello Robot Inc</p> <p>This tool allows Stretch to hold a dry-erase marker.  It is spring-loaded, allowing for compliant interaction between the marker and a whiteboard. </p> <p>The tool can be integrated into your robot URDF by integrating its stretch_description as described in the Stretch ROS documentation.</p> <p></p> <p></p>"},{"location":"stretch-tutorials/stretch_tool_share/dry_erase_holder/#parts-list","title":"Parts List","text":"Item Qty Vendor Expo Dry Erase 1 Amazon M5x50mm Hex Head Bolt 1 McMaster-Carr M5 Nut 2 McMaster-Carr wrist_end_cap_5mm 1 PLA 3D Printer dry_erase_bushing_block 1 PLA 3D Printer Size 30 Rubber Band 2 McMaster-Carr 3/4\" Shaft Collar 1 McMaster-Carr"},{"location":"stretch-tutorials/stretch_tool_share/dry_erase_holder/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D assembly</p> <p></p> <ol> <li>Install the bolt into the dry_erase_bushing block and secure from below with an M5 nut.</li> <li>Attach the dry-erase bushing block to the tool plate, securing from below with the wrist_end_cap_5mm and an M5 nut. Orient the block so the marker points forward. </li> <li>Attach the shaft collar to your dry-erase marker, approximately 8mm from the back of the marker.</li> <li>Slide the marker into the bushing block. Loop a rubber band around the back of the marker and over to one of the pegs on the side of the bushing block. Repeat with the other peg.</li> <li>The marker should now easily spring back when pushed against it. You're ready to write!</li> </ol>"},{"location":"stretch-tutorials/stretch_tool_share/gripper_puller/","title":"Gripper Puller","text":""},{"location":"stretch-tutorials/stretch_tool_share/gripper_puller/#puller","title":"Puller","text":"<p>Created by: Hello Robot Inc</p> <p>This is a simple puller attachment for the Stretch Compliant Gripper. We've used it to pull open many common drawers, cabinet doors, and even a mini-fridge door. You can also use it to push things closed. You can think of it as a circular hook used to pull things or a finger used to push things.</p> <p>It attaches to the 6-32 stud on the side of the gripper. By turning the gripper sideways during manipulation, the hook can drop over the drawer handle, allowing the arm to retract and pull the door open. </p> <p> </p>"},{"location":"stretch-tutorials/stretch_tool_share/gripper_puller/#parts-list","title":"Parts List","text":"Item Qty Vendor 6-32 x 0.5\" BHCS 1 McMaster-Carr 6-32 x 1\" aluminum threaded standoff 1 McMaster-Carr Puller_V1.STL 1 PLA 3D printer"},{"location":"stretch-tutorials/stretch_tool_share/gripper_puller/#assembly-instructions","title":"Assembly instructions","text":"<p>View 3D assembly</p> <p></p> <ol> <li>Screw the standoff onto the gripper's threaded post. Secure tightly and add a drop of light-duty Loctite if desired.</li> <li>Attach the plastic pull to the standoff using the BHCS. </li> </ol>"},{"location":"stretch-tutorials/stretch_tool_share/gripper_removal/","title":"Gripper","text":"<p>The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism is complex and non-linear relative to the motor position.  As shown, it includes mounting features on one side to allow for the attachment of simple rigid tools such as hooks and pullers. </p> <p></p> Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 <p>The attachment features are spaced at 9 mm.</p> <p>The weight of the Stretch Compliant Gripper is 240 g.</p>"},{"location":"stretch-tutorials/stretch_tool_share/gripper_removal/#gripper-removal","title":"Gripper Removal","text":"<p>Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse.</p> <ol> <li>Unplug the Dynamixel cable from the back of the gripper. </li> <li>Remove the 4 screws holding the gripper to the bracket.</li> <li>Remove the gripper from the mounting bracket</li> <li>Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate.</li> </ol> <p></p> <p></p>"},{"location":"stretch-tutorials/stretch_tool_share/updating_urdf/","title":"Updating URDF","text":""},{"location":"stretch-tutorials/stretch_tool_share/updating_urdf/#changing-the-tool","title":"Changing the Tool","text":"<p>If you wish to remove the default gripper and add a different tool, you will typically edit /stretch_description/urdf/stretch_description.xacro. Specifically, you will replace the following line to include the XACRO for the new tool and then follow directions within stretch_ros/stretch_calibration to generate a new calibrated URDF file (stretch.urdf) that includes the new tool.</p> <p><code>&lt;xacro:include filename=\"stretch_gripper.xacro\" /&gt;</code></p> <p>As an example, we provide the <code>stretch_dry_erase_marker.xacro</code> file and its dependent mesh files with stretch_ros. </p> <p>Some of the tools found in the Stretch Body Tool Share include URDF data. To integrate these tools into the URDF for your Stretch</p> <pre><code>cd ~/repos\ngit clone https://github.com/hello-robot/stretch_tool_share\ncd stretch_tool_share/&lt;tool name&gt;\ncp stretch_description/urdf/* ~/catkin_ws/src/stretch_ros/stretch_description/urdf/\ncp stretch_description/meshes/* ~/catkin_ws/src/stretch_ros/stretch_description/meshes/\n</code></pre> <p>Next add the XACRO file for the particular tool to <code>/stretch_description/urdf/stretch_description.xacro</code>. Then you can generate and preview the uncalibrated URDF:</p> <pre><code>cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf\ncp stretch.urdf stretch.urdf.bak\nrosrun stretch_calibration update_urdf_after_xacro_change.sh\n</code></pre> <p>Now visualize the new tool</p> <pre><code>roslaunch stretch_calibration simple_test_head_calibration.launch\n</code></pre>"},{"location":"stretch-web-interface/","title":"Web Interface Deprecated","text":"<p>This repo is DEPRECATED. It has been replaced with a fork called the Stretch Teleop Interface. The original README is saved below.</p>"},{"location":"stretch-web-interface/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>History</li> <li>The Robot and the Operator both use Web Browsers</li> <li>The Web Server</li> <li>Getting Started</li> <li>Installation</li> <li>Quick Start</li> <li>Slower Start</li> <li>Setting Up a Server</li> <li>Credentials for Robots and Operators</li> <li>Running the Server on the Robot's Onboard Computer</li> <li>Running the Server on Amazon Lightsail</li> <li>Licenses</li> </ul>"},{"location":"stretch-web-interface/#overview","title":"Overview","text":"<p>This repository holds code that enables a person (the operator) to remotely teleoperate a Stretch RE1 (the robot) through a recent Chrome/Chromium web browser on an Android mobile phone, laptop, or desktop. The Stretch RE1 is a mobile manipulator from Hello Robot Inc.</p> <p>WARNING: This prototype code has been useful to the community, but is not well tested. There are also security issues, especially if you use the default credentials. Use this code at your own risk. </p> <p></p>"},{"location":"stretch-web-interface/#history","title":"History","text":"<p>When we started Hello Robot Inc. back in 2017, part of our goal was to create a robot that could be intuitively teleoperated from afar. We took an iterative approach, building a series of 7 prototype robots before the Stretch RE1 that we sell today. In conjunction with these prototypes, we developed a series of web interfaces, so that we could control our robots via a web browser and test remote teleoperation. While we eventually deemphasized this aspect of the robot, we thought it could be useful to the community. With this goal in mind, we ported parts of our old web interface code to the Stretch RE1 and made them available in this repository back in June of 2020.</p> <p>Since then, we've been gratified to learn of others working with this code. For example, The Human Centered Robotics Lab at the University Washington has made impressive improvements to the code, which can be found in their repository. We've also learned that the The Human Factors and Aging Laboratory at the University of Illinois at Urbana-Champaign has explored this interface as part of their impressive research to improve the lives of older adults.</p> <p></p>"},{"location":"stretch-web-interface/#the-robot-and-the-operator-both-use-web-browsers","title":"The Robot and the Operator both use Web Browsers","text":"<p>This web interface works via Web Real-Time Communication (WebRTC). Code runs in a browser on the robot, in a browser on the operator's device (e.g., a mobile phone), and on a server. This is analogous to the robot and the operator video conferencing with one another, although they communicate via realtime data in addition to audio and video. By using web browsers, the robot and the operator make use of well-tested high-performance implementations of WebRTC. This symmetry also simplifies development, since a developer can use the same browser-based developer tools on both sides of the communication. The robot's browser and the operator's browser first login to the server, which helps connect them and provides them with the interface code.</p> <p>The robot\u2019s browser uses rosbridge to connect with ROS on the robot. Rosbridge translates JSON from the robot\u2019s browser into ROS communications and vice versa. The JavaScript code used by the robot\u2019s browser to connect with ROS can be found in ros_connect.js under the robot directory, which holds files made available to the robot's browser. </p> <p>With puppeteer the robot can automatically launch and login to its browser. For example, start_robot_browers.js uses puppeteer to launch the robot's browser and login.</p> <p>While the robot\u2019s browser has access to most of the robot via ROS, the operator\u2019s browser can only access the robot indirectly through the robot\u2019s browser. The robotic commands available to the operator\u2019s browser can be found in commands.js under the shared directory, which holds files available to both the operator's browser and the robot's browser. The operator's browser also has access to files in the operator directory.</p> <p></p>"},{"location":"stretch-web-interface/#the-web-server","title":"The Web Server","text":"<p>In the example below, the server runs on the robot. In a production environment, you would use an external server, instead of the robot, to handle things like connecting robots and operators behind firewalls. In a later section, we provide an example of an external server that uses Amazon Lightsail. When used on a production server with proper certificates, this code uses HTTPS and avoids scary messages. </p> <p>The web server uses the Express web framework with Pug templates. The server provides a WebRTC signaling service using socket.io. It uses Redis to store sessions. </p> <p>passport provides authentication for the robot and the operator. mongoose and a MongoDB database store credentials for robots and operators. The stretch_web_interface repository comes with default MongoDB content found at ./mongodb/ for testing behind a firewall. These default contents come with multiple robot and operator accounts. Make sure not to use these default database contents on a deployed system! </p> <p>By default, send_recv_av.js uses a free STUN server provided by Google. The Amazon Lightsail example below uses coturn as a STUN and TURN server.   </p> <p></p>"},{"location":"stretch-web-interface/#getting-started","title":"Getting Started","text":""},{"location":"stretch-web-interface/#installation","title":"Installation","text":"<p>These installation instructions describe how to install both the server and relevant ROS code on the onboard computer of a Stretch RE1 robot. This is suitable for use on a trusted and secure local area network (LAN) behind a strong firewall. </p> <p>The web interface depends on stretch_ros, which is used to control the robot. You should first make sure it is up-to-date and working properly on the robot.</p> <p>Clone the stretch_web_interface repository to ~/catkin_ws/src/ on the robot. </p> <pre><code>cd ~/catkin_ws/src/\ngit clone https://github.com/hello-robot/stretch_web_interface.git\n</code></pre> <p>Run catkin_make.</p> <pre><code>cd ~/catkin_ws/\ncatkin_make\nrospack profile\n</code></pre> <p>Run the installation script. </p> <pre><code>cd ~/catkin_ws/src/stretch_web_interface/bash_scripts/\n./web_interface_installation.sh\n</code></pre> <p>WARNING: The script uninstalls tornado using pip to avoid a rosbridge websocket immediate disconnection issue. This could break other software on your robot.</p> <p></p>"},{"location":"stretch-web-interface/#quick-start","title":"Quick Start","text":"<p>When running on a trusted and secure local area network (LAN) behind a strong firewall, you can use the following insecure method to more conveniently start the system.</p>"},{"location":"stretch-web-interface/#calibrate-the-robot","title":"Calibrate the Robot","text":"<p>First, make sure the robot is calibrated. For example you can run the following command.</p> <pre><code>stretch_robot_home.py\n</code></pre>"},{"location":"stretch-web-interface/#start-ros","title":"Start ROS","text":"<p>Next, in a terminal, run the following command to start ROS. This will start ROS nodes on the robot for the D435i camera, the driver for Stretch RE1, and rosbridge. Rosbridge connects JavaScript running in the robot's browser to ROS using JSON. </p> <pre><code>roslaunch stretch_web_interface web_interface.launch\n</code></pre>"},{"location":"stretch-web-interface/#start-the-web-server-and-the-robots-browser","title":"Start the Web Server and the Robot's Browser","text":"<p>In another terminal, run the following command to start the web server on the robot, launch the robot's browser, and log the robot into the browser. The convenience script calls start_robot_browser.js, which uses puppeteer to log the robot into its browser.</p> <pre><code>roscd stretch_web_interface/bash_scripts/\n./start_web_server_and_robot_browser.sh \n</code></pre> <p>Typically, this script can be exited with Ctrl+C and then restarted without issue.</p> <p>WARNING: start_robot_browser.js contains the default robot credentials in plain text! This is only appropriate for simple testing on a local network behind a firewall. The username and password are public on the Internet, so this is not secure! Deployment would require new credentials and security measures.</p>"},{"location":"stretch-web-interface/#start-the-operators-browser","title":"Start the Operator's Browser","text":"<p>You will now login to a browser as the operator and connect to the robot. You can use a Chrome browser on a recent Android mobile phone or a recent Chrome/Chromium browser on a laptop or desktop. </p> <p>Open the browser goto the robot\u2019s IP address. You can use <code>ifconfig</code> on the robot to determine its IP address.</p> <p>Select \"Advanced\" and then click on \"Proceed to localhost (unsafe)\".</p> <p></p> <p></p> <p></p> <p>Click on \"Login\" and use the following username and password.</p> <pre><code>username:\no1\n\npassword\nxXTgfdH8\n</code></pre> <p>WARNING: This is a default operator account provided for simple testing. Since this username and password are public on the Internet, this is not secure. You should only use this behind a firewall during development and testing. Deployment would require new credentials and security measures.</p> <p></p> <p></p> <p>You should now see a screen like the following. Click on \"no robot connected\" and select the robot \"r1\" to connect to it. </p> <p> </p> <p>You should now see video from the robot on your mobile phone or other device. Click in the designated regions to command the robot to move. You can also click on \"Drive\", \"Arm\" down, \"Arm\" up, \"Hand\" and \"Look\" to move different joints on the robot. </p> <p></p> <p></p>"},{"location":"stretch-web-interface/#slower-start","title":"Slower Start","text":"<p>The following steps describe how to manually start the web server and the robot's browser on the robot, instead of using the convenience script described above. </p>"},{"location":"stretch-web-interface/#calibrate-the-robot_1","title":"Calibrate the Robot","text":"<p>First, make sure the robot is calibrated. For example you can run the following command.</p> <pre><code>stretch_robot_home.py\n</code></pre>"},{"location":"stretch-web-interface/#start-ros_1","title":"Start ROS","text":"<p>Next, in a terminal, run the following command to start the ROS side of things. This will start ROS nodes on the robot for the D435i camera, the driver for Stretch RE1, and rosbridge. Rosbridge connects JavaScript running in the robot's browser to ROS using JSON. </p> <pre><code>roslaunch stretch_web_interface web_interface.launch\n</code></pre>"},{"location":"stretch-web-interface/#start-the-web-server","title":"Start the Web Server","text":"<p>In another terminal, run the following command to start the web server on the robot. </p> <pre><code>roscd stretch_web_interface/bash_scripts/\n./start_desktop_dev_env.sh \n</code></pre>"},{"location":"stretch-web-interface/#start-the-robots-browser","title":"Start the Robot's Browser","text":"<p>Open a Chromium browser on the robot and go to localhost. Select \"Advanced\" and then click on \"Proceed to localhost (unsafe)\".</p> <p></p> <p>Click on \"Login\" and use the following username and password.</p> <pre><code>username:\nr1\n\npassword\nNQUeUb98\n</code></pre> <p>WARNING: This is a default robot account provided for simple testing. Since this username and password are public on the Internet, this is not secure. You should only use this behind a firewall during development and testing. Deployment would require new credentials and security measures.</p> <p></p> <p></p> <p>You should now see video from the robot's camera in the browser window. </p> <p></p>"},{"location":"stretch-web-interface/#start-the-operators-browser_1","title":"Start the Operator's Browser","text":"<p>Please see the instructions above.</p> <p></p>"},{"location":"stretch-web-interface/#setting-up-a-server","title":"Setting Up a Server","text":"<p>The server for the web interface typically runs on the robot's onboard computer or on a remote machine connected to the Internet. </p> <p></p>"},{"location":"stretch-web-interface/#credentials-for-robots-and-operators","title":"Credentials for Robots and Operators","text":"<p>Credentials for robots and operators are stored on the server using MongoDB. </p>"},{"location":"stretch-web-interface/#viewing-and-editing-credentials","title":"Viewing and Editing Credentials","text":"<p>On the server, you can view and edit the credentials using <code>mongodb-compass</code>, which is installed by default. First, use the following command in a terminal to start the application.</p> <pre><code>mongodb-compass\n</code></pre> <p>Next, use \"Connect to Host\" by typing <code>localhost</code> in the Hostname area at the top of the window and then clicking the green \"CONNECT\" button at the bottom right of the window. This should show you various databases. The <code>node-auth</code> database holds the web interface credentials. </p> <p>Clicking on <code>node-auth</code> will show a collection named <code>users</code>.</p> <p>Clicking on <code>users</code> will show the current credentials.</p> <p>If you've only used the default development credentials in this repository, you should see entries for the following: three robots with the usernames r1, r2, and r3; three operators with the usernames o1, o2, and o3; and an administrator with the username admin. Each entry consists of encrypted password information (i.e., salt and hash), a username, a name, a role, a date, and a Boolean indicating whether or not the user has been approved. Without approval, the user should be denied access. The role indicates whether the entry is for a robot or an operator. You can click on the image below to see what this should look like.</p> <p></p>"},{"location":"stretch-web-interface/#creating-new-credentials","title":"Creating New Credentials","text":"<p>First, start the server. Next, go to the web page and click <code>register</code>. Now enter a username and a password. This process creates a new user entry in MongoDB. </p> <p>You can now follow the instructions for viewing credentials above to view the new account you just created. In order for this account to function, you will need to edit the role to be <code>operator</code> or <code>robot</code> and edit approved to be <code>true</code>. You can do this by clicking on the elements with <code>mongodb-compass</code>.</p> <p>Prior to testing anything on the Internet, you should delete all of the default credentials. The default credentials are solely for development on a secure local network behind a firewall.</p>"},{"location":"stretch-web-interface/#backing-up-and-restoring-credentials","title":"Backing Up and Restoring Credentials","text":"<p>On the server, you can backup credentials using a command like the following in a terminal. You should change <code>./</code> to match the directory into which you want to saved the backup directory. </p> <p><code>mongodump --db node-auth --out ./</code></p> <p>You can restore backed up credentials using the following command in a terminal. You'll need to change <code>./</code> to match the directory that holds the backup directory.</p> <p><code>mongorestore -d node-auth ./node-auth/user.bson</code></p> <p></p>"},{"location":"stretch-web-interface/#running-the-server-on-the-robots-onboard-computer","title":"Running the Server on the Robot's Onboard Computer","text":"<p>Running the server on the robot is useful when the robot and the operator are both on the same local area network (LAN). For example, a person with disabilities might operate the robot in their home, or you might be developing new teleoperation code. In these situations, the robot, the operator's browser, and the server should all be behind a strong firewall, reducing security concerns. </p> <p></p>"},{"location":"stretch-web-interface/#running-the-server-on-amazon-lightsail","title":"Running the Server on Amazon Lightsail","text":"<p>Running the server on a remote machine can be useful when the robot and the operator are on separate LANs connected by the Internet. This can enable a person to operate the robot from across the world. In this section, we'll provide an example of setting up the server to run on an Amazon Lightsail instance. This is not a hardened server and is only intended to serve as a helpful example. It likely has significant security shortcomings and is very much a prototype. Use at your own risk. </p> <p>One of the challenges for remote teleoperation is that browsers on different LANs can have difficulty connecting with one another. Peer-to-peer communication may not be achievable due to firewalls and other methods used to help secure networks. For example, home networks, university networks, and corporate networks can all have complex configurations that interfere with peer-to-peer communication. Running the server on a remote machine connected to the Internet helps the robot's browser and the operator's browser connect to one another using standard methods developed for WebRTC video conferencing over the Internet. The server performs a variety of roles, including the following: restricting access to authorized robots and operators; helping operators select from available robots; WebRTC signaling, Session Traversal Utilities for Network Address Translation (STUN), and Traversal Using Relays around Network Address Translation (TURN). Notably, when direct peer-to-peer connectivity fails, TURN relays video, audio, and data between the robot's browser and the operator's browser. Relaying data is robust to networking challenges, but can incur charges due to data usage.</p>"},{"location":"stretch-web-interface/#amazon-lightsail-setup-in-brief","title":"Amazon Lightsail Setup in Brief","text":"<p>This section describes the steps we used to create an Amazon Lightsail instance that runs the server for the web interface.</p> <ul> <li>Obtain a domain name to use for your server.</li> <li>Create a new Amazon Lightsail instance.</li> <li>We used an OS only instance with Ubuntu 20.04, 512 MB RAM, 1 vCPU, 20 GB SSD.</li> <li>Create a static IP address and attach it to your instance.</li> <li>Connect your new instance to SSH, so that you can access it. </li> <li>A command like the following can then be used to login to your instance: <code>ssh -i /path/to/private-key.pem username@public-ip-address</code>.</li> <li>While logged into your instance. </li> <li>Run <code>sudo apt-get update</code> to avoid installation issues.</li> <li>Install the <code>net-tools</code> package, which will be used later. You might also want to install your preferred text editor, such as <code>emacs</code>. <ul> <li><code>sudo apt install net-tools</code></li> </ul> </li> <li>Configure Git.<ul> <li><code>git config --global user.name \"FIRST_NAME LAST_NAME\"</code></li> <li><code>git config --global user.email \"MY_NAME@example.com\"</code></li> </ul> </li> <li>Clone this GitHub repository.<ul> <li><code>cd</code></li> <li><code>mkdir repos</code></li> <li><code>git clone https://github.com/hello-robot/stretch_web_interface.git</code></li> </ul> </li> <li>Use certbot from Let's Encrypt to obtain certificates so that your server can use Hypertext Transfer Protocol Secure (HTTPS). HTTPS is required to fully utilize WebRTC. <ul> <li>You will need to first connect your domain name to the static IP address used by your instance. </li> <li>Follow certbot installation instructions for Ubuntu 20.04.</li> </ul> </li> <li>Run the teleoperation server installation script<ul> <li><code>cd ~/repos/stretch_web_interface/bash_scripts/</code></li> <li><code>./web_server_installation.sh</code></li> </ul> </li> <li>Initialize the database with secure credentials for at least one robot and one operator. For example, you can do the following. </li> <li>Create and export credentials from MongoDB by running a server on your robot. </li> <li>Use <code>scp</code> to copy the exported credentials database from your robot's computer to your Lightsail instance by running a command like <code>scp -ri ./LightsailDefaultKey-us-east-2.pem ./mongodb_credentials ubuntu@public-ip-address:./</code> on your robot's computer.</li> <li>On your Lightsail instance, restore the credentials database with a command like <code>mongorestore -d node-auth ./mongodb_credentials/node-auth/users.bson</code></li> <li>While logged into your instance.</li> <li>Edit the coturn configuration file.<ul> <li>Find your instance's private IP address by running <code>ifconfig -a</code> and looking at the <code>inet</code> IP address. </li> <li>Confirm your instance's public IP address and your domain name by running <code>ping YOUR-DOMAIN-NAME</code> and looking at the IP address. </li> <li>Add the following lines at appropriate locations in <code>/etc/turnserver.conf</code>.</li> <li><code>listening-ip=PRIVATE-IP-ADDRESS</code></li> <li><code>relay-ip=PRIVATE-IP-ADDRESS</code></li> <li><code>external-ip=PUBLIC-IP-ADDRESS</code></li> <li><code>Verbose</code></li> <li><code>lt-cred-mech</code></li> <li><code>pkey=/etc/letsencrypt/live/YOUR-DOMAIN-NAME/privkey.pem</code></li> <li><code>cert=/etc/letsencrypt/live/YOUR-DOMAIN-NAME/cert.pem</code></li> <li><code>no-multicast-peers</code></li> <li><code>secure-stun</code></li> <li><code>mobility</code></li> <li><code>realm=YOUR-DOMAIN-NAME</code></li> </ul> </li> <li>Create TURN server accounts and credentials.<ul> <li>Create an administrator account.</li> <li><code>sudo turnadmin -A -u ADMIN-NAME -p ADMIN-PASSWORD</code></li> <li>Create a TURN user. In the next step, you will add these credentials to <code>./stretch_web_interface/shared/send_recv_av.js</code>.</li> <li><code>sudo turnadmin -a -u TURN-USER-NAME -r YOUR-DOMAIN-NAME -p TURN-USER-PASSWORD</code></li> <li>Open <code>./stretch_web_interface/shared/send_recv_av.js</code> in an editor.</li> <li>Comment out the free STUN server.</li> <li>Uncomment the STUN and TURN servers and fill in the values using your domain name and the credentials you just created (i.e., YOUR-DOMAIN-NAME, TURN-USER-NAME, and TURN-USER-PASSWORD). <ul> <li>The relevant code will look similar to <code>var pcConfig = { iceServers: [ {urls: \"stun:YOUR-DOMAIN-NAME\", username \"TURN-USER-NAME\", credentials: \"TURN-USER-PASSWORD}, {urls: \"turn:YOUR-DOMAIN-NAME\", username \"TURN-USER-NAME\", credentials: \"TURN-USER-PASSWORD}]};</code></li> </ul> </li> </ul> </li> <li>Open the following ports for your Amazon Lightsail instance. These are standard ports for HTTPS, STUN, and TURN. </li> <li><code>HTTPS TCP 443</code></li> <li><code>Custom TCP 3478</code></li> <li><code>Custom TCP 5349</code></li> <li><code>Custom UDP 3478</code></li> <li><code>Custom UDP 5349</code></li> <li>Reboot your instance.</li> <li>Login to your instance and run the following commands to start the server.</li> <li><code>cd ~/repos/stretch_web_interface/bash_scripts/</code></li> <li><code>./start_server_production_env.sh</code></li> <li>Your server should now be running and you can test it by taking the following steps.</li> <li>Turn on and calibrate your robot.</li> <li>Use your robot's Chromium browser to visit your server's domain and login with the robot's credentials that you created. </li> <li>Open a Chrome or Chromium browser of your own on another computer or recent Android phone. Visit your server's domain and login with the operator credentials you created. If you want to test communication between distinct networks, you could turn off your phone's Wi-Fi and then either use your phone or tether to your phone to connect from the mobile phone network to your robot. Please note that this has only been tested with the Chrome browser on recent Android phones. </li> <li>Once you've logged in as an operator, you should be able to select your robot from the drop down list and begin controlling it. </li> <li>After trying it out, be sure to shutdown your Amazon Lightsail instance. It is not hardened and likely has security vulnerabilities. It would be risky to leave it on over a significant length of time.</li> </ul> <p></p>"},{"location":"stretch-web-interface/#licenses","title":"Licenses","text":"<p>This software is intended for use with S T R E T C H (TM) RESEARCH EDITION, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc.</p> <p>For license details for this repository, see the LICENSE files, including TUTORIAL_LICENSE.md, WEBRTC_PROJECT_LICENSE.md, and LICENSE.md. Some other sources and licenses are described by comments found within the code.</p> <p>The Apache 2.0 license applies to all code written by Hello Robot Inc. contained within this repository. We have attempted to note where code was derived from other sources and the governing licenses. </p>  All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending."},{"location":"stretch-web-interface/LICENSE/","title":"LICENSE","text":"<p>The following license applies to the contents of this directory created by Hello Robot Inc. (the \"Contents\"), but does not cover materials from other sources. This software is intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc.</p> <p>Copyright 2020 Hello Robot Inc.</p> <p>The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.</p>"},{"location":"stretch-web-interface/TUTORIAL_LICENSE/","title":"TUTORIAL LICENSE","text":"<p>Some of the code within this repository is derived from tutorial code found via  the following links. This code relates to using mongoose, passport and express.  The original code was released under the MIT License described below.</p> <p>https://github.com/didinj/node-express-passport-mongoose-auth https://www.djamware.com/post/58bd823080aca7585c808ebf/nodejs-expressjs-mongoosejs-and-passportjs-authentication</p> <p>================</p> <p>MIT License</p> <p>Copyright (c) 2017 Didin Jamaludin</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"stretch-web-interface/WEBRTC_PROJECT_LICENSE/","title":"WEBRTC PROJECT LICENSE","text":"<p>The following license covers the original code from which some of the web  interface code was derived (e.g., operator_acquire_av.js, robot_acquire_av.js).  The original code was released in the following respository, which contains WebRTC example code.</p> <p>https://github.com/webrtc/samples</p> <p>======================================</p> <p>Copyright (c) 2014, The WebRTC project authors. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ul> <li> <p>Redistributions of source code must retain the above copyright     notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright     notice, this list of conditions and the following disclaimer in     the documentation and/or other materials provided with the     distribution.</p> </li> <li> <p>Neither the name of Google nor the names of its contributors may     be used to endorse or promote products derived from this software     without specific prior written permission.</p> </li> </ul> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"}]}