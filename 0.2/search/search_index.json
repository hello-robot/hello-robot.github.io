{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome This is your jumping off point for Stretch tutorials, hardware manuals, and APIs. Please contact us at support@hello-robot.com if you don't find what you're looking for. We wish you success in your development journey with Stretch! New to Stretch? We recommend you follow the Getting to Know Stretch Tutorials . Please take the time to learn safe and best practices in operating Stretch. The Stretch robot can potentially be dangerous if used without caution. All users must carefully read the Stretch Safety Guide prior to using the robot. Where to Find Things All of the documentation is accessible and searchable via the navigation menu on this site. Alternatively, you can view the markdown hosted on at the Hello Robot GitHub portal . In addition, it is worth spending some time on the following sites: Resource Description Stretch Community Repository for community shared code Stretch Forum Discourse User Forum Version This is version 0.2 of the Stretch User Documentation. It is written with the following system configuration in mind: Resource Description Model Stretch RE1 or Stretch 2 OS Ubuntu 20.04 ROS Noetic and Melodic Python Python3 Stretch Body '>=0.4 You can access prior documentation suitable for older configurations (eg Ubuntu 18.04, ROS Melodic) here . If you are on an older version and would like to upgrade the robot, follow the Ubuntu 20.04 upgrade guide or contact support. License This documentation is only to be used for an authentic Stretch robot produced and sold by Hello Robot Inc. All Hello Robot documentation and related materials are released under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Home"},{"location":"#welcome","text":"This is your jumping off point for Stretch tutorials, hardware manuals, and APIs. Please contact us at support@hello-robot.com if you don't find what you're looking for. We wish you success in your development journey with Stretch!","title":"Welcome"},{"location":"#new-to-stretch","text":"We recommend you follow the Getting to Know Stretch Tutorials . Please take the time to learn safe and best practices in operating Stretch. The Stretch robot can potentially be dangerous if used without caution. All users must carefully read the Stretch Safety Guide prior to using the robot.","title":"New to Stretch?"},{"location":"#where-to-find-things","text":"All of the documentation is accessible and searchable via the navigation menu on this site. Alternatively, you can view the markdown hosted on at the Hello Robot GitHub portal . In addition, it is worth spending some time on the following sites: Resource Description Stretch Community Repository for community shared code Stretch Forum Discourse User Forum","title":"Where to Find Things"},{"location":"#version","text":"This is version 0.2 of the Stretch User Documentation. It is written with the following system configuration in mind: Resource Description Model Stretch RE1 or Stretch 2 OS Ubuntu 20.04 ROS Noetic and Melodic Python Python3 Stretch Body '>=0.4 You can access prior documentation suitable for older configurations (eg Ubuntu 18.04, ROS Melodic) here . If you are on an older version and would like to upgrade the robot, follow the Ubuntu 20.04 upgrade guide or contact support.","title":"Version"},{"location":"#license","text":"This documentation is only to be used for an authentic Stretch robot produced and sold by Hello Robot Inc. All Hello Robot documentation and related materials are released under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"License"},{"location":"LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains documentation exclusively for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license (the \"License\"); you may not use the Contents except in compliance with the License. You may obtain a copy of the License at https://creativecommons.org/licenses/by-nd/4.0 Unless required by applicable law or agreed to in writing, the Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Patents pending and trademark rights cover the Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\" For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"repos_overview/","text":"Repositories Overview This is the starting off point for exploring the many code repositories that power Stretch. The table below briefly covers the purpose of each repository. Repo Descriptions Repository Description 1 Stretch Body Python SDK that allows you to interact with the hardware. 2 Stretch ROS ROS related code for Stretch. 3 Stretch ROS2 ROS 2 related code for Stretch. 4 Stretch Factory Factory Python tools for debug, testing and calibration. 5 Stretch Firmware Arduino code for the firmware that drives Stretch. 5 Stretch Tool Share Hardware extensions to extend the capabilities of Stretch. 6 Stretch Install Installation scripts for Stretch. 7 Stretch Web Interface Code that allows remote teleoperation through a browser. Contributing We welcome code contributions from the community to improve this documentation or the software stack. To report an issue or contribute to a Stretch repo, please visit the repo's Github page and either file an issue or fork the repository of interest and raise a PR. License This documentation is only to be used for an authentic Stretch robot produced and sold by Hello Robot Inc. All Hello Robot documentation and related materials are released under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Overview"},{"location":"repos_overview/#repositories-overview","text":"This is the starting off point for exploring the many code repositories that power Stretch. The table below briefly covers the purpose of each repository.","title":"Repositories Overview"},{"location":"repos_overview/#repo-descriptions","text":"Repository Description 1 Stretch Body Python SDK that allows you to interact with the hardware. 2 Stretch ROS ROS related code for Stretch. 3 Stretch ROS2 ROS 2 related code for Stretch. 4 Stretch Factory Factory Python tools for debug, testing and calibration. 5 Stretch Firmware Arduino code for the firmware that drives Stretch. 5 Stretch Tool Share Hardware extensions to extend the capabilities of Stretch. 6 Stretch Install Installation scripts for Stretch. 7 Stretch Web Interface Code that allows remote teleoperation through a browser.","title":"Repo Descriptions"},{"location":"repos_overview/#contributing","text":"We welcome code contributions from the community to improve this documentation or the software stack. To report an issue or contribute to a Stretch repo, please visit the repo's Github page and either file an issue or fork the repository of interest and raise a PR.","title":"Contributing"},{"location":"repos_overview/#license","text":"This documentation is only to be used for an authentic Stretch robot produced and sold by Hello Robot Inc. All Hello Robot documentation and related materials are released under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"License"},{"location":"stretch-body/","text":"Overview The stretch_body repository includes Python packages that allow a developer to interact with the hardware of the Stretch RE1 and RE2 robots. These packages are: Package Description hello-robot-stretch-body Python library to interface with Stretch hello-robot-stretch-body-tools Useful commandline tools for using Stretch Python2 version of packages can be installed by: pip2 install -U hello-robot-stretch-body pip2 install -U hello-robot-stretch-body-tools Python3 version of packages can be installed by: pip3 install -U hello-robot-stretch-body pip3 install -U hello-robot-stretch-body-tools See docs.hello-robot.com for documentation on using Stretch. In particular, see the Stretch Body Tutorials for additional information on using these packages. Testing and Development See Stretch Body's README and Stretch Body Commandline Tool's README for information on testing/developing these packages. Changelog See the changelog for information on what changed with each release. License Each subdirectory contains a LICENSE.md file that applies to the directory's contents. This software is intended for use with the Stretch RE1 and RE2 mobile manipulators, which are robots produced and sold by Hello Robot Inc. For further information including inquiries about dual licensing, please contact Hello Robot Inc. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Overview"},{"location":"stretch-body/#overview","text":"The stretch_body repository includes Python packages that allow a developer to interact with the hardware of the Stretch RE1 and RE2 robots. These packages are: Package Description hello-robot-stretch-body Python library to interface with Stretch hello-robot-stretch-body-tools Useful commandline tools for using Stretch Python2 version of packages can be installed by: pip2 install -U hello-robot-stretch-body pip2 install -U hello-robot-stretch-body-tools Python3 version of packages can be installed by: pip3 install -U hello-robot-stretch-body pip3 install -U hello-robot-stretch-body-tools See docs.hello-robot.com for documentation on using Stretch. In particular, see the Stretch Body Tutorials for additional information on using these packages.","title":"Overview"},{"location":"stretch-body/#testing-and-development","text":"See Stretch Body's README and Stretch Body Commandline Tool's README for information on testing/developing these packages.","title":"Testing and Development"},{"location":"stretch-body/#changelog","text":"See the changelog for information on what changed with each release.","title":"Changelog"},{"location":"stretch-body/#license","text":"Each subdirectory contains a LICENSE.md file that applies to the directory's contents. This software is intended for use with the Stretch RE1 and RE2 mobile manipulators, which are robots produced and sold by Hello Robot Inc. For further information including inquiries about dual licensing, please contact Hello Robot Inc. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"License"},{"location":"stretch-body/CHANGELOG/","text":"Changelog The changes between releases of Stretch Body are documented here. 0.4.8 - Sept 14, 2022 This is the initial production release that supports the Stretch RE2 (Mitski batch). This includes the robot_params_RE2V0.py which are the initial robot settings for the RE2 version of the product. It introduces the PrismaticJoint class which consolidates the common Arm and Lift functionality. It changes the units for guarded contact motion from approximate Newtons (suffix _N) to effort_pct - the pecentage [-100,100] of maximum current (A) that a joint should stop at. This change requires RE1 users to migrate their code and robot parameters. See the forum post for more details. It introduces mkdocs.yaml to support serving the repository documenation via MKDocs. It introduces several new features and fixes several bugs, notably: Adds wait_until_at_setpoint() to the Arm and Lift classes Adds use of argparse with all tools Moves Robot thread rates to YAML Cleans up the splined trajectory interface, enables velocity controlled splined trajectories for the Dynamixels Flags a warning for users incorrectly setting the homing offset on DXL servos 0.3.4 - July 20, 2022 Release to add minor features and fix minor bugs: Add a range_pad_t parameter to allow for padding of hardstops for joint homing Clean up tools and warnings to more consistent and legible #140 0.3.0 - June 21, 2022 This release moves Stretch Body to use a new parameter management format. This change will require older systems to migrate their parameters to the new format. For systems that haven't yet migrated, Stretch Body will exit with a warning that they must migrate first by running RE1_migrate_params.py . See the forum post for more details. Features: New param management and RE1.5 support #135 0.2.1 - January 6, 2022 Release to fix two bugs: Fix goto commands in the head jog tool #128 - Fixes goto commands in the head jog tool Fix port_handler location #121 - Fixes dxl buffer resetting under a serial communication failure 0.2.0 - December 28, 2021 This release brings support for waypoint trajectories into master. Support for waypoint trajectories was built up over the last year in the feature/waypoint_trajectories_py3 branch, however, this branch couldn't be merged because the new functionality had flaky performance due to subtle bugs. This branch also attempted to introduce support for Python3 and timestamp synchronization. Support for Python3 and other features were merged in v0.1.0 . The remaining features from this branch have been broken into 7 PRs, each targeting a specific device and squashing any previous bugs through functional and performance testing. They are: Introduce waypoint trajectory RPCs #98 Add individual device threading #105 Trajectory management classes #106 Lift and arm trajectories #110 Dynamixel trajectories #113 Mobile base trajectories #114 Whole body trajectories #115 This release also fixes several bugs. They are: fixed baud map bug #117 fix contact thresh bug #116 Fixed EndOfArm tools unittest #104 Testing: Each PR in this release was tested on multiple robots, but was primarily tested on G2, on Python 2.7/Ubuntu 18.04. 0.1.11 - October 4, 2021 This release gives Stretch Body the ability to support multiple firmware protocols, which at this moment is P0 and P1 firmware. P1 firmware builds on P0 to add waypoint trajectory support and a refactoring of controller functionality into classes. Additionally, this PR fixes how Dynamixel motors calculate velocity from encoder ticks. Features: Support new firmware protocol (P1) #97 Bugfixes: Fix how negative dynamixel velocities are calculated #60 0.1.10 - September 23, 2021 This release introduces 3 features: #68 and #95 : Introduces two new Jupyter notebooks that can be used to interactively explore working with Stretch. See forum post for details. #94 : Introduces Github Action files and docs. Will be enabled in the future to automatically test PRs. #66 : Improves the statistics captured on Stretch Body's performance. Will be used to measure improvements to Stretch Body's communication with low level devices. Bugfixes: #90 : Patches a bug where triggering pimu.trigger_motor_sync() at to high of a rate puts the robot into Runstop mode. #101 : Fixes bugs on startup of Dynamixel devices, ensures status is populated on startup of all devices, and add bool to robot.startup() Testing: All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1. 0.1.6 - August 26, 2021 This release introduces these features: Revised soft limits and collision avoidance Added velocity interfaces for arm and lift Bugfixes: Better error handling for DXL servos and their tools Fix bug where dxls maintain previous motion profile 0.1.4 - July 20, 2021 This release introduces six features and several bugfixes. The features are: Robot self-collision model and tutorial Add ability to runstop individual DXL servos Merged py2 and py3 tools Added instructions for developing/testing Stretch Body Collision avoidance tutorial Improve realsense visualizer Bugfixes: Fix issue where user soft limits overwritten by collision models Pin py2 deps to older version 0.1.0 - May 30, 2021 This release introduces eight major features and several bugfixes. The features are: Python param management Configurable baud rate/GroupRead on Dynamixels Pluggable end effector tools Pluggable end effector support in Xbox Teleop Python logging Soft motion limit Self collision management Unit testing framework - as part of each PR Bugfixes: Multiturn enable_pos bug (#12) & unit tests Misc bugs Other - as part of the features Testing: All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1.","title":"Changelog"},{"location":"stretch-body/CHANGELOG/#changelog","text":"The changes between releases of Stretch Body are documented here.","title":"Changelog"},{"location":"stretch-body/CHANGELOG/#048-sept-14-2022","text":"This is the initial production release that supports the Stretch RE2 (Mitski batch). This includes the robot_params_RE2V0.py which are the initial robot settings for the RE2 version of the product. It introduces the PrismaticJoint class which consolidates the common Arm and Lift functionality. It changes the units for guarded contact motion from approximate Newtons (suffix _N) to effort_pct - the pecentage [-100,100] of maximum current (A) that a joint should stop at. This change requires RE1 users to migrate their code and robot parameters. See the forum post for more details. It introduces mkdocs.yaml to support serving the repository documenation via MKDocs. It introduces several new features and fixes several bugs, notably: Adds wait_until_at_setpoint() to the Arm and Lift classes Adds use of argparse with all tools Moves Robot thread rates to YAML Cleans up the splined trajectory interface, enables velocity controlled splined trajectories for the Dynamixels Flags a warning for users incorrectly setting the homing offset on DXL servos","title":"0.4.8 - Sept 14, 2022"},{"location":"stretch-body/CHANGELOG/#034-july-20-2022","text":"Release to add minor features and fix minor bugs: Add a range_pad_t parameter to allow for padding of hardstops for joint homing Clean up tools and warnings to more consistent and legible #140","title":"0.3.4 - July 20, 2022"},{"location":"stretch-body/CHANGELOG/#030-june-21-2022","text":"This release moves Stretch Body to use a new parameter management format. This change will require older systems to migrate their parameters to the new format. For systems that haven't yet migrated, Stretch Body will exit with a warning that they must migrate first by running RE1_migrate_params.py . See the forum post for more details. Features: New param management and RE1.5 support #135","title":"0.3.0 - June 21, 2022"},{"location":"stretch-body/CHANGELOG/#021-january-6-2022","text":"Release to fix two bugs: Fix goto commands in the head jog tool #128 - Fixes goto commands in the head jog tool Fix port_handler location #121 - Fixes dxl buffer resetting under a serial communication failure","title":"0.2.1 - January 6, 2022"},{"location":"stretch-body/CHANGELOG/#020-december-28-2021","text":"This release brings support for waypoint trajectories into master. Support for waypoint trajectories was built up over the last year in the feature/waypoint_trajectories_py3 branch, however, this branch couldn't be merged because the new functionality had flaky performance due to subtle bugs. This branch also attempted to introduce support for Python3 and timestamp synchronization. Support for Python3 and other features were merged in v0.1.0 . The remaining features from this branch have been broken into 7 PRs, each targeting a specific device and squashing any previous bugs through functional and performance testing. They are: Introduce waypoint trajectory RPCs #98 Add individual device threading #105 Trajectory management classes #106 Lift and arm trajectories #110 Dynamixel trajectories #113 Mobile base trajectories #114 Whole body trajectories #115 This release also fixes several bugs. They are: fixed baud map bug #117 fix contact thresh bug #116 Fixed EndOfArm tools unittest #104 Testing: Each PR in this release was tested on multiple robots, but was primarily tested on G2, on Python 2.7/Ubuntu 18.04.","title":"0.2.0 - December 28, 2021"},{"location":"stretch-body/CHANGELOG/#0111-october-4-2021","text":"This release gives Stretch Body the ability to support multiple firmware protocols, which at this moment is P0 and P1 firmware. P1 firmware builds on P0 to add waypoint trajectory support and a refactoring of controller functionality into classes. Additionally, this PR fixes how Dynamixel motors calculate velocity from encoder ticks. Features: Support new firmware protocol (P1) #97 Bugfixes: Fix how negative dynamixel velocities are calculated #60","title":"0.1.11 - October 4, 2021"},{"location":"stretch-body/CHANGELOG/#0110-september-23-2021","text":"This release introduces 3 features: #68 and #95 : Introduces two new Jupyter notebooks that can be used to interactively explore working with Stretch. See forum post for details. #94 : Introduces Github Action files and docs. Will be enabled in the future to automatically test PRs. #66 : Improves the statistics captured on Stretch Body's performance. Will be used to measure improvements to Stretch Body's communication with low level devices. Bugfixes: #90 : Patches a bug where triggering pimu.trigger_motor_sync() at to high of a rate puts the robot into Runstop mode. #101 : Fixes bugs on startup of Dynamixel devices, ensures status is populated on startup of all devices, and add bool to robot.startup() Testing: All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1.","title":"0.1.10 - September 23, 2021"},{"location":"stretch-body/CHANGELOG/#016-august-26-2021","text":"This release introduces these features: Revised soft limits and collision avoidance Added velocity interfaces for arm and lift Bugfixes: Better error handling for DXL servos and their tools Fix bug where dxls maintain previous motion profile","title":"0.1.6 - August 26, 2021"},{"location":"stretch-body/CHANGELOG/#014-july-20-2021","text":"This release introduces six features and several bugfixes. The features are: Robot self-collision model and tutorial Add ability to runstop individual DXL servos Merged py2 and py3 tools Added instructions for developing/testing Stretch Body Collision avoidance tutorial Improve realsense visualizer Bugfixes: Fix issue where user soft limits overwritten by collision models Pin py2 deps to older version","title":"0.1.4 - July 20, 2021"},{"location":"stretch-body/CHANGELOG/#010-may-30-2021","text":"This release introduces eight major features and several bugfixes. The features are: Python param management Configurable baud rate/GroupRead on Dynamixels Pluggable end effector tools Pluggable end effector support in Xbox Teleop Python logging Soft motion limit Self collision management Unit testing framework - as part of each PR Bugfixes: Multiturn enable_pos bug (#12) & unit tests Misc bugs Other - as part of the features Testing: All unit tests run on Python 2.7.17 on Ubuntu 18.04 on a Stretch RE1.","title":"0.1.0 - May 30, 2021"},{"location":"stretch-body/body/","text":"Stretch Body The stretch_body package provides a low level Python API to the Hello Robot Stretch RE1 hardware. Installing To install stable Stretch Body for Python2, run: $ python -m pip install --upgrade hello-robot-stretch-body To install a pre-release of Stretch Body for Python2, run: $ python -m pip install --upgrade --pre hello-robot-stretch-body Please report feedback on the Issue Tracker or the Forum . For Python3, substitute python with python3 . Running tests There are a number of unit, functional, and performance tests within the test/ folder, separated into test suites by different files. Suites are separated by a device or functionality within Stretch Body that is being tested. In Python2, run python -m unittest test.test_<suite-name> . For Python3, substitute python with python3 . For example, to run the stretch_body.robot.Robot functional tests, run $ git clone https://github.com/hello-robot/stretch_body.git $ cd stretch_body/body $ python -m unittest test.test_robot Developing The source code for Stretch Body resides within the stretch_body/ folder. You can install Stretch Body as \"editable\", and directly edit the source code to test changes. In Python2, run python -m pip install -e . For Python3, substitute python with python3 . For example, to test changes to stretch_body.robot.Robot , run $ git clone https://github.com/hello-robot/stretch_body.git $ cd stretch_body/body $ python -m pip install -e . Now, make desired edits to the stretch_body/body/stretch_body/robot.py file. Software using Stretch Body is now using the modified stretch_body.robot.Robot class. Deploying Increment the version number and run the deploy.sh script.","title":"Index"},{"location":"stretch-body/body/#stretch-body","text":"The stretch_body package provides a low level Python API to the Hello Robot Stretch RE1 hardware.","title":"Stretch Body"},{"location":"stretch-body/body/#installing","text":"To install stable Stretch Body for Python2, run: $ python -m pip install --upgrade hello-robot-stretch-body To install a pre-release of Stretch Body for Python2, run: $ python -m pip install --upgrade --pre hello-robot-stretch-body Please report feedback on the Issue Tracker or the Forum . For Python3, substitute python with python3 .","title":"Installing"},{"location":"stretch-body/body/#running-tests","text":"There are a number of unit, functional, and performance tests within the test/ folder, separated into test suites by different files. Suites are separated by a device or functionality within Stretch Body that is being tested. In Python2, run python -m unittest test.test_<suite-name> . For Python3, substitute python with python3 . For example, to run the stretch_body.robot.Robot functional tests, run $ git clone https://github.com/hello-robot/stretch_body.git $ cd stretch_body/body $ python -m unittest test.test_robot","title":"Running tests"},{"location":"stretch-body/body/#developing","text":"The source code for Stretch Body resides within the stretch_body/ folder. You can install Stretch Body as \"editable\", and directly edit the source code to test changes. In Python2, run python -m pip install -e . For Python3, substitute python with python3 . For example, to test changes to stretch_body.robot.Robot , run $ git clone https://github.com/hello-robot/stretch_body.git $ cd stretch_body/body $ python -m pip install -e . Now, make desired edits to the stretch_body/body/stretch_body/robot.py file. Software using Stretch Body is now using the modified stretch_body.robot.Robot class.","title":"Developing"},{"location":"stretch-body/body/#deploying","text":"Increment the version number and run the deploy.sh script.","title":"Deploying"},{"location":"stretch-body/body/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. This software is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License v3.0 (GNU LGPLv3) as published by the Free Software Foundation. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License v3.0 (GNU LGPLv3) for more details, which can be found via the following link: https://www.gnu.org/licenses/lgpl-3.0.en.html For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-body/body/test/","text":"Unit tests can be run by, for example: python -m unittest test_dynamixel_XL430","title":"Index"},{"location":"stretch-body/docs/jupyter/jupyter_getting_started/","text":"Jupyter Notebook Jupyter is a free, open-source, interactive web tool known as a computational notebook, which researchers can use to combine software code, computational output, explanatory text and multimedia resources in a single document. Launch a Jupyter Notebook App (Linux, MacOS) For Linux and MacOS systems, open a new terminal window. Enter the startup folder by typing cd /some folder name Type jupyter notebook to launch the Jupyter Notebook App. The notebook interface will appear in a new browser window or tab. Launch a Jupyter Notebook App (Windows) Double-click on the Jupyter Notebook desktop launcher (icon shows [IPy]) to start the Jupyter Notebook App. The notebook interface will appear in a new browser window or tab. A secondary terminal window (used only for error logging and for shut down) will be also opened. Executing a notebook Launch the Jupyter Notebook App (see previous section). In the Notebook Dashboard navigate to find the notebook: clicking on its name will open it in a new browser tab. Click on the menu Help -> User Interface Tour for an overview of the Jupyter Notebook App user interface. You can run the notebook document step-by-step (one cell a time) by pressing shift + enter. You can run the whole notebook in a single step by clicking on the menu Cell -> Run All. To restart the kernel (i.e. the computational engine), click on the menu Kernel -> Restart. This can be useful to start over a computation from scratch (e.g. variables are deleted, open files are closed, etc\u2026). Closing a notebook When a notebook is opened, its \u201ccomputational engine\u201d (called the kernel) is automatically started. Closing the notebook browser tab, will not shut down the kernel, instead the kernel will keep running until is explicitly shut down. To shut down a kernel, go to the associated notebook and click on menu File -> Close and Halt. Alternatively, the Notebook Dashboard has a tab named Running that shows all the running notebooks (i.e. kernels) and allows shutting them down (by clicking on a Shutdown button). Shut down the Jupyter Notebook App Closing the browser (or the tab) will not close the Jupyter Notebook App. To completely shut it down you need to close the associated terminal.","title":"Jupyter Notebook"},{"location":"stretch-body/docs/jupyter/jupyter_getting_started/#jupyter-notebook","text":"Jupyter is a free, open-source, interactive web tool known as a computational notebook, which researchers can use to combine software code, computational output, explanatory text and multimedia resources in a single document.","title":"Jupyter Notebook"},{"location":"stretch-body/docs/jupyter/jupyter_getting_started/#launch-a-jupyter-notebook-app-linux-macos","text":"For Linux and MacOS systems, open a new terminal window. Enter the startup folder by typing cd /some folder name Type jupyter notebook to launch the Jupyter Notebook App. The notebook interface will appear in a new browser window or tab.","title":"Launch a Jupyter Notebook App (Linux, MacOS)"},{"location":"stretch-body/docs/jupyter/jupyter_getting_started/#launch-a-jupyter-notebook-app-windows","text":"Double-click on the Jupyter Notebook desktop launcher (icon shows [IPy]) to start the Jupyter Notebook App. The notebook interface will appear in a new browser window or tab. A secondary terminal window (used only for error logging and for shut down) will be also opened.","title":"Launch a Jupyter Notebook App (Windows)"},{"location":"stretch-body/docs/jupyter/jupyter_getting_started/#executing-a-notebook","text":"Launch the Jupyter Notebook App (see previous section). In the Notebook Dashboard navigate to find the notebook: clicking on its name will open it in a new browser tab. Click on the menu Help -> User Interface Tour for an overview of the Jupyter Notebook App user interface. You can run the notebook document step-by-step (one cell a time) by pressing shift + enter. You can run the whole notebook in a single step by clicking on the menu Cell -> Run All. To restart the kernel (i.e. the computational engine), click on the menu Kernel -> Restart. This can be useful to start over a computation from scratch (e.g. variables are deleted, open files are closed, etc\u2026).","title":"Executing a notebook"},{"location":"stretch-body/docs/jupyter/jupyter_getting_started/#closing-a-notebook","text":"When a notebook is opened, its \u201ccomputational engine\u201d (called the kernel) is automatically started. Closing the notebook browser tab, will not shut down the kernel, instead the kernel will keep running until is explicitly shut down. To shut down a kernel, go to the associated notebook and click on menu File -> Close and Halt. Alternatively, the Notebook Dashboard has a tab named Running that shows all the running notebooks (i.e. kernels) and allows shutting them down (by clicking on a Shutdown button).","title":"Closing a notebook"},{"location":"stretch-body/docs/jupyter/jupyter_getting_started/#shut-down-the-jupyter-notebook-app","text":"Closing the browser (or the tab) will not close the Jupyter Notebook App. To completely shut it down you need to close the associated terminal.","title":"Shut down the Jupyter Notebook App"},{"location":"stretch-body/tools/","text":"Stretch Body Command Line Tools This package provides Python tools that work with the Hello Robot Stretch Body package. These tools perform common tasks when working with Stretch RE1 (e.g. homing and stowing), and serve as tutorial code for working on various parts of the robot. Installing To install stable Stretch Body Command Line Tools for Python2, run: $ python -m pip install --upgrade hello-robot-stretch-body-tools To install a pre-release of the Command Line Tools for Python2, run: $ python -m pip install --upgrade --pre hello-robot-stretch-body-tools Please report feedback on the Issue Tracker or the Forum . For Python3, substitute python with python3 . Usage All of the command-line tools reside within the bin/ folder. When this package is installed, they are accessible from anywhere as command-line tools. For example, to perform a robot system check, run: $ stretch_robot_system_check.py Developing The source code for the command-line tools resides within the bin/ folder. You can install the tools package as \"editable\", and directly edit the source code to test changes. In Python2, run python -m pip install -e . For Python3, substitute python with python3 . For example, to test changes to the stretch_robot_home.py script, run $ git clone https://github.com/hello-robot/stretch_body.git $ cd stretch_body/tools $ python -m pip install -e . Now, make desired edits to the stretch_robot_home.py file. Executing the script on the command-line will now run your modified version. Deploying Increment the version number and run the deploy.sh script.","title":"Index"},{"location":"stretch-body/tools/#stretch-body-command-line-tools","text":"This package provides Python tools that work with the Hello Robot Stretch Body package. These tools perform common tasks when working with Stretch RE1 (e.g. homing and stowing), and serve as tutorial code for working on various parts of the robot.","title":"Stretch Body Command Line Tools"},{"location":"stretch-body/tools/#installing","text":"To install stable Stretch Body Command Line Tools for Python2, run: $ python -m pip install --upgrade hello-robot-stretch-body-tools To install a pre-release of the Command Line Tools for Python2, run: $ python -m pip install --upgrade --pre hello-robot-stretch-body-tools Please report feedback on the Issue Tracker or the Forum . For Python3, substitute python with python3 .","title":"Installing"},{"location":"stretch-body/tools/#usage","text":"All of the command-line tools reside within the bin/ folder. When this package is installed, they are accessible from anywhere as command-line tools. For example, to perform a robot system check, run: $ stretch_robot_system_check.py","title":"Usage"},{"location":"stretch-body/tools/#developing","text":"The source code for the command-line tools resides within the bin/ folder. You can install the tools package as \"editable\", and directly edit the source code to test changes. In Python2, run python -m pip install -e . For Python3, substitute python with python3 . For example, to test changes to the stretch_robot_home.py script, run $ git clone https://github.com/hello-robot/stretch_body.git $ cd stretch_body/tools $ python -m pip install -e . Now, make desired edits to the stretch_robot_home.py file. Executing the script on the command-line will now run your modified version.","title":"Developing"},{"location":"stretch-body/tools/#deploying","text":"Increment the version number and run the deploy.sh script.","title":"Deploying"},{"location":"stretch-body/tools/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-factory/","text":"Overview The Stretch Factory package provides low-level Python tools for debug, testing, and calibration of the Hello Robot Stretch RE1 and RE2. These tools are provided for reference only and are intended to be used under the guidance of Hello Robot support engineers. This package can be installed by: pip install hello-robot-stretch-factory The available Stretch Factory tools can be found by tab completing after typing 'REx_'. For example: REx_base_calibrate_imu_collect.py REx_dynamixel_reboot.py REx_stepper_calibration_YAML_to_flash.py REx_base_calibrate_imu_process.py REx_dynamixel_set_baud.py REx_stepper_jog.py REx_base_calibrate_wheel_separation.py REx_firmware_updater.py REx_stepper_mechaduino_menu.py REx_cliff_sensor_calibrate.py REx_gripper_calibrate.py REx_timestamp_manager_analyze.py REx_clock_manager_analyze.py REx_head_calibrate_pan.py REx_usb_reset.py REx_dynamixel_id_change.py REx_hello_dynamixel_jog.py REx_wacc_calibrate.py REx_dynamixel_id_scan.py REx_stepper_calibration_flash_to_YAML.py REx_dynamixel_jog.py REx_stepper_calibration_run.py For useage of these tools, try for example: REx_dynamixel_id_scan.py --help All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Overview"},{"location":"stretch-factory/#overview","text":"The Stretch Factory package provides low-level Python tools for debug, testing, and calibration of the Hello Robot Stretch RE1 and RE2. These tools are provided for reference only and are intended to be used under the guidance of Hello Robot support engineers. This package can be installed by: pip install hello-robot-stretch-factory The available Stretch Factory tools can be found by tab completing after typing 'REx_'. For example: REx_base_calibrate_imu_collect.py REx_dynamixel_reboot.py REx_stepper_calibration_YAML_to_flash.py REx_base_calibrate_imu_process.py REx_dynamixel_set_baud.py REx_stepper_jog.py REx_base_calibrate_wheel_separation.py REx_firmware_updater.py REx_stepper_mechaduino_menu.py REx_cliff_sensor_calibrate.py REx_gripper_calibrate.py REx_timestamp_manager_analyze.py REx_clock_manager_analyze.py REx_head_calibrate_pan.py REx_usb_reset.py REx_dynamixel_id_change.py REx_hello_dynamixel_jog.py REx_wacc_calibrate.py REx_dynamixel_id_scan.py REx_stepper_calibration_flash_to_YAML.py REx_dynamixel_jog.py REx_stepper_calibration_run.py For useage of these tools, try for example: REx_dynamixel_id_scan.py --help All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Overview"},{"location":"stretch-factory/CHANGELOG/","text":"Changelog The changes between releases of Stretch Factory is documented here. 0.3.0 - September 1, 2022 This release moves Stretch Factory to use a new naming scheme for its tools. The prefix REx is now used instead of RE1 . This semantic change is in anticipation of the release of future versions of Stretch (e.g. RE2). In addition, two new tools are introduced: REx_calibrate_guarded_contact.py: Measure the efforts required to move througout the joint workspace and save contact thresholds to Configuration YAML REx_calibrate_range.py: Measure the range of motion of a joint and save to the Configuration YAML. These new tools move the the effort_pct contact model as supported by Stretch Body 0.3 Additional features; Firmware updater tested and supports 20.04 #47 0.2.0 - June 21, 2022 Support new parameter manage scheme #45 Adds new parameter management format. This change will require older systems to migrate their parameters to the new format. For systems that haven't yet migrated, Stretch Body will exit with a warning that they must migrate first by running RE1_migrate_params.py . See the forum post for more details. 0.1.0 Introduce the RE1_firmware_update.py tool 0.0.2 - May 13, 2020 This is the initial release of Stretch Factory. It includes tools to support debug and testing of the Stretch Hardware.","title":"Changelog"},{"location":"stretch-factory/CHANGELOG/#changelog","text":"The changes between releases of Stretch Factory is documented here.","title":"Changelog"},{"location":"stretch-factory/CHANGELOG/#030-september-1-2022","text":"This release moves Stretch Factory to use a new naming scheme for its tools. The prefix REx is now used instead of RE1 . This semantic change is in anticipation of the release of future versions of Stretch (e.g. RE2). In addition, two new tools are introduced: REx_calibrate_guarded_contact.py: Measure the efforts required to move througout the joint workspace and save contact thresholds to Configuration YAML REx_calibrate_range.py: Measure the range of motion of a joint and save to the Configuration YAML. These new tools move the the effort_pct contact model as supported by Stretch Body 0.3 Additional features; Firmware updater tested and supports 20.04 #47","title":"0.3.0 - September 1, 2022"},{"location":"stretch-factory/CHANGELOG/#020-june-21-2022","text":"Support new parameter manage scheme #45 Adds new parameter management format. This change will require older systems to migrate their parameters to the new format. For systems that haven't yet migrated, Stretch Body will exit with a warning that they must migrate first by running RE1_migrate_params.py . See the forum post for more details.","title":"0.2.0 - June 21, 2022"},{"location":"stretch-factory/CHANGELOG/#010","text":"Introduce the RE1_firmware_update.py tool","title":"0.1.0"},{"location":"stretch-factory/CHANGELOG/#002-may-13-2020","text":"This is the initial release of Stretch Factory. It includes tools to support debug and testing of the Stretch Hardware.","title":"0.0.2 - May 13, 2020"},{"location":"stretch-factory/python/","text":"Overview The Stretch Factory package provides Python tools for testing and calibration of the Hello Robot Stretch RE1 and RE2. These tools are provided for reference only and are intended to be used by qualified Hello Robot production engineers. This package can be installed by: python -m pip install -U hello-robot-stretch-factory","title":"Overview"},{"location":"stretch-factory/python/#overview","text":"The Stretch Factory package provides Python tools for testing and calibration of the Hello Robot Stretch RE1 and RE2. These tools are provided for reference only and are intended to be used by qualified Hello Robot production engineers. This package can be installed by: python -m pip install -U hello-robot-stretch-factory","title":"Overview"},{"location":"stretch-factory/python/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation. The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link: https://www.gnu.org/licenses/gpl-3.0.html For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-factory/python/tools/","text":"Stretch Factory Tools The list of tools can be found by tab completion of 'REx' at the command line: >>$ REx REx_base_calibrate_imu_collect.py REx_cliff_sensor_calibrate.py REx_dynamixel_reboot.py REx_hello_dynamixel_jog.py REx_stepper_gains.py REx_base_calibrate_imu_process.py REx_D435i_check.py REx_dynamixel_set_baud.py REx_stepper_calibration_flash_to_YAML.py REx_stepper_jog.py REx_base_calibrate_wheel_separation.py REx_dynamixel_id_change.py REx_firmware_updater.py REx_stepper_calibration_run.py REx_stepper_mechaduino_menu.py REx_calibrate_guarded_contact.py REx_dynamixel_id_scan.py REx_gamepad_configure.py REx_stepper_calibration_YAML_to_flash.p REx_usb_reset.py REx_calibrate_range.py REx_dynamixel_jog.py REx_gripper_calibrate.py REx_stepper_ctrl_tuning.py REx_wacc_calibrate.py These tools are used during the factory system 'bringup' of the robot. They are organized by subsystem: REx_arm* REx_base* REx_dynamixel* REx_lift* REx_stepper* REx_wacc* The tools will generally interact with the lowest level interface of the hardware, make measurements, and write calibration data to the robot's YAML or devices EEPROM. For example, the following script calibrates the wrist accelerometer such that the gravity term is 9.8m/s^2. >>$ REx_wacc_calibrate_gravity.py Ensure base is level and arm is retracted. Hit enter when ready Itr 0 Val 9 .32055700006 ... Itr 99 Val 9 .34092651895 Got a average value of 9 .29669019184 Scalar of 1 .05485391012 Write parameters to stretch_re1_factory_params.yaml ( y/n ) ? [ y ] y Writing yaml... Caution: It is possible to break your robot by running these tools. If not used properly these tools may not respect joint torque and position limits. They may overwrite existing calibration data as well.","title":"Stretch Factory Tools"},{"location":"stretch-factory/python/tools/#stretch-factory-tools","text":"The list of tools can be found by tab completion of 'REx' at the command line: >>$ REx REx_base_calibrate_imu_collect.py REx_cliff_sensor_calibrate.py REx_dynamixel_reboot.py REx_hello_dynamixel_jog.py REx_stepper_gains.py REx_base_calibrate_imu_process.py REx_D435i_check.py REx_dynamixel_set_baud.py REx_stepper_calibration_flash_to_YAML.py REx_stepper_jog.py REx_base_calibrate_wheel_separation.py REx_dynamixel_id_change.py REx_firmware_updater.py REx_stepper_calibration_run.py REx_stepper_mechaduino_menu.py REx_calibrate_guarded_contact.py REx_dynamixel_id_scan.py REx_gamepad_configure.py REx_stepper_calibration_YAML_to_flash.p REx_usb_reset.py REx_calibrate_range.py REx_dynamixel_jog.py REx_gripper_calibrate.py REx_stepper_ctrl_tuning.py REx_wacc_calibrate.py These tools are used during the factory system 'bringup' of the robot. They are organized by subsystem: REx_arm* REx_base* REx_dynamixel* REx_lift* REx_stepper* REx_wacc* The tools will generally interact with the lowest level interface of the hardware, make measurements, and write calibration data to the robot's YAML or devices EEPROM. For example, the following script calibrates the wrist accelerometer such that the gravity term is 9.8m/s^2. >>$ REx_wacc_calibrate_gravity.py Ensure base is level and arm is retracted. Hit enter when ready Itr 0 Val 9 .32055700006 ... Itr 99 Val 9 .34092651895 Got a average value of 9 .29669019184 Scalar of 1 .05485391012 Write parameters to stretch_re1_factory_params.yaml ( y/n ) ? [ y ] y Writing yaml... Caution: It is possible to break your robot by running these tools. If not used properly these tools may not respect joint torque and position limits. They may overwrite existing calibration data as well.","title":"Stretch Factory Tools"},{"location":"stretch-factory/updates/","text":"This directory contains recommended factory updates. Update Description Serial Nos Date 001_ROS_INSTALL Scripts to upgrade to ROS stack for units shipped prior to stretch_ros being available 1003,1004 06/01/2020 002_HEAD_PAN Deprecated. Use update 005 instead. 1002 to 1008 08/01/2020 003_WRIST_SWAP Updating system YAML after installing a new wrist module 1004 9/28/2020 004_HEAD_TILT Debug head tilt 1018 9/28/2020 005_HEAD_PAN_CALIBRATION How to update robot YAML and recalibrate the head pan joint 1001 to 1022 10/8/2020 006_DXL_RUNSTOP Allow robot Dynamixel servos to stop upon runstop activation 1001 to 1022 10/8/2020 007_LIFT_FINGER_GUARD Recalibrate lift after installation of foam finger guards 1005 11/17/2020 008_SYNC_TIMESTAMPS Upgrade firmware and Stretch Body to support synchronized timestamps Prior to 1050 12/08/2020 009_STEPPER_STARTUP Fix bug related to configuration of stepper controllers at startup Prior to 1040 1/4/2020 010_WRIST_SWAP Updating system YAML after installing a new wrist module 1013 1/27/2021 011_HEAD_TILT_SWAP Updating the robot calibration and camera data after install a new head tilt module 1007 2/8/21 012_DEX_WRIST Installing and configuring the beta unit of the dexterous wrist x 2/17/21 013_WACC_INSTALL How to install and configure a replacement Wacc board x 3/7/20","title":"Index"},{"location":"stretch-factory/updates/001_ROS_INSTALL/","text":"001_ROS_INSTALL Background Early Stretch units shipped without the stretch_ros stack installed. These scripts will install the necessary packages and bring the robot up to the current ROS compatible configuration. Installation To upgrade a 'pre-ROS' Stretch to use the latest ROS software stack: >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory >>$ cd updates/STRETCH_UPDATES_001 >>$ ./stretch_install_system.sh >>$ ./stretch_updgrade_pre_ros.sh Then install your URDF to the correct place >>$ rosrun stretch_calibration update_with_most_recent_calibration.sh Now as a sanity check that everything is working, try out the face detection demo .","title":"001_ROS_INSTALL"},{"location":"stretch-factory/updates/001_ROS_INSTALL/#001_ros_install","text":"","title":"001_ROS_INSTALL"},{"location":"stretch-factory/updates/001_ROS_INSTALL/#background","text":"Early Stretch units shipped without the stretch_ros stack installed. These scripts will install the necessary packages and bring the robot up to the current ROS compatible configuration.","title":"Background"},{"location":"stretch-factory/updates/001_ROS_INSTALL/#installation","text":"To upgrade a 'pre-ROS' Stretch to use the latest ROS software stack: >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory >>$ cd updates/STRETCH_UPDATES_001 >>$ ./stretch_install_system.sh >>$ ./stretch_updgrade_pre_ros.sh Then install your URDF to the correct place >>$ rosrun stretch_calibration update_with_most_recent_calibration.sh Now as a sanity check that everything is working, try out the face detection demo .","title":"Installation"},{"location":"stretch-factory/updates/002_HEAD_PAN/","text":"002_HEAD_PAN Note: This update is deprecated. Use update 005 instead. Background Early Stretch robots have a production issue where the range of motion of the head pan is unnecessarily restricted. This is due to: The Dynamixel servo encoder has a range of 0-4096 ticks which corresponds to a 360 degree range of motion. As configured, the servo can not move past the 4096 tick 'rollover point'. Proper installation of the head pan gear train ensures that this rollover point is outside of the normal range-of-motion of the head. In some early units, this rollover point is inside the normal range-of-motion. As a result, the head pan range is limited. In this case, the robot can look all the way to its left, but can not look to its right past approximately 180 degrees --whereas it should be able to look to its right 234 degrees. For reference, the nominal range of motion for the head is described here . Impact The performance is degraded in autonomous actions that require a large range of motion. In particular, when mapping an environment with FUNMAP . The servo may not respect the hardstop of the joint. This can cause it go into an error state due to overload of the servo as it pushes into the hardstop. Fix We will enable hardstop based homing of the head pan. This allows the Dynamixel servo to use Multiturn Mode (and avoid the encoder rollover issue). First, move to the latest Stretch Body package (version >=0.0.17) and the lastest Stretch Body Tools package (version >=0.0.13) >>$ pip2 install hello-robot-stretch-body >>$ pip2 install hello-robot-stretch-body-tools Now update the user YAML to enable homing of the head pan joint. Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml head_pan: #Fix for gear offset range_t: - 0 - 3820 req_calibration: 1 use_multiturn: 1 zero_t: 1155 pwm_homing: - -300 - 300 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well. Next, add this same bit of YAML to the factory image version of the file. This will ensure that when new user accounts are made the fix is applied. This file can be found at /etc/hello-robot/$HELLO_FLEET_ID/stretch_re1_user_params.yaml Quick Test Check that your head is back up and running correctly. Run stretch_head_jog.py >>$ stretch_head_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu a: increment pan 10 deg b: decrement pan 10 deg c: increment tilt 10 deg d: decrement tilt 10 deg e: ahead f: back g: tool h: wheels i: left j: up p: pan go to pos ticks t: tilt go to pos ticks x: home 1 : speed slow 2 : speed default 3 : speed fast 4 : speed max Try out the homing with the 'x' command. Verify that it looks straight ahead ('e') and looks straight back ('f') as expected when commanded from the tool's menu. URDF Calibration Finally, you will want to recalibrate the URDF. This is a slightly more involved process and can take around an hour. The process is described here . That's it, you're all set!","title":"002_HEAD_PAN"},{"location":"stretch-factory/updates/002_HEAD_PAN/#002_head_pan","text":"","title":"002_HEAD_PAN"},{"location":"stretch-factory/updates/002_HEAD_PAN/#note-this-update-is-deprecated-use-update-005-instead","text":"","title":"Note: This update is deprecated. Use update 005 instead."},{"location":"stretch-factory/updates/002_HEAD_PAN/#background","text":"Early Stretch robots have a production issue where the range of motion of the head pan is unnecessarily restricted. This is due to: The Dynamixel servo encoder has a range of 0-4096 ticks which corresponds to a 360 degree range of motion. As configured, the servo can not move past the 4096 tick 'rollover point'. Proper installation of the head pan gear train ensures that this rollover point is outside of the normal range-of-motion of the head. In some early units, this rollover point is inside the normal range-of-motion. As a result, the head pan range is limited. In this case, the robot can look all the way to its left, but can not look to its right past approximately 180 degrees --whereas it should be able to look to its right 234 degrees. For reference, the nominal range of motion for the head is described here .","title":"Background"},{"location":"stretch-factory/updates/002_HEAD_PAN/#impact","text":"The performance is degraded in autonomous actions that require a large range of motion. In particular, when mapping an environment with FUNMAP . The servo may not respect the hardstop of the joint. This can cause it go into an error state due to overload of the servo as it pushes into the hardstop.","title":"Impact"},{"location":"stretch-factory/updates/002_HEAD_PAN/#fix","text":"We will enable hardstop based homing of the head pan. This allows the Dynamixel servo to use Multiturn Mode (and avoid the encoder rollover issue). First, move to the latest Stretch Body package (version >=0.0.17) and the lastest Stretch Body Tools package (version >=0.0.13) >>$ pip2 install hello-robot-stretch-body >>$ pip2 install hello-robot-stretch-body-tools Now update the user YAML to enable homing of the head pan joint. Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml head_pan: #Fix for gear offset range_t: - 0 - 3820 req_calibration: 1 use_multiturn: 1 zero_t: 1155 pwm_homing: - -300 - 300 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well. Next, add this same bit of YAML to the factory image version of the file. This will ensure that when new user accounts are made the fix is applied. This file can be found at /etc/hello-robot/$HELLO_FLEET_ID/stretch_re1_user_params.yaml","title":"Fix"},{"location":"stretch-factory/updates/002_HEAD_PAN/#quick-test","text":"Check that your head is back up and running correctly. Run stretch_head_jog.py >>$ stretch_head_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu a: increment pan 10 deg b: decrement pan 10 deg c: increment tilt 10 deg d: decrement tilt 10 deg e: ahead f: back g: tool h: wheels i: left j: up p: pan go to pos ticks t: tilt go to pos ticks x: home 1 : speed slow 2 : speed default 3 : speed fast 4 : speed max Try out the homing with the 'x' command. Verify that it looks straight ahead ('e') and looks straight back ('f') as expected when commanded from the tool's menu.","title":"Quick Test"},{"location":"stretch-factory/updates/002_HEAD_PAN/#urdf-calibration","text":"Finally, you will want to recalibrate the URDF. This is a slightly more involved process and can take around an hour. The process is described here . That's it, you're all set!","title":"URDF Calibration"},{"location":"stretch-factory/updates/003_WRIST_SWAP/","text":"003_WRIST_SWAP Background After installing a new wrist module the system UDEV needs to be updated Update UDEV First, pull down the files >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory Now copy them over >>$ cd stretch_factory/updates/003_WRIST_SWAP >>$ sudo cp *.rules /etc/udev/rules.d >>$ sudo cp *.rules /etc/hello-robot/stretch-re1-1004/udev Now reboot. After reboot check that the new wrist shows up on the bus >>$ ls /dev/hello-dynamixel-wrist >>$ ls /dev/hello-wacc Test Wrist Then check that the Wacc is reporting sensor data back: >>$ stretch_wacc_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- ------------------------------ Ax ( m/s^2 ) 9 .8684213638 Ay ( m/s^2 ) 0 .506848096848 Az ( m/s^2 ) 0 .361166000366 A0 381 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 25 State 0 Debug 0 Timestamp 1601320914 .65 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1p0 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- Finally, jog the wrist yaw joint: >>$ stretch_wrist_yaw_home.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Moving to first hardstop... Contact at position: -3029 Hit first hardstop, marking to zero ticks Raw position: 14 Moving to calibrated zero: ( rad ) Update Wacc Calibration >>$ pip2 install hello-robot-stretch-factory >>$ RE1_wacc_calibrate.py RE1_wacc_calibrate.py Calibrating Wacc. Ensure arm is retracted and level to ground Hit enter when ready Itr 0 Val 9 .59977857901 ... Itr 99 Val 10 .1095601333 Got a average value of 10 .1372113882 Gravity scalar of 0 .967391 within bounds of 0 .900000 to 1 .100000 Writing yaml... Now copy the updated YAML to /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/stretch-re1-1004 >>$ sudo cp stretch_re1_factory_params.yaml /etc/hello-robot/stretch-re1-1004 You're all set!","title":"003_WRIST_SWAP"},{"location":"stretch-factory/updates/003_WRIST_SWAP/#003_wrist_swap","text":"","title":"003_WRIST_SWAP"},{"location":"stretch-factory/updates/003_WRIST_SWAP/#background","text":"After installing a new wrist module the system UDEV needs to be updated","title":"Background"},{"location":"stretch-factory/updates/003_WRIST_SWAP/#update-udev","text":"First, pull down the files >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory Now copy them over >>$ cd stretch_factory/updates/003_WRIST_SWAP >>$ sudo cp *.rules /etc/udev/rules.d >>$ sudo cp *.rules /etc/hello-robot/stretch-re1-1004/udev Now reboot. After reboot check that the new wrist shows up on the bus >>$ ls /dev/hello-dynamixel-wrist >>$ ls /dev/hello-wacc","title":"Update UDEV"},{"location":"stretch-factory/updates/003_WRIST_SWAP/#test-wrist","text":"Then check that the Wacc is reporting sensor data back: >>$ stretch_wacc_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- ------------------------------ Ax ( m/s^2 ) 9 .8684213638 Ay ( m/s^2 ) 0 .506848096848 Az ( m/s^2 ) 0 .361166000366 A0 381 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 25 State 0 Debug 0 Timestamp 1601320914 .65 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1p0 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- Finally, jog the wrist yaw joint: >>$ stretch_wrist_yaw_home.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Moving to first hardstop... Contact at position: -3029 Hit first hardstop, marking to zero ticks Raw position: 14 Moving to calibrated zero: ( rad )","title":"Test Wrist"},{"location":"stretch-factory/updates/003_WRIST_SWAP/#update-wacc-calibration","text":">>$ pip2 install hello-robot-stretch-factory >>$ RE1_wacc_calibrate.py RE1_wacc_calibrate.py Calibrating Wacc. Ensure arm is retracted and level to ground Hit enter when ready Itr 0 Val 9 .59977857901 ... Itr 99 Val 10 .1095601333 Got a average value of 10 .1372113882 Gravity scalar of 0 .967391 within bounds of 0 .900000 to 1 .100000 Writing yaml... Now copy the updated YAML to /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/stretch-re1-1004 >>$ sudo cp stretch_re1_factory_params.yaml /etc/hello-robot/stretch-re1-1004 You're all set!","title":"Update Wacc Calibration"},{"location":"stretch-factory/updates/004_HEAD_TILT/","text":"004_HEAD_TILT Background Tools to debug the head tilt unit not working (assuming not a mechanical failure or unplugged cable) Check that both the pan (ID 11) and tilt (ID 12) are on the bus >>$ RE1_dynamixel_id_scan.py /dev/hello-dynamixel-head [ Dynamixel ID:000 ] ping Failed. [ Dynamixel ID:001 ] ping Failed. [ Dynamixel ID:002 ] ping Failed. [ Dynamixel ID:003 ] ping Failed. [ Dynamixel ID:004 ] ping Failed. [ Dynamixel ID:005 ] ping Failed. [ Dynamixel ID:006 ] ping Failed. [ Dynamixel ID:007 ] ping Failed. [ Dynamixel ID:008 ] ping Failed. [ Dynamixel ID:009 ] ping Failed. [ Dynamixel ID:010 ] ping Failed. [ Dynamixel ID:011 ] ping Succeeded. Dynamixel model number : 1060 [ Dynamixel ID:012 ] ping Succeeded. Dynamixel model number : 1060 [ Dynamixel ID:013 ] ping Failed. [ Dynamixel ID:014 ] ping Failed. [ Dynamixel ID:015 ] ping Failed. [ Dynamixel ID:016 ] ping Failed. [ Dynamixel ID:017 ] ping Failed. [ Dynamixel ID:018 ] ping Failed. [ Dynamixel ID:019 ] ping Failed. Things to try Directly jog the tilt joint (ID 12) from the menu >>$ RE1_dynamixel_jog.py /dev/hello-dynamixel-head 12 Directly jog the pan joint (ID 11) from the menu >>$ RE1_dynamixel_jog.py /dev/hello-dynamixel-head 11 If you think a servo may have overheated, reboot the servos of the head >>$ RE1_dynamixel_reboot.py /dev/hello-dynamixel-head [ Dynamixel ID:011 ] Reboot Succeeded. [ Dynamixel ID:012 ] Reboot Succeeded. Jog the entire head. Verify the looking ahead, back, etc from the menu work as expcted >>$ stretch_head_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu a: increment pan 10 deg b: decrement pan 10 deg c: increment tilt 10 deg d: decrement tilt 10 deg e: ahead f: back g: tool h: wheels i: left j: up p: pan go to pos ticks t: tilt go to pos ticks x: home 1 : speed slow 2 : speed default 3 : speed fast 4 : speed max -------------------","title":"004_HEAD_TILT"},{"location":"stretch-factory/updates/004_HEAD_TILT/#004_head_tilt","text":"","title":"004_HEAD_TILT"},{"location":"stretch-factory/updates/004_HEAD_TILT/#background","text":"Tools to debug the head tilt unit not working (assuming not a mechanical failure or unplugged cable) Check that both the pan (ID 11) and tilt (ID 12) are on the bus >>$ RE1_dynamixel_id_scan.py /dev/hello-dynamixel-head [ Dynamixel ID:000 ] ping Failed. [ Dynamixel ID:001 ] ping Failed. [ Dynamixel ID:002 ] ping Failed. [ Dynamixel ID:003 ] ping Failed. [ Dynamixel ID:004 ] ping Failed. [ Dynamixel ID:005 ] ping Failed. [ Dynamixel ID:006 ] ping Failed. [ Dynamixel ID:007 ] ping Failed. [ Dynamixel ID:008 ] ping Failed. [ Dynamixel ID:009 ] ping Failed. [ Dynamixel ID:010 ] ping Failed. [ Dynamixel ID:011 ] ping Succeeded. Dynamixel model number : 1060 [ Dynamixel ID:012 ] ping Succeeded. Dynamixel model number : 1060 [ Dynamixel ID:013 ] ping Failed. [ Dynamixel ID:014 ] ping Failed. [ Dynamixel ID:015 ] ping Failed. [ Dynamixel ID:016 ] ping Failed. [ Dynamixel ID:017 ] ping Failed. [ Dynamixel ID:018 ] ping Failed. [ Dynamixel ID:019 ] ping Failed.","title":"Background"},{"location":"stretch-factory/updates/004_HEAD_TILT/#things-to-try","text":"Directly jog the tilt joint (ID 12) from the menu >>$ RE1_dynamixel_jog.py /dev/hello-dynamixel-head 12 Directly jog the pan joint (ID 11) from the menu >>$ RE1_dynamixel_jog.py /dev/hello-dynamixel-head 11 If you think a servo may have overheated, reboot the servos of the head >>$ RE1_dynamixel_reboot.py /dev/hello-dynamixel-head [ Dynamixel ID:011 ] Reboot Succeeded. [ Dynamixel ID:012 ] Reboot Succeeded. Jog the entire head. Verify the looking ahead, back, etc from the menu work as expcted >>$ stretch_head_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu a: increment pan 10 deg b: decrement pan 10 deg c: increment tilt 10 deg d: decrement tilt 10 deg e: ahead f: back g: tool h: wheels i: left j: up p: pan go to pos ticks t: tilt go to pos ticks x: home 1 : speed slow 2 : speed default 3 : speed fast 4 : speed max -------------------","title":"Things to try"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/","text":"005_HEAD_PAN_CALIBRATION Background The URDF calibration is very sensitive to the 'zero' point of the head pan actuator. We've found that recalibration of the zero point may be necessary on occaission -- for example if the gear teeth of the joint have skipped due to very high loading. Starting with Stretch serial number stretch-re1-1023 we've moved to a new method of setting the zero point. This new method allows the user to recalibrate the joint in the field. Update YAML Robots with serial numbers stretch-re1-1001 to stretch-re1-1022 will need to update their user YAML prior to running the recalibration procedure below. Later robots do not need to update their YAML. Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml head_pan: range_t: - 0 - 3827 use_multiturn: 1 zero_t: 1165 pwm_homing: - -300 - 300 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well. Recalibration First, move to the latest Stretch Body package (version >=0.0.17) and the latest Stretch Factory package (version >=0.0.14) >>$ pip2 install hello-robot-stretch-body >>$ pip2 install hello-robot-stretch-factory Now run the recalibration script. This will find the CCW hardstop and mark its position in the servos EEPROM. >>$ RE1_head_calibrate_pan.py About to calibrate the head pan. Doing so will require you to recalibrated your URDF. Proceed ( y/n ) ? y Moving to first hardstop... Contact at position: -3 Hit first hardstop, marking to zero ticks Raw position: 33 Moving to calibrated zero: ( rad ) Recalibration done . Now redo the URDF calibration ( see stretch_ros documentation ) URDF Recalibration Finally, you will want to recalibrate the URDF. This is a slightly more involved process and can take around an hour. The process is described here . That's it, you're all set!","title":"005_HEAD_PAN_CALIBRATION"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/#005_head_pan_calibration","text":"","title":"005_HEAD_PAN_CALIBRATION"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/#background","text":"The URDF calibration is very sensitive to the 'zero' point of the head pan actuator. We've found that recalibration of the zero point may be necessary on occaission -- for example if the gear teeth of the joint have skipped due to very high loading. Starting with Stretch serial number stretch-re1-1023 we've moved to a new method of setting the zero point. This new method allows the user to recalibrate the joint in the field.","title":"Background"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/#update-yaml","text":"Robots with serial numbers stretch-re1-1001 to stretch-re1-1022 will need to update their user YAML prior to running the recalibration procedure below. Later robots do not need to update their YAML. Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml head_pan: range_t: - 0 - 3827 use_multiturn: 1 zero_t: 1165 pwm_homing: - -300 - 300 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well.","title":"Update YAML"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/#recalibration","text":"First, move to the latest Stretch Body package (version >=0.0.17) and the latest Stretch Factory package (version >=0.0.14) >>$ pip2 install hello-robot-stretch-body >>$ pip2 install hello-robot-stretch-factory Now run the recalibration script. This will find the CCW hardstop and mark its position in the servos EEPROM. >>$ RE1_head_calibrate_pan.py About to calibrate the head pan. Doing so will require you to recalibrated your URDF. Proceed ( y/n ) ? y Moving to first hardstop... Contact at position: -3 Hit first hardstop, marking to zero ticks Raw position: 33 Moving to calibrated zero: ( rad ) Recalibration done . Now redo the URDF calibration ( see stretch_ros documentation )","title":"Recalibration"},{"location":"stretch-factory/updates/005_HEAD_PAN_CALIBRATION/#urdf-recalibration","text":"Finally, you will want to recalibrate the URDF. This is a slightly more involved process and can take around an hour. The process is described here . That's it, you're all set!","title":"URDF Recalibration"},{"location":"stretch-factory/updates/006_DXL_RUNSTOP/","text":"006_DXL_RUNSTOP Background For robots with serial number prior to stretch-re1-1023 the Dynamixel servos do not respond to the runstop button. While the robot's hardware architecture prevents integrating the runstop with the Robotis servos, we have implemented a software update that simulates this behavior. This functionality is standard with robots starting with stretch-re1-1023 . Note: This runstop behavior is only effective when there is an instance of the Robot class running. Update YAML Robots with serial numbers stretch-re1-1001 to stretch-re1-1022 will need to update their user YAML Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml robot_sentry: dynamixel_stop_on_runstop: 1 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well. Update Stretch Body First, move to the latest Stretch Body package (version >=0.0.19) >>$ pip2 install hello-robot-stretch-body Now test it out. Try running the Xbox controller and verify that the robot head, wrist, and gripper stop their motion when the runstop is hit. >>$ stretch_xbox_controller_teleop.py That's it!","title":"006_DXL_RUNSTOP"},{"location":"stretch-factory/updates/006_DXL_RUNSTOP/#006_dxl_runstop","text":"","title":"006_DXL_RUNSTOP"},{"location":"stretch-factory/updates/006_DXL_RUNSTOP/#background","text":"For robots with serial number prior to stretch-re1-1023 the Dynamixel servos do not respond to the runstop button. While the robot's hardware architecture prevents integrating the runstop with the Robotis servos, we have implemented a software update that simulates this behavior. This functionality is standard with robots starting with stretch-re1-1023 . Note: This runstop behavior is only effective when there is an instance of the Robot class running.","title":"Background"},{"location":"stretch-factory/updates/006_DXL_RUNSTOP/#update-yaml","text":"Robots with serial numbers stretch-re1-1001 to stretch-re1-1022 will need to update their user YAML Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml robot_sentry: dynamixel_stop_on_runstop: 1 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well.","title":"Update YAML"},{"location":"stretch-factory/updates/006_DXL_RUNSTOP/#update-stretch-body","text":"First, move to the latest Stretch Body package (version >=0.0.19) >>$ pip2 install hello-robot-stretch-body Now test it out. Try running the Xbox controller and verify that the robot head, wrist, and gripper stop their motion when the runstop is hit. >>$ stretch_xbox_controller_teleop.py That's it!","title":"Update Stretch Body"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/","text":"007_LIFT_FINGER_GUARD Background Installing the foam finger guards on the lift will require adjustment of the robot calibration. When the lift homes in the upward direction, it will now stop short of its true hardstop. To adjust for this we will pad the lift range of motion in YAML. We will then test that the URDF calibration is still in spec. If it is out of spec, we will recalibrate the URDF. Update YAML Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml . This will override the default range setting, subtracting 6mm in each direction lift: range_m: - 0.006 - 1.0939 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well. Test New Range of Motion Home the lift and then check that the lift behaves well at the hardstops. Using the menu, jog the lift to each end of range of motion. >>$ stretch_lift_home.py >>$ stretch_lift_jog.py URDF Calibration First update to the latest version of stretch_ros >>$ cd ~/catkin_ws/src/stretch_ros >>$ git pull Now do the calibration: >>$ stretch_robot_home.py >>$ rosrun stretch_calibration update_uncalibrated_urdf.sh >>$ roslaunch stretch_calibration collect_head_calibration_data.launch The robot will collect calibration samples. This will take about 5 minutes. Then: >>$ roslaunch stretch_calibration process_head_calibration_data.launch This will take about an hour. Check that the reported total error at the end of calibration is low (<0.03). If the fit is good, start using the calibration. >>$ rosrun stretch_calibration update_with_most_recent_calibration.sh And visually inspect the fit >>$ >>$ roslaunch stretch_calibration simple_test_head_calibration.launch The full URDF calibration tutorial is found here","title":"007_LIFT_FINGER_GUARD"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/#007_lift_finger_guard","text":"","title":"007_LIFT_FINGER_GUARD"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/#background","text":"Installing the foam finger guards on the lift will require adjustment of the robot calibration. When the lift homes in the upward direction, it will now stop short of its true hardstop. To adjust for this we will pad the lift range of motion in YAML. We will then test that the URDF calibration is still in spec. If it is out of spec, we will recalibrate the URDF.","title":"Background"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/#update-yaml","text":"Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml . This will override the default range setting, subtracting 6mm in each direction lift: range_m: - 0.006 - 1.0939 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well.","title":"Update YAML"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/#test-new-range-of-motion","text":"Home the lift and then check that the lift behaves well at the hardstops. Using the menu, jog the lift to each end of range of motion. >>$ stretch_lift_home.py >>$ stretch_lift_jog.py","title":"Test New Range of Motion"},{"location":"stretch-factory/updates/007_LIFT_FINGER_GUARD/#urdf-calibration","text":"First update to the latest version of stretch_ros >>$ cd ~/catkin_ws/src/stretch_ros >>$ git pull Now do the calibration: >>$ stretch_robot_home.py >>$ rosrun stretch_calibration update_uncalibrated_urdf.sh >>$ roslaunch stretch_calibration collect_head_calibration_data.launch The robot will collect calibration samples. This will take about 5 minutes. Then: >>$ roslaunch stretch_calibration process_head_calibration_data.launch This will take about an hour. Check that the reported total error at the end of calibration is low (<0.03). If the fit is good, start using the calibration. >>$ rosrun stretch_calibration update_with_most_recent_calibration.sh And visually inspect the fit >>$ >>$ roslaunch stretch_calibration simple_test_head_calibration.launch The full URDF calibration tutorial is found here","title":"URDF Calibration"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/","text":"008_SYNC_TIMESTAMPS Background Prior to this update the sensor data coming from Stretch Body was only timestamped using the Linux clock. Timestamping was done at the time of the read of the USB bus and was therefore subject to OS dependent jitter and accuracy. Starting with this update Stretch has the ability to provide synchronized timestamps based on its microcontroller clock and a hardware sync line. Details on the timestamping function are found in this tutorial [Coming soon]. Update Firmware Install the latest version of the firmware for the Wacc, Pimu, and Steppers. NOTE: For now you will want the sync_timestamp branch of the git repository. You'll need to pull it down. >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_firmware -b sync_timestamp Then follow the tutorial for upgrading firmware (Note your Stretch may already have the Arduino IDE installed and configured). Update YAML Robots with serial numbers stretch-re1-1001 to stretch-re1-1022 will need to update their user YAML Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml pimu_clock_manager: n_slew_history: 25 trs: 450.0 use_skew_compensation: 1 wacc_clock_manager: n_slew_history: 25 trs: 687.0 use_skew_compensation: 1 robot_timestamp_manager: sync_mode_enabled: 1 time_align_status: 1 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well. Update Stretch Body NOTE: For now pull down the sync_timestamp branch of the git repository and install that. >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_body -b sync_timestamp >>$ cd ~/repos/stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh FUTURE: First, move to the latest Stretch Body package (version >=0.0.20) >>$ pip2 install hello-robot-stretch-body Try It Out Now test it out. Try running the timestamp jog tool. >>$ stretch_robot_timestamps_jog.py --display For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ Timestamp Manager ----- Sync mode enabled : 1 Status ID : 121 Wall time : 1607575532 .977621 Hardware sync : 1607575532 .939872 Pimu IMU : 1607575532 .930712 Lift Encdoer : 1607575532 .938915 Arm Encoder : 1607575532 .938884 Right Wheel Encoder : 1607575532 .939187 Left Wheel Encoder : 1607575532 .938882 Wacc Accel : 1607575532 .934294 ------ Timestamp Manager ----- Sync mode enabled : 1 Status ID : 125 Wall time : 1607575533 .187324 Hardware sync : 1607575533 .148872 Pimu IMU : 1607575533 .140704 Lift Encdoer : 1607575533 .147775 Arm Encoder : 1607575533 .148192 Right Wheel Encoder : 1607575533 .148234 Left Wheel Encoder : 1607575533 .147883 Wacc Accel : 1607575533 .142721 ... >>$ stretch_robot_timestamps_jog.py --sensor_delta For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Starting sensor timestamp analysis... Sync mode enabled: 1 Time align status: 0 Use skew compensation: 1 --------------------------- DT Pimu IMU :-10152 DT Left Wheel Encoder :-717 DT Right Wheel Encoder :-1068 DT Lift Encoder :-361 DT Arm Encoder :-337 DT Wacc Accel :5703 --------------------------- DT Pimu IMU :-7148 DT Left Wheel Encoder :-708 DT Right Wheel Encoder :-953 DT Lift Encoder :-470 DT Arm Encoder :-864 DT Wacc Accel :84 >>$ stretch_robot_timestamps_jog.py --sensor_stats That's it!","title":"008_SYNC_TIMESTAMPS"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#008_sync_timestamps","text":"","title":"008_SYNC_TIMESTAMPS"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#background","text":"Prior to this update the sensor data coming from Stretch Body was only timestamped using the Linux clock. Timestamping was done at the time of the read of the USB bus and was therefore subject to OS dependent jitter and accuracy. Starting with this update Stretch has the ability to provide synchronized timestamps based on its microcontroller clock and a hardware sync line. Details on the timestamping function are found in this tutorial [Coming soon].","title":"Background"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#update-firmware","text":"Install the latest version of the firmware for the Wacc, Pimu, and Steppers. NOTE: For now you will want the sync_timestamp branch of the git repository. You'll need to pull it down. >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_firmware -b sync_timestamp Then follow the tutorial for upgrading firmware (Note your Stretch may already have the Arduino IDE installed and configured).","title":"Update Firmware"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#update-yaml","text":"Robots with serial numbers stretch-re1-1001 to stretch-re1-1022 will need to update their user YAML Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml pimu_clock_manager: n_slew_history: 25 trs: 450.0 use_skew_compensation: 1 wacc_clock_manager: n_slew_history: 25 trs: 687.0 use_skew_compensation: 1 robot_timestamp_manager: sync_mode_enabled: 1 time_align_status: 1 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well.","title":"Update YAML"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#update-stretch-body","text":"NOTE: For now pull down the sync_timestamp branch of the git repository and install that. >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_body -b sync_timestamp >>$ cd ~/repos/stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh FUTURE: First, move to the latest Stretch Body package (version >=0.0.20) >>$ pip2 install hello-robot-stretch-body","title":"Update Stretch Body"},{"location":"stretch-factory/updates/008_SYNC_TIMESTAMPS/#try-it-out","text":"Now test it out. Try running the timestamp jog tool. >>$ stretch_robot_timestamps_jog.py --display For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ Timestamp Manager ----- Sync mode enabled : 1 Status ID : 121 Wall time : 1607575532 .977621 Hardware sync : 1607575532 .939872 Pimu IMU : 1607575532 .930712 Lift Encdoer : 1607575532 .938915 Arm Encoder : 1607575532 .938884 Right Wheel Encoder : 1607575532 .939187 Left Wheel Encoder : 1607575532 .938882 Wacc Accel : 1607575532 .934294 ------ Timestamp Manager ----- Sync mode enabled : 1 Status ID : 125 Wall time : 1607575533 .187324 Hardware sync : 1607575533 .148872 Pimu IMU : 1607575533 .140704 Lift Encdoer : 1607575533 .147775 Arm Encoder : 1607575533 .148192 Right Wheel Encoder : 1607575533 .148234 Left Wheel Encoder : 1607575533 .147883 Wacc Accel : 1607575533 .142721 ... >>$ stretch_robot_timestamps_jog.py --sensor_delta For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Starting sensor timestamp analysis... Sync mode enabled: 1 Time align status: 0 Use skew compensation: 1 --------------------------- DT Pimu IMU :-10152 DT Left Wheel Encoder :-717 DT Right Wheel Encoder :-1068 DT Lift Encoder :-361 DT Arm Encoder :-337 DT Wacc Accel :5703 --------------------------- DT Pimu IMU :-7148 DT Left Wheel Encoder :-708 DT Right Wheel Encoder :-953 DT Lift Encoder :-470 DT Arm Encoder :-864 DT Wacc Accel :84 >>$ stretch_robot_timestamps_jog.py --sensor_stats That's it!","title":"Try It Out"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/","text":"009_STEPPER_STARTUP Background A bug exists on earlier Stretch RE1 related to the startup conditions of the stepper controller. Power up machine Command base steppers in velocity mode Command base steppers in position mode The position mode command can cause sudden motion of the base as its controller is not correctly initialized. This is a firmware bug. To replicate the bug: Place the base on a thick book or other object to get the wheels off the ground Power up machine from off state Run code below import stretch_body.robot from time import sleep robot = stretch_body . robot . Robot () robot . startup () robot . base . set_translate_velocity ( 0 ) robot . push_command () robot . base . translate_by ( 0.1 ) #Causes the base to lurch forward robot . push_command () Affected Robots This bug affects firmware version Stepper.v0.0.1p0 . To check your firmware version (of the arm for example), run the following and hit enter to print the actuator status: >>$ RE1_stepper_jog.py hello-motor-arm .. Firmware version: Stepper.v0.0.1p0 Fix To fix the bug, the stepper firmware must be updated to version Stepper.v0.0.2p0 or later. Note: Do not attempt to perform a firmware upgrade without contacting Hello Robot first. You will need to: Pull down the latest version of Stretch Factory from PyPi Follow the firmware updater instructions provided here . Verify To verify that the fix, try the test code again import stretch_body.robot from time import sleep robot = stretch_body . robot . Robot () robot . startup () robot . base . set_translate_velocity ( 0 ) robot . push_command () robot . base . translate_by ( 0.1 ) #Causes smooth motion forward robot . push_command ()","title":"009_STEPPER_STARTUP"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/#009_stepper_startup","text":"","title":"009_STEPPER_STARTUP"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/#background","text":"A bug exists on earlier Stretch RE1 related to the startup conditions of the stepper controller. Power up machine Command base steppers in velocity mode Command base steppers in position mode The position mode command can cause sudden motion of the base as its controller is not correctly initialized. This is a firmware bug. To replicate the bug: Place the base on a thick book or other object to get the wheels off the ground Power up machine from off state Run code below import stretch_body.robot from time import sleep robot = stretch_body . robot . Robot () robot . startup () robot . base . set_translate_velocity ( 0 ) robot . push_command () robot . base . translate_by ( 0.1 ) #Causes the base to lurch forward robot . push_command ()","title":"Background"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/#affected-robots","text":"This bug affects firmware version Stepper.v0.0.1p0 . To check your firmware version (of the arm for example), run the following and hit enter to print the actuator status: >>$ RE1_stepper_jog.py hello-motor-arm .. Firmware version: Stepper.v0.0.1p0","title":"Affected Robots"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/#fix","text":"To fix the bug, the stepper firmware must be updated to version Stepper.v0.0.2p0 or later. Note: Do not attempt to perform a firmware upgrade without contacting Hello Robot first. You will need to: Pull down the latest version of Stretch Factory from PyPi Follow the firmware updater instructions provided here .","title":"Fix"},{"location":"stretch-factory/updates/009_STEPPER_STARTUP/#verify","text":"To verify that the fix, try the test code again import stretch_body.robot from time import sleep robot = stretch_body . robot . Robot () robot . startup () robot . base . set_translate_velocity ( 0 ) robot . push_command () robot . base . translate_by ( 0.1 ) #Causes smooth motion forward robot . push_command ()","title":"Verify"},{"location":"stretch-factory/updates/010_WRIST_SWAP/","text":"010_WRIST_SWAP Background After installing a new wrist module the system UDEV needs to be updated Update UDEV First, pull down the files >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory Now copy them over >>$ cd stretch_factory/updates/010_WRIST_SWAP >>$ sudo cp *.rules /etc/udev/rules.d >>$ sudo cp *.rules /etc/hello-robot/stretch-re1-1013/udev Now reboot. After reboot check that the new wrist shows up on the bus >>$ ls /dev/hello-dynamixel-wrist >>$ ls /dev/hello-wacc Test Wrist Then check that the Wacc is reporting sensor data back: >>$ stretch_wacc_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- ------------------------------ Ax ( m/s^2 ) 9 .8684213638 Ay ( m/s^2 ) 0 .506848096848 Az ( m/s^2 ) 0 .361166000366 A0 381 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 25 State 0 Debug 0 Timestamp 1601320914 .65 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1p0 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- Finally, jog the wrist yaw joint: >>$ stretch_wrist_yaw_home.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Moving to first hardstop... Contact at position: -3029 Hit first hardstop, marking to zero ticks Raw position: 14 Moving to calibrated zero: ( rad ) Update Wacc Calibration >>$ pip2 install hello-robot-stretch-factory >>$ RE1_wacc_calibrate.py RE1_wacc_calibrate.py Calibrating Wacc. Ensure arm is retracted and level to ground Hit enter when ready Itr 0 Val 9 .59977857901 ... Itr 99 Val 10 .1095601333 Got a average value of 10 .1372113882 Gravity scalar of 0 .967391 within bounds of 0 .900000 to 1 .100000 Writing yaml... Now copy the updated YAML to /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/stretch-re1-1013 >>$ sudo cp stretch_re1_factory_params.yaml /etc/hello-robot/stretch-re1-1013 You're all set!","title":"010_WRIST_SWAP"},{"location":"stretch-factory/updates/010_WRIST_SWAP/#010_wrist_swap","text":"","title":"010_WRIST_SWAP"},{"location":"stretch-factory/updates/010_WRIST_SWAP/#background","text":"After installing a new wrist module the system UDEV needs to be updated","title":"Background"},{"location":"stretch-factory/updates/010_WRIST_SWAP/#update-udev","text":"First, pull down the files >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory Now copy them over >>$ cd stretch_factory/updates/010_WRIST_SWAP >>$ sudo cp *.rules /etc/udev/rules.d >>$ sudo cp *.rules /etc/hello-robot/stretch-re1-1013/udev Now reboot. After reboot check that the new wrist shows up on the bus >>$ ls /dev/hello-dynamixel-wrist >>$ ls /dev/hello-wacc","title":"Update UDEV"},{"location":"stretch-factory/updates/010_WRIST_SWAP/#test-wrist","text":"Then check that the Wacc is reporting sensor data back: >>$ stretch_wacc_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- ------------------------------ Ax ( m/s^2 ) 9 .8684213638 Ay ( m/s^2 ) 0 .506848096848 Az ( m/s^2 ) 0 .361166000366 A0 381 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 25 State 0 Debug 0 Timestamp 1601320914 .65 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1p0 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- Finally, jog the wrist yaw joint: >>$ stretch_wrist_yaw_home.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Moving to first hardstop... Contact at position: -3029 Hit first hardstop, marking to zero ticks Raw position: 14 Moving to calibrated zero: ( rad )","title":"Test Wrist"},{"location":"stretch-factory/updates/010_WRIST_SWAP/#update-wacc-calibration","text":">>$ pip2 install hello-robot-stretch-factory >>$ RE1_wacc_calibrate.py RE1_wacc_calibrate.py Calibrating Wacc. Ensure arm is retracted and level to ground Hit enter when ready Itr 0 Val 9 .59977857901 ... Itr 99 Val 10 .1095601333 Got a average value of 10 .1372113882 Gravity scalar of 0 .967391 within bounds of 0 .900000 to 1 .100000 Writing yaml... Now copy the updated YAML to /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/stretch-re1-1013 >>$ sudo cp stretch_re1_factory_params.yaml /etc/hello-robot/stretch-re1-1013 You're all set!","title":"Update Wacc Calibration"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/","text":"011_HEAD_TILT_SWAP Background After installing a new head tilt module the URDF calibration needs to be updated. In addition, we will want to store a local copy of the D435i calibration data. Test Head First check that the new head hardware is working correctly. Jog the head around using the command line tool: >>$ stretch_head_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu a: increment pan 10 deg b: decrement pan 10 deg c: increment tilt 10 deg d: decrement tilt 10 deg e: ahead f: back g: tool h: wheels i: left j: up p: pan go to pos ticks t: tilt go to pos ticks x: home 1 : speed slow 2 : speed default 3 : speed fast 4 : speed max ------------------- Next check that the D435i Camera can generate point clouds: >>$ realsense-viewer Update D435i Calibration Data First, pull down the files >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory Now copy them over >>$ cd ~/repos/stretch_factory/updates/011_WRIST_SWAP >>$ sudo cp * ~/stretch_user/stretch-re1-1007/calibration_D435i >>$ sudo cp * /etc/hello-robot/stretch-re1-1007/calibration_D435i Update URDF Calibration The URDF will need re-calibration given the new head hardware. The calibration procedure is described in detail here . You're all set!","title":"011_HEAD_TILT_SWAP"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/#011_head_tilt_swap","text":"","title":"011_HEAD_TILT_SWAP"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/#background","text":"After installing a new head tilt module the URDF calibration needs to be updated. In addition, we will want to store a local copy of the D435i calibration data.","title":"Background"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/#test-head","text":"First check that the new head hardware is working correctly. Jog the head around using the command line tool: >>$ stretch_head_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu a: increment pan 10 deg b: decrement pan 10 deg c: increment tilt 10 deg d: decrement tilt 10 deg e: ahead f: back g: tool h: wheels i: left j: up p: pan go to pos ticks t: tilt go to pos ticks x: home 1 : speed slow 2 : speed default 3 : speed fast 4 : speed max ------------------- Next check that the D435i Camera can generate point clouds: >>$ realsense-viewer","title":"Test Head"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/#update-d435i-calibration-data","text":"First, pull down the files >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory Now copy them over >>$ cd ~/repos/stretch_factory/updates/011_WRIST_SWAP >>$ sudo cp * ~/stretch_user/stretch-re1-1007/calibration_D435i >>$ sudo cp * /etc/hello-robot/stretch-re1-1007/calibration_D435i","title":"Update D435i Calibration Data"},{"location":"stretch-factory/updates/011_HEAD_TILT_SWAP/#update-urdf-calibration","text":"The URDF will need re-calibration given the new head hardware. The calibration procedure is described in detail here . You're all set!","title":"Update URDF Calibration"},{"location":"stretch-factory/updates/012_DEX_WRIST/","text":"012_DEX_WRIST Background This update installs and configures the Beta unit of the Stretch Dex Wrist - Beta. The procedure involves Install and configure the new Wacc board Install Stretch software packages Attach the Dexterous Wrist Update the Dynamixel servo baud rates Update the robot YAML Test the wrist with the XBox controller Configure for use in ROS Install and configure the new Wacc board Robots prior to the 'Joplin' batch will need an upgraded Wacc board that will be provided by Hello Robot. See the update 013_WACC_INSTALL Install Stretch Body Software Packages You'll be installing a local beta version of relevant Stretch Body packages >>$ cd ~/repos >>$ mkdir dex_wrist >>$ cd dex_wrist >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_body >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_tool_share >>$ git clone https://github.com/hello-robot/stretch_factory >> >>$ cd stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh >>$ pip2 install urdfpy >> >>$ pip2 install hello-robot-stretch-tool-share >>$ cd ../../stretch_tool_share/python >>$ ./local_install.sh >>$ cd ~/repos/dex_wrist/stretch_factory/python >> ./local_install.sh Attach the Dexterous Wrist First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Next, note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Next, using a Philips screwdriver, attach the wrist mount bracket to the bottom of the tool plate using the provided M2 bolts. NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Next, raise the wrist module up vertically into the mounting bracket, then sliding it over horizontally so that the bearing mates onto its post. Slide in the 3D printed spacer between the pitch servo and the mounting bracket. Finally, attach the body of the pitch servo to the mounting bracket using the 3 M2.5 screws provided. ( NOTE: Flat head screws provided, socket head screws shown below.) Update the Dynamixel servo baud rates The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600. >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud Update the robot YAML The new wrist requires a number of updates to the robot YAML YAML doesn't allow definition of multiple fields with the same name. Depending on what is already listed in your YAML you may need to manually edit and merge fields. Add the following to you your ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml factory_params : stretch_re1_factory_params.yaml params : - stretch_tool_share.stretch_dex_wrist_beta.params head : baud : 115200 end_of_arm : baud : 115200 tool : tool_stretch_dex_wrist #tool: tool_stretch_gripper robot : use_collision_manager : 1 head_pan : baud : 115200 head_tilt : baud : 115200 wrist_yaw : baud : 115200 stretch_gripper : baud : 115200 range_t : - 0 - 6667 zero_t : 3817 lift : i_feedforward : 0.75 hello-motor-lift : gains : i_safety_feedforward : 0.75 Each user account on Stretch will need to update their YAML as well. It is recommended practice to stored a reference of the YAML in /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/ $HELLO_FLEET_ID >>$ sudo cp *.yaml /etc/hello-robot/ $HELLO_FLEET_ID Configure for use in ROS First pull down the new stretch_ros branch and copy in the tool description: >>$ cd ~/catkin_ws/src/stretch_ros/ >>$ git pull >>$ git checkout feature/pluggable_end_effector >>$ cd ~/repos/dex_wrist/stretch_tool_share/tool_share/stretch_dex_wrist_beta/stretch_description >>$ cp urdf/stretch_dex_wrist_beta.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes Now configure stretch_description.xacro to use the StretchDexWrist tool: >>$ nano ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro to read, <?xml version = \"1.0\" ?> <robot xmlns:xacro = \"http://www.ros.org/wiki/xacro\" name = \"stretch_description\" > <xacro:include filename = \"stretch_dex_wrist_beta.xacro\" /> <xacro:include filename = \"stretch_main.xacro\" /> <xacro:include filename = \"stretch_aruco.xacro\" /> <xacro:include filename = \"stretch_d435i.xacro\" /> <xacro:include filename = \"stretch_laser_range_finder.xacro\" /> <xacro:include filename = \"stretch_respeaker.xacro\" /> </robot> Update your URDF and then export the URDF for Stretch Body to use (you may need to Ctrl-C to exit rosrun ) >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ ./export_urdf.sh Test the wrist with the XBox Controller Try out the new wrist! Note that the new key mapping does not allow for control of the head. >>$ stretch_xbox_controller_teleop.py Ctrl-C to exit. A printable copy of the teleoperation interface is here Test the wrist with RViz Now check that the wrist appears in RVIZ and can be controlled from the keyboard interface: >>$ roslaunch stretch_calibration simple_test_head_calibration.launch You can type 'q' then Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU -----------| | | | i HEAD UP | | j HEAD LEFT l HEAD RIGHT | | , HEAD DOWN | | | | | | 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT | | home page-up | | | | | | 8 LIFT UP | | up-arrow | | 4 BASE FORWARD 6 BASE BACK | | left-arrow right-arrow | | 2 LIFT DOWN | | down-arrow | | | | | | w ARM OUT | | a WRIST FORWARD d WRIST BACK | | x ARM IN | | | | | | c PITCH FORWARD v PITCH BACK | | o ROLL FORWARD p ROLL BACK | | 5 GRIPPER CLOSE | | 0 GRIPPER OPEN | | | | step size: b BIG, m MEDIUM, s SMALL | | q QUIT | | | |-------------------------------------------| Using the Stretch Dex Wrist Additional care should be taken when working with the Dex Wrist as it is now easier to accidentally collide the wrist and gripper with the robot, particularly during lift descent. We've implemented a very coarse collision avoidance behavior in Stretch Body that is turned on by default. It is conservative and doesn't fully prevent collisions however. The collision avoidance can be turned off in the user YAML by: collision_stretch_dex_wrist_to_base : enabled : 0 collision_stretch_dex_wrist_to_self : enabled : 0 You can jog the individual joints of the wrist using the tool: >>$ stretch_dex_wrist_jog.py --pitch Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () For reference, the parameters for the Stretch Dex Wrist (which can be overriden in the user YAML) are found at .local/lib/python2.7/site-packages/stretch_tool_share/stretch_dex_wrist_beta/params.py","title":"012_DEX_WRIST"},{"location":"stretch-factory/updates/012_DEX_WRIST/#012_dex_wrist","text":"","title":"012_DEX_WRIST"},{"location":"stretch-factory/updates/012_DEX_WRIST/#background","text":"This update installs and configures the Beta unit of the Stretch Dex Wrist - Beta. The procedure involves Install and configure the new Wacc board Install Stretch software packages Attach the Dexterous Wrist Update the Dynamixel servo baud rates Update the robot YAML Test the wrist with the XBox controller Configure for use in ROS","title":"Background"},{"location":"stretch-factory/updates/012_DEX_WRIST/#install-and-configure-the-new-wacc-board","text":"Robots prior to the 'Joplin' batch will need an upgraded Wacc board that will be provided by Hello Robot. See the update 013_WACC_INSTALL","title":"Install and configure the new Wacc board"},{"location":"stretch-factory/updates/012_DEX_WRIST/#install-stretch-body-software-packages","text":"You'll be installing a local beta version of relevant Stretch Body packages >>$ cd ~/repos >>$ mkdir dex_wrist >>$ cd dex_wrist >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_body >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_tool_share >>$ git clone https://github.com/hello-robot/stretch_factory >> >>$ cd stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh >>$ pip2 install urdfpy >> >>$ pip2 install hello-robot-stretch-tool-share >>$ cd ../../stretch_tool_share/python >>$ ./local_install.sh >>$ cd ~/repos/dex_wrist/stretch_factory/python >> ./local_install.sh","title":"Install Stretch Body Software Packages"},{"location":"stretch-factory/updates/012_DEX_WRIST/#attach-the-dexterous-wrist","text":"First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Next, note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Next, using a Philips screwdriver, attach the wrist mount bracket to the bottom of the tool plate using the provided M2 bolts. NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Next, raise the wrist module up vertically into the mounting bracket, then sliding it over horizontally so that the bearing mates onto its post. Slide in the 3D printed spacer between the pitch servo and the mounting bracket. Finally, attach the body of the pitch servo to the mounting bracket using the 3 M2.5 screws provided. ( NOTE: Flat head screws provided, socket head screws shown below.)","title":"Attach the Dexterous Wrist"},{"location":"stretch-factory/updates/012_DEX_WRIST/#update-the-dynamixel-servo-baud-rates","text":"The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600. >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud","title":"Update the Dynamixel servo baud rates"},{"location":"stretch-factory/updates/012_DEX_WRIST/#update-the-robot-yaml","text":"The new wrist requires a number of updates to the robot YAML YAML doesn't allow definition of multiple fields with the same name. Depending on what is already listed in your YAML you may need to manually edit and merge fields. Add the following to you your ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml factory_params : stretch_re1_factory_params.yaml params : - stretch_tool_share.stretch_dex_wrist_beta.params head : baud : 115200 end_of_arm : baud : 115200 tool : tool_stretch_dex_wrist #tool: tool_stretch_gripper robot : use_collision_manager : 1 head_pan : baud : 115200 head_tilt : baud : 115200 wrist_yaw : baud : 115200 stretch_gripper : baud : 115200 range_t : - 0 - 6667 zero_t : 3817 lift : i_feedforward : 0.75 hello-motor-lift : gains : i_safety_feedforward : 0.75 Each user account on Stretch will need to update their YAML as well. It is recommended practice to stored a reference of the YAML in /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/ $HELLO_FLEET_ID >>$ sudo cp *.yaml /etc/hello-robot/ $HELLO_FLEET_ID","title":"Update the robot YAML"},{"location":"stretch-factory/updates/012_DEX_WRIST/#configure-for-use-in-ros","text":"First pull down the new stretch_ros branch and copy in the tool description: >>$ cd ~/catkin_ws/src/stretch_ros/ >>$ git pull >>$ git checkout feature/pluggable_end_effector >>$ cd ~/repos/dex_wrist/stretch_tool_share/tool_share/stretch_dex_wrist_beta/stretch_description >>$ cp urdf/stretch_dex_wrist_beta.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes Now configure stretch_description.xacro to use the StretchDexWrist tool: >>$ nano ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro to read, <?xml version = \"1.0\" ?> <robot xmlns:xacro = \"http://www.ros.org/wiki/xacro\" name = \"stretch_description\" > <xacro:include filename = \"stretch_dex_wrist_beta.xacro\" /> <xacro:include filename = \"stretch_main.xacro\" /> <xacro:include filename = \"stretch_aruco.xacro\" /> <xacro:include filename = \"stretch_d435i.xacro\" /> <xacro:include filename = \"stretch_laser_range_finder.xacro\" /> <xacro:include filename = \"stretch_respeaker.xacro\" /> </robot> Update your URDF and then export the URDF for Stretch Body to use (you may need to Ctrl-C to exit rosrun ) >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ ./export_urdf.sh","title":"Configure for use in ROS"},{"location":"stretch-factory/updates/012_DEX_WRIST/#test-the-wrist-with-the-xbox-controller","text":"Try out the new wrist! Note that the new key mapping does not allow for control of the head. >>$ stretch_xbox_controller_teleop.py Ctrl-C to exit. A printable copy of the teleoperation interface is here","title":"Test the wrist with the XBox Controller"},{"location":"stretch-factory/updates/012_DEX_WRIST/#test-the-wrist-with-rviz","text":"Now check that the wrist appears in RVIZ and can be controlled from the keyboard interface: >>$ roslaunch stretch_calibration simple_test_head_calibration.launch You can type 'q' then Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU -----------| | | | i HEAD UP | | j HEAD LEFT l HEAD RIGHT | | , HEAD DOWN | | | | | | 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT | | home page-up | | | | | | 8 LIFT UP | | up-arrow | | 4 BASE FORWARD 6 BASE BACK | | left-arrow right-arrow | | 2 LIFT DOWN | | down-arrow | | | | | | w ARM OUT | | a WRIST FORWARD d WRIST BACK | | x ARM IN | | | | | | c PITCH FORWARD v PITCH BACK | | o ROLL FORWARD p ROLL BACK | | 5 GRIPPER CLOSE | | 0 GRIPPER OPEN | | | | step size: b BIG, m MEDIUM, s SMALL | | q QUIT | | | |-------------------------------------------|","title":"Test the wrist with RViz"},{"location":"stretch-factory/updates/012_DEX_WRIST/#using-the-stretch-dex-wrist","text":"Additional care should be taken when working with the Dex Wrist as it is now easier to accidentally collide the wrist and gripper with the robot, particularly during lift descent. We've implemented a very coarse collision avoidance behavior in Stretch Body that is turned on by default. It is conservative and doesn't fully prevent collisions however. The collision avoidance can be turned off in the user YAML by: collision_stretch_dex_wrist_to_base : enabled : 0 collision_stretch_dex_wrist_to_self : enabled : 0 You can jog the individual joints of the wrist using the tool: >>$ stretch_dex_wrist_jog.py --pitch Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () For reference, the parameters for the Stretch Dex Wrist (which can be overriden in the user YAML) are found at .local/lib/python2.7/site-packages/stretch_tool_share/stretch_dex_wrist_beta/params.py","title":"Using the Stretch Dex Wrist"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/","text":"012_DEX_WRIST Background This update installs and configures the Beta unit of the Stretch Dex Wrist - Beta. The procedure involves Install and configure the new Wacc board Install Stretch software packages Attach the Dexterous Wrist Update the Dynamixel servo baud rates Update the robot YAML Test the wrist with the XBox controller Configure for use in ROS Install and configure the new Wacc board Robots prior to the 'Joplin' batch will need an upgraded Wacc board that will be provided by Hello Robot. See the update 013_WACC_INSTALL Install Stretch Body Software Packages You'll be installing a local beta version of relevant Stretch Body packages >>$ cd ~/repos >>$ mkdir dex_wrist >>$ cd dex_wrist >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_body >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_tool_share >>$ git clone https://github.com/hello-robot/stretch_factory >> >>$ cd stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh >>$ pip2 install urdfpy >> >>$ pip2 install hello-robot-stretch-tool-share >>$ cd ../../stretch_tool_share/python >>$ ./local_install.sh >>$ cd ~/repos/dex_wrist/stretch_factory/python >> ./local_install.sh Attach the Dexterous Wrist First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Next, note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Next, using a Philips screwdriver, attach the wrist mount bracket to the bottom of the tool plate using the provided M2 bolts. NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Next, raise the wrist module up vertically into the mounting bracket, then sliding it over horizontally so that the bearing mates onto its post. Slide in the 3D printed spacer between the pitch servo and the mounting bracket. Finally, attach the body of the pitch servo to the mounting bracket using the 3 M2.5 screws provided. ( NOTE: Flat head screws provided, socket head screws shown below.) Update the Dynamixel servo baud rates The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600. >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud Update the robot YAML The new wrist requires a number of updates to the robot YAML YAML doesn't allow definition of multiple fields with the same name. Depending on what is already listed in your YAML you may need to manually edit and merge fields. Add the following to you your ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml factory_params : stretch_re1_factory_params.yaml params : - stretch_tool_share.stretch_dex_wrist_beta.params head : baud : 115200 end_of_arm : baud : 115200 tool : tool_stretch_dex_wrist #tool: tool_stretch_gripper robot : use_collision_manager : 1 head_pan : baud : 115200 head_tilt : baud : 115200 wrist_yaw : baud : 115200 stretch_gripper : baud : 115200 range_t : - 0 - 6667 zero_t : 3817 lift : i_feedforward : 0.75 hello-motor-lift : gains : i_safety_feedforward : 0.75 Each user account on Stretch will need to update their YAML as well. It is recommended practice to stored a reference of the YAML in /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/ $HELLO_FLEET_ID >>$ sudo cp *.yaml /etc/hello-robot/ $HELLO_FLEET_ID Configure for use in ROS First pull down the new stretch_ros branch and copy in the tool description: >>$ cd ~/catkin_ws/src/stretch_ros/ >>$ git pull >>$ git checkout feature/pluggable_end_effector >>$ cd ~/repos/dex_wrist/stretch_tool_share/tool_share/stretch_dex_wrist_beta/stretch_description >>$ cp urdf/stretch_dex_wrist_beta.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes Now configure stretch_description.xacro to use the StretchDexWrist tool: >>$ nano ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro to read, <?xml version = \"1.0\" ?> <robot xmlns:xacro = \"http://www.ros.org/wiki/xacro\" name = \"stretch_description\" > <xacro:include filename = \"stretch_dex_wrist_beta.xacro\" /> <xacro:include filename = \"stretch_main.xacro\" /> <xacro:include filename = \"stretch_aruco.xacro\" /> <xacro:include filename = \"stretch_d435i.xacro\" /> <xacro:include filename = \"stretch_laser_range_finder.xacro\" /> <xacro:include filename = \"stretch_respeaker.xacro\" /> </robot> Update your URDF and then export the URDF for Stretch Body to use (you may need to Ctrl-C to exit rosrun ) >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ ./export_urdf.sh Test the wrist with the XBox Controller Try out the new wrist! Note that the new key mapping does not allow for control of the head. >>$ stretch_xbox_controller_teleop.py Ctrl-C to exit. A printable copy of the teleoperation interface is here Test the wrist with RViz Now check that the wrist appears in RVIZ and can be controlled from the keyboard interface: >>$ roslaunch stretch_calibration simple_test_head_calibration.launch You can type 'q' then Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU -----------| | | | i HEAD UP | | j HEAD LEFT l HEAD RIGHT | | , HEAD DOWN | | | | | | 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT | | home page-up | | | | | | 8 LIFT UP | | up-arrow | | 4 BASE FORWARD 6 BASE BACK | | left-arrow right-arrow | | 2 LIFT DOWN | | down-arrow | | | | | | w ARM OUT | | a WRIST FORWARD d WRIST BACK | | x ARM IN | | | | | | c PITCH FORWARD v PITCH BACK | | o ROLL FORWARD p ROLL BACK | | 5 GRIPPER CLOSE | | 0 GRIPPER OPEN | | | | step size: b BIG, m MEDIUM, s SMALL | | q QUIT | | | |-------------------------------------------| Using the Stretch Dex Wrist Additional care should be taken when working with the Dex Wrist as it is now easier to accidentally collide the wrist and gripper with the robot, particularly during lift descent. We've implemented a very coarse collision avoidance behavior in Stretch Body that is turned on by default. It is conservative and doesn't fully prevent collisions however. The collision avoidance can be turned off in the user YAML by: collision_stretch_dex_wrist_to_base : enabled : 0 collision_stretch_dex_wrist_to_self : enabled : 0 You can jog the individual joints of the wrist using the tool: >>$ stretch_dex_wrist_jog.py --pitch Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () For reference, the parameters for the Stretch Dex Wrist (which can be overriden in the user YAML) are found at .local/lib/python2.7/site-packages/stretch_tool_share/stretch_dex_wrist_beta/params.py","title":"012_DEX_WRIST"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#012_dex_wrist","text":"","title":"012_DEX_WRIST"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#background","text":"This update installs and configures the Beta unit of the Stretch Dex Wrist - Beta. The procedure involves Install and configure the new Wacc board Install Stretch software packages Attach the Dexterous Wrist Update the Dynamixel servo baud rates Update the robot YAML Test the wrist with the XBox controller Configure for use in ROS","title":"Background"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#install-and-configure-the-new-wacc-board","text":"Robots prior to the 'Joplin' batch will need an upgraded Wacc board that will be provided by Hello Robot. See the update 013_WACC_INSTALL","title":"Install and configure the new Wacc board"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#install-stretch-body-software-packages","text":"You'll be installing a local beta version of relevant Stretch Body packages >>$ cd ~/repos >>$ mkdir dex_wrist >>$ cd dex_wrist >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_body >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_tool_share >>$ git clone https://github.com/hello-robot/stretch_factory >> >>$ cd stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh >>$ pip2 install urdfpy >> >>$ pip2 install hello-robot-stretch-tool-share >>$ cd ../../stretch_tool_share/python >>$ ./local_install.sh >>$ cd ~/repos/dex_wrist/stretch_factory/python >> ./local_install.sh","title":"Install Stretch Body Software Packages"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#attach-the-dexterous-wrist","text":"First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Next, note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Next, using a Philips screwdriver, attach the wrist mount bracket to the bottom of the tool plate using the provided M2 bolts. NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Next, raise the wrist module up vertically into the mounting bracket, then sliding it over horizontally so that the bearing mates onto its post. Slide in the 3D printed spacer between the pitch servo and the mounting bracket. Finally, attach the body of the pitch servo to the mounting bracket using the 3 M2.5 screws provided. ( NOTE: Flat head screws provided, socket head screws shown below.)","title":"Attach the Dexterous Wrist"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#update-the-dynamixel-servo-baud-rates","text":"The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600. >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud","title":"Update the Dynamixel servo baud rates"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#update-the-robot-yaml","text":"The new wrist requires a number of updates to the robot YAML YAML doesn't allow definition of multiple fields with the same name. Depending on what is already listed in your YAML you may need to manually edit and merge fields. Add the following to you your ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml factory_params : stretch_re1_factory_params.yaml params : - stretch_tool_share.stretch_dex_wrist_beta.params head : baud : 115200 end_of_arm : baud : 115200 tool : tool_stretch_dex_wrist #tool: tool_stretch_gripper robot : use_collision_manager : 1 head_pan : baud : 115200 head_tilt : baud : 115200 wrist_yaw : baud : 115200 stretch_gripper : baud : 115200 range_t : - 0 - 6667 zero_t : 3817 lift : i_feedforward : 0.75 hello-motor-lift : gains : i_safety_feedforward : 0.75 Each user account on Stretch will need to update their YAML as well. It is recommended practice to stored a reference of the YAML in /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/ $HELLO_FLEET_ID >>$ sudo cp *.yaml /etc/hello-robot/ $HELLO_FLEET_ID","title":"Update the robot YAML"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#configure-for-use-in-ros","text":"First pull down the new stretch_ros branch and copy in the tool description: >>$ cd ~/catkin_ws/src/stretch_ros/ >>$ git pull >>$ git checkout feature/pluggable_end_effector >>$ cd ~/repos/dex_wrist/stretch_tool_share/tool_share/stretch_dex_wrist_beta/stretch_description >>$ cp urdf/stretch_dex_wrist_beta.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes Now configure stretch_description.xacro to use the StretchDexWrist tool: >>$ nano ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro to read, <?xml version = \"1.0\" ?> <robot xmlns:xacro = \"http://www.ros.org/wiki/xacro\" name = \"stretch_description\" > <xacro:include filename = \"stretch_dex_wrist_beta.xacro\" /> <xacro:include filename = \"stretch_main.xacro\" /> <xacro:include filename = \"stretch_aruco.xacro\" /> <xacro:include filename = \"stretch_d435i.xacro\" /> <xacro:include filename = \"stretch_laser_range_finder.xacro\" /> <xacro:include filename = \"stretch_respeaker.xacro\" /> </robot> Update your URDF and then export the URDF for Stretch Body to use (you may need to Ctrl-C to exit rosrun ) >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ ./export_urdf.sh","title":"Configure for use in ROS"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#test-the-wrist-with-the-xbox-controller","text":"Try out the new wrist! Note that the new key mapping does not allow for control of the head. >>$ stretch_xbox_controller_teleop.py Ctrl-C to exit. A printable copy of the teleoperation interface is here","title":"Test the wrist with the XBox Controller"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#test-the-wrist-with-rviz","text":"Now check that the wrist appears in RVIZ and can be controlled from the keyboard interface: >>$ roslaunch stretch_calibration simple_test_head_calibration.launch You can type 'q' then Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU -----------| | | | i HEAD UP | | j HEAD LEFT l HEAD RIGHT | | , HEAD DOWN | | | | | | 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT | | home page-up | | | | | | 8 LIFT UP | | up-arrow | | 4 BASE FORWARD 6 BASE BACK | | left-arrow right-arrow | | 2 LIFT DOWN | | down-arrow | | | | | | w ARM OUT | | a WRIST FORWARD d WRIST BACK | | x ARM IN | | | | | | c PITCH FORWARD v PITCH BACK | | o ROLL FORWARD p ROLL BACK | | 5 GRIPPER CLOSE | | 0 GRIPPER OPEN | | | | step size: b BIG, m MEDIUM, s SMALL | | q QUIT | | | |-------------------------------------------|","title":"Test the wrist with RViz"},{"location":"stretch-factory/updates/012_DEX_WRIST_BETA/#using-the-stretch-dex-wrist","text":"Additional care should be taken when working with the Dex Wrist as it is now easier to accidentally collide the wrist and gripper with the robot, particularly during lift descent. We've implemented a very coarse collision avoidance behavior in Stretch Body that is turned on by default. It is conservative and doesn't fully prevent collisions however. The collision avoidance can be turned off in the user YAML by: collision_stretch_dex_wrist_to_base : enabled : 0 collision_stretch_dex_wrist_to_self : enabled : 0 You can jog the individual joints of the wrist using the tool: >>$ stretch_dex_wrist_jog.py --pitch Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () For reference, the parameters for the Stretch Dex Wrist (which can be overriden in the user YAML) are found at .local/lib/python2.7/site-packages/stretch_tool_share/stretch_dex_wrist_beta/params.py","title":"Using the Stretch Dex Wrist"},{"location":"stretch-factory/updates/013_WACC_INSTALL/","text":"013_WACC_INSTALL Background This update installs and configures a new Wacc (Wrist + Accelerometer) board. You will need Replacement Wacc board USB-A to USB-micro cable 1.5mm Hex wrench 2.5mm Hex wrench Small flat head screw driver or similar Loctite 242 (blue) Update the Wacc board serial numbers First, on the robot run: >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory >>$ chmod a+rw $HELLO_FLEET_PATH / $HELLO_FLEET_ID /udev/* Now attach the USB cable into a USB port of the robot trunk. Run the updating tool as shown below. You will - Plug in USB when prompted - Hit enter - Unplug and plug in USB when prompted - Hit enter >>$ cd stretch_factory/updates/013_WACC_INSTALL >>$ sudo dmesg -c >>$ ./add_new_wacc_pcba.py ---------------------- Adding WACC PCBA to robot: stretch-re1-1039 Plug / Reset Dynamixel device now... Press return when done [ 1035443 .643968 ] usb 1 -1.3.2.1: new high-speed USB device number 12 using xhci_hcd [ 1035443 .844182 ] usb 1 -1.3.2.1: New USB device found, idVendor = 1a40, idProduct = 0101 [ 1035443 .844199 ] usb 1 -1.3.2.1: New USB device strings: Mfr = 0 , Product = 1 , SerialNumber = 0 [ 1035443 .844208 ] usb 1 -1.3.2.1: Product: USB 2 .0 Hub [ 1035443 .845851 ] hub 1 -1.3.2.1:1.0: USB hub found [ 1035443 .845923 ] hub 1 -1.3.2.1:1.0: 4 ports detected [ 1035444 .252052 ] usb 1 -1.3.2.1.2: new full-speed USB device number 15 using xhci_hcd [ 1035444 .479616 ] usb 1 -1.3.2.1.2: New USB device found, idVendor = 0403 , idProduct = 6001 [ 1035444 .479625 ] usb 1 -1.3.2.1.2: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 3 [ 1035444 .479631 ] usb 1 -1.3.2.1.2: Product: FT232R USB UART [ 1035444 .479636 ] usb 1 -1.3.2.1.2: Manufacturer: FTDI [ 1035444 .479640 ] usb 1 -1.3.2.1.2: SerialNumber: AQ00X8TJ [ 1035444 .483900 ] ftdi_sio 1 -1.3.2.1.2:1.0: FTDI USB Serial Device converter detected [ 1035444 .484043 ] usb 1 -1.3.2.1.2: Detected FT232RL [ 1035444 .484466 ] usb 1 -1.3.2.1.2: FTDI USB Serial Device converter now attached to ttyUSB3 [ 1035445 .459995 ] usb 1 -1.3.2.1.3: new full-speed USB device number 17 using xhci_hcd --------------------------- Found Dynamixel device with SerialNumber AQ00X8TJ Writing UDEV for AQ00X8TJ Overwriting existing entry... Plug / Reset in Arduino device now... Press return when done --------------------------- Found Arduino device with SerialNumber C209885C50524653312E3120FF101E39 Writing UDEV for hello-wacc C209885C50524653312E3120FF101E39 Overwriting existing entry... --------------------------- Found Arduino device with SerialNumber C209885C50524653312E3120FF101E39 Writing UDEV for hello-wacc C209885C50524653312E3120FF101E39 Overwriting existing entry... Install the new Wacc board Power down the robot from Ubuntu and turn off the main power switch. Remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Manually pose the lift height and arm such that the wrist can sit on a table top in order to support the wrist weight during disassembly Using the 1.5mm wrench, remove the two M2 bolts holding the plastic cap to the end of the wrist Using the 2.5mm wrench, remove the two M4 bolts (blue arrows) holding the wrist module to the end of arm Slide the wrist module out of the arm tube while supporting the weight of the module so that it remains parallel to the ground. Take care that the Wacc board clears the surrounding metal structure (shown in blue) With the screw driver, push back and dislodge the JST power cable and USB cable from the back of the Wacc board. Remove the JST servo cable at the front of the board. Using the 1.5mm wrench, remove the 4 M2 bolts holding the Wacc board to the sheetmetal frame. Attach the replacement board onto the sheetmetal frame using the provided 4 M2 bolts Reattach the USB and power cables to back of Wacc Carefully route the Dynamixel servo cable out of the arm as shown such that no cables are pinched when attaching the plastic cap. Carefully slide the wrist module back into the arm. Ensure that the cables are fully seated. Apply Loctite to the two M4 bolts. Secure the wrist module to the arm with the bolts. then attach the plastic cap with the two M2 bolts. Check the Wacc functionality Power the robot back on and check that the board is on the bus >>$ ls /dev/hello-dynamixel-wrist hello-dynamixel-wrist >>$ ls /dev/hello-wacc hello-wacc Then check that the Wacc is reporting sensor data back: >>$ stretch_wacc_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- ------------------------------ Ax ( m/s^2 ) 9 .8684213638 Ay ( m/s^2 ) 0 .506848096848 Az ( m/s^2 ) 0 .361166000366 A0 381 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 25 State 0 Debug 0 Timestamp 1601320914 .65 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1p0 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- Finally, home the wrist yaw joint to ensure that it is working. >>$ stretch_wrist_yaw_home.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Moving to first hardstop... Contact at position: -3029 Hit first hardstop, marking to zero ticks Raw position: 14 Moving to calibrated zero: ( rad ) Update Wacc Calibration >>$ RE1_wacc_calibrate.py RE1_wacc_calibrate.py Calibrating Wacc. Ensure arm is retracted and level to ground Hit enter when ready Itr 0 Val 9 .59977857901 ... Itr 99 Val 10 .1095601333 Got a average value of 10 .1372113882 Gravity scalar of 0 .967391 within bounds of 0 .900000 to 1 .100000 Writing yaml... Note: If the RE1* tools are not present you can install them as >>$ pip2 install hello-robot-stretch-factory","title":"013_WACC_INSTALL"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#013_wacc_install","text":"","title":"013_WACC_INSTALL"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#background","text":"This update installs and configures a new Wacc (Wrist + Accelerometer) board. You will need Replacement Wacc board USB-A to USB-micro cable 1.5mm Hex wrench 2.5mm Hex wrench Small flat head screw driver or similar Loctite 242 (blue)","title":"Background"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#update-the-wacc-board-serial-numbers","text":"First, on the robot run: >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory >>$ chmod a+rw $HELLO_FLEET_PATH / $HELLO_FLEET_ID /udev/* Now attach the USB cable into a USB port of the robot trunk. Run the updating tool as shown below. You will - Plug in USB when prompted - Hit enter - Unplug and plug in USB when prompted - Hit enter >>$ cd stretch_factory/updates/013_WACC_INSTALL >>$ sudo dmesg -c >>$ ./add_new_wacc_pcba.py ---------------------- Adding WACC PCBA to robot: stretch-re1-1039 Plug / Reset Dynamixel device now... Press return when done [ 1035443 .643968 ] usb 1 -1.3.2.1: new high-speed USB device number 12 using xhci_hcd [ 1035443 .844182 ] usb 1 -1.3.2.1: New USB device found, idVendor = 1a40, idProduct = 0101 [ 1035443 .844199 ] usb 1 -1.3.2.1: New USB device strings: Mfr = 0 , Product = 1 , SerialNumber = 0 [ 1035443 .844208 ] usb 1 -1.3.2.1: Product: USB 2 .0 Hub [ 1035443 .845851 ] hub 1 -1.3.2.1:1.0: USB hub found [ 1035443 .845923 ] hub 1 -1.3.2.1:1.0: 4 ports detected [ 1035444 .252052 ] usb 1 -1.3.2.1.2: new full-speed USB device number 15 using xhci_hcd [ 1035444 .479616 ] usb 1 -1.3.2.1.2: New USB device found, idVendor = 0403 , idProduct = 6001 [ 1035444 .479625 ] usb 1 -1.3.2.1.2: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 3 [ 1035444 .479631 ] usb 1 -1.3.2.1.2: Product: FT232R USB UART [ 1035444 .479636 ] usb 1 -1.3.2.1.2: Manufacturer: FTDI [ 1035444 .479640 ] usb 1 -1.3.2.1.2: SerialNumber: AQ00X8TJ [ 1035444 .483900 ] ftdi_sio 1 -1.3.2.1.2:1.0: FTDI USB Serial Device converter detected [ 1035444 .484043 ] usb 1 -1.3.2.1.2: Detected FT232RL [ 1035444 .484466 ] usb 1 -1.3.2.1.2: FTDI USB Serial Device converter now attached to ttyUSB3 [ 1035445 .459995 ] usb 1 -1.3.2.1.3: new full-speed USB device number 17 using xhci_hcd --------------------------- Found Dynamixel device with SerialNumber AQ00X8TJ Writing UDEV for AQ00X8TJ Overwriting existing entry... Plug / Reset in Arduino device now... Press return when done --------------------------- Found Arduino device with SerialNumber C209885C50524653312E3120FF101E39 Writing UDEV for hello-wacc C209885C50524653312E3120FF101E39 Overwriting existing entry... --------------------------- Found Arduino device with SerialNumber C209885C50524653312E3120FF101E39 Writing UDEV for hello-wacc C209885C50524653312E3120FF101E39 Overwriting existing entry...","title":"Update the Wacc board serial numbers"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#install-the-new-wacc-board","text":"Power down the robot from Ubuntu and turn off the main power switch. Remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Manually pose the lift height and arm such that the wrist can sit on a table top in order to support the wrist weight during disassembly Using the 1.5mm wrench, remove the two M2 bolts holding the plastic cap to the end of the wrist Using the 2.5mm wrench, remove the two M4 bolts (blue arrows) holding the wrist module to the end of arm Slide the wrist module out of the arm tube while supporting the weight of the module so that it remains parallel to the ground. Take care that the Wacc board clears the surrounding metal structure (shown in blue) With the screw driver, push back and dislodge the JST power cable and USB cable from the back of the Wacc board. Remove the JST servo cable at the front of the board. Using the 1.5mm wrench, remove the 4 M2 bolts holding the Wacc board to the sheetmetal frame. Attach the replacement board onto the sheetmetal frame using the provided 4 M2 bolts Reattach the USB and power cables to back of Wacc Carefully route the Dynamixel servo cable out of the arm as shown such that no cables are pinched when attaching the plastic cap. Carefully slide the wrist module back into the arm. Ensure that the cables are fully seated. Apply Loctite to the two M4 bolts. Secure the wrist module to the arm with the bolts. then attach the plastic cap with the two M2 bolts.","title":"Install the new Wacc board"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#check-the-wacc-functionality","text":"Power the robot back on and check that the board is on the bus >>$ ls /dev/hello-dynamixel-wrist hello-dynamixel-wrist >>$ ls /dev/hello-wacc hello-wacc Then check that the Wacc is reporting sensor data back: >>$ stretch_wacc_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- ------------------------------ Ax ( m/s^2 ) 9 .8684213638 Ay ( m/s^2 ) 0 .506848096848 Az ( m/s^2 ) 0 .361166000366 A0 381 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 25 State 0 Debug 0 Timestamp 1601320914 .65 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1p0 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- Finally, home the wrist yaw joint to ensure that it is working. >>$ stretch_wrist_yaw_home.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Moving to first hardstop... Contact at position: -3029 Hit first hardstop, marking to zero ticks Raw position: 14 Moving to calibrated zero: ( rad )","title":"Check the Wacc functionality"},{"location":"stretch-factory/updates/013_WACC_INSTALL/#update-wacc-calibration","text":">>$ RE1_wacc_calibrate.py RE1_wacc_calibrate.py Calibrating Wacc. Ensure arm is retracted and level to ground Hit enter when ready Itr 0 Val 9 .59977857901 ... Itr 99 Val 10 .1095601333 Got a average value of 10 .1372113882 Gravity scalar of 0 .967391 within bounds of 0 .900000 to 1 .100000 Writing yaml... Note: If the RE1* tools are not present you can install them as >>$ pip2 install hello-robot-stretch-factory","title":"Update Wacc Calibration"},{"location":"stretch-factory/updates/014_WAYPOINTS/","text":"014_WAYPOINTS Background x Update Firmware Install the latest version of the firmware for the Wacc, Pimu, and Steppers. NOTE: For now you will want the sync_timestamp branch of the git repository. You'll need to pull it down. >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_firmware -b via_trajectory Then follow the tutorial for upgrading firmware (Note your Stretch may already have the Arduino IDE installed and configured). Update YAML Robots with serial numbers stretch-re1-1001 to stretch-re1-1022 will need to update their user YAML Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml pimu_clock_manager: n_slew_history: 25 trs: 450.0 use_skew_compensation: 1 wacc_clock_manager: n_slew_history: 25 trs: 687.0 use_skew_compensation: 1 robot_timestamp_manager: sync_mode_enabled: 1 time_align_status: 1 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well. Update Stretch Body NOTE: For now pull down the sync_timestamp branch of the git repository and install that. >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_body -b sync_timestamp >>$ cd ~/repos/stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh FUTURE: First, move to the latest Stretch Body package (version >=0.0.20) >>$ pip2 install hello-robot-stretch-body Try It Out Now test it out. Try running the timestamp jog tool. >>$ stretch_robot_timestamps_jog.py --display For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ Timestamp Manager ----- Sync mode enabled : 1 Status ID : 121 Wall time : 1607575532 .977621 Hardware sync : 1607575532 .939872 Pimu IMU : 1607575532 .930712 Lift Encdoer : 1607575532 .938915 Arm Encoder : 1607575532 .938884 Right Wheel Encoder : 1607575532 .939187 Left Wheel Encoder : 1607575532 .938882 Wacc Accel : 1607575532 .934294 ------ Timestamp Manager ----- Sync mode enabled : 1 Status ID : 125 Wall time : 1607575533 .187324 Hardware sync : 1607575533 .148872 Pimu IMU : 1607575533 .140704 Lift Encdoer : 1607575533 .147775 Arm Encoder : 1607575533 .148192 Right Wheel Encoder : 1607575533 .148234 Left Wheel Encoder : 1607575533 .147883 Wacc Accel : 1607575533 .142721 ... >>$ stretch_robot_timestamps_jog.py --sensor_delta For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Starting sensor timestamp analysis... Sync mode enabled: 1 Time align status: 0 Use skew compensation: 1 --------------------------- DT Pimu IMU :-10152 DT Left Wheel Encoder :-717 DT Right Wheel Encoder :-1068 DT Lift Encoder :-361 DT Arm Encoder :-337 DT Wacc Accel :5703 --------------------------- DT Pimu IMU :-7148 DT Left Wheel Encoder :-708 DT Right Wheel Encoder :-953 DT Lift Encoder :-470 DT Arm Encoder :-864 DT Wacc Accel :84 >>$ stretch_robot_timestamps_jog.py --sensor_stats That's it!","title":"014_WAYPOINTS"},{"location":"stretch-factory/updates/014_WAYPOINTS/#014_waypoints","text":"","title":"014_WAYPOINTS"},{"location":"stretch-factory/updates/014_WAYPOINTS/#background","text":"x","title":"Background"},{"location":"stretch-factory/updates/014_WAYPOINTS/#update-firmware","text":"Install the latest version of the firmware for the Wacc, Pimu, and Steppers. NOTE: For now you will want the sync_timestamp branch of the git repository. You'll need to pull it down. >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_firmware -b via_trajectory Then follow the tutorial for upgrading firmware (Note your Stretch may already have the Arduino IDE installed and configured).","title":"Update Firmware"},{"location":"stretch-factory/updates/014_WAYPOINTS/#update-yaml","text":"Robots with serial numbers stretch-re1-1001 to stretch-re1-1022 will need to update their user YAML Add the following to ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml pimu_clock_manager: n_slew_history: 25 trs: 450.0 use_skew_compensation: 1 wacc_clock_manager: n_slew_history: 25 trs: 687.0 use_skew_compensation: 1 robot_timestamp_manager: sync_mode_enabled: 1 time_align_status: 1 Note : This fix is only applied to the current user account. If there are other existing user accounts they will want to apply this fix as well.","title":"Update YAML"},{"location":"stretch-factory/updates/014_WAYPOINTS/#update-stretch-body","text":"NOTE: For now pull down the sync_timestamp branch of the git repository and install that. >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_body -b sync_timestamp >>$ cd ~/repos/stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh FUTURE: First, move to the latest Stretch Body package (version >=0.0.20) >>$ pip2 install hello-robot-stretch-body","title":"Update Stretch Body"},{"location":"stretch-factory/updates/014_WAYPOINTS/#try-it-out","text":"Now test it out. Try running the timestamp jog tool. >>$ stretch_robot_timestamps_jog.py --display For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ------ Timestamp Manager ----- Sync mode enabled : 1 Status ID : 121 Wall time : 1607575532 .977621 Hardware sync : 1607575532 .939872 Pimu IMU : 1607575532 .930712 Lift Encdoer : 1607575532 .938915 Arm Encoder : 1607575532 .938884 Right Wheel Encoder : 1607575532 .939187 Left Wheel Encoder : 1607575532 .938882 Wacc Accel : 1607575532 .934294 ------ Timestamp Manager ----- Sync mode enabled : 1 Status ID : 125 Wall time : 1607575533 .187324 Hardware sync : 1607575533 .148872 Pimu IMU : 1607575533 .140704 Lift Encdoer : 1607575533 .147775 Arm Encoder : 1607575533 .148192 Right Wheel Encoder : 1607575533 .148234 Left Wheel Encoder : 1607575533 .147883 Wacc Accel : 1607575533 .142721 ... >>$ stretch_robot_timestamps_jog.py --sensor_delta For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. Starting sensor timestamp analysis... Sync mode enabled: 1 Time align status: 0 Use skew compensation: 1 --------------------------- DT Pimu IMU :-10152 DT Left Wheel Encoder :-717 DT Right Wheel Encoder :-1068 DT Lift Encoder :-361 DT Arm Encoder :-337 DT Wacc Accel :5703 --------------------------- DT Pimu IMU :-7148 DT Left Wheel Encoder :-708 DT Right Wheel Encoder :-953 DT Lift Encoder :-470 DT Arm Encoder :-864 DT Wacc Accel :84 >>$ stretch_robot_timestamps_jog.py --sensor_stats That's it!","title":"Try It Out"},{"location":"stretch-factory/updates/015_DEX_WRIST/","text":"012_DEX_WRIST Background This update installs and configures the Beta unit of the Stretch Dex Wrist - Beta. The procedure involves Install and configure the new Wacc board Install Stretch software packages Attach the Dexterous Wrist Update the Dynamixel servo baud rates Update the robot YAML Test the wrist with the XBox controller Configure for use in ROS Install and configure the new Wacc board Robots prior to the 'Joplin' batch will need an upgraded Wacc board that will be provided by Hello Robot. See the update 013_WACC_INSTALL Install Stretch Body Software Packages You'll be installing a local beta version of relevant Stretch Body packages >>$ cd ~/repos >>$ mkdir dex_wrist >>$ cd dex_wrist >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_body >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_tool_share >>$ git clone https://github.com/hello-robot/stretch_factory >> >>$ cd stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh >>$ pip2 install urdfpy >> >>$ pip2 install hello-robot-stretch-tool-share >>$ cd ../../stretch_tool_share/python >>$ ./local_install.sh >>$ cd ~/repos/dex_wrist/stretch_factory/python >> ./local_install.sh Attach the Dexterous Wrist First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Next, note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Next, using a Philips screwdriver, attach the wrist mount bracket to the bottom of the tool plate using the provided M2 bolts. NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Next, raise the wrist module up vertically into the mounting bracket, then sliding it over horizontally so that the bearing mates onto its post. Slide in the 3D printed spacer between the pitch servo and the mounting bracket. Finally, attach the body of the pitch servo to the mounting bracket using the 3 M2.5 screws provided. ( NOTE: Flat head screws provided, socket head screws shown below.) Update the Dynamixel servo baud rates The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600. >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud Update the robot YAML The new wrist requires a number of updates to the robot YAML YAML doesn't allow definition of multiple fields with the same name. Depending on what is already listed in your YAML you may need to manually edit and merge fields. Add the following to you your ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml factory_params : stretch_re1_factory_params.yaml params : - stretch_tool_share.stretch_dex_wrist_beta.params head : baud : 115200 end_of_arm : baud : 115200 tool : tool_stretch_dex_wrist #tool: tool_stretch_gripper robot : use_collision_manager : 1 head_pan : baud : 115200 head_tilt : baud : 115200 wrist_yaw : baud : 115200 stretch_gripper : baud : 115200 range_t : - 0 - 6667 zero_t : 3817 lift : i_feedforward : 0.75 hello-motor-lift : gains : i_safety_feedforward : 0.75 Each user account on Stretch will need to update their YAML as well. It is recommended practice to stored a reference of the YAML in /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/ $HELLO_FLEET_ID >>$ sudo cp *.yaml /etc/hello-robot/ $HELLO_FLEET_ID Configure for use in ROS First pull down the new stretch_ros branch and copy in the tool description: >>$ cd ~/catkin_ws/src/stretch_ros/ >>$ git pull >>$ git checkout feature/pluggable_end_effector >>$ cd ~/repos/dex_wrist/stretch_tool_share/tool_share/stretch_dex_wrist_beta/stretch_description >>$ cp urdf/stretch_dex_wrist_beta.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes Now configure stretch_description.xacro to use the StretchDexWrist tool: >>$ nano ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro to read, <?xml version = \"1.0\" ?> <robot xmlns:xacro = \"http://www.ros.org/wiki/xacro\" name = \"stretch_description\" > <xacro:include filename = \"stretch_dex_wrist_beta.xacro\" /> <xacro:include filename = \"stretch_main.xacro\" /> <xacro:include filename = \"stretch_aruco.xacro\" /> <xacro:include filename = \"stretch_d435i.xacro\" /> <xacro:include filename = \"stretch_laser_range_finder.xacro\" /> <xacro:include filename = \"stretch_respeaker.xacro\" /> </robot> Update your URDF and then export the URDF for Stretch Body to use (you may need to Ctrl-C to exit rosrun ) >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ ./export_urdf.sh Test the wrist with the XBox Controller Try out the new wrist! Note that the new key mapping does not allow for control of the head. >>$ stretch_xbox_controller_teleop.py Ctrl-C to exit. A printable copy of the teleoperation interface is here Test the wrist with RViz Now check that the wrist appears in RVIZ and can be controlled from the keyboard interface: >>$ roslaunch stretch_calibration simple_test_head_calibration.launch You can type 'q' then Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU -----------| | | | i HEAD UP | | j HEAD LEFT l HEAD RIGHT | | , HEAD DOWN | | | | | | 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT | | home page-up | | | | | | 8 LIFT UP | | up-arrow | | 4 BASE FORWARD 6 BASE BACK | | left-arrow right-arrow | | 2 LIFT DOWN | | down-arrow | | | | | | w ARM OUT | | a WRIST FORWARD d WRIST BACK | | x ARM IN | | | | | | c PITCH FORWARD v PITCH BACK | | o ROLL FORWARD p ROLL BACK | | 5 GRIPPER CLOSE | | 0 GRIPPER OPEN | | | | step size: b BIG, m MEDIUM, s SMALL | | q QUIT | | | |-------------------------------------------| Using the Stretch Dex Wrist Additional care should be taken when working with the Dex Wrist as it is now easier to accidentally collide the wrist and gripper with the robot, particularly during lift descent. We've implemented a very coarse collision avoidance behavior in Stretch Body that is turned on by default. It is conservative and doesn't fully prevent collisions however. The collision avoidance can be turned off in the user YAML by: collision_stretch_dex_wrist_to_base : enabled : 0 collision_stretch_dex_wrist_to_self : enabled : 0 You can jog the individual joints of the wrist using the tool: >>$ stretch_dex_wrist_jog.py --pitch Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () For reference, the parameters for the Stretch Dex Wrist (which can be overriden in the user YAML) are found at .local/lib/python2.7/site-packages/stretch_tool_share/stretch_dex_wrist_beta/params.py","title":"012_DEX_WRIST"},{"location":"stretch-factory/updates/015_DEX_WRIST/#012_dex_wrist","text":"","title":"012_DEX_WRIST"},{"location":"stretch-factory/updates/015_DEX_WRIST/#background","text":"This update installs and configures the Beta unit of the Stretch Dex Wrist - Beta. The procedure involves Install and configure the new Wacc board Install Stretch software packages Attach the Dexterous Wrist Update the Dynamixel servo baud rates Update the robot YAML Test the wrist with the XBox controller Configure for use in ROS","title":"Background"},{"location":"stretch-factory/updates/015_DEX_WRIST/#install-and-configure-the-new-wacc-board","text":"Robots prior to the 'Joplin' batch will need an upgraded Wacc board that will be provided by Hello Robot. See the update 013_WACC_INSTALL","title":"Install and configure the new Wacc board"},{"location":"stretch-factory/updates/015_DEX_WRIST/#install-stretch-body-software-packages","text":"You'll be installing a local beta version of relevant Stretch Body packages >>$ cd ~/repos >>$ mkdir dex_wrist >>$ cd dex_wrist >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_body >>$ git clone --branch feature/pluggable_end_effectors https://github.com/hello-robot/stretch_tool_share >>$ git clone https://github.com/hello-robot/stretch_factory >> >>$ cd stretch_body/body >>$ ./local_install.sh >>$ cd ../tools >>$ ./local_install.sh >>$ pip2 install urdfpy >> >>$ pip2 install hello-robot-stretch-tool-share >>$ cd ../../stretch_tool_share/python >>$ ./local_install.sh >>$ cd ~/repos/dex_wrist/stretch_factory/python >> ./local_install.sh","title":"Install Stretch Body Software Packages"},{"location":"stretch-factory/updates/015_DEX_WRIST/#attach-the-dexterous-wrist","text":"First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Next, note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Next, using a Philips screwdriver, attach the wrist mount bracket to the bottom of the tool plate using the provided M2 bolts. NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Next, raise the wrist module up vertically into the mounting bracket, then sliding it over horizontally so that the bearing mates onto its post. Slide in the 3D printed spacer between the pitch servo and the mounting bracket. Finally, attach the body of the pitch servo to the mounting bracket using the 3 M2.5 screws provided. ( NOTE: Flat head screws provided, socket head screws shown below.)","title":"Attach the Dexterous Wrist"},{"location":"stretch-factory/updates/015_DEX_WRIST/#update-the-dynamixel-servo-baud-rates","text":"The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600. >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud >>$ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud","title":"Update the Dynamixel servo baud rates"},{"location":"stretch-factory/updates/015_DEX_WRIST/#update-the-robot-yaml","text":"The new wrist requires a number of updates to the robot YAML YAML doesn't allow definition of multiple fields with the same name. Depending on what is already listed in your YAML you may need to manually edit and merge fields. Add the following to you your ~/stretch_user/$HELLO_FLEET_ID/stretch_re1_user_params.yaml factory_params : stretch_re1_factory_params.yaml params : - stretch_tool_share.stretch_dex_wrist_beta.params head : baud : 115200 end_of_arm : baud : 115200 tool : tool_stretch_dex_wrist #tool: tool_stretch_gripper robot : use_collision_manager : 1 head_pan : baud : 115200 head_tilt : baud : 115200 wrist_yaw : baud : 115200 stretch_gripper : baud : 115200 range_t : - 0 - 6667 zero_t : 3817 lift : i_feedforward : 0.75 hello-motor-lift : gains : i_safety_feedforward : 0.75 Each user account on Stretch will need to update their YAML as well. It is recommended practice to stored a reference of the YAML in /etc so that it will be available to other (new) user accounts. >>$ cd ~/stretch_user/ $HELLO_FLEET_ID >>$ sudo cp *.yaml /etc/hello-robot/ $HELLO_FLEET_ID","title":"Update the robot YAML"},{"location":"stretch-factory/updates/015_DEX_WRIST/#configure-for-use-in-ros","text":"First pull down the new stretch_ros branch and copy in the tool description: >>$ cd ~/catkin_ws/src/stretch_ros/ >>$ git pull >>$ git checkout feature/pluggable_end_effector >>$ cd ~/repos/dex_wrist/stretch_tool_share/tool_share/stretch_dex_wrist_beta/stretch_description >>$ cp urdf/stretch_dex_wrist_beta.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes Now configure stretch_description.xacro to use the StretchDexWrist tool: >>$ nano ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro to read, <?xml version = \"1.0\" ?> <robot xmlns:xacro = \"http://www.ros.org/wiki/xacro\" name = \"stretch_description\" > <xacro:include filename = \"stretch_dex_wrist_beta.xacro\" /> <xacro:include filename = \"stretch_main.xacro\" /> <xacro:include filename = \"stretch_aruco.xacro\" /> <xacro:include filename = \"stretch_d435i.xacro\" /> <xacro:include filename = \"stretch_laser_range_finder.xacro\" /> <xacro:include filename = \"stretch_respeaker.xacro\" /> </robot> Update your URDF and then export the URDF for Stretch Body to use (you may need to Ctrl-C to exit rosrun ) >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ ./export_urdf.sh","title":"Configure for use in ROS"},{"location":"stretch-factory/updates/015_DEX_WRIST/#test-the-wrist-with-the-xbox-controller","text":"Try out the new wrist! Note that the new key mapping does not allow for control of the head. >>$ stretch_xbox_controller_teleop.py Ctrl-C to exit. A printable copy of the teleoperation interface is here","title":"Test the wrist with the XBox Controller"},{"location":"stretch-factory/updates/015_DEX_WRIST/#test-the-wrist-with-rviz","text":"Now check that the wrist appears in RVIZ and can be controlled from the keyboard interface: >>$ roslaunch stretch_calibration simple_test_head_calibration.launch You can type 'q' then Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU -----------| | | | i HEAD UP | | j HEAD LEFT l HEAD RIGHT | | , HEAD DOWN | | | | | | 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT | | home page-up | | | | | | 8 LIFT UP | | up-arrow | | 4 BASE FORWARD 6 BASE BACK | | left-arrow right-arrow | | 2 LIFT DOWN | | down-arrow | | | | | | w ARM OUT | | a WRIST FORWARD d WRIST BACK | | x ARM IN | | | | | | c PITCH FORWARD v PITCH BACK | | o ROLL FORWARD p ROLL BACK | | 5 GRIPPER CLOSE | | 0 GRIPPER OPEN | | | | step size: b BIG, m MEDIUM, s SMALL | | q QUIT | | | |-------------------------------------------|","title":"Test the wrist with RViz"},{"location":"stretch-factory/updates/015_DEX_WRIST/#using-the-stretch-dex-wrist","text":"Additional care should be taken when working with the Dex Wrist as it is now easier to accidentally collide the wrist and gripper with the robot, particularly during lift descent. We've implemented a very coarse collision avoidance behavior in Stretch Body that is turned on by default. It is conservative and doesn't fully prevent collisions however. The collision avoidance can be turned off in the user YAML by: collision_stretch_dex_wrist_to_base : enabled : 0 collision_stretch_dex_wrist_to_self : enabled : 0 You can jog the individual joints of the wrist using the tool: >>$ stretch_dex_wrist_jog.py --pitch Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () For reference, the parameters for the Stretch Dex Wrist (which can be overriden in the user YAML) are found at .local/lib/python2.7/site-packages/stretch_tool_share/stretch_dex_wrist_beta/params.py","title":"Using the Stretch Dex Wrist"},{"location":"stretch-factory/updates/016_WHEEL_STEPPER/","text":"016_WHEEL_STEPPER Background This update configures the robot (stretch-re1-1065) to use a new left wheel module. Clone the repo First, configure the software: >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory >>$ cd stretch_factory/updates/016_WHEEL_STEPPER >>$ ./configure_new_stepper.py Test the Motor Next power down the robot. Power the robot back on and check that the board is on the bus: >>$ ls /dev/hello-motor-left-wheel /dev/hello-motor-left-wheel Finally, check that the base moves correctly. Use the f and b commands to jog the base forward and back. stretch_base_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. -------------- m: menu 1 : rate slow 2 : rate default 3 : rate fast 4 : rate max w: CW/CCW 90 deg x: forward-> back 0 .5m y: spin at 22 .5deg/s f / b / l / r : small forward / back / left / right F / B / L / R : large forward / back / left / right o: freewheel p: pretty print q: quit Input?","title":"016_WHEEL_STEPPER"},{"location":"stretch-factory/updates/016_WHEEL_STEPPER/#016_wheel_stepper","text":"","title":"016_WHEEL_STEPPER"},{"location":"stretch-factory/updates/016_WHEEL_STEPPER/#background","text":"This update configures the robot (stretch-re1-1065) to use a new left wheel module.","title":"Background"},{"location":"stretch-factory/updates/016_WHEEL_STEPPER/#clone-the-repo","text":"First, configure the software: >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_factory >>$ cd stretch_factory/updates/016_WHEEL_STEPPER >>$ ./configure_new_stepper.py","title":"Clone the repo"},{"location":"stretch-factory/updates/016_WHEEL_STEPPER/#test-the-motor","text":"Next power down the robot. Power the robot back on and check that the board is on the bus: >>$ ls /dev/hello-motor-left-wheel /dev/hello-motor-left-wheel Finally, check that the base moves correctly. Use the f and b commands to jog the base forward and back. stretch_base_jog.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. -------------- m: menu 1 : rate slow 2 : rate default 3 : rate fast 4 : rate max w: CW/CCW 90 deg x: forward-> back 0 .5m y: spin at 22 .5deg/s f / b / l / r : small forward / back / left / right F / B / L / R : large forward / back / left / right o: freewheel p: pretty print q: quit Input?","title":"Test the Motor"},{"location":"stretch-factory/updates/017_D435I_TEST/","text":"017_D435I_TEST Background This update describes a series of tests to evaluate if the Stretch D435i camera is working properly. Check USB version Reboot the robot. After reboot, check that the camera is detected as USB 3.2: >>$ rs-enumerate-devices | grep Usb Usb Type Descriptor : 3 .2 Test data collection Create a data collection configuration file: >>$ cd >>$ nano data_collect.cfg And add the following: #Video streams DEPTH,1280,720,15,Z16,0 INFRARED,640,480,15,Y8,1 INFRARED,640,480,15,Y8,2 COLOR,1280,720,15,RGB8,0 # IMU streams will produce data in m/sec^2 & rad/sec ACCEL,1,1,63,MOTION_XYZ32F GYRO,1,1,200,MOTION_XYZ32F Next clear the system log sudo dmesg -c And collect 1000 frames from the camera rs-data-collect -c ./data_collect.cfg -f ./log.csv -t 60 -m 1000","title":"017_D435I_TEST"},{"location":"stretch-factory/updates/017_D435I_TEST/#017_d435i_test","text":"","title":"017_D435I_TEST"},{"location":"stretch-factory/updates/017_D435I_TEST/#background","text":"This update describes a series of tests to evaluate if the Stretch D435i camera is working properly.","title":"Background"},{"location":"stretch-factory/updates/017_D435I_TEST/#check-usb-version","text":"Reboot the robot. After reboot, check that the camera is detected as USB 3.2: >>$ rs-enumerate-devices | grep Usb Usb Type Descriptor : 3 .2","title":"Check USB version"},{"location":"stretch-factory/updates/017_D435I_TEST/#test-data-collection","text":"Create a data collection configuration file: >>$ cd >>$ nano data_collect.cfg And add the following: #Video streams DEPTH,1280,720,15,Z16,0 INFRARED,640,480,15,Y8,1 INFRARED,640,480,15,Y8,2 COLOR,1280,720,15,RGB8,0 # IMU streams will produce data in m/sec^2 & rad/sec ACCEL,1,1,63,MOTION_XYZ32F GYRO,1,1,200,MOTION_XYZ32F Next clear the system log sudo dmesg -c And collect 1000 frames from the camera rs-data-collect -c ./data_collect.cfg -f ./log.csv -t 60 -m 1000","title":"Test data collection"},{"location":"stretch-factory/updates/018_GRIPPER_CALIBRATION/","text":"018_GRIPPER_CALIBRATION Background The Stretch gripper is calibrated to define the closed position, 'zero' or home position, and fully open position of the gripper. Recalibrating may be necessary if replacing the gripper, or if the gripper is not performing correctly. Gripper Calibration First, move to the latest Stretch Factory package (version >=0.0.14) >>$ pip2 install hello-robot-stretch-factory Now run the gripper calibration script. This script will take you through the step-by-step process of calibration. 1. First, the script automatically finds the gripper closed position. >>$ RE1_gripper_calibrate.py Hit enter to find zero Moving to first hardstop... Contact at position: 0 Hit first hardstop, marking to zero ticks Homing is now 4895 Raw position: 12 Moving to calibrated zero: ( rad ) Next you are asked to manually set the 'zero' or home position, entering '1' or '2' to open or close the fingers until they barely don't touch, as pictured. --------------------------------------------------- Enter 1 to open fingers. Enter 2 to close fingers. Enter 3 when the fingertips are just barely not touching. 1 1 1 1 1 2 3 ( 'Setting zero at:' , 5639 ) Next you are asked to manually set the fully open position, entering '1' or '2' to open or close the fingers until the fingers are fully open and stop moving. --------------------------------------------------- Enter 1 to open fingers. Enter 2 to close fingers. Enter 3 when the fingertips are fully open, and no further opening motion is possible 1 1 . . . 1 2 3 ( 'Setting open at:' , 8500 ) Finally, the script will move the gripper to the closed, open, and zero positions. Verify that the gripper moves to the expected poses and save the calibration. Hit enter to close Hit enter to open Hit enter to go to zero Save calibration [ y ] ?y","title":"018_GRIPPER_CALIBRATION"},{"location":"stretch-factory/updates/018_GRIPPER_CALIBRATION/#018_gripper_calibration","text":"","title":"018_GRIPPER_CALIBRATION"},{"location":"stretch-factory/updates/018_GRIPPER_CALIBRATION/#background","text":"The Stretch gripper is calibrated to define the closed position, 'zero' or home position, and fully open position of the gripper. Recalibrating may be necessary if replacing the gripper, or if the gripper is not performing correctly.","title":"Background"},{"location":"stretch-factory/updates/018_GRIPPER_CALIBRATION/#gripper-calibration","text":"First, move to the latest Stretch Factory package (version >=0.0.14) >>$ pip2 install hello-robot-stretch-factory Now run the gripper calibration script. This script will take you through the step-by-step process of calibration. 1. First, the script automatically finds the gripper closed position. >>$ RE1_gripper_calibrate.py Hit enter to find zero Moving to first hardstop... Contact at position: 0 Hit first hardstop, marking to zero ticks Homing is now 4895 Raw position: 12 Moving to calibrated zero: ( rad ) Next you are asked to manually set the 'zero' or home position, entering '1' or '2' to open or close the fingers until they barely don't touch, as pictured. --------------------------------------------------- Enter 1 to open fingers. Enter 2 to close fingers. Enter 3 when the fingertips are just barely not touching. 1 1 1 1 1 2 3 ( 'Setting zero at:' , 5639 ) Next you are asked to manually set the fully open position, entering '1' or '2' to open or close the fingers until the fingers are fully open and stop moving. --------------------------------------------------- Enter 1 to open fingers. Enter 2 to close fingers. Enter 3 when the fingertips are fully open, and no further opening motion is possible 1 1 . . . 1 2 3 ( 'Setting open at:' , 8500 ) Finally, the script will move the gripper to the closed, open, and zero positions. Verify that the gripper moves to the expected poses and save the calibration. Hit enter to close Hit enter to open Hit enter to go to zero Save calibration [ y ] ?y","title":"Gripper Calibration"},{"location":"stretch-firmware/","text":"Overview The Stretch Firmware repository provides the Arduino based firmware for the Stretch robot. Minor version updates to Stretch Body may occasionally require the robot's firmware to also be updated. The repository includes the firmware for the three Stretch PCBA types: hello_stepper: firmware for stepper motor controller based on the Mechaduino project hello_wacc: firmware for wrist accelerometer board (Wacc) in the wrist hello_pimu: firmware for power and imu board (Pimu) in the base License For details, see the LICENSE.md file in the root directory. All materials within this repository are licensed with the GNU General Public License v3.0 (GNU GPLv3) except where other third-party licenses must apply. We thank people who have contributed to this work via open-source code and open hardware. We especially thank the Mechaduino project and Tropical Labs . The motor controller firmware and hardware are derived from the excellent firmware and hardware created for the Mechaduino project by Tropical Labs. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Overview"},{"location":"stretch-firmware/#overview","text":"The Stretch Firmware repository provides the Arduino based firmware for the Stretch robot. Minor version updates to Stretch Body may occasionally require the robot's firmware to also be updated. The repository includes the firmware for the three Stretch PCBA types: hello_stepper: firmware for stepper motor controller based on the Mechaduino project hello_wacc: firmware for wrist accelerometer board (Wacc) in the wrist hello_pimu: firmware for power and imu board (Pimu) in the base","title":"Overview"},{"location":"stretch-firmware/#license","text":"For details, see the LICENSE.md file in the root directory. All materials within this repository are licensed with the GNU General Public License v3.0 (GNU GPLv3) except where other third-party licenses must apply. We thank people who have contributed to this work via open-source code and open hardware. We especially thank the Mechaduino project and Tropical Labs . The motor controller firmware and hardware are derived from the excellent firmware and hardware created for the Mechaduino project by Tropical Labs. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"License"},{"location":"stretch-firmware/LICENSE/","text":"The following license (GPLv3) applies to the entire contents of this directory (the \"Contents\") except where other third-party licenses must apply. The Contents include firmware and hardware for use with the Stretch RE1 and RE2 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. We thank people who have contributed to this work via open-source code and open hardware. We especially thank the Mechaduino project and Tropical Labs . The motor controller firmware and hardware are derived from the excellent firmware and hardware created for the Mechaduino project by Tropical Labs. The Mechaduino firmware and hardware were originally released using the Creative Commons Attribution Share-Alike 4.0 License (CC BY-SA 4.0) . With approval from Tropical Labs, we have licensed our derived firmware and hardware using the GNU General Public License v3.0 (GNU GPLv3) as described below. As stated by Creative Commons, this is permitted due to the GPLv3 being a one-way BY-SA compatible license, such that \"you may license your contributions to adaptations of BY-SA 4.0 materials under GPLv3.\" . Copyright 2020 Hello Robot Inc. The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation. The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link: https://www.gnu.org/licenses/gpl-3.0.html For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/","text":"Arduino Core for SAMD21 CPU This repository contains the source code and configuration files of the Arduino Core for Atmel's SAMD21 processor (used on the Arduino/Genuino Zero, MKR1000 and MKRZero boards). Installation on Arduino IDE This core is available as a package in the Arduino IDE cores manager. Just open the \"Boards Manager\" and install the package called: \"Arduino SAMD Boards (32-bit ARM Cortex-M0+)\" Support There is a dedicated section of the Arduino Forum for general discussion and project assistance: http://forum.arduino.cc/index.php?board=98.0 Bugs or Issues If you find a bug you can submit an issue here on github: https://github.com/arduino/ArduinoCore-samd/issues Before posting a new issue, please check if the same problem has been already reported by someone else to avoid duplicates. Contributions Contributions are always welcome. The preferred way to receive code cotribution is by submitting a Pull Request on github. Hourly builds This repository is under a Continuous Integration system that every hour checks if there are updates and builds a release for testing (the so called \"Hourly builds\"). The hourly builds are available through Boards Manager. If you want to install them: 1. Open the Preferences of the Arduino IDE. 2. Add this URL http://downloads.arduino.cc/Hourly/samd/package_samd-hourly-build_index.json in the Additional Boards Manager URLs field, and click OK. 3. Open the Boards Manager (menu Tools->Board->Board Manager...) 4. Install Arduino SAMD core - Hourly build 5. Select one of the boards under SAMD Hourly build XX in Tools->Board menu 6. Compile/Upload as usual If you already installed an hourly build and you want to update it with the latest: 1. Open the Boards Manager (menu Tools->Board->Board Manager...) 2. Remove Arduino SAMD core - Hourly build 3. Install again Arduino SAMD core - Hourly build , the Board Manager will download the latest build replacing the old one. License and credits This core has been developed by Arduino LLC in collaboration with Atmel. Copyright (c) 2015 Arduino LLC. All right reserved. This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation; either version 2.1 of the License, or (at your option) any later version. This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to the Free Software Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA","title":"Arduino Core for SAMD21 CPU"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#arduino-core-for-samd21-cpu","text":"This repository contains the source code and configuration files of the Arduino Core for Atmel's SAMD21 processor (used on the Arduino/Genuino Zero, MKR1000 and MKRZero boards).","title":"Arduino Core for SAMD21 CPU"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#installation-on-arduino-ide","text":"This core is available as a package in the Arduino IDE cores manager. Just open the \"Boards Manager\" and install the package called: \"Arduino SAMD Boards (32-bit ARM Cortex-M0+)\"","title":"Installation on Arduino IDE"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#support","text":"There is a dedicated section of the Arduino Forum for general discussion and project assistance: http://forum.arduino.cc/index.php?board=98.0","title":"Support"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#bugs-or-issues","text":"If you find a bug you can submit an issue here on github: https://github.com/arduino/ArduinoCore-samd/issues Before posting a new issue, please check if the same problem has been already reported by someone else to avoid duplicates.","title":"Bugs or Issues"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#contributions","text":"Contributions are always welcome. The preferred way to receive code cotribution is by submitting a Pull Request on github.","title":"Contributions"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#hourly-builds","text":"This repository is under a Continuous Integration system that every hour checks if there are updates and builds a release for testing (the so called \"Hourly builds\"). The hourly builds are available through Boards Manager. If you want to install them: 1. Open the Preferences of the Arduino IDE. 2. Add this URL http://downloads.arduino.cc/Hourly/samd/package_samd-hourly-build_index.json in the Additional Boards Manager URLs field, and click OK. 3. Open the Boards Manager (menu Tools->Board->Board Manager...) 4. Install Arduino SAMD core - Hourly build 5. Select one of the boards under SAMD Hourly build XX in Tools->Board menu 6. Compile/Upload as usual If you already installed an hourly build and you want to update it with the latest: 1. Open the Boards Manager (menu Tools->Board->Board Manager...) 2. Remove Arduino SAMD core - Hourly build 3. Install again Arduino SAMD core - Hourly build , the Board Manager will download the latest build replacing the old one.","title":"Hourly builds"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/#license-and-credits","text":"This core has been developed by Arduino LLC in collaboration with Atmel. Copyright (c) 2015 Arduino LLC. All right reserved. This library is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation; either version 2.1 of the License, or (at your option) any later version. This library is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU Lesser General Public License along with this library; if not, write to the Free Software Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA","title":"License and credits"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/","text":"Arduino Zero Bootloader 1- Prerequisites The project build is based on Makefile system. Makefile is present at project root and try to handle multi-platform cases. Multi-plaform GCC is provided by ARM here: https://launchpad.net/gcc-arm-embedded/+download Atmel Studio contains both make and ARM GCC toolchain. You don't need to install them in this specific use case. For all builds and platforms you will need to have the Arduino IDE installed and the board support package for \"Arduino SAMD Boards (32-bits ARM Cortex-M0+)\". You can install the latter from the former's \"Boards Manager\" UI. Windows Native command line Make binary can be obtained here: http://gnuwin32.sourceforge.net/packages/make.htm Cygwin/MSys/MSys2/Babun/etc... It is available natively in all distributions. Atmel Studio An Atmel Studio 7 Makefile-based project is present at project root, just open samd21_sam_ba.atsln file in AS7. Linux Make is usually available by default. OS X Make is available through XCode package. 2- Selecting available SAM-BA interfaces By default both USB and UART are made available, but this parameter can be modified in sam_ba_monitor.h, line 31: Set the define SAM_BA_INTERFACE to * SAM_BA_UART_ONLY for only UART interface * SAM_BA_USBCDC_ONLY for only USB CDC interface * SAM_BA_BOTH_INTERFACES for enabling both the interfaces 3- Behaviour This bootloader implements the double-tap on Reset button. By quickly pressing this button two times, the board will reset and stay in bootloader, waiting for communication on either USB or USART. The USB port in use is the USB Native port, close to the Reset button. The USART in use is the one available on pins D0/D1, labelled respectively RX/TX. Communication parameters are a baudrate at 115200, 8bits of data, no parity and 1 stop bit (8N1). 4- Description Pinmap The following pins are used by the program : PA25 : input/output (USB DP) PA24 : input/output (USB DM) PA11 : input (USART RX) PA10 : output (USART TX) The application board shall avoid driving the PA25, PA24, PB23 and PB22 signals while the boot program is running (after a POR for example). Clock system CPU runs at 48MHz from Generic Clock Generator 0 on DFLL48M. Generic Clock Generator 1 is using external 32kHz oscillator and is the source of DFLL48M. USB and USART are using Generic Clock Generator 0 also. Memory Mapping Bootloader code will be located at 0x0 and executed before any applicative code. Applications compiled to be executed along with the bootloader will start at 0x2000 (see linker script bootloader_samd21x18.ld). Before jumping to the application, the bootloader changes the VTOR register to use the interrupt vectors of the application @0x2000.<- not required as application code is taking care of this. 5- How to build If not specified the makefile builds for Arduino Zero : make if you want to make a custom bootloader for a derivative board you must supply all the necessary information in a board_definitions_xxx.h file, and add the corresponding case in board_definitions.h . For example for the Arduino MKR1000 we use board_definitions_arduino_mkr1000.h and it is build with the following command: BOARD_ID=arduino_mkr1000 NAME=samd21_sam_ba_arduino_mkr1000 make clean all","title":"Arduino Zero Bootloader"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#arduino-zero-bootloader","text":"","title":"Arduino Zero Bootloader"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#1-prerequisites","text":"The project build is based on Makefile system. Makefile is present at project root and try to handle multi-platform cases. Multi-plaform GCC is provided by ARM here: https://launchpad.net/gcc-arm-embedded/+download Atmel Studio contains both make and ARM GCC toolchain. You don't need to install them in this specific use case. For all builds and platforms you will need to have the Arduino IDE installed and the board support package for \"Arduino SAMD Boards (32-bits ARM Cortex-M0+)\". You can install the latter from the former's \"Boards Manager\" UI.","title":"1- Prerequisites"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#windows","text":"Native command line Make binary can be obtained here: http://gnuwin32.sourceforge.net/packages/make.htm Cygwin/MSys/MSys2/Babun/etc... It is available natively in all distributions. Atmel Studio An Atmel Studio 7 Makefile-based project is present at project root, just open samd21_sam_ba.atsln file in AS7.","title":"Windows"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#linux","text":"Make is usually available by default.","title":"Linux"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#os-x","text":"Make is available through XCode package.","title":"OS X"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#2-selecting-available-sam-ba-interfaces","text":"By default both USB and UART are made available, but this parameter can be modified in sam_ba_monitor.h, line 31: Set the define SAM_BA_INTERFACE to * SAM_BA_UART_ONLY for only UART interface * SAM_BA_USBCDC_ONLY for only USB CDC interface * SAM_BA_BOTH_INTERFACES for enabling both the interfaces","title":"2- Selecting available SAM-BA interfaces"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#3-behaviour","text":"This bootloader implements the double-tap on Reset button. By quickly pressing this button two times, the board will reset and stay in bootloader, waiting for communication on either USB or USART. The USB port in use is the USB Native port, close to the Reset button. The USART in use is the one available on pins D0/D1, labelled respectively RX/TX. Communication parameters are a baudrate at 115200, 8bits of data, no parity and 1 stop bit (8N1).","title":"3- Behaviour"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#4-description","text":"Pinmap The following pins are used by the program : PA25 : input/output (USB DP) PA24 : input/output (USB DM) PA11 : input (USART RX) PA10 : output (USART TX) The application board shall avoid driving the PA25, PA24, PB23 and PB22 signals while the boot program is running (after a POR for example). Clock system CPU runs at 48MHz from Generic Clock Generator 0 on DFLL48M. Generic Clock Generator 1 is using external 32kHz oscillator and is the source of DFLL48M. USB and USART are using Generic Clock Generator 0 also. Memory Mapping Bootloader code will be located at 0x0 and executed before any applicative code. Applications compiled to be executed along with the bootloader will start at 0x2000 (see linker script bootloader_samd21x18.ld). Before jumping to the application, the bootloader changes the VTOR register to use the interrupt vectors of the application @0x2000.<- not required as application code is taking care of this.","title":"4- Description"},{"location":"stretch-firmware/arduino/hardware/hello-robot/samd/bootloaders/zero/#5-how-to-build","text":"If not specified the makefile builds for Arduino Zero : make if you want to make a custom bootloader for a derivative board you must supply all the necessary information in a board_definitions_xxx.h file, and add the corresponding case in board_definitions.h . For example for the Arduino MKR1000 we use board_definitions_arduino_mkr1000.h and it is build with the following command: BOARD_ID=arduino_mkr1000 NAME=samd21_sam_ba_arduino_mkr1000 make clean all","title":"5- How to build"},{"location":"stretch-firmware/arduino/hello_pimu/","text":"Stretch RE1/RE2 - Pimu Firmware The Stretch Pimu Firmware runs on the Pimu PCBA. Pimu stands for Power+IMU. The firmware is responsible for Monitoring / filtering / calibration of 9DOF IMU Battery voltage and current monitoring Buzzer Base case fan control Runstop monitoring Monitoring cliff sensors","title":"Stretch RE1/RE2 - Pimu Firmware"},{"location":"stretch-firmware/arduino/hello_pimu/#stretch-re1re2-pimu-firmware","text":"The Stretch Pimu Firmware runs on the Pimu PCBA. Pimu stands for Power+IMU. The firmware is responsible for Monitoring / filtering / calibration of 9DOF IMU Battery voltage and current monitoring Buzzer Base case fan control Runstop monitoring Monitoring cliff sensors","title":"Stretch RE1/RE2 - Pimu Firmware"},{"location":"stretch-firmware/arduino/hello_stepper/","text":"Stretch RE1/RE2 - Stepper Firmware The Stretch Stepper Firmware runs on the Stepper PCBA found on the back of the Stretch stepper motors. It is a modified version of the open source Mechaduino project , which provides closed loop current control of a stepper motor. The firmware is organized so as to allow switching, via serial command, between 'stock Mechaduino mode' with its menu interface and the Hello Robot firmware with its RPC interface. The primary hardware modifications are: Higher current capacity by use of parallel motor drivers Locking JST connectors Addition of a 'sync' line for multi-dof synchronization The primary software modifications are: RPC and serialization layer to manage controller parameters, controller commands, etc Trapezoidal trajectory generators based on the library MotionGenerator Handling of runstop and motor synchronization Implementation of a guarded move behavior Various controller variants, controller and parameter management functions","title":"Stretch RE1/RE2 - Stepper Firmware"},{"location":"stretch-firmware/arduino/hello_stepper/#stretch-re1re2-stepper-firmware","text":"The Stretch Stepper Firmware runs on the Stepper PCBA found on the back of the Stretch stepper motors. It is a modified version of the open source Mechaduino project , which provides closed loop current control of a stepper motor. The firmware is organized so as to allow switching, via serial command, between 'stock Mechaduino mode' with its menu interface and the Hello Robot firmware with its RPC interface. The primary hardware modifications are: Higher current capacity by use of parallel motor drivers Locking JST connectors Addition of a 'sync' line for multi-dof synchronization The primary software modifications are: RPC and serialization layer to manage controller parameters, controller commands, etc Trapezoidal trajectory generators based on the library MotionGenerator Handling of runstop and motor synchronization Implementation of a guarded move behavior Various controller variants, controller and parameter management functions","title":"Stretch RE1/RE2 - Stepper Firmware"},{"location":"stretch-firmware/arduino/hello_wacc/","text":"Stretch RE1/RE2 - Wacc Firmware The Stretch Wacc Firmware runs on the Wacc PCBA found inside the robot arm. Wacc stands for the 'Wrist + Accelerometer' board. The Wacc firmware provides Monitoring of the wrist accelerometer Monitoring the Arduino expansion header in the wrist The Wacc expansion header is electrically routed to potentially provide Analog in (x1) Digital in (x1) Digital out (x2) Serial SPI Serial I2C Serial UART See the Stretch RE1 Hardware User Guide for pin-out, electrical, and connector information. Using the Expansion DIO Header Factory Interface By default the Expansion DIO header is configured in firmware to provide two digital inputs, two digital outputs, and an analog input. This can be seen in the Wacc_status structure. struct __attribute__ (( packed )) Wacc_Status { float ax ; //Accelerometer AX float ay ; //Accelerometer AY float az ; //Accelerometer AZ int16_t a0 ; //expansion header analog in uint8_t d0 ; //expansion header digital in uint8_t d1 ; //expansion header digital in uint8_t d2 ; //expansion header digital out uint8_t d3 ; //expansion header digital out uint32_t single_tap_count ; //Accelerometer tap count uint32_t state ; uint32_t timestamp ; //ms, overflows every 50 days uint32_t debug ; }; The user can interact with these pins through the Stretch Body python interface . Custom Interface Advanced users may want to create a custom interface to the Expansion DIO header that utilizes SPI, I2C, or UART. For this, see the provided Stretch Firmware tutorials.","title":"Stretch RE1/RE2 - Wacc Firmware"},{"location":"stretch-firmware/arduino/hello_wacc/#stretch-re1re2-wacc-firmware","text":"The Stretch Wacc Firmware runs on the Wacc PCBA found inside the robot arm. Wacc stands for the 'Wrist + Accelerometer' board. The Wacc firmware provides Monitoring of the wrist accelerometer Monitoring the Arduino expansion header in the wrist The Wacc expansion header is electrically routed to potentially provide Analog in (x1) Digital in (x1) Digital out (x2) Serial SPI Serial I2C Serial UART See the Stretch RE1 Hardware User Guide for pin-out, electrical, and connector information.","title":"Stretch RE1/RE2 - Wacc Firmware"},{"location":"stretch-firmware/arduino/hello_wacc/#using-the-expansion-dio-header","text":"","title":"Using the Expansion DIO Header"},{"location":"stretch-firmware/arduino/hello_wacc/#factory-interface","text":"By default the Expansion DIO header is configured in firmware to provide two digital inputs, two digital outputs, and an analog input. This can be seen in the Wacc_status structure. struct __attribute__ (( packed )) Wacc_Status { float ax ; //Accelerometer AX float ay ; //Accelerometer AY float az ; //Accelerometer AZ int16_t a0 ; //expansion header analog in uint8_t d0 ; //expansion header digital in uint8_t d1 ; //expansion header digital in uint8_t d2 ; //expansion header digital out uint8_t d3 ; //expansion header digital out uint32_t single_tap_count ; //Accelerometer tap count uint32_t state ; uint32_t timestamp ; //ms, overflows every 50 days uint32_t debug ; }; The user can interact with these pins through the Stretch Body python interface .","title":"Factory Interface"},{"location":"stretch-firmware/arduino/hello_wacc/#custom-interface","text":"Advanced users may want to create a custom interface to the Expansion DIO header that utilizes SPI, I2C, or UART. For this, see the provided Stretch Firmware tutorials.","title":"Custom Interface"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ADXL343/","text":"Adafruit ADXL343 Accelerometer Driver This driver is for the Adafruit ADXL343 Breakout (http://www.adafruit.com/products/), and is based on Adafruit's Unified Sensor Library (Adafruit_Sensor). About the ADXL343 The ADXL343 is a digital accelerometer that supports both SPI and I2C mode, with adjustable data rata and 'range' (+/-2/4/8/16g). The Adafruit_ADXL343 driver takes advantage of I2C mode to reduce the total pin count required to use the sensor. More information on the ADXL345 can be found in the datasheet: http://www.analog.com/static/imported-files/data_sheets/ADXL343.pdf What is the Adafruit Unified Sensor Library? The Adafruit Unified Sensor Library (https://github.com/adafruit/Adafruit_Sensor) provides a common interface and data type for any supported sensor. It defines some basic information about the sensor (sensor limits, etc.), and returns standard SI units of a specific type and scale for each supported sensor type. It provides a simple abstraction layer between your application and the actual sensor HW, allowing you to drop in any comparable sensor with only one or two lines of code to change in your project (essentially the constructor since the functions to read sensor data and get information about the sensor are defined in the base Adafruit_Sensor class). This is imporant useful for two reasons: 1.) You can use the data right away because it's already converted to SI units that you understand and can compare, rather than meaningless values like 0..1023. 2.) Because SI units are standardised in the sensor library, you can also do quick sanity checks working with new sensors, or drop in any comparable sensor if you need better sensitivity or if a lower cost unit becomes available, etc. Light sensors will always report units in lux, gyroscopes will always report units in rad/s, etc. ... freeing you up to focus on the data, rather than digging through the datasheet to understand what the sensor's raw numbers really mean. About this Driver Adafruit invests time and resources providing this open source code. Please support Adafruit and open-source hardware by purchasing products from Adafruit! Written by Kevin (KTOWN) Townsend for Adafruit Industries.","title":"Adafruit ADXL343 Accelerometer Driver ![Build Status](https://github.com/adafruit/Adafruit_ADXL343/workflows/Arduino%20Library%20CI/badge.svg)"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ADXL343/#adafruit-adxl343-accelerometer-driver","text":"This driver is for the Adafruit ADXL343 Breakout (http://www.adafruit.com/products/), and is based on Adafruit's Unified Sensor Library (Adafruit_Sensor).","title":"Adafruit ADXL343 Accelerometer Driver"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ADXL343/#about-the-adxl343","text":"The ADXL343 is a digital accelerometer that supports both SPI and I2C mode, with adjustable data rata and 'range' (+/-2/4/8/16g). The Adafruit_ADXL343 driver takes advantage of I2C mode to reduce the total pin count required to use the sensor. More information on the ADXL345 can be found in the datasheet: http://www.analog.com/static/imported-files/data_sheets/ADXL343.pdf","title":"About the ADXL343"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ADXL343/#what-is-the-adafruit-unified-sensor-library","text":"The Adafruit Unified Sensor Library (https://github.com/adafruit/Adafruit_Sensor) provides a common interface and data type for any supported sensor. It defines some basic information about the sensor (sensor limits, etc.), and returns standard SI units of a specific type and scale for each supported sensor type. It provides a simple abstraction layer between your application and the actual sensor HW, allowing you to drop in any comparable sensor with only one or two lines of code to change in your project (essentially the constructor since the functions to read sensor data and get information about the sensor are defined in the base Adafruit_Sensor class). This is imporant useful for two reasons: 1.) You can use the data right away because it's already converted to SI units that you understand and can compare, rather than meaningless values like 0..1023. 2.) Because SI units are standardised in the sensor library, you can also do quick sanity checks working with new sensors, or drop in any comparable sensor if you need better sensitivity or if a lower cost unit becomes available, etc. Light sensors will always report units in lux, gyroscopes will always report units in rad/s, etc. ... freeing you up to focus on the data, rather than digging through the datasheet to understand what the sensor's raw numbers really mean.","title":"What is the Adafruit Unified Sensor Library?"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ADXL343/#about-this-driver","text":"Adafruit invests time and resources providing this open source code. Please support Adafruit and open-source hardware by purchasing products from Adafruit! Written by Kevin (KTOWN) Townsend for Adafruit Industries.","title":"About this Driver"},{"location":"stretch-firmware/arduino/libraries/Adafruit_FXAS21002C/","text":"Adafruit_FXAS21002C Driver for the Adafruit FXAS21002C 3-Axis gyroscope breakout","title":"Adafruit_FXAS21002C"},{"location":"stretch-firmware/arduino/libraries/Adafruit_FXAS21002C/#adafruit_fxas21002c","text":"Driver for the Adafruit FXAS21002C 3-Axis gyroscope breakout","title":"Adafruit_FXAS21002C"},{"location":"stretch-firmware/arduino/libraries/Adafruit_FXOS8700/","text":"Adafruit_FXOS8700 Driver for the Adafruit FXOS8700 Accelerometer/Magnetometer Breakout","title":"Adafruit_FXOS8700"},{"location":"stretch-firmware/arduino/libraries/Adafruit_FXOS8700/#adafruit_fxos8700","text":"Driver for the Adafruit FXOS8700 Accelerometer/Magnetometer Breakout","title":"Adafruit_FXOS8700"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/","text":"Adafruit NeoPixel Library Arduino library for controlling single-wire-based LED pixels and strip such as the Adafruit 60 LED/meter Digital LED strip , the Adafruit FLORA RGB Smart Pixel , the Adafruit Breadboard-friendly RGB Smart Pixel , the Adafruit NeoPixel Stick , and the Adafruit NeoPixel Shield . After downloading, rename folder to 'Adafruit_NeoPixel' and install in Arduino Libraries folder. Restart Arduino IDE, then open File->Sketchbook->Library->Adafruit_NeoPixel->strandtest sketch. Compatibility notes: Port A is not supported on any AVR processors at this time Installation First Method In the Arduino IDE, navigate to Sketch > Include Library > Manage Libraries Then the Library Manager will open and you will find a list of libraries that are already installed or ready for installation. Then search for Neopixel strip using the search bar. Click on the text area and then select the specific version and install it. Second Method Navigate to the Releases page . Download the latest release. Extract the zip file In the Arduino IDE, navigate to Sketch > Include Library > Add .ZIP Library Features Simple to use Controlling NeoPixels \u201cfrom scratch\u201d is quite a challenge, so we provide a library letting you focus on the fun and interesting bits. Give back The library is free; you don\u2019t have to pay for anything. Adafruit invests time and resources providing this open source code, please support Adafruit and open-source hardware by purchasing products from Adafruit! Supported Chipsets We have included code for the following chips - sometimes these break for exciting reasons that we can't control in which case please open an issue! AVR ATmega and ATtiny (any 8-bit) - 8 MHz, 12 MHz and 16 MHz Teensy 3.x and LC Arduino Due Arduino 101 ATSAMD21 (Arduino Zero/M0 and other SAMD21 boards) @ 48 MHz ATSAMD51 @ 120 MHz Adafruit STM32 Feather @ 120 MHz ESP8266 any speed ESP32 any speed Nordic nRF52 (Adafruit Feather nRF52), nRF51 (micro:bit) Infineon XMC1100 BootKit @ 32 MHz Infineon XMC1100 2Go @ 32 MHz Infineon XMC1300 BootKit @ 32 MHz Infineon XMC4700 RelaxKit, XMC4800 RelaxKit, XMC4800 IoT Amazon FreeRTOS Kit @ 144 MHz Check forks for other architectures not listed here! GNU Lesser General Public License Adafruit_NeoPixel is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. Functions begin() updateLength() updateType() show() delay_ns() setPin() setPixelColor() fill() ColorHSV() getPixelColor() setBrightness() getBrightness() clear() gamma32() Examples There are many examples implemented in this library. One of the examples is below. You can find other examples here Simple #include <Adafruit_NeoPixel.h> #ifdef __AVR__ #include <avr/power.h> #endif #define PIN 6 #define NUMPIXELS 16 Adafruit_NeoPixel pixels ( NUMPIXELS , PIN , NEO_GRB + NEO_KHZ800 ); #define DELAYVAL 500 void setup () { #if defined(__AVR_ATtiny85__) && (F_CPU == 16000000) clock_prescale_set ( clock_div_1 ); #endif pixels . begin (); } void loop () { pixels . clear (); for ( int i = 0 ; i < NUMPIXELS ; i ++ ) { pixels . setPixelColor ( i , pixels . Color ( 0 , 150 , 0 )); pixels . show (); delay ( DELAYVAL ); } } Contributing If you want to contribute to this project: Report bugs and errors Ask for enhancements Create issues and pull requests Tell others about this library Contribute new protocols Please read CONTRIBUTING.md for details on our code of conduct, and the process for submitting pull requests to us. Roadmap The PRIME DIRECTIVE is to maintain backward compatibility with existing Arduino sketches -- many are hosted elsewhere and don't track changes here, some are in print and can never be changed! Please don't reformat code for the sake of reformatting code. The resulting large \"visual diff\" makes it impossible to untangle actual bug fixes from merely rearranged lines. (Exception for first item in wishlist below.) Things I'd Like To Do But There's No Official Timeline So Please Don't Count On Any Of This Ever Being Canonical: For the show() function (with all the delicate pixel timing stuff), break out each architecture into separate source files rather than the current unmaintainable tangle of #ifdef statements! Please don't use updateLength() or updateType() in new code. They should not have been implemented this way (use the C++ 'new' operator with the regular constructor instead) and are only sticking around because of the Prime Directive. setPin() is OK for now though, it's a trick we can use to 'recycle' pixel memory across multiple strips. In the M0 and M4 code, use the hardware systick counter for bit timing rather than hand-tweaked NOPs (a temporary kludge at the time because I wasn't reading systick correctly). (As of 1.4.2, systick is used on M4 devices and it appears to be overclock-compatible. Not for M0 yet, which is why this item is still here.) As currently written, brightness scaling is still a \"destructive\" operation -- pixel values are altered in RAM and the original value as set can't be accurately read back, only approximated, which has been confusing and frustrating to users. It was done this way at the time because NeoPixel timing is strict, AVR microcontrollers (all we had at the time) are limited, and assembly language is hard. All the 32-bit architectures should have no problem handling nondestructive brightness scaling -- calculating each byte immediately before it's sent out the wire, maintaining the original set value in RAM -- the work just hasn't been done. There's a fair chance even the AVR code could manage it with some intense focus. (The DotStar library achieves nondestructive brightness scaling because it doesn't have to manage data timing so carefully...every architecture, even ATtiny, just takes whatever cycles it needs for the multiply/shift operations.) Credits This library is written by Phil \"Paint Your Dragon\" Burgess for Adafruit Industries, with contributions by PJRC, Michael Miller and other members of the open source community. License Adafruit_NeoPixel is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. Adafruit_NeoPixel is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU Lesser General Public License along with NeoPixel. If not, see this","title":"Adafruit NeoPixel Library [![Build Status](https://github.com/adafruit/Adafruit_NeoPixel/workflows/Arduino%20Library%20CI/badge.svg)](https://github.com/adafruit/Adafruit_NeoPixel/actions)[![Documentation](https://github.com/adafruit/ci-arduino/blob/master/assets/doxygen_badge.svg)](http://adafruit.github.io/Adafruit_NeoPixel/html/index.html)"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#adafruit-neopixel-library","text":"Arduino library for controlling single-wire-based LED pixels and strip such as the Adafruit 60 LED/meter Digital LED strip , the Adafruit FLORA RGB Smart Pixel , the Adafruit Breadboard-friendly RGB Smart Pixel , the Adafruit NeoPixel Stick , and the Adafruit NeoPixel Shield . After downloading, rename folder to 'Adafruit_NeoPixel' and install in Arduino Libraries folder. Restart Arduino IDE, then open File->Sketchbook->Library->Adafruit_NeoPixel->strandtest sketch. Compatibility notes: Port A is not supported on any AVR processors at this time","title":"Adafruit NeoPixel Library"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#installation","text":"","title":"Installation"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#first-method","text":"In the Arduino IDE, navigate to Sketch > Include Library > Manage Libraries Then the Library Manager will open and you will find a list of libraries that are already installed or ready for installation. Then search for Neopixel strip using the search bar. Click on the text area and then select the specific version and install it.","title":"First Method"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#second-method","text":"Navigate to the Releases page . Download the latest release. Extract the zip file In the Arduino IDE, navigate to Sketch > Include Library > Add .ZIP Library","title":"Second Method"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#features","text":"","title":"Features"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#simple-to-use","text":"Controlling NeoPixels \u201cfrom scratch\u201d is quite a challenge, so we provide a library letting you focus on the fun and interesting bits.","title":"Simple to use"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#give-back","text":"The library is free; you don\u2019t have to pay for anything. Adafruit invests time and resources providing this open source code, please support Adafruit and open-source hardware by purchasing products from Adafruit!","title":"Give back"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#supported-chipsets","text":"We have included code for the following chips - sometimes these break for exciting reasons that we can't control in which case please open an issue! AVR ATmega and ATtiny (any 8-bit) - 8 MHz, 12 MHz and 16 MHz Teensy 3.x and LC Arduino Due Arduino 101 ATSAMD21 (Arduino Zero/M0 and other SAMD21 boards) @ 48 MHz ATSAMD51 @ 120 MHz Adafruit STM32 Feather @ 120 MHz ESP8266 any speed ESP32 any speed Nordic nRF52 (Adafruit Feather nRF52), nRF51 (micro:bit) Infineon XMC1100 BootKit @ 32 MHz Infineon XMC1100 2Go @ 32 MHz Infineon XMC1300 BootKit @ 32 MHz Infineon XMC4700 RelaxKit, XMC4800 RelaxKit, XMC4800 IoT Amazon FreeRTOS Kit @ 144 MHz Check forks for other architectures not listed here!","title":"Supported Chipsets"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#gnu-lesser-general-public-license","text":"Adafruit_NeoPixel is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.","title":"GNU Lesser General Public License"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#functions","text":"begin() updateLength() updateType() show() delay_ns() setPin() setPixelColor() fill() ColorHSV() getPixelColor() setBrightness() getBrightness() clear() gamma32()","title":"Functions"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#examples","text":"There are many examples implemented in this library. One of the examples is below. You can find other examples here","title":"Examples"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#simple","text":"#include <Adafruit_NeoPixel.h> #ifdef __AVR__ #include <avr/power.h> #endif #define PIN 6 #define NUMPIXELS 16 Adafruit_NeoPixel pixels ( NUMPIXELS , PIN , NEO_GRB + NEO_KHZ800 ); #define DELAYVAL 500 void setup () { #if defined(__AVR_ATtiny85__) && (F_CPU == 16000000) clock_prescale_set ( clock_div_1 ); #endif pixels . begin (); } void loop () { pixels . clear (); for ( int i = 0 ; i < NUMPIXELS ; i ++ ) { pixels . setPixelColor ( i , pixels . Color ( 0 , 150 , 0 )); pixels . show (); delay ( DELAYVAL ); } }","title":"Simple"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#contributing","text":"If you want to contribute to this project: Report bugs and errors Ask for enhancements Create issues and pull requests Tell others about this library Contribute new protocols Please read CONTRIBUTING.md for details on our code of conduct, and the process for submitting pull requests to us.","title":"Contributing"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#roadmap","text":"The PRIME DIRECTIVE is to maintain backward compatibility with existing Arduino sketches -- many are hosted elsewhere and don't track changes here, some are in print and can never be changed! Please don't reformat code for the sake of reformatting code. The resulting large \"visual diff\" makes it impossible to untangle actual bug fixes from merely rearranged lines. (Exception for first item in wishlist below.) Things I'd Like To Do But There's No Official Timeline So Please Don't Count On Any Of This Ever Being Canonical: For the show() function (with all the delicate pixel timing stuff), break out each architecture into separate source files rather than the current unmaintainable tangle of #ifdef statements! Please don't use updateLength() or updateType() in new code. They should not have been implemented this way (use the C++ 'new' operator with the regular constructor instead) and are only sticking around because of the Prime Directive. setPin() is OK for now though, it's a trick we can use to 'recycle' pixel memory across multiple strips. In the M0 and M4 code, use the hardware systick counter for bit timing rather than hand-tweaked NOPs (a temporary kludge at the time because I wasn't reading systick correctly). (As of 1.4.2, systick is used on M4 devices and it appears to be overclock-compatible. Not for M0 yet, which is why this item is still here.) As currently written, brightness scaling is still a \"destructive\" operation -- pixel values are altered in RAM and the original value as set can't be accurately read back, only approximated, which has been confusing and frustrating to users. It was done this way at the time because NeoPixel timing is strict, AVR microcontrollers (all we had at the time) are limited, and assembly language is hard. All the 32-bit architectures should have no problem handling nondestructive brightness scaling -- calculating each byte immediately before it's sent out the wire, maintaining the original set value in RAM -- the work just hasn't been done. There's a fair chance even the AVR code could manage it with some intense focus. (The DotStar library achieves nondestructive brightness scaling because it doesn't have to manage data timing so carefully...every architecture, even ATtiny, just takes whatever cycles it needs for the multiply/shift operations.)","title":"Roadmap"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#credits","text":"This library is written by Phil \"Paint Your Dragon\" Burgess for Adafruit Industries, with contributions by PJRC, Michael Miller and other members of the open source community.","title":"Credits"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/#license","text":"Adafruit_NeoPixel is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. Adafruit_NeoPixel is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU Lesser General Public License along with NeoPixel. If not, see this","title":"License"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/CONTRIBUTING/","text":"Contribution Guidelines This library is the culmination of the expertise of many members of the open source community who have dedicated their time and hard work. The best way to ask for help or propose a new idea is to create a new issue while creating a Pull Request with your code changes allows you to share your own innovations with the rest of the community. The following are some guidelines to observe when creating issues or PRs: Be friendly; it is important that we can all enjoy a safe space as we are all working on the same project and it is okay for people to have different ideas Use code blocks ; it helps us help you when we can read your code! On that note also refrain from pasting more than 30 lines of code in a post, instead create a gist if you need to share large snippets Use reasonable titles; refrain from using overly long or capitalized titles as they are usually annoying and do little to encourage others to help :smile: Be detailed; refrain from mentioning code problems without sharing your source code and always give information regarding your board and version of the library","title":"Contribution Guidelines"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel/CONTRIBUTING/#contribution-guidelines","text":"This library is the culmination of the expertise of many members of the open source community who have dedicated their time and hard work. The best way to ask for help or propose a new idea is to create a new issue while creating a Pull Request with your code changes allows you to share your own innovations with the rest of the community. The following are some guidelines to observe when creating issues or PRs: Be friendly; it is important that we can all enjoy a safe space as we are all working on the same project and it is okay for people to have different ideas Use code blocks ; it helps us help you when we can read your code! On that note also refrain from pasting more than 30 lines of code in a post, instead create a gist if you need to share large snippets Use reasonable titles; refrain from using overly long or capitalized titles as they are usually annoying and do little to encourage others to help :smile: Be detailed; refrain from mentioning code problems without sharing your source code and always give information regarding your board and version of the library","title":"Contribution Guidelines"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel_ZeroDMA/","text":"Adafruit_NeoPixel_ZeroDMA DMA-based NeoPixel library for SAMD21 and SAMD51 microcontrollers (Feather M0, M4, etc.) Doesn't require stopping interrupts, so millis() and micros() don't lose time, soft PWM (for servos, etc.) still operate normally. Requires LATEST Adafruit_NeoPixel and Adafruit_ZeroDMA libraries also be installed (Adafruit SAMD board support automatically includes Adafruit_ZeroDMA). THIS ONLY WORKS ON CERTAIN PINS. THIS IS NORMAL. Library uses SERCOM peripherals for SPI output, and the hardware only supports this on specific pins (plus, some SERCOMs are in use for Serial, I2C, etc.). AS OF VERSION 1.2.0: the selection of pins is more restrictive than before, to better avoid collisions with other peripherals (I2C, etc.)...with few exceptions, you can use DMA NeoPixels with impunity. And more boards are supported now. COMPATIBLE BOARDS AND PINS: Feather M0: pins 5, 6, 12 and MOSI*. Feather M0 Express: pins 6, 12 and MOSI*. Feather M4: pins 12, A2, A4 and MOSI*. ItsyBitsy M0: pins 5, 12 and MOSI*. ItsyBitsy M4: pins 2, 5, 12 and MOSI*. Metro M0 or Arduino Zero: pins 5, 12 and MOSI*. Metro M4: pins 6, 11, A3 and MOSI*. Metro M4 AirLift: pins 6, 11 and MOSI*. Grand Central: pins 11, 14, 23 and MOSI*. HalloWing M0: pins 4 (NEOPIX), 6 and MOSI*. HalloWing M4: pins 6, 8, A5 and MOSI*. MONSTER M4SK: pin 2. PyPortal, PyPortal Titano: pin 3 (SENSE connector). PyGamer, PyGamer Advance: pins 12 and A4. PyBadge, PyBadge AirLift: pins A4, MOSI*. Crickit M0: pins 8, 11, A8 and A11. Trellis M4: pin 10 (keypad NeoPixels). Circuit Playground M0: pin A2. Trinket M0: pin 4 (can't use simultaneously with I2C, SPI or Serial1). Gemma M0: pin D0 (can't use simultaneously with I2C, SPI or Serial1). QT Py: MOSI* and pin 16 (underside pad, can't use w/optional SPI flash). Arduino NANO 33 IoT: pins 4, 6, 7, A2, A3, MOSI*. * If using the MOSI pin on these boards, the corresponding SPI peripheral is not usable. A few add-ons (usually TFT display shields/wings) rely on SPI, so avoid NeoPixeling from this pin in such situations. MOSI is really only offered anymore to maintain partial compatibility with older projects that might've used the earlier library, which was less selective about such things. OTHER THINGS TO KNOW: DMA NeoPixels use a LOT of RAM: 12 bytes/pixel for RGB, 16 bytes/pixel for RGBW, about 4X as much as regular NeoPixel library (plus a little bit extra for structures & stuff). 0/1 bit timing does not precisely match NeoPixel/WS2812/SK6812 datasheet specs, but it seems to work well enough. Use at your own peril. Have not tested this yet with multiple instances (DMA-driven NeoPixels on multiple pins), but in theory it should work. Should also be OK mixing DMA and non-DMA NeoPixels in same sketch (just use different constructor and pins for each). Currently this only supports strip declaration with length & pin known at compile time, so it's not a 100% drop-in replacement for all NeoPixel code right now. But probably 99%+ of all sketches are written that way, so it's perfectly usable for most. The stock NeoPixel library has the option of setting the length & pin number at run-time (so these can be stored in a config file or in EEPROM)...but those functions are now considered deprecated, so we should be OK going forward.","title":"Adafruit_NeoPixel_ZeroDMA"},{"location":"stretch-firmware/arduino/libraries/Adafruit_NeoPixel_ZeroDMA/#adafruit_neopixel_zerodma","text":"DMA-based NeoPixel library for SAMD21 and SAMD51 microcontrollers (Feather M0, M4, etc.) Doesn't require stopping interrupts, so millis() and micros() don't lose time, soft PWM (for servos, etc.) still operate normally. Requires LATEST Adafruit_NeoPixel and Adafruit_ZeroDMA libraries also be installed (Adafruit SAMD board support automatically includes Adafruit_ZeroDMA). THIS ONLY WORKS ON CERTAIN PINS. THIS IS NORMAL. Library uses SERCOM peripherals for SPI output, and the hardware only supports this on specific pins (plus, some SERCOMs are in use for Serial, I2C, etc.). AS OF VERSION 1.2.0: the selection of pins is more restrictive than before, to better avoid collisions with other peripherals (I2C, etc.)...with few exceptions, you can use DMA NeoPixels with impunity. And more boards are supported now. COMPATIBLE BOARDS AND PINS: Feather M0: pins 5, 6, 12 and MOSI*. Feather M0 Express: pins 6, 12 and MOSI*. Feather M4: pins 12, A2, A4 and MOSI*. ItsyBitsy M0: pins 5, 12 and MOSI*. ItsyBitsy M4: pins 2, 5, 12 and MOSI*. Metro M0 or Arduino Zero: pins 5, 12 and MOSI*. Metro M4: pins 6, 11, A3 and MOSI*. Metro M4 AirLift: pins 6, 11 and MOSI*. Grand Central: pins 11, 14, 23 and MOSI*. HalloWing M0: pins 4 (NEOPIX), 6 and MOSI*. HalloWing M4: pins 6, 8, A5 and MOSI*. MONSTER M4SK: pin 2. PyPortal, PyPortal Titano: pin 3 (SENSE connector). PyGamer, PyGamer Advance: pins 12 and A4. PyBadge, PyBadge AirLift: pins A4, MOSI*. Crickit M0: pins 8, 11, A8 and A11. Trellis M4: pin 10 (keypad NeoPixels). Circuit Playground M0: pin A2. Trinket M0: pin 4 (can't use simultaneously with I2C, SPI or Serial1). Gemma M0: pin D0 (can't use simultaneously with I2C, SPI or Serial1). QT Py: MOSI* and pin 16 (underside pad, can't use w/optional SPI flash). Arduino NANO 33 IoT: pins 4, 6, 7, A2, A3, MOSI*. * If using the MOSI pin on these boards, the corresponding SPI peripheral is not usable. A few add-ons (usually TFT display shields/wings) rely on SPI, so avoid NeoPixeling from this pin in such situations. MOSI is really only offered anymore to maintain partial compatibility with older projects that might've used the earlier library, which was less selective about such things. OTHER THINGS TO KNOW: DMA NeoPixels use a LOT of RAM: 12 bytes/pixel for RGB, 16 bytes/pixel for RGBW, about 4X as much as regular NeoPixel library (plus a little bit extra for structures & stuff). 0/1 bit timing does not precisely match NeoPixel/WS2812/SK6812 datasheet specs, but it seems to work well enough. Use at your own peril. Have not tested this yet with multiple instances (DMA-driven NeoPixels on multiple pins), but in theory it should work. Should also be OK mixing DMA and non-DMA NeoPixels in same sketch (just use different constructor and pins for each). Currently this only supports strip declaration with length & pin known at compile time, so it's not a 100% drop-in replacement for all NeoPixel code right now. But probably 99%+ of all sketches are written that way, so it's perfectly usable for most. The stock NeoPixel library has the option of setting the length & pin number at run-time (so these can be stored in a config file or in EEPROM)...but those functions are now considered deprecated, so we should be OK going forward.","title":"Adafruit_NeoPixel_ZeroDMA"},{"location":"stretch-firmware/arduino/libraries/Adafruit_Unified_Sensor/","text":"Adafruit Unified Sensor Driver Many small embedded systems exist to collect data from sensors, analyse the data, and either take an appropriate action or send that sensor data to another system for processing. One of the many challenges of embedded systems design is the fact that parts you used today may be out of production tomorrow, or system requirements may change and you may need to choose a different sensor down the road. Creating new drivers is a relatively easy task, but integrating them into existing systems is both error prone and time consuming since sensors rarely use the exact same units of measurement. By reducing all data to a single sensors_event_t 'type' and settling on specific, standardised SI units for each sensor family the same sensor types return values that are comparable with any other similar sensor. This enables you to switch sensor models with very little impact on the rest of the system, which can help mitigate some of the risks and problems of sensor availability and code reuse. The unified sensor abstraction layer is also useful for data-logging and data-transmission since you only have one well-known type to log or transmit over the air or wire. Unified Sensor Drivers The following drivers are based on the Adafruit Unified Sensor Driver: Accelerometers - Adafruit_ADXL345 - Adafruit_LSM303DLHC - Adafruit_MMA8451_Library Gyroscope - Adafruit_L3GD20_U Light - Adafruit_TSL2561 - Adafruit_TSL2591_Library Magnetometers - Adafruit_LSM303DLHC - Adafruit_HMC5883_Unified Barometric Pressure - Adafruit_BMP085_Unified - Adafruit_BMP183_Unified_Library Humidity & Temperature - Adafruit_DHT_Unified How Does it Work? Any driver that supports the Adafruit unified sensor abstraction layer will implement the Adafruit_Sensor base class. There are two main typedefs and one enum defined in Adafruit_Sensor.h that are used to 'abstract' away the sensor details and values: Sensor Types (sensors_type_t) These pre-defined sensor types are used to properly handle the two related typedefs below, and allows us determine what types of units the sensor uses, etc. /** Sensor types */ typedef enum { SENSOR_TYPE_ACCELEROMETER = (1), SENSOR_TYPE_MAGNETIC_FIELD = (2), SENSOR_TYPE_ORIENTATION = (3), SENSOR_TYPE_GYROSCOPE = (4), SENSOR_TYPE_LIGHT = (5), SENSOR_TYPE_PRESSURE = (6), SENSOR_TYPE_PROXIMITY = (8), SENSOR_TYPE_GRAVITY = (9), SENSOR_TYPE_LINEAR_ACCELERATION = (10), SENSOR_TYPE_ROTATION_VECTOR = (11), SENSOR_TYPE_RELATIVE_HUMIDITY = (12), SENSOR_TYPE_AMBIENT_TEMPERATURE = (13), SENSOR_TYPE_VOLTAGE = (15), SENSOR_TYPE_CURRENT = (16), SENSOR_TYPE_COLOR = (17) } sensors_type_t; Sensor Details (sensor_t) This typedef describes the specific capabilities of this sensor, and allows us to know what sensor we are using beneath the abstraction layer. /* Sensor details (40 bytes) */ /** struct sensor_s is used to describe basic information about a specific sensor. */ typedef struct { char name[12]; int32_t version; int32_t sensor_id; int32_t type; float max_value; float min_value; float resolution; int32_t min_delay; } sensor_t; The individual fields are intended to be used as follows: name : The sensor name or ID, up to a maximum of twelve characters (ex. \"MPL115A2\") version : The version of the sensor HW and the driver to allow us to differentiate versions of the board or driver sensor_id : A unique sensor identifier that is used to differentiate this specific sensor instance from any others that are present on the system or in the sensor network type : The sensor type, based on sensors_type_t in sensors.h max_value : The maximum value that this sensor can return (in the appropriate SI unit) min_value : The minimum value that this sensor can return (in the appropriate SI unit) resolution : The smallest difference between two values that this sensor can report (in the appropriate SI unit) min_delay : The minimum delay in microseconds between two sensor events, or '0' if there is no constant sensor rate Sensor Data/Events (sensors_event_t) This typedef is used to return sensor data from any sensor supported by the abstraction layer, using standard SI units and scales. /* Sensor event (36 bytes) */ /** struct sensor_event_s is used to provide a single sensor event in a common format. */ typedef struct { int32_t version; int32_t sensor_id; int32_t type; int32_t reserved0; int32_t timestamp; union { float data[4]; sensors_vec_t acceleration; sensors_vec_t magnetic; sensors_vec_t orientation; sensors_vec_t gyro; float temperature; float distance; float light; float pressure; float relative_humidity; float current; float voltage; sensors_color_t color; }; } sensors_event_t; It includes the following fields: version : Contain 'sizeof(sensors_event_t)' to identify which version of the API we're using in case this changes in the future sensor_id : A unique sensor identifier that is used to differentiate this specific sensor instance from any others that are present on the system or in the sensor network (must match the sensor_id value in the corresponding sensor_t enum above!) type : the sensor type, based on sensors_type_t in sensors.h timestamp : time in milliseconds when the sensor value was read data[4] : An array of four 32-bit values that allows us to encapsulate any type of sensor data via a simple union (further described below) Required Functions In addition to the two standard types and the sensor type enum, all drivers based on Adafruit_Sensor must also implement the following two functions: bool getEvent(sensors_event_t*); Calling this function will populate the supplied sensors_event_t reference with the latest available sensor data. You should call this function as often as you want to update your data. void getSensor(sensor_t*); Calling this function will provide some basic information about the sensor (the sensor name, driver version, min and max values, etc. Standardised SI values for sensors_event_t A key part of the abstraction layer is the standardisation of values on SI units of a particular scale, which is accomplished via the data[4] union in sensors_event_t above. This 16 byte union includes fields for each main sensor type, and uses the following SI units and scales: acceleration : values are in meter per second per second (m/s^2) magnetic : values are in micro-Tesla (uT) orientation : values are in degrees gyro : values are in rad/s temperature : values in degrees centigrade (Celsius) distance : values are in centimeters light : values are in SI lux units pressure : values are in hectopascal (hPa) relative_humidity : values are in percent current : values are in milliamps (mA) voltage : values are in volts (V) color : values are in 0..1.0 RGB channel luminosity and 32-bit RGBA format The Unified Driver Abstraction Layer in Practice Using the unified sensor abstraction layer is relatively easy once a compliant driver has been created. Every compliant sensor can now be read using a single, well-known 'type' (sensors_event_t), and there is a standardised way of interrogating a sensor about its specific capabilities (via sensor_t). An example of reading the TSL2561 light sensor can be seen below: Adafruit_TSL2561 tsl = Adafruit_TSL2561(TSL2561_ADDR_FLOAT, 12345); ... /* Get a new sensor event */ sensors_event_t event; tsl.getEvent(&event); /* Display the results (light is measured in lux) */ if (event.light) { Serial.print(event.light); Serial.println(\" lux\"); } else { /* If event.light = 0 lux the sensor is probably saturated and no reliable data could be generated! */ Serial.println(\"Sensor overload\"); } Similarly, we can get the basic technical capabilities of this sensor with the following code: sensor_t sensor; sensor_t sensor; tsl.getSensor(&sensor); /* Display the sensor details */ Serial.println(\"------------------------------------\"); Serial.print (\"Sensor: \"); Serial.println(sensor.name); Serial.print (\"Driver Ver: \"); Serial.println(sensor.version); Serial.print (\"Unique ID: \"); Serial.println(sensor.sensor_id); Serial.print (\"Max Value: \"); Serial.print(sensor.max_value); Serial.println(\" lux\"); Serial.print (\"Min Value: \"); Serial.print(sensor.min_value); Serial.println(\" lux\"); Serial.print (\"Resolution: \"); Serial.print(sensor.resolution); Serial.println(\" lux\"); Serial.println(\"------------------------------------\"); Serial.println(\"\");","title":"Adafruit Unified Sensor Driver #"},{"location":"stretch-firmware/arduino/libraries/Adafruit_Unified_Sensor/#adafruit-unified-sensor-driver","text":"Many small embedded systems exist to collect data from sensors, analyse the data, and either take an appropriate action or send that sensor data to another system for processing. One of the many challenges of embedded systems design is the fact that parts you used today may be out of production tomorrow, or system requirements may change and you may need to choose a different sensor down the road. Creating new drivers is a relatively easy task, but integrating them into existing systems is both error prone and time consuming since sensors rarely use the exact same units of measurement. By reducing all data to a single sensors_event_t 'type' and settling on specific, standardised SI units for each sensor family the same sensor types return values that are comparable with any other similar sensor. This enables you to switch sensor models with very little impact on the rest of the system, which can help mitigate some of the risks and problems of sensor availability and code reuse. The unified sensor abstraction layer is also useful for data-logging and data-transmission since you only have one well-known type to log or transmit over the air or wire.","title":"Adafruit Unified Sensor Driver"},{"location":"stretch-firmware/arduino/libraries/Adafruit_Unified_Sensor/#unified-sensor-drivers","text":"The following drivers are based on the Adafruit Unified Sensor Driver: Accelerometers - Adafruit_ADXL345 - Adafruit_LSM303DLHC - Adafruit_MMA8451_Library Gyroscope - Adafruit_L3GD20_U Light - Adafruit_TSL2561 - Adafruit_TSL2591_Library Magnetometers - Adafruit_LSM303DLHC - Adafruit_HMC5883_Unified Barometric Pressure - Adafruit_BMP085_Unified - Adafruit_BMP183_Unified_Library Humidity & Temperature - Adafruit_DHT_Unified","title":"Unified Sensor Drivers"},{"location":"stretch-firmware/arduino/libraries/Adafruit_Unified_Sensor/#how-does-it-work","text":"Any driver that supports the Adafruit unified sensor abstraction layer will implement the Adafruit_Sensor base class. There are two main typedefs and one enum defined in Adafruit_Sensor.h that are used to 'abstract' away the sensor details and values: Sensor Types (sensors_type_t) These pre-defined sensor types are used to properly handle the two related typedefs below, and allows us determine what types of units the sensor uses, etc. /** Sensor types */ typedef enum { SENSOR_TYPE_ACCELEROMETER = (1), SENSOR_TYPE_MAGNETIC_FIELD = (2), SENSOR_TYPE_ORIENTATION = (3), SENSOR_TYPE_GYROSCOPE = (4), SENSOR_TYPE_LIGHT = (5), SENSOR_TYPE_PRESSURE = (6), SENSOR_TYPE_PROXIMITY = (8), SENSOR_TYPE_GRAVITY = (9), SENSOR_TYPE_LINEAR_ACCELERATION = (10), SENSOR_TYPE_ROTATION_VECTOR = (11), SENSOR_TYPE_RELATIVE_HUMIDITY = (12), SENSOR_TYPE_AMBIENT_TEMPERATURE = (13), SENSOR_TYPE_VOLTAGE = (15), SENSOR_TYPE_CURRENT = (16), SENSOR_TYPE_COLOR = (17) } sensors_type_t; Sensor Details (sensor_t) This typedef describes the specific capabilities of this sensor, and allows us to know what sensor we are using beneath the abstraction layer. /* Sensor details (40 bytes) */ /** struct sensor_s is used to describe basic information about a specific sensor. */ typedef struct { char name[12]; int32_t version; int32_t sensor_id; int32_t type; float max_value; float min_value; float resolution; int32_t min_delay; } sensor_t; The individual fields are intended to be used as follows: name : The sensor name or ID, up to a maximum of twelve characters (ex. \"MPL115A2\") version : The version of the sensor HW and the driver to allow us to differentiate versions of the board or driver sensor_id : A unique sensor identifier that is used to differentiate this specific sensor instance from any others that are present on the system or in the sensor network type : The sensor type, based on sensors_type_t in sensors.h max_value : The maximum value that this sensor can return (in the appropriate SI unit) min_value : The minimum value that this sensor can return (in the appropriate SI unit) resolution : The smallest difference between two values that this sensor can report (in the appropriate SI unit) min_delay : The minimum delay in microseconds between two sensor events, or '0' if there is no constant sensor rate Sensor Data/Events (sensors_event_t) This typedef is used to return sensor data from any sensor supported by the abstraction layer, using standard SI units and scales. /* Sensor event (36 bytes) */ /** struct sensor_event_s is used to provide a single sensor event in a common format. */ typedef struct { int32_t version; int32_t sensor_id; int32_t type; int32_t reserved0; int32_t timestamp; union { float data[4]; sensors_vec_t acceleration; sensors_vec_t magnetic; sensors_vec_t orientation; sensors_vec_t gyro; float temperature; float distance; float light; float pressure; float relative_humidity; float current; float voltage; sensors_color_t color; }; } sensors_event_t; It includes the following fields: version : Contain 'sizeof(sensors_event_t)' to identify which version of the API we're using in case this changes in the future sensor_id : A unique sensor identifier that is used to differentiate this specific sensor instance from any others that are present on the system or in the sensor network (must match the sensor_id value in the corresponding sensor_t enum above!) type : the sensor type, based on sensors_type_t in sensors.h timestamp : time in milliseconds when the sensor value was read data[4] : An array of four 32-bit values that allows us to encapsulate any type of sensor data via a simple union (further described below) Required Functions In addition to the two standard types and the sensor type enum, all drivers based on Adafruit_Sensor must also implement the following two functions: bool getEvent(sensors_event_t*); Calling this function will populate the supplied sensors_event_t reference with the latest available sensor data. You should call this function as often as you want to update your data. void getSensor(sensor_t*); Calling this function will provide some basic information about the sensor (the sensor name, driver version, min and max values, etc. Standardised SI values for sensors_event_t A key part of the abstraction layer is the standardisation of values on SI units of a particular scale, which is accomplished via the data[4] union in sensors_event_t above. This 16 byte union includes fields for each main sensor type, and uses the following SI units and scales: acceleration : values are in meter per second per second (m/s^2) magnetic : values are in micro-Tesla (uT) orientation : values are in degrees gyro : values are in rad/s temperature : values in degrees centigrade (Celsius) distance : values are in centimeters light : values are in SI lux units pressure : values are in hectopascal (hPa) relative_humidity : values are in percent current : values are in milliamps (mA) voltage : values are in volts (V) color : values are in 0..1.0 RGB channel luminosity and 32-bit RGBA format","title":"How Does it Work?"},{"location":"stretch-firmware/arduino/libraries/Adafruit_Unified_Sensor/#the-unified-driver-abstraction-layer-in-practice","text":"Using the unified sensor abstraction layer is relatively easy once a compliant driver has been created. Every compliant sensor can now be read using a single, well-known 'type' (sensors_event_t), and there is a standardised way of interrogating a sensor about its specific capabilities (via sensor_t). An example of reading the TSL2561 light sensor can be seen below: Adafruit_TSL2561 tsl = Adafruit_TSL2561(TSL2561_ADDR_FLOAT, 12345); ... /* Get a new sensor event */ sensors_event_t event; tsl.getEvent(&event); /* Display the results (light is measured in lux) */ if (event.light) { Serial.print(event.light); Serial.println(\" lux\"); } else { /* If event.light = 0 lux the sensor is probably saturated and no reliable data could be generated! */ Serial.println(\"Sensor overload\"); } Similarly, we can get the basic technical capabilities of this sensor with the following code: sensor_t sensor; sensor_t sensor; tsl.getSensor(&sensor); /* Display the sensor details */ Serial.println(\"------------------------------------\"); Serial.print (\"Sensor: \"); Serial.println(sensor.name); Serial.print (\"Driver Ver: \"); Serial.println(sensor.version); Serial.print (\"Unique ID: \"); Serial.println(sensor.sensor_id); Serial.print (\"Max Value: \"); Serial.print(sensor.max_value); Serial.println(\" lux\"); Serial.print (\"Min Value: \"); Serial.print(sensor.min_value); Serial.println(\" lux\"); Serial.print (\"Resolution: \"); Serial.print(sensor.resolution); Serial.println(\" lux\"); Serial.println(\"------------------------------------\"); Serial.println(\"\");","title":"The Unified Driver Abstraction Layer in Practice"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ZeroDMA/","text":"Adafruit_ZeroDMA library DMA helper/wrapped for ATSAMD21 such as Arduino Zero & Feather M0 Current version of this library no longer requires Adafruit_ASFcore as a prerequisite. However...IT BREAKS COMPATIBILITY WITH PRIOR VERSIONS. Function names, calling sequence and return types/values have changed. See examples! Item(s) in 'utility' directory are much pared-down derivatives of Atmel ASFcore 3 files. Please keep their original copyright and license intact when editing.","title":"Adafruit_ZeroDMA library [![Build Status](https://github.com/adafruit/Adafruit_ZeroDMA/workflows/Arduino%20Library%20CI/badge.svg)](https://github.com/adafruit/Adafruit_ZeroDMA/actions)"},{"location":"stretch-firmware/arduino/libraries/Adafruit_ZeroDMA/#adafruit_zerodma-library","text":"DMA helper/wrapped for ATSAMD21 such as Arduino Zero & Feather M0 Current version of this library no longer requires Adafruit_ASFcore as a prerequisite. However...IT BREAKS COMPATIBILITY WITH PRIOR VERSIONS. Function names, calling sequence and return types/values have changed. See examples! Item(s) in 'utility' directory are much pared-down derivatives of Atmel ASFcore 3 files. Please keep their original copyright and license intact when editing.","title":"Adafruit_ZeroDMA library"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/","text":"FlashStorage library for Arduino The FlashStorage library aims to provide a convenient way to store and retrieve user's data using the non-volatile flash memory of microcontrollers. The flash memory, due to his properties, is generally used to store the firmware code, but it can also be used to store user data. Supported hardware Currently, only ATSAMD21 cpu is supported (and consequently every board based on this cpu like the Arduino Zero or Aduino MKR1000). Limited number of writes The flash memory has a limited amount of write cycles. Typical flash memories can perform about 10000 writes cycles to the same flash block before starting to \"wear out\" and begin to lose the ability to retain data. So BEWARE: IMPROPER USE OF THIS LIBRARY CAN QUICKLY AND PERMANENTLY DESTROY THE FLASH MEMORY OF YOUR MICRO , in particular you should avoid to call the write() function too often and make sure that in the entire life of the micro the number of calls to write stay well below the above limit of 10000 (it's a good rule-of-thumb to keep that number in mind even if the manufacturer of the micro guarantees a bigger number of cycles). The same caution must be taken if you're using the EEPROM API emulation (see below) with the EEPROM.commit() function. Usage First of all you must declare a global FlashStorage object for each piece of data you intend to store in the flash memory. For example if you want to store the age of a person you must declare an age_storage like this: FlashStorage ( age_storage , int ); this instruction means \"create a FlashStorage to store an int variable and call it age_storage \". Now you can use age_storage as a place to safely store an integer: void readAndStoreUserAge () { Serial . println ( \"Please enter your age:\" ); String age = Serial . readStringUntil ( '\\n' ); age_storage . write ( age . toInt ()); // <-- save the age } after a reset of the microcontroller to retrieve the stored age you can use: int user_age = age_storage . read (); Using the alternative EEPROM-like API If you include FlashAsEEPROM.h you'll get an EEPROM emulation with the internal flash memory. See EmulateEEPROM sketch for an example. The API is very similar to the well known Arduino EEPROM.h API but with two additional functions: EEPROM.isValid() returns true if data in the EEPROM is valid or, in other words, if the data has been written at least once, otherwise EEPROM data is \"undefined\" and the function returns false . EEPROM.commit() store the EEPROM data in flash. Use this with care: Every call writes the complete EEPROM data to flash. This will reduce the remainig flash-write-cycles. Don't call this method in a loop or you will kill your flash soon . License This library is released under LGPL-2.1. FAQ Can I use a single FlashStorage object to store more stuff? Yes, you can declare a struct with more fields and create a FlashStorage object to store the entire structure. See the StoreNameAndSurname sketch for an example on how to do it. The content of the FlashStorage is erased each time a new sketch is uploaded? Yes, every time you upload a new sketch, the previous content of the FlashStorage is erased. Do you recommend to use FLASH instead of EEPROM? No. If your micro provides an EEPROM it's almost always better to use that because it's a kind of memory designed with the specific purpose to store user data (it has a longer lifetime, number of write cycles, etc...). In the absence of an EEPROM you can use this library to use a piece of the flash memory as an alternative to EEPROM but you must always keep in mind his limits.","title":"FlashStorage library for Arduino"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#flashstorage-library-for-arduino","text":"The FlashStorage library aims to provide a convenient way to store and retrieve user's data using the non-volatile flash memory of microcontrollers. The flash memory, due to his properties, is generally used to store the firmware code, but it can also be used to store user data.","title":"FlashStorage library for Arduino"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#supported-hardware","text":"Currently, only ATSAMD21 cpu is supported (and consequently every board based on this cpu like the Arduino Zero or Aduino MKR1000).","title":"Supported hardware"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#limited-number-of-writes","text":"The flash memory has a limited amount of write cycles. Typical flash memories can perform about 10000 writes cycles to the same flash block before starting to \"wear out\" and begin to lose the ability to retain data. So BEWARE: IMPROPER USE OF THIS LIBRARY CAN QUICKLY AND PERMANENTLY DESTROY THE FLASH MEMORY OF YOUR MICRO , in particular you should avoid to call the write() function too often and make sure that in the entire life of the micro the number of calls to write stay well below the above limit of 10000 (it's a good rule-of-thumb to keep that number in mind even if the manufacturer of the micro guarantees a bigger number of cycles). The same caution must be taken if you're using the EEPROM API emulation (see below) with the EEPROM.commit() function.","title":"Limited number of writes"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#usage","text":"First of all you must declare a global FlashStorage object for each piece of data you intend to store in the flash memory. For example if you want to store the age of a person you must declare an age_storage like this: FlashStorage ( age_storage , int ); this instruction means \"create a FlashStorage to store an int variable and call it age_storage \". Now you can use age_storage as a place to safely store an integer: void readAndStoreUserAge () { Serial . println ( \"Please enter your age:\" ); String age = Serial . readStringUntil ( '\\n' ); age_storage . write ( age . toInt ()); // <-- save the age } after a reset of the microcontroller to retrieve the stored age you can use: int user_age = age_storage . read ();","title":"Usage"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#using-the-alternative-eeprom-like-api","text":"If you include FlashAsEEPROM.h you'll get an EEPROM emulation with the internal flash memory. See EmulateEEPROM sketch for an example. The API is very similar to the well known Arduino EEPROM.h API but with two additional functions: EEPROM.isValid() returns true if data in the EEPROM is valid or, in other words, if the data has been written at least once, otherwise EEPROM data is \"undefined\" and the function returns false . EEPROM.commit() store the EEPROM data in flash. Use this with care: Every call writes the complete EEPROM data to flash. This will reduce the remainig flash-write-cycles. Don't call this method in a loop or you will kill your flash soon .","title":"Using the alternative EEPROM-like API"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#license","text":"This library is released under LGPL-2.1.","title":"License"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#faq","text":"","title":"FAQ"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#can-i-use-a-single-flashstorage-object-to-store-more-stuff","text":"Yes, you can declare a struct with more fields and create a FlashStorage object to store the entire structure. See the StoreNameAndSurname sketch for an example on how to do it.","title":"Can I use a single FlashStorage object to store more stuff?"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#the-content-of-the-flashstorage-is-erased-each-time-a-new-sketch-is-uploaded","text":"Yes, every time you upload a new sketch, the previous content of the FlashStorage is erased.","title":"The content of the FlashStorage is erased each time a new sketch is uploaded?"},{"location":"stretch-firmware/arduino/libraries/FlashStorage/readme/#do-you-recommend-to-use-flash-instead-of-eeprom","text":"No. If your micro provides an EEPROM it's almost always better to use that because it's a kind of memory designed with the specific purpose to store user data (it has a longer lifetime, number of write cycles, etc...). In the absence of an EEPROM you can use this library to use a piece of the flash memory as an alternative to EEPROM but you must always keep in mind his limits.","title":"Do you recommend to use FLASH instead of EEPROM?"},{"location":"stretch-firmware/docs/tutorial_data_transfer/","text":"NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board. NOTE : These tutorials may require the latest version of Stretch Body. If necessary, please update your install. Integrating Custom Data In this tutorial we explore how to plumb custom data to and from the Arduino based Stretch Wacc (Wrist + Accelerometer) board. This enables users to integrate custom sensors and actuators on to the Wrist Expansion header . How Data Transfer Happens in Stretch Body Data is transferred between Stretch Body Python and the Arduino based firmware using USB based serial. As an example, consider the transfer of a Status data message from the Wacc to Stretch Body. This involves: Stretch Body requests new Status data via wacc.pull_status() This generates an RPC call to the Wacc board The status data on the Wacc board gets 'packed' and transmitted back to Stretch Body Stretch Body unpacks the data into the status dictionary of the Wacc Python class . Similarly, data can go the other direction (e.g., Command messages ). Fortunately, the data transfer is managed automatically for the developer. In order to integrate your custom data you will: Extend the firmware Status and Command structs to include your data Derive your own class from the Stretch Body Wacc class that packs and unpacks your custom data Calculator Example As a simple example we will extend the Wacc to be an embedded calculator. The implementation is found in the Wacc_Calc Arduino sketch and the corresponding WaccCalc Python class . Arduino Define data structures First, we define the data to be sent back and forth in Common.h of the firmware. struct __attribute__ (( packed )) Calc_Command { float var1 ; float var2 ; uint8_t op ; }; struct __attribute__ (( packed )) Calc_Status { float result ; }; Our calculator will perform the computation: result=op(var1,var2) . Now add these structs to the Status and Command structs in Common.h : struct __attribute__ (( packed )) Wacc_Command { Calc_Command calc ; ... }; struct __attribute__ (( packed )) Wacc_Status { Calc_Status calc ; ... }; The ordering of the data is important. Your custom data should be at the start of the struct as the Python class will unpack this data first. Define calculator function Next we add the calculator function to Wacc.cpp : float my_calc ( uint8_t op , float var1 , float var2 ) { if ( op == 0 ) return var1 + var2 ; if ( op == 1 ) return var1 * var2 ; if ( op == 2 ) return var1 / var2 ; return 0 ; } Integrate calculator into the control loop Finally, we integrate our calculator into the embedded control loop in Wacc.cpp . The function stepWaccController() in Wacc.cpp is called by Timer5 at 700Hz. The calculator is fairly lightweight so its computation time should not interfere with the existing Wacc timing. Heavier computation would require careful integration with an eye to loop timing. Add the call to stepWaccController() -- just prior to the Status data being copied out for transmittal back. void stepWaccController () { ... stat . calc . result = my_calc ( cmd . calc . op , cmd . calc . var1 , cmd . calc . var2 ); memcpy (( uint8_t * ) ( & stat_out ),( uint8_t * ) ( & stat ), sizeof ( Wacc_Status )); } The variable cmd , which contains the command to the calculator, is automatically updated with fresh data via the RPC mechanism. Bump protocol versions The packet definition of data exchanged between Python and the Arduino is tagged with a protocol version. This allows the Python Device to ensure it is exchanging compatible data. The mainline release of Stretch Firmware starts with protocol version 0 and increments with each new protocol release. To avoid conflicts, for this tutorial we pick an arbitrary large number (99). In Common.h , we bump from Protocol '0' #define FIRMWARE_VERSION \"Wacc.v0.0.1p0\" to Protocol '99' #define FIRMWARE_VERSION \"Wacc.v0.0.1p99\" Python Now we will implement WaccCalc which derives from the Wacc Python class . from stretch_body.wacc import * from stretch_body.transport import * class WaccCalc ( Wacc ): \"\"\" This class demonstrates how to extend the Wacc class with custom data See the corresponding tutorial for more information. \"\"\" def __init__ ( self , verbose = False ): Wacc . __init__ ( self , verbose = verbose , ext_status_cb = self . ext_unpack_status , #Set callback to unpack status ext_command_cb = self . ext_pack_command ) #Set callback to pack command self . _command [ 'calc' ] = { 'op' : 0 , 'var1' : 0 , 'var2' : 0 } #Extend command dictionary with custom fields self . status [ 'calc' ] = 0.0 #Extend status dictionary with custom fields self . valid_firmware_protocol = 'pMyCalc' def calculate ( self , op , var1 , var2 ): \"\"\" 0: addition 1: multiplication 2: division \"\"\" self . _command [ 'calc' ][ 'op' ] = int ( op ) self . _command [ 'calc' ][ 'var1' ] = float ( var1 ) self . _command [ 'calc' ][ 'var2' ] = float ( var2 ) self . _dirty_command = True def pretty_print ( self ): Wacc . pretty_print ( self ) print 'Calc:' , self . status [ 'calc' ] def ext_unpack_status ( self , s ): \"\"\" s: byte array to unpack return: number of bytes unpacked \"\"\" sidx = 0 self . status [ 'calc' ] = unpack_float_t ( s [ sidx :]) return 4 def ext_pack_command ( self , s , sidx ): \"\"\" s: byte array to pack in to sidx: index to start packing at return: new sidx \"\"\" pack_float_t ( s , sidx , self . _command [ 'calc' ][ 'var1' ]) sidx += 4 pack_float_t ( s , sidx , self . _command [ 'calc' ][ 'var2' ]) sidx += 4 pack_uint8_t ( s , sidx , self . _command [ 'calc' ][ 'op' ]) sidx += 1 return sidx The class registers two callbacks for packing Command data and unpacking Status data. They are fairly self-explanatory and can be easily extended to match your custom data. Looking at the unpacking code: def ext_unpack_status ( self , s ): \"\"\" s: byte array to unpack return: number of bytes unpacked \"\"\" sidx = 0 self . status [ 'calc' ] = unpack_float_t ( s [ sidx :]) return 4 , we see the use of unpack_float_t . This, and other functions to unpack data, are found in stretch_body.transport . It is important that the data types and order match exactly those declared in the firmware. For example in Common.h we have: struct __attribute__ (( packed )) Calc_Command { float var1 ; float var2 ; uint8_t op ; }; , and on the Python side we have pack_float_t ( s , sidx , self . _command [ 'calc' ][ 'var1' ]) sidx += 4 pack_float_t ( s , sidx , self . _command [ 'calc' ][ 'var2' ]) sidx += 4 pack_uint8_t ( s , sidx , self . _command [ 'calc' ][ 'op' ]) sidx += 1 Test the Calculator - iPython We're ready to try out our calculator. First, Install and setup the Arduino IDE if it isn't already as described in the Updating Firmware tutorial . Open the Wacc_Calc Arduino sketch in the Arduino IDE. Select the hello_wacc board, the ttyACMx port that maps to the Wacc board. Then burn the firmware as described in the the Updating Firmware tutorial . Now, lets try it out: >>$ cd ~/repos/stretch_firmware/tutorial/python/ >>$ ipython And from iPython In [ 1 ]: import wacc_calc In [ 2 ]: w = wacc_calc . WaccCalc () In [ 3 ]: w . startup () In [ 4 ]: w . calculate ( op = 0 , var1 = 100.0 , var2 = 200.0 ) In [ 5 ]: w . push_command () In [ 6 ]: w . pull_status () In [ 7 ]: print 'Result is' , w . status [ 'calc' ] Result is 300.0 In [ 8 ]: w . calculate ( op = 1 , var1 = 100.0 , var2 = 200.0 ) In [ 9 ]: w . push_command () In [ 10 ]: w . pull_status () In [ 11 ]: print 'Result is' , w . status [ 'calc' ] Result is 20000.0 In [ 12 ]: w . calculate ( op = 2 , var1 = 100.0 , var2 = 200.0 ) In [ 13 ]: w . push_command () In [ 14 ]: w . pull_status () In [ 15 ]: print 'Result is' , w . status [ 'calc' ] Result is 0.5 Test the Calculator - Script Alternatively you can use the provided tool, stretch_wacc_calc_jog.py . Here you can use the calculator through the menu. hello-robot@stretch-re1-100x:~$ cd repos/stretch_firmware/tutorial/python/ hello-robot@stretch-re1-100x:~/repos/stretch_firmware/tutorial/python$ ./stretch_wacc_calc_jog.py ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- X ---Calculate Op ( Var1,Var2 ) --- Op = 0 : Add Op = 1 : Mult Op = 2 : Div Enter Op 1 Enter Var1 12 Enter Var2 13 Adding YAML Parameters The WaccCalc class can use the Stretch Body YAML files as well. For example, add to your stretch_re1_user_params.yaml : wacc : calc_scalar : 2.0 Now let's scale the commands to the Arduino according to this YAML parameter. In WaccCalc : def calculate ( self , op , var1 , var2 ): \"\"\" 0: addition 1: multiplication 2: division \"\"\" self . _command [ 'calc' ][ 'op' ] = int ( op ) self . _command [ 'calc' ][ 'var1' ] = self . params [ 'calc_scalar' ] * float ( var1 ) self . _command [ 'calc' ][ 'var2' ] = self . params [ 'calc_scalar' ] * float ( var2 ) self . _dirty_command = True Run the iPython as above and you'll see the values multiplied by the YAML scalar. Final Steps We want the Stretch Body Robot to use WaccCalc and not Wacc . To do this, add the following to your stretch_re1_user_params.yaml : robot: custom_wacc: py_class_name: WaccCalc py_module_name: wacc_calc This tells Robot which module and class to create its Wacc instance from. Now, test that it works from iPython In [ 1 ]: import stretch_body.robot In [ 2 ]: robot = stretch_body . robot . Robot () Starting TransportConnection on : / dev / hello - wacc In [ 3 ]: robot . startup () In [ 4 ]: robot . wacc . calculate ( op = 0 , var1 = 100.0 , var2 = 200.0 ) In [ 5 ]: robot . push_command () In [ 6 ]: print 'Result' , robot . wacc . status [ 'calc' ] Result 300.0 In [ 7 ]: robot . stop () All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Data Transfer"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#integrating-custom-data","text":"In this tutorial we explore how to plumb custom data to and from the Arduino based Stretch Wacc (Wrist + Accelerometer) board. This enables users to integrate custom sensors and actuators on to the Wrist Expansion header .","title":"Integrating Custom Data"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#how-data-transfer-happens-in-stretch-body","text":"Data is transferred between Stretch Body Python and the Arduino based firmware using USB based serial. As an example, consider the transfer of a Status data message from the Wacc to Stretch Body. This involves: Stretch Body requests new Status data via wacc.pull_status() This generates an RPC call to the Wacc board The status data on the Wacc board gets 'packed' and transmitted back to Stretch Body Stretch Body unpacks the data into the status dictionary of the Wacc Python class . Similarly, data can go the other direction (e.g., Command messages ). Fortunately, the data transfer is managed automatically for the developer. In order to integrate your custom data you will: Extend the firmware Status and Command structs to include your data Derive your own class from the Stretch Body Wacc class that packs and unpacks your custom data","title":"How Data Transfer Happens in Stretch Body"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#calculator-example","text":"As a simple example we will extend the Wacc to be an embedded calculator. The implementation is found in the Wacc_Calc Arduino sketch and the corresponding WaccCalc Python class .","title":"Calculator Example"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#arduino","text":"","title":"Arduino"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#define-data-structures","text":"First, we define the data to be sent back and forth in Common.h of the firmware. struct __attribute__ (( packed )) Calc_Command { float var1 ; float var2 ; uint8_t op ; }; struct __attribute__ (( packed )) Calc_Status { float result ; }; Our calculator will perform the computation: result=op(var1,var2) . Now add these structs to the Status and Command structs in Common.h : struct __attribute__ (( packed )) Wacc_Command { Calc_Command calc ; ... }; struct __attribute__ (( packed )) Wacc_Status { Calc_Status calc ; ... }; The ordering of the data is important. Your custom data should be at the start of the struct as the Python class will unpack this data first.","title":"Define data structures"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#define-calculator-function","text":"Next we add the calculator function to Wacc.cpp : float my_calc ( uint8_t op , float var1 , float var2 ) { if ( op == 0 ) return var1 + var2 ; if ( op == 1 ) return var1 * var2 ; if ( op == 2 ) return var1 / var2 ; return 0 ; }","title":"Define calculator function"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#integrate-calculator-into-the-control-loop","text":"Finally, we integrate our calculator into the embedded control loop in Wacc.cpp . The function stepWaccController() in Wacc.cpp is called by Timer5 at 700Hz. The calculator is fairly lightweight so its computation time should not interfere with the existing Wacc timing. Heavier computation would require careful integration with an eye to loop timing. Add the call to stepWaccController() -- just prior to the Status data being copied out for transmittal back. void stepWaccController () { ... stat . calc . result = my_calc ( cmd . calc . op , cmd . calc . var1 , cmd . calc . var2 ); memcpy (( uint8_t * ) ( & stat_out ),( uint8_t * ) ( & stat ), sizeof ( Wacc_Status )); } The variable cmd , which contains the command to the calculator, is automatically updated with fresh data via the RPC mechanism.","title":"Integrate calculator into the control loop"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#bump-protocol-versions","text":"The packet definition of data exchanged between Python and the Arduino is tagged with a protocol version. This allows the Python Device to ensure it is exchanging compatible data. The mainline release of Stretch Firmware starts with protocol version 0 and increments with each new protocol release. To avoid conflicts, for this tutorial we pick an arbitrary large number (99). In Common.h , we bump from Protocol '0' #define FIRMWARE_VERSION \"Wacc.v0.0.1p0\" to Protocol '99' #define FIRMWARE_VERSION \"Wacc.v0.0.1p99\"","title":"Bump protocol versions"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#python","text":"Now we will implement WaccCalc which derives from the Wacc Python class . from stretch_body.wacc import * from stretch_body.transport import * class WaccCalc ( Wacc ): \"\"\" This class demonstrates how to extend the Wacc class with custom data See the corresponding tutorial for more information. \"\"\" def __init__ ( self , verbose = False ): Wacc . __init__ ( self , verbose = verbose , ext_status_cb = self . ext_unpack_status , #Set callback to unpack status ext_command_cb = self . ext_pack_command ) #Set callback to pack command self . _command [ 'calc' ] = { 'op' : 0 , 'var1' : 0 , 'var2' : 0 } #Extend command dictionary with custom fields self . status [ 'calc' ] = 0.0 #Extend status dictionary with custom fields self . valid_firmware_protocol = 'pMyCalc' def calculate ( self , op , var1 , var2 ): \"\"\" 0: addition 1: multiplication 2: division \"\"\" self . _command [ 'calc' ][ 'op' ] = int ( op ) self . _command [ 'calc' ][ 'var1' ] = float ( var1 ) self . _command [ 'calc' ][ 'var2' ] = float ( var2 ) self . _dirty_command = True def pretty_print ( self ): Wacc . pretty_print ( self ) print 'Calc:' , self . status [ 'calc' ] def ext_unpack_status ( self , s ): \"\"\" s: byte array to unpack return: number of bytes unpacked \"\"\" sidx = 0 self . status [ 'calc' ] = unpack_float_t ( s [ sidx :]) return 4 def ext_pack_command ( self , s , sidx ): \"\"\" s: byte array to pack in to sidx: index to start packing at return: new sidx \"\"\" pack_float_t ( s , sidx , self . _command [ 'calc' ][ 'var1' ]) sidx += 4 pack_float_t ( s , sidx , self . _command [ 'calc' ][ 'var2' ]) sidx += 4 pack_uint8_t ( s , sidx , self . _command [ 'calc' ][ 'op' ]) sidx += 1 return sidx The class registers two callbacks for packing Command data and unpacking Status data. They are fairly self-explanatory and can be easily extended to match your custom data. Looking at the unpacking code: def ext_unpack_status ( self , s ): \"\"\" s: byte array to unpack return: number of bytes unpacked \"\"\" sidx = 0 self . status [ 'calc' ] = unpack_float_t ( s [ sidx :]) return 4 , we see the use of unpack_float_t . This, and other functions to unpack data, are found in stretch_body.transport . It is important that the data types and order match exactly those declared in the firmware. For example in Common.h we have: struct __attribute__ (( packed )) Calc_Command { float var1 ; float var2 ; uint8_t op ; }; , and on the Python side we have pack_float_t ( s , sidx , self . _command [ 'calc' ][ 'var1' ]) sidx += 4 pack_float_t ( s , sidx , self . _command [ 'calc' ][ 'var2' ]) sidx += 4 pack_uint8_t ( s , sidx , self . _command [ 'calc' ][ 'op' ]) sidx += 1","title":"Python"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#test-the-calculator-ipython","text":"We're ready to try out our calculator. First, Install and setup the Arduino IDE if it isn't already as described in the Updating Firmware tutorial . Open the Wacc_Calc Arduino sketch in the Arduino IDE. Select the hello_wacc board, the ttyACMx port that maps to the Wacc board. Then burn the firmware as described in the the Updating Firmware tutorial . Now, lets try it out: >>$ cd ~/repos/stretch_firmware/tutorial/python/ >>$ ipython And from iPython In [ 1 ]: import wacc_calc In [ 2 ]: w = wacc_calc . WaccCalc () In [ 3 ]: w . startup () In [ 4 ]: w . calculate ( op = 0 , var1 = 100.0 , var2 = 200.0 ) In [ 5 ]: w . push_command () In [ 6 ]: w . pull_status () In [ 7 ]: print 'Result is' , w . status [ 'calc' ] Result is 300.0 In [ 8 ]: w . calculate ( op = 1 , var1 = 100.0 , var2 = 200.0 ) In [ 9 ]: w . push_command () In [ 10 ]: w . pull_status () In [ 11 ]: print 'Result is' , w . status [ 'calc' ] Result is 20000.0 In [ 12 ]: w . calculate ( op = 2 , var1 = 100.0 , var2 = 200.0 ) In [ 13 ]: w . push_command () In [ 14 ]: w . pull_status () In [ 15 ]: print 'Result is' , w . status [ 'calc' ] Result is 0.5","title":"Test the Calculator - iPython"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#test-the-calculator-script","text":"Alternatively you can use the provided tool, stretch_wacc_calc_jog.py . Here you can use the calculator through the menu. hello-robot@stretch-re1-100x:~$ cd repos/stretch_firmware/tutorial/python/ hello-robot@stretch-re1-100x:~/repos/stretch_firmware/tutorial/python$ ./stretch_wacc_calc_jog.py ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- X ---Calculate Op ( Var1,Var2 ) --- Op = 0 : Add Op = 1 : Mult Op = 2 : Div Enter Op 1 Enter Var1 12 Enter Var2 13","title":"Test the Calculator - Script"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#_1","text":"","title":""},{"location":"stretch-firmware/docs/tutorial_data_transfer/#adding-yaml-parameters","text":"The WaccCalc class can use the Stretch Body YAML files as well. For example, add to your stretch_re1_user_params.yaml : wacc : calc_scalar : 2.0 Now let's scale the commands to the Arduino according to this YAML parameter. In WaccCalc : def calculate ( self , op , var1 , var2 ): \"\"\" 0: addition 1: multiplication 2: division \"\"\" self . _command [ 'calc' ][ 'op' ] = int ( op ) self . _command [ 'calc' ][ 'var1' ] = self . params [ 'calc_scalar' ] * float ( var1 ) self . _command [ 'calc' ][ 'var2' ] = self . params [ 'calc_scalar' ] * float ( var2 ) self . _dirty_command = True Run the iPython as above and you'll see the values multiplied by the YAML scalar.","title":"Adding YAML Parameters"},{"location":"stretch-firmware/docs/tutorial_data_transfer/#final-steps","text":"We want the Stretch Body Robot to use WaccCalc and not Wacc . To do this, add the following to your stretch_re1_user_params.yaml : robot: custom_wacc: py_class_name: WaccCalc py_module_name: wacc_calc This tells Robot which module and class to create its Wacc instance from. Now, test that it works from iPython In [ 1 ]: import stretch_body.robot In [ 2 ]: robot = stretch_body . robot . Robot () Starting TransportConnection on : / dev / hello - wacc In [ 3 ]: robot . startup () In [ 4 ]: robot . wacc . calculate ( op = 0 , var1 = 100.0 , var2 = 200.0 ) In [ 5 ]: robot . push_command () In [ 6 ]: print 'Result' , robot . wacc . status [ 'calc' ] Result 300.0 In [ 7 ]: robot . stop () All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Final Steps"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/","text":"NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board. NOTE : These tutorials may require the latest version of Stretch Body. If necessary, please update your install. Integrating an I2C Device This tutorial illustrates the integration of a I2C device on to the Wrist Expansion header . It extends t. We recommend doing the Data Transfer tutorial first. Calculator via I2C In this tutorial we will run the calculator from the Data Transfer tutorial on an Adafruit Metro M0 Express (Arduino Zero) that is running as an I2C slave. It will take a calculator Command from the Wacc and return the result in Status message. Flash Firmware First, program the Metro with the provided sketch, zero_wacc_i2c . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Arduino UNO' from the IDE under Tools/Board Next, program the Wacc with the provided sketch, hello_wacc_i2c . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Adafruit M0 Express' from the IDE under Tools/Board Code Walk-through The code is straightforward and is a natural extension of the code described in the Data Transfer tutorial. A few sections to highlight in the hello_wacc_i2c sketch are: In setupWacc() we add code to configure the I2C. Wire . begin (); In Wacc.cpp we add the I2C code: #include <Wire.h> uint8_t i2c_out [ 9 ]; //I2C data out uint8_t i2c_in [ 9 ]; //I2C data in uint8_t ds_i2c_cnt = 0 ; //Down sample counter float FS_I2C = 10 ; //Rate to run transactions (Hz) int buf_idx = 0 ; void i2cTransaction () { //Send the commmand memcpy ( i2c_out , ( uint8_t * ) ( & cmd . calc ), sizeof ( Calc_Command )); Wire . beginTransmission ( 4 ); // transmit to device #4 for ( int i = 0 ; i < sizeof ( Calc_Command ); i ++ ) { Wire . write ( i2c_out [ i ]); } Wire . endTransmission (); //Get the result Wire . requestFrom ( 4 , sizeof ( Calc_Status )); int buf_idx = 0 ; while ( Wire . available () && buf_idx < sizeof ( Calc_Status )) // loop through all but the last { uint8_t x = Wire . read (); i2c_in [ buf_idx ++ ] = x ; } memcpy (( uint8_t * ) ( & stat . calc ), i2c_in , sizeof ( Calc_Status )); } Here, the 9 bytes of the Calc_Command are transferred out and the 4 bytes of the Calc_Status are received. Note: This simple communication protocol is not robust to handshaking errors, etc Note: The Wacc uses I2C to also communicate with its onboard accelerometer -- the ADXL343 . In our example we are using I2C address 4 to communicate with our Metro slave. The ADXL343 is configured to use addrex 0xA6 for a write and 0xA7 for a read. Finally, we call the i2cTransaction() function at a rate of FS_I2C by adding to stepWaccController() : if ( ds_i2c_cnt ++ >= ( FS_CTRL / FS_I2C )) { ds_i2c_cnt = 0 ; i2cTransaction (); } Wire Up the Boards Next, wire the Metro to the Expansion Header as: Stretch Expansion Header Uno SCL SCL SDA SDA GND GND Test the Calculator Now, test the setup using the provided tool, stretch_wacc_calc_jog.py . As shown below, The Metro performs the calculation of 12*13 and the result is report back to Stretch Body. hello-robot@stretch-re1-100x:~$ cd repos/stretch_firmware/tutorial/python/ hello-robot@stretch-re1-100x:~/repos/stretch_firmware/tutorial/python$ ./stretch_wacc_calc_jog.py ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- X ---Calculate Op ( Var1,Var2 ) --- Op = 0 : Add Op = 1 : Mult Op = 2 : Div Enter Op 1 Enter Var1 12 Enter Var2 13 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- ------------------------------ Ax ( m/s^2 ) 0 .0488623343408 Ay ( m/s^2 ) 0 .155020624399 Az ( m/s^2 ) -10.0049753189 A0 349 D0 ( In ) 0 D1 ( In ) 1 D2 ( Out ) 70 D3 ( Out ) 0 Single Tap Count 26 State 0 Debug 0 Timestamp 1591588745 .27 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1pMySPI Calc: 156 .0 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"I2C Sensor"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#integrating-an-i2c-device","text":"This tutorial illustrates the integration of a I2C device on to the Wrist Expansion header . It extends t. We recommend doing the Data Transfer tutorial first.","title":"Integrating an I2C Device"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#calculator-via-i2c","text":"In this tutorial we will run the calculator from the Data Transfer tutorial on an Adafruit Metro M0 Express (Arduino Zero) that is running as an I2C slave. It will take a calculator Command from the Wacc and return the result in Status message.","title":"Calculator via I2C"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#flash-firmware","text":"First, program the Metro with the provided sketch, zero_wacc_i2c . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Arduino UNO' from the IDE under Tools/Board Next, program the Wacc with the provided sketch, hello_wacc_i2c . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Adafruit M0 Express' from the IDE under Tools/Board","title":"Flash Firmware"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#code-walk-through","text":"The code is straightforward and is a natural extension of the code described in the Data Transfer tutorial. A few sections to highlight in the hello_wacc_i2c sketch are: In setupWacc() we add code to configure the I2C. Wire . begin (); In Wacc.cpp we add the I2C code: #include <Wire.h> uint8_t i2c_out [ 9 ]; //I2C data out uint8_t i2c_in [ 9 ]; //I2C data in uint8_t ds_i2c_cnt = 0 ; //Down sample counter float FS_I2C = 10 ; //Rate to run transactions (Hz) int buf_idx = 0 ; void i2cTransaction () { //Send the commmand memcpy ( i2c_out , ( uint8_t * ) ( & cmd . calc ), sizeof ( Calc_Command )); Wire . beginTransmission ( 4 ); // transmit to device #4 for ( int i = 0 ; i < sizeof ( Calc_Command ); i ++ ) { Wire . write ( i2c_out [ i ]); } Wire . endTransmission (); //Get the result Wire . requestFrom ( 4 , sizeof ( Calc_Status )); int buf_idx = 0 ; while ( Wire . available () && buf_idx < sizeof ( Calc_Status )) // loop through all but the last { uint8_t x = Wire . read (); i2c_in [ buf_idx ++ ] = x ; } memcpy (( uint8_t * ) ( & stat . calc ), i2c_in , sizeof ( Calc_Status )); } Here, the 9 bytes of the Calc_Command are transferred out and the 4 bytes of the Calc_Status are received. Note: This simple communication protocol is not robust to handshaking errors, etc Note: The Wacc uses I2C to also communicate with its onboard accelerometer -- the ADXL343 . In our example we are using I2C address 4 to communicate with our Metro slave. The ADXL343 is configured to use addrex 0xA6 for a write and 0xA7 for a read. Finally, we call the i2cTransaction() function at a rate of FS_I2C by adding to stepWaccController() : if ( ds_i2c_cnt ++ >= ( FS_CTRL / FS_I2C )) { ds_i2c_cnt = 0 ; i2cTransaction (); }","title":"Code Walk-through"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#wire-up-the-boards","text":"Next, wire the Metro to the Expansion Header as: Stretch Expansion Header Uno SCL SCL SDA SDA GND GND","title":"Wire Up the Boards"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#test-the-calculator","text":"Now, test the setup using the provided tool, stretch_wacc_calc_jog.py . As shown below, The Metro performs the calculation of 12*13 and the result is report back to Stretch Body.","title":"Test the Calculator"},{"location":"stretch-firmware/docs/tutorial_i2c_sensor/#hello-robotstretch-re1-100x-cd-reposstretch_firmwaretutorialpython-hello-robotstretch-re1-100xreposstretch_firmwaretutorialpython-stretch_wacc_calc_jogpy-menu-m-menu-r-reset-board-a-set-d2-on-b-set-d2-off-c-set-d3-on-d-set-d3-off-x-do-calculation-x-calculate-opvar1var2-op0-add-op1-mult-op2-div-enter-op-1-enter-var1-12-enter-var2-13-menu-m-menu-r-reset-board-a-set-d2-on-b-set-d2-off-c-set-d3-on-d-set-d3-off-x-do-calculation-ax-ms2-00488623343408-ay-ms2-0155020624399-az-ms2-100049753189-a0-349-d0-in-0-d1-in-1-d2-out-70-d3-out-0-single-tap-count-26-state-0-debug-0-timestamp-159158874527-board-version-waccguthriev1-firmware-version-waccv001pmyspi-calc-1560-menu-m-menu-r-reset-board-a-set-d2-on-b-set-d2-off-c-set-d3-on-d-set-d3-off-x-do-calculation-","text":"All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"hello-robot@stretch-re1-100x:~$ cd repos/stretch_firmware/tutorial/python/\nhello-robot@stretch-re1-100x:~/repos/stretch_firmware/tutorial/python$ ./stretch_wacc_calc_jog.py \n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\nX\n---Calculate Op(Var1,Var2) ---\nOp=0: Add\nOp=1: Mult\nOp=2: Div\nEnter Op\n1\nEnter Var1\n12\nEnter Var2\n13\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\n\n------------------------------\nAx (m/s^2) 0.0488623343408\nAy (m/s^2) 0.155020624399\nAz (m/s^2) -10.0049753189\nA0 349\nD0 (In) 0\nD1 (In) 1\nD2 (Out) 70\nD3 (Out) 0\nSingle Tap Count 26\nState  0\nDebug 0\nTimestamp 1591588745.27\nBoard version: Wacc.Guthrie.V1\nFirmware version: Wacc.v0.0.1pMySPI\nCalc: 156.0\n------ MENU -------\nm: menu\nr: reset board\na: set D2 on\nb: set D2 off\nc: set D3 on\nd: set D3 off\nX: do calculation\n-------------------\n"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/","text":"NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board. NOTE : These tutorials may require the latest version of Stretch Body. If necessary, please update your install. Integrating a Serial Device This tutorial illustrates the integration of a UART device on to the Wrist Expansion header . We recommend first reading Data Transfer tutorial to understand how data is transfered back and forth from Stretch Body to the SAMD uC. In this tutorial we will extend the Stretch Body Wacc class to send 10 floats down to the custom serial device. The serial device will echo the 10 floats back up to Stretch Body. For the purposes of the tutorial we will wire the Wacc up in a loopback configuration as a stand-in for an actual physical serial device. Loopback Hardware Setup Connect the UART TX pin to the UART RX pin of the Stretch Expansion Header (or the Metro M0 if emulating the Wacc). Connector information for the Expansion Header is found in the Hardware Guide . Flash Firmware Pull down the latest version of Stretch Firmware >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_firmware Next, program the (emulated) Wacc with the provided sketch, hello_wacc_serial . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Hello Wacc' from the IDE under Tools/Board Firmware Code Walk-through First, in the sketch setup() we configure enable the SerialExt device SerialExt . begin ( 115200 ); Next, in Common.h we define Command data to send down from Stretch Body to the serial device. We also define Status data to report back to Stretch Body. For our example we'll send 10 floats down and 10 floats back up. struct __attribute__ (( packed )) SerialExtCommand { float data [ 10 ]; }; struct __attribute__ (( packed )) SerialExtStatus { float data [ 10 ]; }; In Wacc.cpp we add the code that will communicate with the serial device. Whenever Stretch Body pushes a new command down the Wacc, this function will get called. Here we are writing the Command data to the SerialExt port. We then send back in the Status message the data we read from SerialExt. As it is a physical loopback connection, the data back in Status will be the same as the Command message. void serial_comms () { char * data_down = ( char * ) & ( cmd . serial . data [ 0 ]); char * data_up = ( char * )( stat . serial . data ); for ( int i = 0 ; i < 40 ; i ++ ) { SerialExt . write ( data_down + i , 1 ); } for ( int i = 0 ; i < 40 ; i ++ ) { if ( SerialExt . available ()) { data_up [ i ] = SerialExt . read (); } } } Stretch Body Code Walk-through We provide an example class WaccSerialExt that extends the Wacc class of Stretch Body. This class provides two call backs that will get called on pull_status and push_command respectively. Here we see the packing and unpacking of the 10 floats found in Common.h def ext_unpack_status ( self , s ): \"\"\" s: byte array to unpack return: number of bytes unpacked \"\"\" sidx = 0 for i in range ( self . n_float ): self . status [ 'serial_ext' ][ i ] = unpack_float_t ( s [ sidx :]) sidx += 4 return sidx def ext_pack_command ( self , s , sidx ): \"\"\" s: byte array to pack in to sidx: index to start packing at return: new sidx \"\"\" for i in range ( self . n_float ): pack_float_t ( s , sidx , self . _command [ 'serial_ext' ][ i ]) sidx += 4 return sidx We also define a function that generates a new 'command' down to the serial device. In this case it just increments the 10 floats by one: def serial_data_increment ( self ): for i in range ( self . n_float ): self . _command [ 'serial_ext' ][ i ] = float ( self . _command [ 'serial_ext' ][ i ] + 1 ) self . _dirty_command = True Finally, we do a simple test of the class with the tool stretch_wacc_serial_jog.py from wacc_serial_ext import WaccSerialExt w = WaccSerialExt () w . startup () try : while True : print ( 'Hit enter to do TX/RX cycle' ) raw_input () w . serial_data_increment () w . push_command () print ( 'TX to SerialExt' , w . _command [ 'serial_ext' ]) w . pull_status () print ( 'RX from SerialExt' , w . status [ 'serial_ext' ]) except ( KeyboardInterrupt , SystemExit ): w . stop () Run it from the command line and verify that the 10 floats are being command down to your serial device, through the loopback connection, and back, up to the WaccSerialExt class: >>$ cd ~/repos/stretch_firmware/tutorial/python >>$ ./stretch_wacc_serial_jog.py Hit enter to do TX/RX cycle .. ( 'TX to SerialExt' , [ 2 .0, 3 .0, 4 .0, 5 .0, 6 .0, 7 .0, 8 .0, 9 .0, 10 .0, 11 .0 ]) ( 'RX from SerialExt' , [ 1 .0, 2 .0, 3 .0, 4 .0, 5 .0, 6 .0, 7 .0, 8 .0, 9 .0, 10 .0 ]) Hit enter to do TX/RX cycle ( 'TX to SerialExt' , [ 3 .0, 4 .0, 5 .0, 6 .0, 7 .0, 8 .0, 9 .0, 10 .0, 11 .0, 12 .0 ]) ( 'RX from SerialExt' , [ 2 .0, 3 .0, 4 .0, 5 .0, 6 .0, 7 .0, 8 .0, 9 .0, 10 .0, 11 .0 ]) Hit enter to do TX/RX cycle .. NOTE: It takes one control cycle for the command values to be reported back to the status All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Serial Sensor"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#integrating-a-serial-device","text":"This tutorial illustrates the integration of a UART device on to the Wrist Expansion header . We recommend first reading Data Transfer tutorial to understand how data is transfered back and forth from Stretch Body to the SAMD uC. In this tutorial we will extend the Stretch Body Wacc class to send 10 floats down to the custom serial device. The serial device will echo the 10 floats back up to Stretch Body. For the purposes of the tutorial we will wire the Wacc up in a loopback configuration as a stand-in for an actual physical serial device.","title":"Integrating a Serial Device"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#loopback-hardware-setup","text":"Connect the UART TX pin to the UART RX pin of the Stretch Expansion Header (or the Metro M0 if emulating the Wacc). Connector information for the Expansion Header is found in the Hardware Guide .","title":"Loopback Hardware Setup"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#flash-firmware","text":"Pull down the latest version of Stretch Firmware >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_firmware Next, program the (emulated) Wacc with the provided sketch, hello_wacc_serial . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Hello Wacc' from the IDE under Tools/Board","title":"Flash Firmware"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#firmware-code-walk-through","text":"First, in the sketch setup() we configure enable the SerialExt device SerialExt . begin ( 115200 ); Next, in Common.h we define Command data to send down from Stretch Body to the serial device. We also define Status data to report back to Stretch Body. For our example we'll send 10 floats down and 10 floats back up. struct __attribute__ (( packed )) SerialExtCommand { float data [ 10 ]; }; struct __attribute__ (( packed )) SerialExtStatus { float data [ 10 ]; }; In Wacc.cpp we add the code that will communicate with the serial device. Whenever Stretch Body pushes a new command down the Wacc, this function will get called. Here we are writing the Command data to the SerialExt port. We then send back in the Status message the data we read from SerialExt. As it is a physical loopback connection, the data back in Status will be the same as the Command message. void serial_comms () { char * data_down = ( char * ) & ( cmd . serial . data [ 0 ]); char * data_up = ( char * )( stat . serial . data ); for ( int i = 0 ; i < 40 ; i ++ ) { SerialExt . write ( data_down + i , 1 ); } for ( int i = 0 ; i < 40 ; i ++ ) { if ( SerialExt . available ()) { data_up [ i ] = SerialExt . read (); } } }","title":"Firmware Code Walk-through"},{"location":"stretch-firmware/docs/tutorial_serial_sensor/#stretch-body-code-walk-through","text":"We provide an example class WaccSerialExt that extends the Wacc class of Stretch Body. This class provides two call backs that will get called on pull_status and push_command respectively. Here we see the packing and unpacking of the 10 floats found in Common.h def ext_unpack_status ( self , s ): \"\"\" s: byte array to unpack return: number of bytes unpacked \"\"\" sidx = 0 for i in range ( self . n_float ): self . status [ 'serial_ext' ][ i ] = unpack_float_t ( s [ sidx :]) sidx += 4 return sidx def ext_pack_command ( self , s , sidx ): \"\"\" s: byte array to pack in to sidx: index to start packing at return: new sidx \"\"\" for i in range ( self . n_float ): pack_float_t ( s , sidx , self . _command [ 'serial_ext' ][ i ]) sidx += 4 return sidx We also define a function that generates a new 'command' down to the serial device. In this case it just increments the 10 floats by one: def serial_data_increment ( self ): for i in range ( self . n_float ): self . _command [ 'serial_ext' ][ i ] = float ( self . _command [ 'serial_ext' ][ i ] + 1 ) self . _dirty_command = True Finally, we do a simple test of the class with the tool stretch_wacc_serial_jog.py from wacc_serial_ext import WaccSerialExt w = WaccSerialExt () w . startup () try : while True : print ( 'Hit enter to do TX/RX cycle' ) raw_input () w . serial_data_increment () w . push_command () print ( 'TX to SerialExt' , w . _command [ 'serial_ext' ]) w . pull_status () print ( 'RX from SerialExt' , w . status [ 'serial_ext' ]) except ( KeyboardInterrupt , SystemExit ): w . stop () Run it from the command line and verify that the 10 floats are being command down to your serial device, through the loopback connection, and back, up to the WaccSerialExt class: >>$ cd ~/repos/stretch_firmware/tutorial/python >>$ ./stretch_wacc_serial_jog.py Hit enter to do TX/RX cycle .. ( 'TX to SerialExt' , [ 2 .0, 3 .0, 4 .0, 5 .0, 6 .0, 7 .0, 8 .0, 9 .0, 10 .0, 11 .0 ]) ( 'RX from SerialExt' , [ 1 .0, 2 .0, 3 .0, 4 .0, 5 .0, 6 .0, 7 .0, 8 .0, 9 .0, 10 .0 ]) Hit enter to do TX/RX cycle ( 'TX to SerialExt' , [ 3 .0, 4 .0, 5 .0, 6 .0, 7 .0, 8 .0, 9 .0, 10 .0, 11 .0, 12 .0 ]) ( 'RX from SerialExt' , [ 2 .0, 3 .0, 4 .0, 5 .0, 6 .0, 7 .0, 8 .0, 9 .0, 10 .0, 11 .0 ]) Hit enter to do TX/RX cycle .. NOTE: It takes one control cycle for the command values to be reported back to the status All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Stretch Body Code Walk-through"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/","text":"NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board. NOTE : These tutorials may require the latest version of Stretch Body. If necessary, please update your install. Integrating an SPI Device This tutorial illustrates the integration of a SPI device on to the Wrist Expansion header . It extends the Data Transfer tutorial. We recommend doing this tutorial prior to doing this one. Calculator via SPI In this tutorial we will run the calculator from the Data Transfer tutorial on an Adafruit Metro M0 Express (Arduino Zero) that is running as an SPI slave. It will take a calculator Command from the Wacc and return the result in Status message. Flash Firmware First, program the Metro with the provided sketch, zero_wacc_spi . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Adafruit M0 Express' from the IDE under Tools/Board Next, program the Wacc with the provided sketch, hello_wacc_spi . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Hello Wacc' from the IDE under Tools/Board Code Walk-through The code is straightforward and is a natural extension of the code described in the Data Transfer tutorial. A few sections to highlight in the hello_wacc_spi sketch are: First, in the sketch setup() we configure the slave select pin to be an output pinMode ( HEADER_SPI_SS , OUTPUT ); Next in setupWacc() we add code to configure the SPI SPISettings settingsA ( 100000 , MSBFIRST , SPI_MODE1 ); SPI . begin (); SPI . beginTransaction ( settingsA ); In Wacc.cpp we add the SPI code: #include <SPI.h> uint8_t spi_out [ 9 ]; //SPI data out uint8_t spi_in [ 9 ]; //SPI data in uint8_t ds_spi_cnt = 0 ; //Down sample counter float FS_SPI = 10 ; //Rate to run transactions (Hz) void spiTransaction () { digitalWrite ( HEADER_SPI_SS , LOW ); SPI . transfer ( 'X' ); //Mark start of transaction memcpy ( spi_out , ( uint8_t * ) ( & cmd . calc ), sizeof ( Calc_Command )); for ( uint8_t idx = 0 ; idx < 9 ; idx ++ ) spi_in [ idx ] = SPI . transfer ( spi_out [ idx ]); digitalWrite ( HEADER_SPI_SS , HIGH ); memcpy (( uint8_t * ) ( & stat . calc ), spi_in + 1 , sizeof ( Calc_Status )); } Here, the 9 bytes of the Calc_Command are transferred out and the 4 bytes of the Calc_Status are received. Note: This simple communication protocol is not robust, using an 'X' to demarcate the start of a transaction Finally, we call the spiTransaction() function at a rate of FS_SPI by adding to stepWaccController() : if ( ds_spi_cnt ++ >= ( FS_CTRL / FS_SPI )) { ds_spi_cnt = 0 ; spiTransaction (); } Wire Up the Boards Next, wire the Metro to the Expansion Header as: Stretch Expansion Header Uno SS SS MISO MISO MOSI MOSI SCK SCK GND GND Note: Other Arduino boards can be used. However 5V Arduinos boards will need their SPI lines level shifted to 3V3. Test the Calculator Now, test the setup using the provided tool, stretch_wacc_calc_jog.py . As shown below, The Arduino Uno performs the calculation of 12*13 and the result is report back to Stretch Body. hello-robot@stretch-re1-100x:~$ cd repos/stretch_firmware/tutorial/python/ hello-robot@stretch-re1-100x:~/repos/stretch_firmware/tutorial/python$ ./stretch_wacc_calc_jog.py ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- X ---Calculate Op ( Var1,Var2 ) --- Op = 0 : Add Op = 1 : Mult Op = 2 : Div Enter Op 1 Enter Var1 12 Enter Var2 13 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- ------------------------------ Ax ( m/s^2 ) 0 .0488623343408 Ay ( m/s^2 ) 0 .155020624399 Az ( m/s^2 ) -10.0049753189 A0 349 D0 ( In ) 0 D1 ( In ) 1 D2 ( Out ) 70 D3 ( Out ) 0 Single Tap Count 26 State 0 Debug 0 Timestamp 1591588745 .27 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1pMySPI Calc: 156 .0 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"SPI Sensor"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#integrating-an-spi-device","text":"This tutorial illustrates the integration of a SPI device on to the Wrist Expansion header . It extends the Data Transfer tutorial. We recommend doing this tutorial prior to doing this one.","title":"Integrating an SPI Device"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#calculator-via-spi","text":"In this tutorial we will run the calculator from the Data Transfer tutorial on an Adafruit Metro M0 Express (Arduino Zero) that is running as an SPI slave. It will take a calculator Command from the Wacc and return the result in Status message.","title":"Calculator via SPI"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#flash-firmware","text":"First, program the Metro with the provided sketch, zero_wacc_spi . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Adafruit M0 Express' from the IDE under Tools/Board Next, program the Wacc with the provided sketch, hello_wacc_spi . Be sure to: Select the board's port from the IDE under Tools/Port Select the board 'Hello Wacc' from the IDE under Tools/Board","title":"Flash Firmware"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#code-walk-through","text":"The code is straightforward and is a natural extension of the code described in the Data Transfer tutorial. A few sections to highlight in the hello_wacc_spi sketch are: First, in the sketch setup() we configure the slave select pin to be an output pinMode ( HEADER_SPI_SS , OUTPUT ); Next in setupWacc() we add code to configure the SPI SPISettings settingsA ( 100000 , MSBFIRST , SPI_MODE1 ); SPI . begin (); SPI . beginTransaction ( settingsA ); In Wacc.cpp we add the SPI code: #include <SPI.h> uint8_t spi_out [ 9 ]; //SPI data out uint8_t spi_in [ 9 ]; //SPI data in uint8_t ds_spi_cnt = 0 ; //Down sample counter float FS_SPI = 10 ; //Rate to run transactions (Hz) void spiTransaction () { digitalWrite ( HEADER_SPI_SS , LOW ); SPI . transfer ( 'X' ); //Mark start of transaction memcpy ( spi_out , ( uint8_t * ) ( & cmd . calc ), sizeof ( Calc_Command )); for ( uint8_t idx = 0 ; idx < 9 ; idx ++ ) spi_in [ idx ] = SPI . transfer ( spi_out [ idx ]); digitalWrite ( HEADER_SPI_SS , HIGH ); memcpy (( uint8_t * ) ( & stat . calc ), spi_in + 1 , sizeof ( Calc_Status )); } Here, the 9 bytes of the Calc_Command are transferred out and the 4 bytes of the Calc_Status are received. Note: This simple communication protocol is not robust, using an 'X' to demarcate the start of a transaction Finally, we call the spiTransaction() function at a rate of FS_SPI by adding to stepWaccController() : if ( ds_spi_cnt ++ >= ( FS_CTRL / FS_SPI )) { ds_spi_cnt = 0 ; spiTransaction (); }","title":"Code Walk-through"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#wire-up-the-boards","text":"Next, wire the Metro to the Expansion Header as: Stretch Expansion Header Uno SS SS MISO MISO MOSI MOSI SCK SCK GND GND Note: Other Arduino boards can be used. However 5V Arduinos boards will need their SPI lines level shifted to 3V3.","title":"Wire Up the Boards"},{"location":"stretch-firmware/docs/tutorial_spi_sensor/#test-the-calculator","text":"Now, test the setup using the provided tool, stretch_wacc_calc_jog.py . As shown below, The Arduino Uno performs the calculation of 12*13 and the result is report back to Stretch Body. hello-robot@stretch-re1-100x:~$ cd repos/stretch_firmware/tutorial/python/ hello-robot@stretch-re1-100x:~/repos/stretch_firmware/tutorial/python$ ./stretch_wacc_calc_jog.py ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- X ---Calculate Op ( Var1,Var2 ) --- Op = 0 : Add Op = 1 : Mult Op = 2 : Div Enter Op 1 Enter Var1 12 Enter Var2 13 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- ------------------------------ Ax ( m/s^2 ) 0 .0488623343408 Ay ( m/s^2 ) 0 .155020624399 Az ( m/s^2 ) -10.0049753189 A0 349 D0 ( In ) 0 D1 ( In ) 1 D2 ( Out ) 70 D3 ( Out ) 0 Single Tap Count 26 State 0 Debug 0 Timestamp 1591588745 .27 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1pMySPI Calc: 156 .0 ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off X: do calculation ------------------- All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Test the Calculator"},{"location":"stretch-firmware/docs/tutorial_updating_firmware/","text":"Updating Stretch Firmware Stretch has 6 Arduino based microcontroller boards that may require a firmware update: Board USB Device Arduino Sketch Left wheel stepper /dev/hello-motor-left-wheel hello_stepper.ino Right wheel stepper /dev/hello-motor-right-wheel hello_stepper.ino Lift stepper /dev/hello-motor-lift hello_stepper.ino Arm stepper /dev/hello-motor-arm hello_stepper.ino Pimu /dev/hello-pimu hello_pimu.ino Wacc /dev/hello-wacc hello_wacc.ino Firmware updates are managed through the REx_firmware_updater.py tool, which uses the Arduino Cli tool to flash the firmware. Before doing an upgrade first ensure the latest Stretch Body is installed as well as Stretch Factory >>$ pip install hello-robot-stretch-factory --upgrade --no-cache-dir >>$ pip install hello-robot-stretch-body --upgrade --no-cache-dir Firmware Update Tool The firmware update tool will automatically recommend and install the latest version of firmware found on GitHub. It will only recommend versions that are compatible with the currently installed Stretch Body. >>$ REx_firmware_updater.py --help usage: REx_firmware_updater.py [ -h ] [ --current | --available | --recommended | --install | --install_version | --install_branch | --install_path INSTALL_PATH | --mgmt ] [ --pimu ] [ --wacc ] [ --arm ] [ --lift ] [ --left_wheel ] [ --right_wheel ] Upload Stretch firmware to microcontrollers optional arguments: -h, --help show this help message and exit --current Display the currently installed firmware versions --available Display the available firmware versions --recommended Display the recommended firmware --install Install the recommended firmware --install_version Install a specific firmware version --install_branch Install the HEAD of a specific branch --install_path INSTALL_PATH Install the firmware on the provided path ( eg ./stretch_firmware/arduino ) --mgmt Display overview on firmware management --pimu Upload Pimu firmware --wacc Upload Wacc firmware --arm Upload Arm Stepper firmware --lift Upload Lift Stepper firmware --left_wheel Upload Left Wheel Stepper firmware --right_wheel Upload Right Wheel Stepper firmware Updating to the Latest Firmware To update to the latest version of firmware: >>$ REx_firmware_updater.py --install Collecting information... ######################################## Recommended Firmware Updates ######################################## DEVICE | INSTALLED | RECOMMENDED | ACTION -------------------------------------------------------------------------------------------------------------- HELLO-WACC | Wacc.v0.0.2p1 | Wacc.v0.0.1p0 | Downgrade recommended HELLO-MOTOR-LIFT | Stepper.v0.1.0p1 | Stepper.v0.0.4p0 | Downgrade recommended HELLO-PIMU | Pimu.v0.0.2p1 | Pimu.v0.0.1p0 | Downgrade recommended HELLO-MOTOR-ARM | Stepper.v0.1.0p1 | Stepper.v0.0.4p0 | Downgrade recommended HELLO-MOTOR-LEFT-WHEEL | Stepper.v0.1.0p1 | Stepper.v0.0.4p0 | Downgrade recommended HELLO-MOTOR-RIGHT-WHEEL | Stepper.v0.1.0p1 | Stepper.v0.0.4p0 | Downgrade recommended ######################################### UPDATING FIRMWARE TO... ########################################### HELLO-WACC | Downgrading to Wacc.v0.0.1p0 HELLO-MOTOR-LEFT-WHEEL | Downgrading to Stepper.v0.0.4p0 HELLO-MOTOR-RIGHT-WHEEL | Downgrading to Stepper.v0.0.4p0 HELLO-MOTOR-LIFT | Downgrading to Stepper.v0.0.4p0 HELLO-PIMU | Downgrading to Pimu.v0.0.1p0 HELLO-MOTOR-ARM | Downgrading to Stepper.v0.0.4p0 ------------------------------------------------ WARNING: ( 1 ) Updating robot firmware should only be done by experienced users WARNING: ( 2 ) Do not have other robot processes running during update WARNING: ( 3 ) Leave robot powered on during update WARNING: ( 4 ) Ensure Lift has support clamp in place WARNING: ( 5 ) Lift may make a loud noise during programming. This is normal. Proceed with update?? [ y/N ] : Review the recommendations and warnings before proceeding ('y'). If you prefer to only update one or more boards you can specify it on the command line. For example: REx_firmware_updater.py --install --arm --wacc Other Useful Commands Running REx_firmware_updater.py --current will report the currently install firmware: >> $REx_firmware_updater .py --current ######################################## Currently Installed Firmware ######################################## ------------ HELLO-WACC ------------ Installed Firmware: Wacc.v0.0.2p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-MOTOR-LIFT ------------ Installed Firmware: Stepper.v0.1.0p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-PIMU ------------ Installed Firmware: Pimu.v0.0.2p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-MOTOR-ARM ------------ Installed Firmware: Stepper.v0.1.0p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-MOTOR-LEFT-WHEEL ------------ Installed Firmware: Stepper.v0.1.0p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-MOTOR-RIGHT-WHEEL ------------ Installed Firmware: Stepper.v0.1.0p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID Running REx_firmware_updater.py --available will list the available versions on GitHub: >>$ REx_firmware_updater.py --available Collecting information... ####################### Currently Tagged Versions of Stretch Firmware on Master Branch ####################### ---- HELLO-WACC ---- Wacc.v0.0.1p0 ---- HELLO-MOTOR-LIFT ---- Stepper.v0.0.1p0 Stepper.v0.0.2p0 Stepper.v0.0.3p0 Stepper.v0.0.4p0 ---- HELLO-PIMU ---- Pimu.v0.0.1p0 ---- HELLO-MOTOR-ARM ---- Stepper.v0.0.1p0 Stepper.v0.0.2p0 Stepper.v0.0.3p0 Stepper.v0.0.4p0 ---- HELLO-MOTOR-LEFT-WHEEL ---- Stepper.v0.0.1p0 Stepper.v0.0.2p0 Stepper.v0.0.3p0 Stepper.v0.0.4p0 ---- HELLO-MOTOR-RIGHT-WHEEL ---- Stepper.v0.0.1p0 Stepper.v0.0.2p0 Stepper.v0.0.3p0 Stepper.v0.0.4p0 All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Updating Firmware"},{"location":"stretch-firmware/docs/tutorial_updating_firmware/#updating-stretch-firmware","text":"Stretch has 6 Arduino based microcontroller boards that may require a firmware update: Board USB Device Arduino Sketch Left wheel stepper /dev/hello-motor-left-wheel hello_stepper.ino Right wheel stepper /dev/hello-motor-right-wheel hello_stepper.ino Lift stepper /dev/hello-motor-lift hello_stepper.ino Arm stepper /dev/hello-motor-arm hello_stepper.ino Pimu /dev/hello-pimu hello_pimu.ino Wacc /dev/hello-wacc hello_wacc.ino Firmware updates are managed through the REx_firmware_updater.py tool, which uses the Arduino Cli tool to flash the firmware. Before doing an upgrade first ensure the latest Stretch Body is installed as well as Stretch Factory >>$ pip install hello-robot-stretch-factory --upgrade --no-cache-dir >>$ pip install hello-robot-stretch-body --upgrade --no-cache-dir","title":"Updating Stretch Firmware"},{"location":"stretch-firmware/docs/tutorial_updating_firmware/#firmware-update-tool","text":"The firmware update tool will automatically recommend and install the latest version of firmware found on GitHub. It will only recommend versions that are compatible with the currently installed Stretch Body. >>$ REx_firmware_updater.py --help usage: REx_firmware_updater.py [ -h ] [ --current | --available | --recommended | --install | --install_version | --install_branch | --install_path INSTALL_PATH | --mgmt ] [ --pimu ] [ --wacc ] [ --arm ] [ --lift ] [ --left_wheel ] [ --right_wheel ] Upload Stretch firmware to microcontrollers optional arguments: -h, --help show this help message and exit --current Display the currently installed firmware versions --available Display the available firmware versions --recommended Display the recommended firmware --install Install the recommended firmware --install_version Install a specific firmware version --install_branch Install the HEAD of a specific branch --install_path INSTALL_PATH Install the firmware on the provided path ( eg ./stretch_firmware/arduino ) --mgmt Display overview on firmware management --pimu Upload Pimu firmware --wacc Upload Wacc firmware --arm Upload Arm Stepper firmware --lift Upload Lift Stepper firmware --left_wheel Upload Left Wheel Stepper firmware --right_wheel Upload Right Wheel Stepper firmware","title":"Firmware Update Tool"},{"location":"stretch-firmware/docs/tutorial_updating_firmware/#updating-to-the-latest-firmware","text":"To update to the latest version of firmware: >>$ REx_firmware_updater.py --install Collecting information... ######################################## Recommended Firmware Updates ######################################## DEVICE | INSTALLED | RECOMMENDED | ACTION -------------------------------------------------------------------------------------------------------------- HELLO-WACC | Wacc.v0.0.2p1 | Wacc.v0.0.1p0 | Downgrade recommended HELLO-MOTOR-LIFT | Stepper.v0.1.0p1 | Stepper.v0.0.4p0 | Downgrade recommended HELLO-PIMU | Pimu.v0.0.2p1 | Pimu.v0.0.1p0 | Downgrade recommended HELLO-MOTOR-ARM | Stepper.v0.1.0p1 | Stepper.v0.0.4p0 | Downgrade recommended HELLO-MOTOR-LEFT-WHEEL | Stepper.v0.1.0p1 | Stepper.v0.0.4p0 | Downgrade recommended HELLO-MOTOR-RIGHT-WHEEL | Stepper.v0.1.0p1 | Stepper.v0.0.4p0 | Downgrade recommended ######################################### UPDATING FIRMWARE TO... ########################################### HELLO-WACC | Downgrading to Wacc.v0.0.1p0 HELLO-MOTOR-LEFT-WHEEL | Downgrading to Stepper.v0.0.4p0 HELLO-MOTOR-RIGHT-WHEEL | Downgrading to Stepper.v0.0.4p0 HELLO-MOTOR-LIFT | Downgrading to Stepper.v0.0.4p0 HELLO-PIMU | Downgrading to Pimu.v0.0.1p0 HELLO-MOTOR-ARM | Downgrading to Stepper.v0.0.4p0 ------------------------------------------------ WARNING: ( 1 ) Updating robot firmware should only be done by experienced users WARNING: ( 2 ) Do not have other robot processes running during update WARNING: ( 3 ) Leave robot powered on during update WARNING: ( 4 ) Ensure Lift has support clamp in place WARNING: ( 5 ) Lift may make a loud noise during programming. This is normal. Proceed with update?? [ y/N ] : Review the recommendations and warnings before proceeding ('y'). If you prefer to only update one or more boards you can specify it on the command line. For example: REx_firmware_updater.py --install --arm --wacc","title":"Updating to the Latest Firmware"},{"location":"stretch-firmware/docs/tutorial_updating_firmware/#other-useful-commands","text":"Running REx_firmware_updater.py --current will report the currently install firmware: >> $REx_firmware_updater .py --current ######################################## Currently Installed Firmware ######################################## ------------ HELLO-WACC ------------ Installed Firmware: Wacc.v0.0.2p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-MOTOR-LIFT ------------ Installed Firmware: Stepper.v0.1.0p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-PIMU ------------ Installed Firmware: Pimu.v0.0.2p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-MOTOR-ARM ------------ Installed Firmware: Stepper.v0.1.0p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-MOTOR-LEFT-WHEEL ------------ Installed Firmware: Stepper.v0.1.0p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID ------------ HELLO-MOTOR-RIGHT-WHEEL ------------ Installed Firmware: Stepper.v0.1.0p1 Installed Stretch Body supports protocols: p0 , p1 Installed protocol p1 : VALID Running REx_firmware_updater.py --available will list the available versions on GitHub: >>$ REx_firmware_updater.py --available Collecting information... ####################### Currently Tagged Versions of Stretch Firmware on Master Branch ####################### ---- HELLO-WACC ---- Wacc.v0.0.1p0 ---- HELLO-MOTOR-LIFT ---- Stepper.v0.0.1p0 Stepper.v0.0.2p0 Stepper.v0.0.3p0 Stepper.v0.0.4p0 ---- HELLO-PIMU ---- Pimu.v0.0.1p0 ---- HELLO-MOTOR-ARM ---- Stepper.v0.0.1p0 Stepper.v0.0.2p0 Stepper.v0.0.3p0 Stepper.v0.0.4p0 ---- HELLO-MOTOR-LEFT-WHEEL ---- Stepper.v0.0.1p0 Stepper.v0.0.2p0 Stepper.v0.0.3p0 Stepper.v0.0.4p0 ---- HELLO-MOTOR-RIGHT-WHEEL ---- Stepper.v0.0.1p0 Stepper.v0.0.2p0 Stepper.v0.0.3p0 Stepper.v0.0.4p0 All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Other Useful Commands"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/","text":"Emulating the Wacc In this tutorial we describe how to emulate the Arduino based Stretch Wacc (Wrist + Accelerometer) board using an off-the-shelf Arduino. This enables users to develop and test custom code for the Wacc board prior to deploying it to the actual robot hardware. Emulation involves: Installing the Wacc firmware on an Arduino Zero compatible board ( Adafruit Metro M0 Express ) Mapping the UDEV rules to the Metro This tutorial assumes the Metro board is plugged into one of Stretch's external USB ports. It is also possible to install Stretch Body on a developer Ubuntu machine if the robot is not available (see below). Hardware We will emulate the Wacc on the Metro board as both boards use the Atmel SAMD21G18A-AUT processor. The primary difference between the Wacc and the Metro is that the Wacc integrates an ADXL343 3 axis accelerometer on its I2C bus. As this chip isn't interfaced to our emulation hardware, this data will be missing. The Wacc also lacks the On/Off switch and RGB LED of the Metro. Only a subset of its pins of the Wacc (shown below) are used compared to the Metro. You can attach your custom hardware to the Metro as if it were the Stretch Expansion Header. Firmware First, we'll install the factory Wacc firmware on the Metro. Custom Wacc firmware could be installed using the same process. First, download the firmware repo onto the development machine if it isn't already there: >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_firmware Next, install and setup the Arduino IDE has described here . Now, with the Metro M0 plugged in to a USB port on Stretch: Open the hello_wacc sketch Select Tools/Board/Hello Wacc Select the Metro board under Tools/Port Upload the firmware. The red LED on the Metro should flash at 1Hz. Note: Take care to not accidentally burn firmware to the wrong board. You can check the mapping of boards to ports by: >>$ ls -l /dev/hello* Setup UDEV Next we need for the Metro to appear as a device named /dev/hello-wacc . For this: Run 'sudo dmesg -c' to clear the system log Hit the reset button on the Metro Run 'sudo dmesg | grep Serial You'll see the Metro serial number: >>$ sudo dmesg -c | grep Serial [ 810971 .303719 ] usb 1 -3: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 3 [ 810971 .303725 ] usb 1 -3: SerialNumber: 9261CC655150484735202020FF0C270C Copy the serial number. Edit the udev file: >>$ sudo nano /etc/udev/rules.d/95-hello-arduino.rules Find the entry for hello-wacc and update ATTRS{serial} to the new serial number. KERNEL == \"ttyACM*\" , ATTRS { idVendor }== \"2341\" , ATTRS { idProduct }== \"804d\" ,MODE: = \"0666\" , ATTRS { serial }== \"SERIAL\" , SYMLINK += \"hello-wacc\" , ENV { ID_MM_DEVICE_IGNORE }= \"1\" , where SERIAL is the serial number captured from dmesg. Now reload the udev rule >>$ sudo udevadm control --reload Now hit the reset button of the Metro and check that it is mapped: >>$ ls -l /dev/hello-wacc lrwxrwxrwx 1 root root 7 Jun 10 20 :04 /dev/hello-wacc -> ttyACM0 Note: When switching back to Stretch's internal Wacc board you'll need to restore the UDEV file. This can be done by: >>$ sudo cp $HELLO_FLEET_PATH / $HELLO_FLEET_ID /udev/95-hello-arduino.rules /etc/udev/rules.d/ Test the Emulated Wacc Now, check that your emulated Wacc is working. You should see the Timestamp and A0 values moving when you hit 'Enter'. Now you're ready to integrate your custom hardware. >>$ stretch_wacc_jog.py ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- ------------------------------ Ax ( m/s^2 ) 0 .0 Ay ( m/s^2 ) 0 .0 Az ( m/s^2 ) 0 .0 A0 375 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 0 State 0 Debug 0 Timestamp 1591846120 .85 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1p0 All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Wacc Emulaton"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#emulating-the-wacc","text":"In this tutorial we describe how to emulate the Arduino based Stretch Wacc (Wrist + Accelerometer) board using an off-the-shelf Arduino. This enables users to develop and test custom code for the Wacc board prior to deploying it to the actual robot hardware. Emulation involves: Installing the Wacc firmware on an Arduino Zero compatible board ( Adafruit Metro M0 Express ) Mapping the UDEV rules to the Metro This tutorial assumes the Metro board is plugged into one of Stretch's external USB ports. It is also possible to install Stretch Body on a developer Ubuntu machine if the robot is not available (see below).","title":"Emulating the Wacc"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#hardware","text":"We will emulate the Wacc on the Metro board as both boards use the Atmel SAMD21G18A-AUT processor. The primary difference between the Wacc and the Metro is that the Wacc integrates an ADXL343 3 axis accelerometer on its I2C bus. As this chip isn't interfaced to our emulation hardware, this data will be missing. The Wacc also lacks the On/Off switch and RGB LED of the Metro. Only a subset of its pins of the Wacc (shown below) are used compared to the Metro. You can attach your custom hardware to the Metro as if it were the Stretch Expansion Header.","title":"Hardware"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#firmware","text":"First, we'll install the factory Wacc firmware on the Metro. Custom Wacc firmware could be installed using the same process. First, download the firmware repo onto the development machine if it isn't already there: >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_firmware Next, install and setup the Arduino IDE has described here . Now, with the Metro M0 plugged in to a USB port on Stretch: Open the hello_wacc sketch Select Tools/Board/Hello Wacc Select the Metro board under Tools/Port Upload the firmware. The red LED on the Metro should flash at 1Hz. Note: Take care to not accidentally burn firmware to the wrong board. You can check the mapping of boards to ports by: >>$ ls -l /dev/hello*","title":"Firmware"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#setup-udev","text":"Next we need for the Metro to appear as a device named /dev/hello-wacc . For this: Run 'sudo dmesg -c' to clear the system log Hit the reset button on the Metro Run 'sudo dmesg | grep Serial You'll see the Metro serial number: >>$ sudo dmesg -c | grep Serial [ 810971 .303719 ] usb 1 -3: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 3 [ 810971 .303725 ] usb 1 -3: SerialNumber: 9261CC655150484735202020FF0C270C Copy the serial number. Edit the udev file: >>$ sudo nano /etc/udev/rules.d/95-hello-arduino.rules Find the entry for hello-wacc and update ATTRS{serial} to the new serial number. KERNEL == \"ttyACM*\" , ATTRS { idVendor }== \"2341\" , ATTRS { idProduct }== \"804d\" ,MODE: = \"0666\" , ATTRS { serial }== \"SERIAL\" , SYMLINK += \"hello-wacc\" , ENV { ID_MM_DEVICE_IGNORE }= \"1\" , where SERIAL is the serial number captured from dmesg. Now reload the udev rule >>$ sudo udevadm control --reload Now hit the reset button of the Metro and check that it is mapped: >>$ ls -l /dev/hello-wacc lrwxrwxrwx 1 root root 7 Jun 10 20 :04 /dev/hello-wacc -> ttyACM0 Note: When switching back to Stretch's internal Wacc board you'll need to restore the UDEV file. This can be done by: >>$ sudo cp $HELLO_FLEET_PATH / $HELLO_FLEET_ID /udev/95-hello-arduino.rules /etc/udev/rules.d/","title":"Setup UDEV"},{"location":"stretch-firmware/docs/tutorial_wacc_emulation/#test-the-emulated-wacc","text":"Now, check that your emulated Wacc is working. You should see the Timestamp and A0 values moving when you hit 'Enter'. Now you're ready to integrate your custom hardware. >>$ stretch_wacc_jog.py ------ MENU ------- m: menu r: reset board a: set D2 on b: set D2 off c: set D3 on d: set D3 off ------------------- ------------------------------ Ax ( m/s^2 ) 0 .0 Ay ( m/s^2 ) 0 .0 Az ( m/s^2 ) 0 .0 A0 375 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 0 State 0 Debug 0 Timestamp 1591846120 .85 Board version: Wacc.Guthrie.V1 Firmware version: Wacc.v0.0.1p0 All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Test the Emulated Wacc"},{"location":"stretch-firmware/docs/tutorials_overview/","text":"Integrating Custom Hardware These tutorials demonstrate how to use the Stretch wrist expansion header to integrate custom hardware into Stretch Body . NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board. Tutorial Description Data Transfer How to plumb your custom data from the wrist Arduino through Stretch Body Wrist Board Emulation How to emulate the Stretch wrist board using an off the shelf Arduino SPI Sensor How to integrate an SPI sensor I2C Sensor How to integrate an I2C sensor Serial Sensor How to integrate a Serial UART sensor","title":"Overview"},{"location":"stretch-firmware/docs/tutorials_overview/#integrating-custom-hardware","text":"These tutorials demonstrate how to use the Stretch wrist expansion header to integrate custom hardware into Stretch Body . NOTE It is possible to brick the Wacc board by incorrectly configuring the hardware peripherals of the SAMD uC. Therefore, when integrating your custom hardware into the Wacc we strongly recommend emulating the Wacc board until the functionality is complete. The tutorial Wacc Emulation describes how to configure an Adafruit Metro M0 Express to behave as a stand-in for a Wacc board. Tutorial Description Data Transfer How to plumb your custom data from the wrist Arduino through Stretch Body Wrist Board Emulation How to emulate the Stretch wrist board using an off the shelf Arduino SPI Sensor How to integrate an SPI sensor I2C Sensor How to integrate an I2C sensor Serial Sensor How to integrate a Serial UART sensor","title":"Integrating Custom Hardware"},{"location":"stretch-hardware-guides/","text":"Overview The Stretch Hardware Guides repository maintains documentation on the use and specifications of the Stretch RE1 & RE2 hardware. Stretch RE2 Stretch RE1 Model Guide Description RE2 Safety Guide Safety guide for users of the Stretch RE2 Battery Maintenance Guide Guide to care for and charge the Stretch RE2 Batteries RE2 Hardware Guide Specification and functional description of the Stretch RE2 Hardware RE2 Dex Wrist Guide Installing, configuring, and working with the Stretch RE2 Dex Wrist Model Guide Description RE1 Safety Guide Safety guide for users of the Stretch RE1 Battery Maintenance Guide Guide to care for and charge the Stretch RE1 Batteries RE1 Hardware Guide Specification and functional description of the Stretch RE1 Hardware RE1 Dex Wrist Guide Installing, configuring, and working with the Stretch RE1 Dex Wrist License For details, see the LICENSE.md file in the root directory. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Overview"},{"location":"stretch-hardware-guides/#overview","text":"The Stretch Hardware Guides repository maintains documentation on the use and specifications of the Stretch RE1 & RE2 hardware. Stretch RE2 Stretch RE1 Model Guide Description RE2 Safety Guide Safety guide for users of the Stretch RE2 Battery Maintenance Guide Guide to care for and charge the Stretch RE2 Batteries RE2 Hardware Guide Specification and functional description of the Stretch RE2 Hardware RE2 Dex Wrist Guide Installing, configuring, and working with the Stretch RE2 Dex Wrist Model Guide Description RE1 Safety Guide Safety guide for users of the Stretch RE1 Battery Maintenance Guide Guide to care for and charge the Stretch RE1 Batteries RE1 Hardware Guide Specification and functional description of the Stretch RE1 Hardware RE1 Dex Wrist Guide Installing, configuring, and working with the Stretch RE1 Dex Wrist","title":"Overview"},{"location":"stretch-hardware-guides/#license","text":"For details, see the LICENSE.md file in the root directory. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"License"},{"location":"stretch-hardware-guides/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains documentation exclusively for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Creative Commons Attribution-NoDerivatives 4.0 International (CC BY-ND 4.0) license (the \"License\"); you may not use the Contents except in compliance with the License. You may obtain a copy of the License at https://creativecommons.org/licenses/by-nd/4.0 Unless required by applicable law or agreed to in writing, the Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Patents pending and trademark rights cover the Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\" For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/","text":"Stretch RE1 - Battery Maintenance Guide Overview Stretch RE1 utilizes two 12V AGM SLA batteries that provide a combined 18AH of capacity. Maintaining an adequate level of charge on the battery system will enhance the battery lifetime. The run time for a fully charged system is dependent on the load use case. The majority of battery power is consumed by the NUC computer as Stretch uses relatively low power motors. A fully charged robot running a high CPU load can run approximately 2 hours before requiring a recharge. A fully charged robot powered on but with minimal load can run approximately 5 hours before requiring a recharge. Charger Stretch ships with a NOCO Genius 10 charger. Earlier versions of Stretch use the NOCO G7200. These two chargers are functionally very similar. Please review the battery charger user manuals prior to following the guidance in this document. Genius 10 Manual G7200 Manual Stretch utilizes four of the available modes on these chargers. Mode Function STANDBY Charger not charging the robot 12V AGM Charging robot powered down SUPPLY 1) Power the robot during tethered use 2) Repair damaged batteries. REPAIR Repair damaged batteries. === \"Unordered list\" * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci === \"Ordered list\" 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci NOCO Genius 10 - Interface Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached 2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached 2) Press and hold MODE button for 3s 3) Press MODE button until SUPPLY indicator is illuminated 4) Attach charger REPAIR 1) From STANDBY, charger attached 2) Press and hold MODE button for 3s 3) Press MODE button until REPAIR indicator is illuminated NOCO G7200 - Interface Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached 2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached 2) Press and hold MODE button for 3s 3) Press MODE button until SUPPLY indicator is illuminated 4) Attach charger REPAIR 1) From STANDBY, charger attached 2) Press and hold MODE button for 3s 3) Press MODE button until REPAIR indicator is illuminated Charging Best Practices It is possible to accidentally deeply discharge the batteries by leaving the robot on for long durations without the charger attached. This is similar to leaving the lights on your car where the battery will continue to drain until fully discharged. We recommend following the best practices below to avoid deep discharge of the batteries and to ensure they have a long lifespan. Use Case Best Practice Reason Robot is in use - tethered Leave the charger attached in SUPPLY mode while developing on the robot whenever possible. Shutdown and power off the robot when development is done. Running the robot while attempting to charge in 12V AGM mode can cause issues and is generally bad for battery health. SUPPLY mode is preferred whenever the robot needs to be powered on. Robot is in use - untethered Regularly check the battery voltage using the command line tool. Shutdown the computer and power off the robot when voltage falls below 11.5V. Attach the charger in 12V AGM mode. Charge to 100% before resuming operation. The 12V AGM charge mode expects the battery voltage to be above 10.5-11V in order to operate. Robot is not in use Shutdown the computer and turn off the robot power. Leave the charger attached and place it in 12V AGM mode. Leaving the robot power on may cause the batteries to deep discharge The charger will maintain a \u2018trickle charge\u2019 on the battery, keeping the charge at 100%. Robot is coming out of storage Attach charger in 12V AGM mode and charge for 2-3 hours until charger reports 100% SLA batteries naturally lose charge over time due to \u2018self-discharge\u2019. When To Plug in the Charger We recommend keeping the charger attached whenever the robot is not running untethered. When the battery voltage drops below \u2018low voltage\u2019 threshold the robot will produce an intermittent double beep sound. This is a reminder to the user to plug in the charger. If desired, the intermittent beep functionality can be disabled by setting the stop_at_low_voltage field in the User YAML to 0 . Troubleshooting Issue How to Diagnose Cause Corrective Procedure Robot shows no power on activity Nothing happens when you toggle on the robot\u2019s power switch. There is no visible illumination of LEDs, motion of the laser range finder, or audible noise of the robot fans. The robot fuse may have blown. When the batteries drain the current required to maintain power goes up, which can ultimately blow the fuse. Proceed to \u201cChanging the Fuse\u201d steps below Robot powers on momentarily When you toggle on the robot\u2019s power switch some activity occurs (illumination of LEDs, audible noise of robot fans, etc) but the computer fails to boot. The battery voltage is too low to maintain power. As the power draw increases during power-on, the voltage dips and causes the system to shut down. Connect the battery charger in 12V AGM mode and leave until fully charged. Battery won\u2019t charge in 12V AGM mode When the robot is powered down and the charger is connected in 12V AGM mode, the charger eventually switches to a different mode. The battery voltage is too low for the charger to function correctly in normal operation. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger reports 100% charge but the batteries are discharged When the robot is powered down and the charger is connected in 12V AGM mode, the charger status shows 100%. However the robot fails to turn on properly. Damage to the batteries (usually caused by excessively low voltage) may artificially raise the open circuit voltage of the battery, causing the battery to appear fully charged, while providing low capacity. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger will not charge or stay in any mode. When placed in 12V AGM, SUPPLY, or REPAIR mode, it continually reverts to STANDBY mode after ~ 20 minutes. Charger may be defective. Contact Hello Robot Support for a replacement. Recovering from Low Battery Voltage Turn off the robot power switch and detach the charger from the robot Place charger in SUPPLY Mode Allow robot to charge for 4-8 hours, or up to 24 hours for extreme discharge Switch the charger to 12V AGM mode Charge until at 100% Additional Information Powering Down the Robot The recommended power down procedure is Place a clamp on the mast below the shoulder to prevent dropping Shutdown the computer from the Desktop or via SSH When the laser range finder has stopped spinning, turn off the main power switch Replacing the Fuse Stretch RE1 has an automotive fuse inside the base that may need to be replaced. The type of fuse depends on your build version of the RE1 Build Version Fuse Type Recommended Fuse Guthrie 8A 5x20mm Fast Blow Glass Bussman S505-8-R Hank and later 7.5A ATM Fast Blow Blade Bussman VP/ATM-7-1/2-RP The fuse location is shown below. For guidance on replacing the fuse, contact Hello Robot support: support@hello-robot.com . Checking the Battery Charge The battery charger LEDs provide an approximate indicator of battery charge when it is in 12V AGM mode. Checking the Battery Voltage Battery voltage is not always an accurate indicator of battery charge but it can be a useful proxy. A charged battery will typically report a voltage of 12-12.8V and will maintain that voltage across load conditions. Meanwhile, a partially charged battery may report anywhere from 10-12.8V but its voltage will drop rapidly when loaded. Measuring Battery Voltage from the Command Line The battery voltage and current draw can be checked from the command line: $ stretch_robot_battery_check.py [Pass] Voltage with 12.9889035225 [Pass] Current with 2.46239192784 [Pass] CPU Temp with 56.0 Measuring Battery Voltage with a DMM When troubleshooting a deeply discharged battery it may be useful to directly measure the battery voltage with a digital multimeter (DMM). To do this we recommend detaching the charger cable at its inline connector and applying the DMM to the connector contacts as shown. NOTE: Caution should be taken as it is possible to short the battery when doing this. Repairing Damaged Batteries It is possible for Stretch's batteries to become damaged due to repeated deep discharge. If the robot has continued issues maintaining a charge we recommend attempting the following procedure: Turn off the robot power switch and detach the charger from the robot Place charger in SUPPLY Mode Attach the charger and allow robot to charge for 4-8 hours Place the charger in REPAIR mode Allow robot to charge until the repair cycle completes and the charger returns to standby - up to 4 hours Place the charger back in 12V AGM mode and allow batteries to charger to 100% Replacing Dead Batteries It is possible for a mechanically skilled person to replace the Stretch batteries should it be necessary . Please contact Hello Robot Support for more information (support@hello-robot.com) All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Battery Maintenance Guide"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#stretch-re1-battery-maintenance-guide","text":"","title":"Stretch RE1 - Battery Maintenance Guide"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#overview","text":"Stretch RE1 utilizes two 12V AGM SLA batteries that provide a combined 18AH of capacity. Maintaining an adequate level of charge on the battery system will enhance the battery lifetime. The run time for a fully charged system is dependent on the load use case. The majority of battery power is consumed by the NUC computer as Stretch uses relatively low power motors. A fully charged robot running a high CPU load can run approximately 2 hours before requiring a recharge. A fully charged robot powered on but with minimal load can run approximately 5 hours before requiring a recharge.","title":"Overview"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#charger","text":"Stretch ships with a NOCO Genius 10 charger. Earlier versions of Stretch use the NOCO G7200. These two chargers are functionally very similar. Please review the battery charger user manuals prior to following the guidance in this document. Genius 10 Manual G7200 Manual Stretch utilizes four of the available modes on these chargers. Mode Function STANDBY Charger not charging the robot 12V AGM Charging robot powered down SUPPLY 1) Power the robot during tethered use 2) Repair damaged batteries. REPAIR Repair damaged batteries. === \"Unordered list\" * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci === \"Ordered list\" 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci","title":"Charger"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#noco-genius-10-interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached 2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached 2) Press and hold MODE button for 3s 3) Press MODE button until SUPPLY indicator is illuminated 4) Attach charger REPAIR 1) From STANDBY, charger attached 2) Press and hold MODE button for 3s 3) Press MODE button until REPAIR indicator is illuminated","title":"NOCO Genius 10 - Interface"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#noco-g7200-interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached 2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached 2) Press and hold MODE button for 3s 3) Press MODE button until SUPPLY indicator is illuminated 4) Attach charger REPAIR 1) From STANDBY, charger attached 2) Press and hold MODE button for 3s 3) Press MODE button until REPAIR indicator is illuminated","title":"NOCO G7200 - Interface"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#charging-best-practices","text":"It is possible to accidentally deeply discharge the batteries by leaving the robot on for long durations without the charger attached. This is similar to leaving the lights on your car where the battery will continue to drain until fully discharged. We recommend following the best practices below to avoid deep discharge of the batteries and to ensure they have a long lifespan. Use Case Best Practice Reason Robot is in use - tethered Leave the charger attached in SUPPLY mode while developing on the robot whenever possible. Shutdown and power off the robot when development is done. Running the robot while attempting to charge in 12V AGM mode can cause issues and is generally bad for battery health. SUPPLY mode is preferred whenever the robot needs to be powered on. Robot is in use - untethered Regularly check the battery voltage using the command line tool. Shutdown the computer and power off the robot when voltage falls below 11.5V. Attach the charger in 12V AGM mode. Charge to 100% before resuming operation. The 12V AGM charge mode expects the battery voltage to be above 10.5-11V in order to operate. Robot is not in use Shutdown the computer and turn off the robot power. Leave the charger attached and place it in 12V AGM mode. Leaving the robot power on may cause the batteries to deep discharge The charger will maintain a \u2018trickle charge\u2019 on the battery, keeping the charge at 100%. Robot is coming out of storage Attach charger in 12V AGM mode and charge for 2-3 hours until charger reports 100% SLA batteries naturally lose charge over time due to \u2018self-discharge\u2019.","title":"Charging Best Practices"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#when-to-plug-in-the-charger","text":"We recommend keeping the charger attached whenever the robot is not running untethered. When the battery voltage drops below \u2018low voltage\u2019 threshold the robot will produce an intermittent double beep sound. This is a reminder to the user to plug in the charger. If desired, the intermittent beep functionality can be disabled by setting the stop_at_low_voltage field in the User YAML to 0 .","title":"When To Plug in the Charger"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#troubleshooting","text":"Issue How to Diagnose Cause Corrective Procedure Robot shows no power on activity Nothing happens when you toggle on the robot\u2019s power switch. There is no visible illumination of LEDs, motion of the laser range finder, or audible noise of the robot fans. The robot fuse may have blown. When the batteries drain the current required to maintain power goes up, which can ultimately blow the fuse. Proceed to \u201cChanging the Fuse\u201d steps below Robot powers on momentarily When you toggle on the robot\u2019s power switch some activity occurs (illumination of LEDs, audible noise of robot fans, etc) but the computer fails to boot. The battery voltage is too low to maintain power. As the power draw increases during power-on, the voltage dips and causes the system to shut down. Connect the battery charger in 12V AGM mode and leave until fully charged. Battery won\u2019t charge in 12V AGM mode When the robot is powered down and the charger is connected in 12V AGM mode, the charger eventually switches to a different mode. The battery voltage is too low for the charger to function correctly in normal operation. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger reports 100% charge but the batteries are discharged When the robot is powered down and the charger is connected in 12V AGM mode, the charger status shows 100%. However the robot fails to turn on properly. Damage to the batteries (usually caused by excessively low voltage) may artificially raise the open circuit voltage of the battery, causing the battery to appear fully charged, while providing low capacity. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger will not charge or stay in any mode. When placed in 12V AGM, SUPPLY, or REPAIR mode, it continually reverts to STANDBY mode after ~ 20 minutes. Charger may be defective. Contact Hello Robot Support for a replacement.","title":"Troubleshooting"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#recovering-from-low-battery-voltage","text":"Turn off the robot power switch and detach the charger from the robot Place charger in SUPPLY Mode Allow robot to charge for 4-8 hours, or up to 24 hours for extreme discharge Switch the charger to 12V AGM mode Charge until at 100%","title":"Recovering from Low Battery Voltage"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#additional-information","text":"","title":"Additional Information"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#powering-down-the-robot","text":"The recommended power down procedure is Place a clamp on the mast below the shoulder to prevent dropping Shutdown the computer from the Desktop or via SSH When the laser range finder has stopped spinning, turn off the main power switch","title":"Powering Down the Robot"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#replacing-the-fuse","text":"Stretch RE1 has an automotive fuse inside the base that may need to be replaced. The type of fuse depends on your build version of the RE1 Build Version Fuse Type Recommended Fuse Guthrie 8A 5x20mm Fast Blow Glass Bussman S505-8-R Hank and later 7.5A ATM Fast Blow Blade Bussman VP/ATM-7-1/2-RP The fuse location is shown below. For guidance on replacing the fuse, contact Hello Robot support: support@hello-robot.com .","title":"Replacing the Fuse"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#checking-the-battery-charge","text":"The battery charger LEDs provide an approximate indicator of battery charge when it is in 12V AGM mode.","title":"Checking the Battery Charge"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#checking-the-battery-voltage","text":"Battery voltage is not always an accurate indicator of battery charge but it can be a useful proxy. A charged battery will typically report a voltage of 12-12.8V and will maintain that voltage across load conditions. Meanwhile, a partially charged battery may report anywhere from 10-12.8V but its voltage will drop rapidly when loaded. Measuring Battery Voltage from the Command Line The battery voltage and current draw can be checked from the command line: $ stretch_robot_battery_check.py [Pass] Voltage with 12.9889035225 [Pass] Current with 2.46239192784 [Pass] CPU Temp with 56.0 Measuring Battery Voltage with a DMM When troubleshooting a deeply discharged battery it may be useful to directly measure the battery voltage with a digital multimeter (DMM). To do this we recommend detaching the charger cable at its inline connector and applying the DMM to the connector contacts as shown. NOTE: Caution should be taken as it is possible to short the battery when doing this.","title":"Checking the Battery Voltage"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#repairing-damaged-batteries","text":"It is possible for Stretch's batteries to become damaged due to repeated deep discharge. If the robot has continued issues maintaining a charge we recommend attempting the following procedure: Turn off the robot power switch and detach the charger from the robot Place charger in SUPPLY Mode Attach the charger and allow robot to charge for 4-8 hours Place the charger in REPAIR mode Allow robot to charge until the repair cycle completes and the charger returns to standby - up to 4 hours Place the charger back in 12V AGM mode and allow batteries to charger to 100%","title":"Repairing Damaged Batteries"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re1/#replacing-dead-batteries","text":"It is possible for a mechanically skilled person to replace the Stretch batteries should it be necessary . Please contact Hello Robot Support for more information (support@hello-robot.com) All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Replacing Dead Batteries"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/","text":"Stretch RE2 - Battery Maintenance Guide Overview Stretch RE2 utilizes two 12V AGM SLA batteries that provide a combined 18AH of capacity. Maintaining an adequate level of charge on the battery system will enhance the battery lifetime. The run time for a fully charged system is dependent on the load use case. The majority of battery power is consumed by the NUC computer as Stretch uses relatively low power motors. A fully charged robot running a high CPU load can run approximately 2 hours before requiring a recharge. A fully charged robot powered on but with minimal load can run approximately 5 hours before requiring a recharge. State of Battery Charge An accurate measure of the battery charge isn't available on Stretch (as this requires a coulomb counting system). A coarse approximation of battery charge is given by the battery voltage. For RE2 during normal operation with a moderate load the relationship between voltage and charge is roughly: Voltage Charge LED Light Bar 11.5 - 13.4V 80%+ Bright green 11.0 - 11.5V 60%+ Yellow 10.5 - 11.0V 40%+ Orange Below 10.5V <40% Red As shown below, the LED lightbar color provides an indication of the battery voltage. When To Plug in the Charger We recommend plugging in the charger whenever: The battery voltage is below 11V (lightbar is orange). The robot is not running untethered The robot is producing a periodic beeping sound Accidental Full Discharge Stretch includes a few feature to help prevent accidental full discharge (for example, if the robot is left on overnight not plugged to its charger) Audible warning : Stretch will beep every 2 seconds if the battery voltage drops below 10.5V Hard power off : Stretch will completely power off if the battery voltage drops below 9.75V Charger Stretch ships with a NOCO Genius 10 charger. Please review the battery charger user manual prior to following the guidance in this document. Genius 10 Manual Stretch utilizes four of the available modes on these chargers. Mode Function STANDBY Charger not charging the robot 12V AGM Charging robot powered down SUPPLY 1) Power the robot during tethered use 2) Repair damaged batteries. REPAIR Repair damaged batteries. NOCO Genius 10 - Interface Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached 2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached 2) Press and hold MODE button for 3s 3) Press MODE button until SUPPLY indicator is illuminated 4) Attach charger REPAIR 1) From STANDBY, charger attached 2) Press and hold MODE button for 3s 3) Press MODE button until REPAIR indicator is illuminated Charging Best Practices It is possible to accidentally deeply discharge the batteries by leaving the robot on for long durations without the charger attached. This is similar to leaving the lights on your car where the battery will continue to drain until fully discharged. We recommend following the best practices below to avoid deep discharge of the batteries and to ensure they have a long lifespan. Use Case Best Practice Reason Robot is in use - tethered Leave the charger attached in SUPPLY mode while developing on the robot whenever possible. Shutdown and power off the robot when development is done. Running the robot while attempting to charge in 12V AGM mode can cause issues and is generally bad for battery health. SUPPLY mode is preferred whenever the robot needs to be powered on. Robot is in use - untethered Regularly check the battery voltage using the command line tool. Shutdown the computer and power off the robot when voltage falls below 11.5V. Attach the charger in 12V AGM mode. Charge to 100% before resuming operation. The 12V AGM charge mode expects the battery voltage to be above 10.5-11V in order to operate. Robot is not in use Shutdown the computer and turn off the robot power. Leave the charger attached and place it in 12V AGM mode. Leaving the robot power on may cause the batteries to deep discharge The charger will maintain a \u2018trickle charge\u2019 on the battery, keeping the charge at 100%. Robot is coming out of storage Attach charger in 12V AGM mode and charge for 2-3 hours until charger reports 100% SLA batteries naturally lose charge over time due to \u2018self-discharge\u2019. Troubleshooting Issue How to Diagnose Cause Corrective Procedure Robot shows no power on activity Nothing happens when you toggle on the robot\u2019s power switch. There is no visible illumination of LEDs, motion of the laser range finder, or audible noise of the robot fans. The robot fuse may have blown. When the batteries drain the current required to maintain power goes up, which can ultimately blow the fuse. Proceed to \u201cChanging the Fuse\u201d steps below Robot powers on momentarily When you toggle on the robot\u2019s power switch some activity occurs (illumination of LEDs, audible noise of robot fans, etc) but the computer fails to boot. The battery voltage is too low to maintain power. As the power draw increases during power-on, the voltage dips and causes the system to shut down. Connect the battery charger in 12V AGM mode and leave until fully charged. Battery won\u2019t charge in 12V AGM mode When the robot is powered down and the charger is connected in 12V AGM mode, the charger eventually switches to a different mode. The battery voltage is too low for the charger to function correctly in normal operation. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger reports 100% charge but the batteries are discharged When the robot is powered down and the charger is connected in 12V AGM mode, the charger status shows 100%. However the robot fails to turn on properly. Damage to the batteries (usually caused by excessively low voltage) may artificially raise the open circuit voltage of the battery, causing the battery to appear fully charged, while providing low capacity. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger will not charge or stay in any mode. When placed in 12V AGM, SUPPLY, or REPAIR mode, it continually reverts to STANDBY mode after ~ 20 minutes. Charger may be defective. Contact Hello Robot Support for a replacement. Recovering from Low Battery Voltage Turn off the robot power switch and detach the charger from the robot Place charger in SUPPLY Mode Allow robot to charge for 4-8 hours, or up to 24 hours for extreme discharge Switch the charger to 12V AGM mode Charge until at 100% Additional Information Powering Down the Robot The recommended power down procedure is Place a clamp on the mast below the shoulder to prevent dropping Shutdown the computer from the Desktop or via SSH When the laser range finder has stopped spinning, turn off the main power switch Replacing the Fuse Stretch RE2 has two automotive fuses inside the base that may need to be replaced. Fuse Type Recommended Fuse 7.5A ATM Fast Blow Blade Bussman VP/ATM-7-1/2-RP The fuse locations are shown below. For guidance on replacing the fuse, contact Hello Robot support: support@hello-robot.com . Checking the Battery Charge The battery charger LEDs provide an approximate indicator of battery charge when it is in 12V AGM mode. Checking the Battery Voltage Battery voltage is not always an accurate indicator of battery charge but it can be a useful proxy. A charged battery will typically report a voltage of 12-12.8V and will maintain that voltage across load conditions. Meanwhile, a partially charged battery may report anywhere from 10-12.8V but its voltage will drop rapidly when loaded. Measuring Battery Voltage from the Command Line The battery voltage and current draw can be checked from the command line: $ stretch_robot_battery_check.py [Pass] Voltage with 12.9889035225 [Pass] Current with 2.46239192784 [Pass] CPU Temp with 56.0 Measuring Battery Voltage with a DMM When troubleshooting a deeply discharged battery it may be useful to directly measure the battery voltage with a digital multimeter (DMM). To do this we recommend Detach the charger cable at its inline connector Apply the DMM to the connector contacts as shown Plug the charge cable into the charge port of the robot NOTE: Caution should be taken as it is possible to short the battery when doing this. Repairing Damaged Batteries It is possible for Stretch's batteries to become damaged due to repeated deep discharge. If the robot has continued issues maintaining a charge we recommend attempting the following procedure: Turn off the robot power switch and detach the charger from the robot Place charger in SUPPLY Mode Attach the charger and allow robot to charge for 4-8 hours Place the charger in REPAIR mode Allow robot to charge until the repair cycle completes and the charger returns to standby - up to 4 hours Place the charger back in 12V AGM mode and allow batteries to charger to 100% Replacing Damaged Batteries It is possible for a mechanically skilled person to replace the Stretch batteries should it be necessary . Please contact Hello Robot Support for more information (support@hello-robot.com) All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Battery Maintenance Guide"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#stretch-re2-battery-maintenance-guide","text":"","title":"Stretch RE2 - Battery Maintenance Guide"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#overview","text":"Stretch RE2 utilizes two 12V AGM SLA batteries that provide a combined 18AH of capacity. Maintaining an adequate level of charge on the battery system will enhance the battery lifetime. The run time for a fully charged system is dependent on the load use case. The majority of battery power is consumed by the NUC computer as Stretch uses relatively low power motors. A fully charged robot running a high CPU load can run approximately 2 hours before requiring a recharge. A fully charged robot powered on but with minimal load can run approximately 5 hours before requiring a recharge.","title":"Overview"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#state-of-battery-charge","text":"An accurate measure of the battery charge isn't available on Stretch (as this requires a coulomb counting system). A coarse approximation of battery charge is given by the battery voltage. For RE2 during normal operation with a moderate load the relationship between voltage and charge is roughly: Voltage Charge LED Light Bar 11.5 - 13.4V 80%+ Bright green 11.0 - 11.5V 60%+ Yellow 10.5 - 11.0V 40%+ Orange Below 10.5V <40% Red As shown below, the LED lightbar color provides an indication of the battery voltage.","title":"State of Battery Charge"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#when-to-plug-in-the-charger","text":"We recommend plugging in the charger whenever: The battery voltage is below 11V (lightbar is orange). The robot is not running untethered The robot is producing a periodic beeping sound","title":"When To Plug in the Charger"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#accidental-full-discharge","text":"Stretch includes a few feature to help prevent accidental full discharge (for example, if the robot is left on overnight not plugged to its charger) Audible warning : Stretch will beep every 2 seconds if the battery voltage drops below 10.5V Hard power off : Stretch will completely power off if the battery voltage drops below 9.75V","title":"Accidental Full Discharge"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#charger","text":"Stretch ships with a NOCO Genius 10 charger. Please review the battery charger user manual prior to following the guidance in this document. Genius 10 Manual Stretch utilizes four of the available modes on these chargers. Mode Function STANDBY Charger not charging the robot 12V AGM Charging robot powered down SUPPLY 1) Power the robot during tethered use 2) Repair damaged batteries. REPAIR Repair damaged batteries.","title":"Charger"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#noco-genius-10-interface","text":"Mode Procedure STANDBY Illuminates when charger not charging 12V AGM 1) From STANDBY, charger attached 2) Press MODE button repeatedly until 12V AGM indicator is illuminated SUPPLY 1) From STANDBY, charger not attached 2) Press and hold MODE button for 3s 3) Press MODE button until SUPPLY indicator is illuminated 4) Attach charger REPAIR 1) From STANDBY, charger attached 2) Press and hold MODE button for 3s 3) Press MODE button until REPAIR indicator is illuminated","title":"NOCO Genius 10 - Interface"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#charging-best-practices","text":"It is possible to accidentally deeply discharge the batteries by leaving the robot on for long durations without the charger attached. This is similar to leaving the lights on your car where the battery will continue to drain until fully discharged. We recommend following the best practices below to avoid deep discharge of the batteries and to ensure they have a long lifespan. Use Case Best Practice Reason Robot is in use - tethered Leave the charger attached in SUPPLY mode while developing on the robot whenever possible. Shutdown and power off the robot when development is done. Running the robot while attempting to charge in 12V AGM mode can cause issues and is generally bad for battery health. SUPPLY mode is preferred whenever the robot needs to be powered on. Robot is in use - untethered Regularly check the battery voltage using the command line tool. Shutdown the computer and power off the robot when voltage falls below 11.5V. Attach the charger in 12V AGM mode. Charge to 100% before resuming operation. The 12V AGM charge mode expects the battery voltage to be above 10.5-11V in order to operate. Robot is not in use Shutdown the computer and turn off the robot power. Leave the charger attached and place it in 12V AGM mode. Leaving the robot power on may cause the batteries to deep discharge The charger will maintain a \u2018trickle charge\u2019 on the battery, keeping the charge at 100%. Robot is coming out of storage Attach charger in 12V AGM mode and charge for 2-3 hours until charger reports 100% SLA batteries naturally lose charge over time due to \u2018self-discharge\u2019.","title":"Charging Best Practices"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#troubleshooting","text":"Issue How to Diagnose Cause Corrective Procedure Robot shows no power on activity Nothing happens when you toggle on the robot\u2019s power switch. There is no visible illumination of LEDs, motion of the laser range finder, or audible noise of the robot fans. The robot fuse may have blown. When the batteries drain the current required to maintain power goes up, which can ultimately blow the fuse. Proceed to \u201cChanging the Fuse\u201d steps below Robot powers on momentarily When you toggle on the robot\u2019s power switch some activity occurs (illumination of LEDs, audible noise of robot fans, etc) but the computer fails to boot. The battery voltage is too low to maintain power. As the power draw increases during power-on, the voltage dips and causes the system to shut down. Connect the battery charger in 12V AGM mode and leave until fully charged. Battery won\u2019t charge in 12V AGM mode When the robot is powered down and the charger is connected in 12V AGM mode, the charger eventually switches to a different mode. The battery voltage is too low for the charger to function correctly in normal operation. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger reports 100% charge but the batteries are discharged When the robot is powered down and the charger is connected in 12V AGM mode, the charger status shows 100%. However the robot fails to turn on properly. Damage to the batteries (usually caused by excessively low voltage) may artificially raise the open circuit voltage of the battery, causing the battery to appear fully charged, while providing low capacity. Proceed to the \u201cRecovering from Low Battery Voltage\u201d steps below. Charger will not charge or stay in any mode. When placed in 12V AGM, SUPPLY, or REPAIR mode, it continually reverts to STANDBY mode after ~ 20 minutes. Charger may be defective. Contact Hello Robot Support for a replacement.","title":"Troubleshooting"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#recovering-from-low-battery-voltage","text":"Turn off the robot power switch and detach the charger from the robot Place charger in SUPPLY Mode Allow robot to charge for 4-8 hours, or up to 24 hours for extreme discharge Switch the charger to 12V AGM mode Charge until at 100%","title":"Recovering from Low Battery Voltage"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#additional-information","text":"","title":"Additional Information"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#powering-down-the-robot","text":"The recommended power down procedure is Place a clamp on the mast below the shoulder to prevent dropping Shutdown the computer from the Desktop or via SSH When the laser range finder has stopped spinning, turn off the main power switch","title":"Powering Down the Robot"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#replacing-the-fuse","text":"Stretch RE2 has two automotive fuses inside the base that may need to be replaced. Fuse Type Recommended Fuse 7.5A ATM Fast Blow Blade Bussman VP/ATM-7-1/2-RP The fuse locations are shown below. For guidance on replacing the fuse, contact Hello Robot support: support@hello-robot.com .","title":"Replacing the Fuse"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#checking-the-battery-charge","text":"The battery charger LEDs provide an approximate indicator of battery charge when it is in 12V AGM mode.","title":"Checking the Battery Charge"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#checking-the-battery-voltage","text":"Battery voltage is not always an accurate indicator of battery charge but it can be a useful proxy. A charged battery will typically report a voltage of 12-12.8V and will maintain that voltage across load conditions. Meanwhile, a partially charged battery may report anywhere from 10-12.8V but its voltage will drop rapidly when loaded. Measuring Battery Voltage from the Command Line The battery voltage and current draw can be checked from the command line: $ stretch_robot_battery_check.py [Pass] Voltage with 12.9889035225 [Pass] Current with 2.46239192784 [Pass] CPU Temp with 56.0 Measuring Battery Voltage with a DMM When troubleshooting a deeply discharged battery it may be useful to directly measure the battery voltage with a digital multimeter (DMM). To do this we recommend Detach the charger cable at its inline connector Apply the DMM to the connector contacts as shown Plug the charge cable into the charge port of the robot NOTE: Caution should be taken as it is possible to short the battery when doing this.","title":"Checking the Battery Voltage"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#repairing-damaged-batteries","text":"It is possible for Stretch's batteries to become damaged due to repeated deep discharge. If the robot has continued issues maintaining a charge we recommend attempting the following procedure: Turn off the robot power switch and detach the charger from the robot Place charger in SUPPLY Mode Attach the charger and allow robot to charge for 4-8 hours Place the charger in REPAIR mode Allow robot to charge until the repair cycle completes and the charger returns to standby - up to 4 hours Place the charger back in 12V AGM mode and allow batteries to charger to 100%","title":"Repairing Damaged Batteries"},{"location":"stretch-hardware-guides/docs/battery_maintenance_guide_re2/#replacing-damaged-batteries","text":"It is possible for a mechanically skilled person to replace the Stretch batteries should it be necessary . Please contact Hello Robot Support for more information (support@hello-robot.com) All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Replacing Damaged Batteries"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/","text":"Stretch RE1 - Dex Wrist User Guide In this guide, we will cover the installation, configuration, and use of the Stretch Dex Wrist. Overview The Stretch Dex Wrist is an optional add-on to the RE1. It adds pitch and roll degrees of freedom to the standard wrist yaw joint. It also includes a slightly modified version of the standard Stretch Compliant Gripper. NOTE: If your robot did not ship with the Stretch Dex Wrist pre-installed you will want to first proceed to the Appendix: Installation and Configuration at the end of this guide. Functional Specification Working with the Dex Wrist Safe Use The Dex Wrist requires added attention to safety. Its additional dexterity introduces new pinch points around the wrist pitch and roll degrees of freedom. NOTE: Please review the Robot Safety Guide prior to working with the Dex Wrist. In addition to these precautions, the Dex Wrist requires attention to pinch points between: The wrist pitch and wrist yaw structures during yaw motion The gripper and wrist pitch structures during pitch motion The Dex Wrist includes a pinch point safety marking as a reminder to users: Avoiding Collisions The added dexterity of the Dex Wrist introduces new opportunities for self-collision between the robot tool and the robot. These include Running the tool into the base during lift downward motion Running the tool into the ground Running the tool into the wrist yaw structure We recommend becoming familiar with the potential collision points of the Dex Wrist by commanding careful motions through the stretch_xbox_controller_teleop.py tool. With Stretch Body v0.1.0 we introduce a simple collision avoidance controller . The collision avoidance behavior acts to dynamically set the robot joint limits according to simple models of its kinematic state. The avoidance behavior is defined in collision_model.py For performance reasons this collision avoidance behavior is coarse and does not prevent all self-collisions and considered 'experimental'. The collision avoidance is off by default for the standard Stretch RE1. For robots with the wrist we turn it on by default. It be turned on or off by modifying the following in your user YAML: robot : use_collision_manager : 1 XBox Teleoperation The Dex Wrist can be teleoperated using the XBox controller. When the Dex Wrist is installed the stretch_xbox_controller_teleop.py tool will automatically remap control of the pan-tilt head to control of the pitch-roll wrist. $ stretch_xbox_controller_teleop.py The new key mapping is shown below. A printable version is available here . Stretch Body Interface The new WristPitch and WristRoll joints are accessed from Stretch Body in the same manner as the WristYaw joint. Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () You can jog the individual joints of the wrist with the Stretch Body interface using the stretch_dex_wrist_jog.py tool that installs with the Stretch Tool Share: $ stretch_dex_wrist_jog.py --pitch $ stretch_dex_wrist_jog.py --yaw $ stretch_dex_wrist_jog.py --roll For reference, the parameters for the Stretch Dex Wrist (which can be overridden in the user YAML) can be seen in params.py . Stretch ROS Interface The Dex Wrist can be controlled via ROS as well, as shown in the keyboard teleoperation code . To test the interface: $ roslaunch stretch_calibration simple_test_head_calibration.launch You can use Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN c PITCH FORWARD v PITCH BACK o ROLL FORWARD p ROLL BACK 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- Appendix: Installation and Configuration Robots that did not ship with the Dex Wrist installed will require additional hardware and software installation. Production Batch Variation Earlier production 'batches' of Stretch will require a hardware upgrade prior to use the Dex Wrist. To check your robot's batch, run: $ stretch_about.py Refer to this table to determine what changes are required for your robot. Batch Name Upgrade Wacc Board Update Baud Rate Guthrie Y Y Hank Y Y Irma Y Y Joplin N Y Kendrick or later N N Upgrade Wacc Board If your robot requires a Wacc Board upgrade please follow the instructions here with the assistance of Hello Robot support. This must be done before attaching the Dex Wrist to our robot. Update Baud Rate The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600 (depending on your production batch). Use the commands below. $ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600. Changing baud to 115200 Success at changing baud $ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600. Changing baud to 115200 Success at changing baud $ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600. Changing baud to 115200 Success at changing baud Attaching the Dex Wrist Power down your Stretch before installing the Dex Wrist. The Dex Wrist mounts to the bottom of the Stretch Wrist Tool Plate . Installation requires 8 M2x6mm Torx FHCS bolts (provided) 4 M2.5x4mm Torx FHCS bolts (provided) 2 M2.5x8mm SHCS bolts (provided) T6 Torx wrench (provided) T8 Torx wrench (provided) 2mm Hex key (provided) First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Mounting Bracket Note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Using the T6 Torx wrench, attach the wrist mount bracket (A) to the bottom of the tool plate using the provided M2x6mm bolts (B). NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Now route the Dynamixel cable coming from the Stretch Wrist Yaw through the hollow bore of the wrist yaw joint. NOTE: During this step ensure the Dynamixel cable from the wrist yaw exits out the back (towards the shoulder) Next, raise the wrist module up vertically into the mounting bracket , then sliding it over horizontally so that the bearing mates onto its post. Now rotate the wrist yaw joint so the wrist pitch servo body is accessible. Attach the pitch servo to the mounting bracket using the 4 M2.5x4mm screws (C) using the T8 Torx wrench. Finally, route the Dynamixel cable into the wrist pitch servo (pink) and install the cable clip (D) using the M2.5x8mm bolts and the 2mm hex wrench. Software Configuration Robots that did not ship with the Dex Wrist pre-installed will require their software to be updated and configured. NOTE: Each user account on the robot will need to run the following steps to configure the Dex Wrist. Upgrade Stretch Body Ensure the latest version of Stretch Body and Stretch Factory are installed $ pip2 install hello-robot-stretch-body -U --no-cache-dir $ pip2 install hello-robot-stretch-body-tools -U --no-cache-dir $ pip2 install hello-robot-stretch-factory -U --no-cache-dir $ pip2 install hello-robot-stretch-tool-share -U --no-cache-dir Backup User YAML $ cd $HELLO_FLEET_PATH / $HELLO_FLEET_ID $ cp stretch_re1_user_params.yaml stretch_re1_user_params.yaml.bak Run Installation Script $ cd ~/repos $ git clone https://github.com/hello-robot/stretch_install $ cd ./stretch_install $ git pull $ ./stretch_new_dex_wrist_install.sh NOTE: The factory gripper calibration may not provide the full range of motion in some cases. If necessary you can dial in the gripper calibration with the tool RE1_gripper_calibrate.py All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"DexWrist User Guide"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#stretch-re1-dex-wrist-user-guide","text":"In this guide, we will cover the installation, configuration, and use of the Stretch Dex Wrist.","title":"Stretch RE1 - Dex Wrist User Guide"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#overview","text":"The Stretch Dex Wrist is an optional add-on to the RE1. It adds pitch and roll degrees of freedom to the standard wrist yaw joint. It also includes a slightly modified version of the standard Stretch Compliant Gripper. NOTE: If your robot did not ship with the Stretch Dex Wrist pre-installed you will want to first proceed to the Appendix: Installation and Configuration at the end of this guide.","title":"Overview"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#functional-specification","text":"","title":"Functional Specification"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#working-with-the-dex-wrist","text":"","title":"Working with the Dex Wrist"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#safe-use","text":"The Dex Wrist requires added attention to safety. Its additional dexterity introduces new pinch points around the wrist pitch and roll degrees of freedom. NOTE: Please review the Robot Safety Guide prior to working with the Dex Wrist. In addition to these precautions, the Dex Wrist requires attention to pinch points between: The wrist pitch and wrist yaw structures during yaw motion The gripper and wrist pitch structures during pitch motion The Dex Wrist includes a pinch point safety marking as a reminder to users:","title":"Safe Use"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#avoiding-collisions","text":"The added dexterity of the Dex Wrist introduces new opportunities for self-collision between the robot tool and the robot. These include Running the tool into the base during lift downward motion Running the tool into the ground Running the tool into the wrist yaw structure We recommend becoming familiar with the potential collision points of the Dex Wrist by commanding careful motions through the stretch_xbox_controller_teleop.py tool. With Stretch Body v0.1.0 we introduce a simple collision avoidance controller . The collision avoidance behavior acts to dynamically set the robot joint limits according to simple models of its kinematic state. The avoidance behavior is defined in collision_model.py For performance reasons this collision avoidance behavior is coarse and does not prevent all self-collisions and considered 'experimental'. The collision avoidance is off by default for the standard Stretch RE1. For robots with the wrist we turn it on by default. It be turned on or off by modifying the following in your user YAML: robot : use_collision_manager : 1","title":"Avoiding Collisions"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#xbox-teleoperation","text":"The Dex Wrist can be teleoperated using the XBox controller. When the Dex Wrist is installed the stretch_xbox_controller_teleop.py tool will automatically remap control of the pan-tilt head to control of the pitch-roll wrist. $ stretch_xbox_controller_teleop.py The new key mapping is shown below. A printable version is available here .","title":"XBox Teleoperation"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#stretch-body-interface","text":"The new WristPitch and WristRoll joints are accessed from Stretch Body in the same manner as the WristYaw joint. Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () You can jog the individual joints of the wrist with the Stretch Body interface using the stretch_dex_wrist_jog.py tool that installs with the Stretch Tool Share: $ stretch_dex_wrist_jog.py --pitch $ stretch_dex_wrist_jog.py --yaw $ stretch_dex_wrist_jog.py --roll For reference, the parameters for the Stretch Dex Wrist (which can be overridden in the user YAML) can be seen in params.py .","title":"Stretch Body Interface"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#stretch-ros-interface","text":"The Dex Wrist can be controlled via ROS as well, as shown in the keyboard teleoperation code . To test the interface: $ roslaunch stretch_calibration simple_test_head_calibration.launch You can use Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN c PITCH FORWARD v PITCH BACK o ROLL FORWARD p ROLL BACK 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT -------------------------------------------","title":"Stretch ROS Interface"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#appendix-installation-and-configuration","text":"Robots that did not ship with the Dex Wrist installed will require additional hardware and software installation.","title":"Appendix: Installation and Configuration"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#production-batch-variation","text":"Earlier production 'batches' of Stretch will require a hardware upgrade prior to use the Dex Wrist. To check your robot's batch, run: $ stretch_about.py Refer to this table to determine what changes are required for your robot. Batch Name Upgrade Wacc Board Update Baud Rate Guthrie Y Y Hank Y Y Irma Y Y Joplin N Y Kendrick or later N N","title":"Production Batch Variation"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#upgrade-wacc-board","text":"If your robot requires a Wacc Board upgrade please follow the instructions here with the assistance of Hello Robot support. This must be done before attaching the Dex Wrist to our robot.","title":"Upgrade Wacc Board"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#update-baud-rate","text":"The new wrist requires moving to 115200 Baud communication for all Dynamixel servos from the previous 57600 (depending on your production batch). Use the commands below. $ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 11 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600. Changing baud to 115200 Success at changing baud $ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-head 12 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600. Changing baud to 115200 Success at changing baud $ RE1_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600. Changing baud to 115200 Success at changing baud","title":"Update Baud Rate"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#attaching-the-dex-wrist","text":"Power down your Stretch before installing the Dex Wrist. The Dex Wrist mounts to the bottom of the Stretch Wrist Tool Plate . Installation requires 8 M2x6mm Torx FHCS bolts (provided) 4 M2.5x4mm Torx FHCS bolts (provided) 2 M2.5x8mm SHCS bolts (provided) T6 Torx wrench (provided) T8 Torx wrench (provided) 2mm Hex key (provided) First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide .","title":"Attaching the Dex Wrist"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#mounting-bracket","text":"Note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Using the T6 Torx wrench, attach the wrist mount bracket (A) to the bottom of the tool plate using the provided M2x6mm bolts (B). NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Now route the Dynamixel cable coming from the Stretch Wrist Yaw through the hollow bore of the wrist yaw joint. NOTE: During this step ensure the Dynamixel cable from the wrist yaw exits out the back (towards the shoulder) Next, raise the wrist module up vertically into the mounting bracket , then sliding it over horizontally so that the bearing mates onto its post. Now rotate the wrist yaw joint so the wrist pitch servo body is accessible. Attach the pitch servo to the mounting bracket using the 4 M2.5x4mm screws (C) using the T8 Torx wrench. Finally, route the Dynamixel cable into the wrist pitch servo (pink) and install the cable clip (D) using the M2.5x8mm bolts and the 2mm hex wrench.","title":"Mounting Bracket"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#software-configuration","text":"Robots that did not ship with the Dex Wrist pre-installed will require their software to be updated and configured. NOTE: Each user account on the robot will need to run the following steps to configure the Dex Wrist.","title":"Software Configuration"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#upgrade-stretch-body","text":"Ensure the latest version of Stretch Body and Stretch Factory are installed $ pip2 install hello-robot-stretch-body -U --no-cache-dir $ pip2 install hello-robot-stretch-body-tools -U --no-cache-dir $ pip2 install hello-robot-stretch-factory -U --no-cache-dir $ pip2 install hello-robot-stretch-tool-share -U --no-cache-dir","title":"Upgrade Stretch Body"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#backup-user-yaml","text":"$ cd $HELLO_FLEET_PATH / $HELLO_FLEET_ID $ cp stretch_re1_user_params.yaml stretch_re1_user_params.yaml.bak","title":"Backup User YAML"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re1/#run-installation-script","text":"$ cd ~/repos $ git clone https://github.com/hello-robot/stretch_install $ cd ./stretch_install $ git pull $ ./stretch_new_dex_wrist_install.sh NOTE: The factory gripper calibration may not provide the full range of motion in some cases. If necessary you can dial in the gripper calibration with the tool RE1_gripper_calibrate.py All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Run Installation Script"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/","text":"Stretch RE2 - Dex Wrist User Guide In this guide, we will cover the installation, configuration, and use of the Stretch Dex Wrist. Overview The Stretch Dex Wrist is an optional add-on to the RE2. It adds pitch and roll degrees of freedom to the standard wrist yaw joint. It also includes a slightly modified version of the standard Stretch Compliant Gripper. NOTE: If your robot did not ship with the Stretch Dex Wrist pre-installed you will want to first proceed to the Appendix: Installation and Configuration at the end of this guide. Functional Specification Working with the Dex Wrist Safe Use The Dex Wrist requires added attention to safety. Its additional dexterity introduces new pinch points around the wrist pitch and roll degrees of freedom. NOTE: Please review the Robot Safety Guide prior to working with the Dex Wrist. In addition to these precautions, the Dex Wrist requires attention to pinch points between: The wrist pitch and wrist yaw structures during yaw motion The gripper and wrist pitch structures during pitch motion The Dex Wrist includes a pinch point safety marking as a reminder to users: Avoiding Collisions The added dexterity of the Dex Wrist introduces new opportunities for self-collision between the robot tool and the robot. These include Running the tool into the base during lift downward motion Running the tool into the ground Running the tool into the wrist yaw structure We recommend becoming familiar with the potential collision points of the Dex Wrist by commanding careful motions through the stretch_xbox_controller_teleop.py tool. With Stretch Body v0.1.0 we introduce a simple collision avoidance controller . The collision avoidance behavior acts to dynamically set the robot joint limits according to simple models of its kinematic state. The avoidance behavior is defined in collision_model.py For performance reasons this collision avoidance behavior is coarse and does not prevent all self-collisions and considered 'experimental'. The collision avoidance is off by default for the standard Stretch RE1. For robots with the wrist we turn it on by default. It be turned on or off by modifying the following in your user YAML: robot : use_collision_manager : 1 XBox Teleoperation The Dex Wrist can be teleoperated using the XBox controller. When the Dex Wrist is installed the stretch_xbox_controller_teleop.py tool will automatically remap control of the pan-tilt head to control of the pitch-roll wrist. $ stretch_xbox_controller_teleop.py The new key mapping is shown below. A printable version is available here . Stretch Body Interface The new WristPitch and WristRoll joints are accessed from Stretch Body in the same manner as the WristYaw joint. Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () You can jog the individual joints of the wrist with the Stretch Body interface using the stretch_dex_wrist_jog.py tool that installs with the Stretch Tool Share: $ stretch_dex_wrist_jog.py --pitch $ stretch_dex_wrist_jog.py --yaw $ stretch_dex_wrist_jog.py --roll For reference, the parameters for the Stretch Dex Wrist (which can be overridden in the user YAML) can be seen in params.py . Stretch ROS Interface The Dex Wrist can be controlled via ROS as well, as shown in the keyboard teleoperation code . To test the interface: $ roslaunch stretch_calibration simple_test_head_calibration.launch You can use Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN c PITCH FORWARD v PITCH BACK o ROLL FORWARD p ROLL BACK 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- Appendix: Installation and Configuration Robots that did not ship with the Dex Wrist installed will require additional hardware and software installation. Attaching the Dex Wrist Power down your Stretch before installing the Dex Wrist. The Dex Wrist mounts to the bottom of the Stretch Wrist Tool Plate . Installation requires 8 M2x6mm Torx FHCS bolts (provided) 4 M2.5x4mm Torx FHCS bolts (provided) 2 M2.5x8mm SHCS bolts (provided) T6 Torx wrench (provided) T8 Torx wrench (provided) 2mm Hex key (provided) First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide . Mounting Bracket Note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Using the T6 Torx wrench, attach the wrist mount bracket (A) to the bottom of the tool plate using the provided M2x6mm bolts (B). NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Now route the Dynamixel cable coming from the Stretch Wrist Yaw through the hollow bore of the wrist yaw joint. NOTE: During this step ensure the Dynamixel cable from the wrist yaw exits out the back (towards the shoulder) Next, raise the wrist module up vertically into the mounting bracket , then sliding it over horizontally so that the bearing mates onto its post. Now rotate the wrist yaw joint so the wrist pitch servo body is accessible. Attach the pitch servo to the mounting bracket using the 4 M2.5x4mm screws (C) using the T8 Torx wrench. Finally, route the Dynamixel cable into the wrist pitch servo (pink) and install the cable clip (D) using the M2.5x8mm bolts and the 2mm hex wrench. Software Configuration Robots that did not ship with the Dex Wrist pre-installed will require their software to be updated and configured. NOTE: Each user account on the robot will need to run the following steps to configure the Dex Wrist. Upgrade Stretch Body Ensure the latest version of Stretch Body and Stretch Factory are installed $ pip3 install hello-robot-stretch-body -U $ pip3 install hello-robot-stretch-body-tools -U $ pip3 install hello-robot-stretch-factory -U $ pip3 install hello-robot-stretch-tool-share -U Backup User YAML $ cd $HELLO_FLEET_PATH / $HELLO_FLEET_ID $ cp stretch_user_params.yaml stretch_user_params.yaml.bak Run Installation Script $ cd ~/repos $ git clone https://github.com/hello-robot/stretch_install $ cd ./stretch_install $ git pull $ ./stretch_new_dex_wrist_install.sh NOTE: The factory gripper calibration may not provide the full range of motion in some cases. If necessary you can dial in the gripper calibration with the tool RE1_gripper_calibrate.py All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"DexWrist User Guide"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#stretch-re2-dex-wrist-user-guide","text":"In this guide, we will cover the installation, configuration, and use of the Stretch Dex Wrist.","title":"Stretch RE2 - Dex Wrist User Guide"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#overview","text":"The Stretch Dex Wrist is an optional add-on to the RE2. It adds pitch and roll degrees of freedom to the standard wrist yaw joint. It also includes a slightly modified version of the standard Stretch Compliant Gripper. NOTE: If your robot did not ship with the Stretch Dex Wrist pre-installed you will want to first proceed to the Appendix: Installation and Configuration at the end of this guide.","title":"Overview"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#functional-specification","text":"","title":"Functional Specification"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#working-with-the-dex-wrist","text":"","title":"Working with the Dex Wrist"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#safe-use","text":"The Dex Wrist requires added attention to safety. Its additional dexterity introduces new pinch points around the wrist pitch and roll degrees of freedom. NOTE: Please review the Robot Safety Guide prior to working with the Dex Wrist. In addition to these precautions, the Dex Wrist requires attention to pinch points between: The wrist pitch and wrist yaw structures during yaw motion The gripper and wrist pitch structures during pitch motion The Dex Wrist includes a pinch point safety marking as a reminder to users:","title":"Safe Use"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#avoiding-collisions","text":"The added dexterity of the Dex Wrist introduces new opportunities for self-collision between the robot tool and the robot. These include Running the tool into the base during lift downward motion Running the tool into the ground Running the tool into the wrist yaw structure We recommend becoming familiar with the potential collision points of the Dex Wrist by commanding careful motions through the stretch_xbox_controller_teleop.py tool. With Stretch Body v0.1.0 we introduce a simple collision avoidance controller . The collision avoidance behavior acts to dynamically set the robot joint limits according to simple models of its kinematic state. The avoidance behavior is defined in collision_model.py For performance reasons this collision avoidance behavior is coarse and does not prevent all self-collisions and considered 'experimental'. The collision avoidance is off by default for the standard Stretch RE1. For robots with the wrist we turn it on by default. It be turned on or off by modifying the following in your user YAML: robot : use_collision_manager : 1","title":"Avoiding Collisions"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#xbox-teleoperation","text":"The Dex Wrist can be teleoperated using the XBox controller. When the Dex Wrist is installed the stretch_xbox_controller_teleop.py tool will automatically remap control of the pan-tilt head to control of the pitch-roll wrist. $ stretch_xbox_controller_teleop.py The new key mapping is shown below. A printable version is available here .","title":"XBox Teleoperation"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#stretch-body-interface","text":"The new WristPitch and WristRoll joints are accessed from Stretch Body in the same manner as the WristYaw joint. Control of the Stretch Dex Wrist uses the same interfaces as the rest of the Stretch Body Robot joints. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move arm to safe manipulation location robot . stow () robot . lift . move_to ( 0.4 ) robot . push_command () time . sleep ( 2.0 ) #Pose the Dex Wrist robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'wrist_pitch' , 0 ) robot . end_of_arm . move_to ( 'wrist_roll' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) time . sleep ( 2.0 ) #Go back to stow and shutdown robot . stow () robot . stop () You can jog the individual joints of the wrist with the Stretch Body interface using the stretch_dex_wrist_jog.py tool that installs with the Stretch Tool Share: $ stretch_dex_wrist_jog.py --pitch $ stretch_dex_wrist_jog.py --yaw $ stretch_dex_wrist_jog.py --roll For reference, the parameters for the Stretch Dex Wrist (which can be overridden in the user YAML) can be seen in params.py .","title":"Stretch Body Interface"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#stretch-ros-interface","text":"The Dex Wrist can be controlled via ROS as well, as shown in the keyboard teleoperation code . To test the interface: $ roslaunch stretch_calibration simple_test_head_calibration.launch You can use Ctrl-C to exit when done. The menu interface is: ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN c PITCH FORWARD v PITCH BACK o ROLL FORWARD p ROLL BACK 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT -------------------------------------------","title":"Stretch ROS Interface"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#appendix-installation-and-configuration","text":"Robots that did not ship with the Dex Wrist installed will require additional hardware and software installation.","title":"Appendix: Installation and Configuration"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#attaching-the-dex-wrist","text":"Power down your Stretch before installing the Dex Wrist. The Dex Wrist mounts to the bottom of the Stretch Wrist Tool Plate . Installation requires 8 M2x6mm Torx FHCS bolts (provided) 4 M2.5x4mm Torx FHCS bolts (provided) 2 M2.5x8mm SHCS bolts (provided) T6 Torx wrench (provided) T8 Torx wrench (provided) 2mm Hex key (provided) First, remove the standard Stretch Gripper if it is still attached according to the Hardware User Guide .","title":"Attaching the Dex Wrist"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#mounting-bracket","text":"Note where the forward direction is on the wrist yaw tool plate. The forward direction is indicated by the additional alignment hole that is just outside the bolt pattern (shown pointing down in the image) Using the T6 Torx wrench, attach the wrist mount bracket (A) to the bottom of the tool plate using the provided M2x6mm bolts (B). NOTE: ensure that the forward direction of the bracket (also indicated by an alignment hole) matches the forward direction of the tool plate. Now route the Dynamixel cable coming from the Stretch Wrist Yaw through the hollow bore of the wrist yaw joint. NOTE: During this step ensure the Dynamixel cable from the wrist yaw exits out the back (towards the shoulder) Next, raise the wrist module up vertically into the mounting bracket , then sliding it over horizontally so that the bearing mates onto its post. Now rotate the wrist yaw joint so the wrist pitch servo body is accessible. Attach the pitch servo to the mounting bracket using the 4 M2.5x4mm screws (C) using the T8 Torx wrench. Finally, route the Dynamixel cable into the wrist pitch servo (pink) and install the cable clip (D) using the M2.5x8mm bolts and the 2mm hex wrench.","title":"Mounting Bracket"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#software-configuration","text":"Robots that did not ship with the Dex Wrist pre-installed will require their software to be updated and configured. NOTE: Each user account on the robot will need to run the following steps to configure the Dex Wrist.","title":"Software Configuration"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#upgrade-stretch-body","text":"Ensure the latest version of Stretch Body and Stretch Factory are installed $ pip3 install hello-robot-stretch-body -U $ pip3 install hello-robot-stretch-body-tools -U $ pip3 install hello-robot-stretch-factory -U $ pip3 install hello-robot-stretch-tool-share -U","title":"Upgrade Stretch Body"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#backup-user-yaml","text":"$ cd $HELLO_FLEET_PATH / $HELLO_FLEET_ID $ cp stretch_user_params.yaml stretch_user_params.yaml.bak","title":"Backup User YAML"},{"location":"stretch-hardware-guides/docs/dex_wrist_guide_re2/#run-installation-script","text":"$ cd ~/repos $ git clone https://github.com/hello-robot/stretch_install $ cd ./stretch_install $ git pull $ ./stretch_new_dex_wrist_install.sh NOTE: The factory gripper calibration may not provide the full range of motion in some cases. If necessary you can dial in the gripper calibration with the tool RE1_gripper_calibrate.py All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Run Installation Script"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/","text":"Stretch RE1 - Hardware Guide This manual provides the engineering data and user guidance for working with the Hello Robot Stretch RE1 hardware. Disclaimer The Hello Robot Stretch is intended for use in the research of mobile manipulation applications by users experienced in the use and programming of research robots. This product is not intended for general use in the home by consumers, and lacks the required certifications for such use. Please see the section on Regulatory Compliance for further details. Functional Specification Body Plan Hardware Architecture Robot Subsystems Base The base is a two wheel differential drive with a passive Mecanum wheel for a caster. It includes four cliff sensors to allow detection of stairs, thresholds, etc. Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm The base has 6 M4 threaded inserts available for mounting user accessories such as a tray. The mounting pattern is shown below. The inserts are recessed 1mm from the top of the base shell. Base IMU The base has a 9 DOF IMU using the 9 DOF FXOS8700 + FXAS21002 chipset. The IMU orientation is as shown below: Trunk Development and charge ports are at the back of the base in the trunk. The trunk cover slides into place vertically and is non-latching. The trunk height has been designed to accommodate one or more USB based Intel Neural Compute Sticks. Two mounting holes are provided inside the trunk. These allow the user to strain relief tethered cables (eg, HDMI and keyboard) during development. It is recommended to strain relief such cables to prevent accidental damage during base motion. Item Notes A Vent Intake vent for computer fan B 6 Port USB Hub USB 3.0 , powered 5V/3A C Ethernet Connected to computer NIC D On/Off Robot power on / off. Switch is illuminated when on. E Charge Rated for upplied 12V/7A charger F HDMI Connected to computer HDMI G Mounting points M4 threaded holes Head The head provides the audio interface to the robot, a pan tilt depth camera, a runstop, as well as a developer interface to allow the addition of additional user hardware. Item Notes A Pan tilt depth camera Intel RealSense D435i Two Dynamixel XL430-W250-T servos B Speakers C Mounting holes 2x M4 threaded, spacing 25mm D Developer Interface USB2.0-A with 5V@500mA fused JST XHP-2, 12V@3A fused Pin 1: 12V Pin 2: GND E Microphone array With programmable 12 RGB LED ring F Runstop G Audio volume control Pan Tilt The head pan-tilt unit utilizes two Dynamixel XL430-W250-T servos. It incorporates a small fan in order to ensure proper cooling of the servo and camera during dynamic repeated motions of the tilt DOF. The nominal \u2018zero\u2019 position is of the head is shown below, along with the corresponding range of motion. DOF Range (deg) Min(deg) Max (deg) Pan 346 -234 112 Tilt 115 -25 90 ReSpeaker Microphone Array The ReSPeaker has 12 RGB LEDs that can be controlled programatically. By default they display sound intensity and direction of the microphone array. The ReSpeaker has 4 mems microphones mounted on a 64.61mm circle at 45 degree spacing. The drawing below shows the position and orientation of the microphone array relative to the head pan axis. Runstop The runstop allows the user to pause the motion of the four primary DOF (base, lift, and arm) by tapping the illuminated button on the head. When the runstop is enabled, these DOF are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume. Lift The lift degree of freedom provides vertical translation of the arm. It is driven by a closed loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive. The \u2018shoulder\u2019 includes two mounting holes and a small delivery tray. NOTE : When using the shoulder mounting screws do not use M4 bolts longer than 7mm in length. Otherwise damage may occur. Item Notes A Delivery tray B Mounting holes Threaded M4. Spacing 34.5 mm. Length not to exceed 7mm C Aruco Tag Size 40x40 mm Arm The arm comprises 5 telescoping carbon fiber links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion. The arm exhibits a small amount of play (lash) in the X, Y, Z, and theta directions which is a normal characteristic of its design. Despite this it can achieve good repeatability, in part because its gravity loading is fairly constant. The retracted arm and wrist combined are designed to fit within the footprint of the base. The arm is designed to have: Reach: 0.52m Wrist The wrist includes: Yaw DOF to allow for stowing of the tool 2 Aruco tags for calibration and visual localization of the tool Expansion port with Arduino expansion header USB-A connector Tool plate with dual sided mounting Dynamixel X-Series TTL bus Wrist Control Interface The wrist yaw degree-of-freedom uses a Dynamixel XL430 servo . Additional Dynamixel servos can be daisy chained off of this servo, allowing for one more additional degree-of-freedoms to be easily integrated onto the robot (such as the provided Stretch Gripper). Stretch comes with a XL430 compatible control cable preinstalled into this servo. If a different cable needs to be installed the servo cap can be removed as shown. Wrist Tool Plate The tool plate allows for mounting on the top or the bottom using the M2 bolt pattern. The mounting pattern is compatible with Robotis Dynamixel frames as well: FR12-H101K FR12-S102K FR12-S101K Wrist Yaw Range of Motion The wrist yaw DOF is calibrated so that the index hole faces forward at the 'zero' position. From this pose the wrist has a ROM of +256/-76 degrees as shown. Wrist Accelerometer The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The sensor is mounted inside the distal link of the arm as shown below. Wrist Expansion USB The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V. Wrist Expansion Header The wrist includes an expansion header that provides access to pins of the wrist Arduino board. The header connector can be accessed by removing the cap at the end of the arm. The header is wired to a Atmel SAMD21G18A-AUT ( datasheet ) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow: General purpose digital I/O Analog input In addition, the firmware can be configured for other pin functions, including: Serial SPI Serial I2C Serial UART The Stretch Firmware Manual covers this modification. The header pins utilize 3V3 TTL logic. They do not have interface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded The pin mapping is: Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input Analog in (A0) The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN) . It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305 ). Pin 1 & 10 are indicated below. The expansion DIO schematic shown below. Wrist Mounts Gripper The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position. As shown, it includes mounting features on one side to allow for attachment of simple rigid tools such as hooks and pullers . Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 The attachment features are spaced at 9mm. The weight of the Stretch Compliant Gripper is 240g. Gripper Removal Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse. Unplug the Dynamixel cable from the back of the gripper. Remove the 4 screws holding the gripper to the bracket. Remove the gripper from the mounting bracket Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate. Robot Care Battery Maintenance Please review the Battery Maintenance Guide for proper care and charging of the Stretch batteries. Belt Tension A neoprene timing belt drives the arm up and down the lift. It may detension over long periods of time if it experiences sustained loading. In this case, slack will become visually apparent in the belt as the lift moves. The belt is very straightforward to re-tension. Please contact support@hello-robot.com for tensioning instructions. Keeping the Robot Clean The robot surfaces can be wiped down with an alcohol wipe or a moist rag from time to time in order to remove and debris or oils that accumulate on the shells or mast. The drive wheels can accumulate dust over time and begin to lose traction. They should be periodically wiped down as well. When possible, the Trunk cover for the base should be kept on in order to keep dust and debris out of the Trunk connectors. If the D435i camera requires cleaning use appropriate lens cleaning fluid and a microfiber cloth. Keeping the Robot Calibrated The robot comes pre-calibrated with a robot-specific URDF. This calibration allows the D435i depth sensor to accurately estimate where the robot wrist, and body, is in the depth image. The robot may become slightly uncalibrated over time for a variety of reasons: Normal wear and tear and loosening of joints of the robot The head structure is accidentally load and the structure becomes very slightly bent The wrist and should structure become accidentally highly loaded and become slightly bent The calibration accuracy can be checked using the provided ROS tools. If necessary, the user can recalibrate the robot. See the Stretch URDF Calibration Guide for more information. Transporting the Robot Stretch was designed to be easily transported in the back of a car, up a stair case, or around a building. For short trips, the robot can be simply rolled around by grabbing its mast. It may be picked up by its mast and carried up stairs as well. For safety, please use two people to lift the robot. For longer trips it is recommended to transport the robot in its original cardboard box with foam packaging. The metal protective cage that surrounds the head is only necessary if the robot might be shipped and the box will not remain upright. System Check It is useful to periodically run stretch_robot_system_check.py. This will check that the robot's hardware devices are present and within normal operating conditions. $ stretch_robot_system_check.py ---- Checking Devices ---- [Pass] : hello-wacc [Pass] : hello-motor-left-wheel [Pass] : hello-motor-arm [Pass] : hello-dynamixel-wrist [Pass] : hello-motor-right-wheel [Pass] : hello-motor-lift [Pass] : hello-pimu [Pass] : hello-respeaker [Pass] : hello-lrf [Pass] : hello-dynamixel-head ---- Checking Pimu ---- [Pass] Voltage = 12.8763639927 [Pass] Current = 3.25908634593 [Pass] Temperature = 36.3404559783 [Pass] Cliff-0 = -4.72064208984 [Pass] Cliff-1 = -8.56213378906 [Pass] Cliff-2 = 1.08505249023 [Pass] Cliff-3 = 5.68453979492 [Pass] IMU AZ = -9.80407142639 ---- Checking EndOfArm ---- [Dynamixel ID:013] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: wrist_yaw [Pass] Calibrated: wrist_yaw [Dynamixel ID:014] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: stretch_gripper [Pass] Calibrated: stretch_gripper ---- Checking Head ---- [Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: head_tilt [Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: head_pan ---- Checking Wacc ---- [Pass] AX = 9.4840593338 ---- Checking hello-motor-left-wheel ---- [Pass] Position = 43.9992256165 ---- Checking hello-motor-right-wheel ---- [Pass] Position = 15.1164712906 ---- Checking hello-motor-arm ---- [Pass] Position = 59.7719421387 [Pass] Position Calibrated = True ---- Checking hello-motor-lift ---- [Pass] Position = 83.7744064331 [Pass] Position Calibrated = True ---- Checking for Intel D435i ---- Bus 002 Device 016: ID 8086:0b3a Intel Corp. [Pass] : Device found Regulatory Compliance The Stretch Research Edition 1 (Stretch RE1) is not certified for use as a consumer device in the U.S. Unless stated otherwise, the Stretch RE1 is not subjected to compliance testing nor certified to meet any requirements, such as requirements for EMI, EMC, or ESD. Per FCC 47 CFR, Part 15, Subpart B, section 15.103(c) , we claim the Stretch Research Edition 1 as an exempted device, since it is a digital device used exclusively as industrial, commercial, or medical test equipment, where test equipment is equipment intended primarily for purposes of performing scientific investigations. OET BULLETIN NO. 62 , titled \"UNDERSTANDING THE FCC REGULATIONS FOR COMPUTERS AND OTHER DIGITAL DEVICES\" from December 1993 provides further clarification of the Section 15.103(c) exemption: \u201c Test equipment includes devices used for maintenance, research, evaluation, simulation and other analytical or scientific applications in areas such as industrial plants, public utilities, hospitals, universities, laboratories, automotive service centers and electronic repair shops.\u201d All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Hardware Guide"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#stretch-re1-hardware-guide","text":"This manual provides the engineering data and user guidance for working with the Hello Robot Stretch RE1 hardware.","title":"Stretch RE1 - Hardware Guide"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#disclaimer","text":"The Hello Robot Stretch is intended for use in the research of mobile manipulation applications by users experienced in the use and programming of research robots. This product is not intended for general use in the home by consumers, and lacks the required certifications for such use. Please see the section on Regulatory Compliance for further details.","title":"Disclaimer"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#functional-specification","text":"","title":"Functional Specification"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#body-plan","text":"","title":"Body Plan"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#hardware-architecture","text":"","title":"Hardware Architecture"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#robot-subsystems","text":"","title":"Robot Subsystems"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#base","text":"The base is a two wheel differential drive with a passive Mecanum wheel for a caster. It includes four cliff sensors to allow detection of stairs, thresholds, etc. Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm The base has 6 M4 threaded inserts available for mounting user accessories such as a tray. The mounting pattern is shown below. The inserts are recessed 1mm from the top of the base shell.","title":"Base"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#base-imu","text":"The base has a 9 DOF IMU using the 9 DOF FXOS8700 + FXAS21002 chipset. The IMU orientation is as shown below:","title":"Base IMU"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#trunk","text":"Development and charge ports are at the back of the base in the trunk. The trunk cover slides into place vertically and is non-latching. The trunk height has been designed to accommodate one or more USB based Intel Neural Compute Sticks. Two mounting holes are provided inside the trunk. These allow the user to strain relief tethered cables (eg, HDMI and keyboard) during development. It is recommended to strain relief such cables to prevent accidental damage during base motion. Item Notes A Vent Intake vent for computer fan B 6 Port USB Hub USB 3.0 , powered 5V/3A C Ethernet Connected to computer NIC D On/Off Robot power on / off. Switch is illuminated when on. E Charge Rated for upplied 12V/7A charger F HDMI Connected to computer HDMI G Mounting points M4 threaded holes","title":"Trunk"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#head","text":"The head provides the audio interface to the robot, a pan tilt depth camera, a runstop, as well as a developer interface to allow the addition of additional user hardware. Item Notes A Pan tilt depth camera Intel RealSense D435i Two Dynamixel XL430-W250-T servos B Speakers C Mounting holes 2x M4 threaded, spacing 25mm D Developer Interface USB2.0-A with 5V@500mA fused JST XHP-2, 12V@3A fused Pin 1: 12V Pin 2: GND E Microphone array With programmable 12 RGB LED ring F Runstop G Audio volume control","title":"Head"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#pan-tilt","text":"The head pan-tilt unit utilizes two Dynamixel XL430-W250-T servos. It incorporates a small fan in order to ensure proper cooling of the servo and camera during dynamic repeated motions of the tilt DOF. The nominal \u2018zero\u2019 position is of the head is shown below, along with the corresponding range of motion. DOF Range (deg) Min(deg) Max (deg) Pan 346 -234 112 Tilt 115 -25 90","title":"Pan Tilt"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#respeaker-microphone-array","text":"The ReSPeaker has 12 RGB LEDs that can be controlled programatically. By default they display sound intensity and direction of the microphone array. The ReSpeaker has 4 mems microphones mounted on a 64.61mm circle at 45 degree spacing. The drawing below shows the position and orientation of the microphone array relative to the head pan axis.","title":"ReSpeaker Microphone Array"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#runstop","text":"The runstop allows the user to pause the motion of the four primary DOF (base, lift, and arm) by tapping the illuminated button on the head. When the runstop is enabled, these DOF are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume.","title":"Runstop"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#lift","text":"The lift degree of freedom provides vertical translation of the arm. It is driven by a closed loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive. The \u2018shoulder\u2019 includes two mounting holes and a small delivery tray. NOTE : When using the shoulder mounting screws do not use M4 bolts longer than 7mm in length. Otherwise damage may occur. Item Notes A Delivery tray B Mounting holes Threaded M4. Spacing 34.5 mm. Length not to exceed 7mm C Aruco Tag Size 40x40 mm","title":"Lift"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#arm","text":"The arm comprises 5 telescoping carbon fiber links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion. The arm exhibits a small amount of play (lash) in the X, Y, Z, and theta directions which is a normal characteristic of its design. Despite this it can achieve good repeatability, in part because its gravity loading is fairly constant. The retracted arm and wrist combined are designed to fit within the footprint of the base. The arm is designed to have: Reach: 0.52m","title":"Arm"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist","text":"The wrist includes: Yaw DOF to allow for stowing of the tool 2 Aruco tags for calibration and visual localization of the tool Expansion port with Arduino expansion header USB-A connector Tool plate with dual sided mounting Dynamixel X-Series TTL bus","title":"Wrist"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-control-interface","text":"The wrist yaw degree-of-freedom uses a Dynamixel XL430 servo . Additional Dynamixel servos can be daisy chained off of this servo, allowing for one more additional degree-of-freedoms to be easily integrated onto the robot (such as the provided Stretch Gripper). Stretch comes with a XL430 compatible control cable preinstalled into this servo. If a different cable needs to be installed the servo cap can be removed as shown.","title":"Wrist Control Interface"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-tool-plate","text":"The tool plate allows for mounting on the top or the bottom using the M2 bolt pattern. The mounting pattern is compatible with Robotis Dynamixel frames as well: FR12-H101K FR12-S102K FR12-S101K","title":"Wrist Tool Plate"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-yaw-range-of-motion","text":"The wrist yaw DOF is calibrated so that the index hole faces forward at the 'zero' position. From this pose the wrist has a ROM of +256/-76 degrees as shown.","title":"Wrist Yaw Range of Motion"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-accelerometer","text":"The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The sensor is mounted inside the distal link of the arm as shown below.","title":"Wrist Accelerometer"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-expansion-usb","text":"The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V.","title":"Wrist Expansion USB"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-expansion-header","text":"The wrist includes an expansion header that provides access to pins of the wrist Arduino board. The header connector can be accessed by removing the cap at the end of the arm. The header is wired to a Atmel SAMD21G18A-AUT ( datasheet ) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow: General purpose digital I/O Analog input In addition, the firmware can be configured for other pin functions, including: Serial SPI Serial I2C Serial UART The Stretch Firmware Manual covers this modification. The header pins utilize 3V3 TTL logic. They do not have interface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded The pin mapping is: Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input Analog in (A0) The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN) . It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305 ). Pin 1 & 10 are indicated below. The expansion DIO schematic shown below.","title":"Wrist Expansion Header"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#wrist-mounts","text":"","title":"Wrist Mounts"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#gripper","text":"The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position. As shown, it includes mounting features on one side to allow for attachment of simple rigid tools such as hooks and pullers . Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 The attachment features are spaced at 9mm. The weight of the Stretch Compliant Gripper is 240g.","title":"Gripper"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#gripper-removal","text":"Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse. Unplug the Dynamixel cable from the back of the gripper. Remove the 4 screws holding the gripper to the bracket. Remove the gripper from the mounting bracket Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate.","title":"Gripper Removal"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#robot-care","text":"","title":"Robot Care"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#battery-maintenance","text":"Please review the Battery Maintenance Guide for proper care and charging of the Stretch batteries.","title":"Battery Maintenance"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#belt-tension","text":"A neoprene timing belt drives the arm up and down the lift. It may detension over long periods of time if it experiences sustained loading. In this case, slack will become visually apparent in the belt as the lift moves. The belt is very straightforward to re-tension. Please contact support@hello-robot.com for tensioning instructions.","title":"Belt Tension"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#keeping-the-robot-clean","text":"The robot surfaces can be wiped down with an alcohol wipe or a moist rag from time to time in order to remove and debris or oils that accumulate on the shells or mast. The drive wheels can accumulate dust over time and begin to lose traction. They should be periodically wiped down as well. When possible, the Trunk cover for the base should be kept on in order to keep dust and debris out of the Trunk connectors. If the D435i camera requires cleaning use appropriate lens cleaning fluid and a microfiber cloth.","title":"Keeping the Robot Clean"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#keeping-the-robot-calibrated","text":"The robot comes pre-calibrated with a robot-specific URDF. This calibration allows the D435i depth sensor to accurately estimate where the robot wrist, and body, is in the depth image. The robot may become slightly uncalibrated over time for a variety of reasons: Normal wear and tear and loosening of joints of the robot The head structure is accidentally load and the structure becomes very slightly bent The wrist and should structure become accidentally highly loaded and become slightly bent The calibration accuracy can be checked using the provided ROS tools. If necessary, the user can recalibrate the robot. See the Stretch URDF Calibration Guide for more information.","title":"Keeping the Robot Calibrated"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#transporting-the-robot","text":"Stretch was designed to be easily transported in the back of a car, up a stair case, or around a building. For short trips, the robot can be simply rolled around by grabbing its mast. It may be picked up by its mast and carried up stairs as well. For safety, please use two people to lift the robot. For longer trips it is recommended to transport the robot in its original cardboard box with foam packaging. The metal protective cage that surrounds the head is only necessary if the robot might be shipped and the box will not remain upright.","title":"Transporting the Robot"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#system-check","text":"It is useful to periodically run stretch_robot_system_check.py. This will check that the robot's hardware devices are present and within normal operating conditions. $ stretch_robot_system_check.py ---- Checking Devices ---- [Pass] : hello-wacc [Pass] : hello-motor-left-wheel [Pass] : hello-motor-arm [Pass] : hello-dynamixel-wrist [Pass] : hello-motor-right-wheel [Pass] : hello-motor-lift [Pass] : hello-pimu [Pass] : hello-respeaker [Pass] : hello-lrf [Pass] : hello-dynamixel-head ---- Checking Pimu ---- [Pass] Voltage = 12.8763639927 [Pass] Current = 3.25908634593 [Pass] Temperature = 36.3404559783 [Pass] Cliff-0 = -4.72064208984 [Pass] Cliff-1 = -8.56213378906 [Pass] Cliff-2 = 1.08505249023 [Pass] Cliff-3 = 5.68453979492 [Pass] IMU AZ = -9.80407142639 ---- Checking EndOfArm ---- [Dynamixel ID:013] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: wrist_yaw [Pass] Calibrated: wrist_yaw [Dynamixel ID:014] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: stretch_gripper [Pass] Calibrated: stretch_gripper ---- Checking Head ---- [Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: head_tilt [Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: head_pan ---- Checking Wacc ---- [Pass] AX = 9.4840593338 ---- Checking hello-motor-left-wheel ---- [Pass] Position = 43.9992256165 ---- Checking hello-motor-right-wheel ---- [Pass] Position = 15.1164712906 ---- Checking hello-motor-arm ---- [Pass] Position = 59.7719421387 [Pass] Position Calibrated = True ---- Checking hello-motor-lift ---- [Pass] Position = 83.7744064331 [Pass] Position Calibrated = True ---- Checking for Intel D435i ---- Bus 002 Device 016: ID 8086:0b3a Intel Corp. [Pass] : Device found","title":"System Check"},{"location":"stretch-hardware-guides/docs/hardware_guide_re1/#regulatory-compliance","text":"The Stretch Research Edition 1 (Stretch RE1) is not certified for use as a consumer device in the U.S. Unless stated otherwise, the Stretch RE1 is not subjected to compliance testing nor certified to meet any requirements, such as requirements for EMI, EMC, or ESD. Per FCC 47 CFR, Part 15, Subpart B, section 15.103(c) , we claim the Stretch Research Edition 1 as an exempted device, since it is a digital device used exclusively as industrial, commercial, or medical test equipment, where test equipment is equipment intended primarily for purposes of performing scientific investigations. OET BULLETIN NO. 62 , titled \"UNDERSTANDING THE FCC REGULATIONS FOR COMPUTERS AND OTHER DIGITAL DEVICES\" from December 1993 provides further clarification of the Section 15.103(c) exemption: \u201c Test equipment includes devices used for maintenance, research, evaluation, simulation and other analytical or scientific applications in areas such as industrial plants, public utilities, hospitals, universities, laboratories, automotive service centers and electronic repair shops.\u201d All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Regulatory Compliance"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/","text":"Stretch RE2 - Hardware Guide This manual provides the engineering data and user guidance for working with the Hello Robot Stretch RE2 hardware. Disclaimer The Hello Robot Stretch is intended for use in the research of mobile manipulation applications by users experienced in the use and programming of research robots. This product is not intended for general use in the home by consumers, and lacks the required certifications for such use. Please see the section on Regulatory Compliance for further details. Functional Specification Body Plan Hardware Architecture Robot Subsystems Base The base is a two wheel differential drive with a passive Mecanum wheel for a caster. It includes four cliff sensors to allow detection of stairs, thresholds, etc. Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm The base has 6 M4 threaded inserts available for mounting user accessories such as a tray. The mounting pattern is shown below. The inserts are recessed 1mm from the top of the base shell. Base IMU The base has a 9 DOF IMU using the 9 DOF FXOS8700 + FXAS21002 chipset. The IMU orientation is as shown below: Trunk Development and charge ports are at the back of the base in the trunk. Item Notes A Vent Intake vent for computer fan B 6 Port USB Hub USB 3.0 , powered 5V/3A C Ethernet Connected to computer NIC D On/Off Robot power on / off. Switch is illuminated when on. E Charge Rated for upplied 12V/7A charger F HDMI Connected to computer HDMI G LED Light Bar Indicates battery voltage H 12V access plug Allows customer cable access to 12V Aux on Pimu PCBA Head The head provides the audio interface to the robot, a pan tilt depth camera, a runstop, as well as a developer interface to allow the addition of additional user hardware. Item Notes A Pan tilt depth camera Intel RealSense D435i Two Dynamixel XL430-W250-T servos B Speakers D Developer Interface Volume control, USB2.0-A with 5V@500mA fused JST XHP-2, 12V@3A fused Pin 1: 12V Pin 2: GND E Microphone array With programmable 12 RGB LED ring F Runstop Pan Tilt The head pan-tilt unit utilizes two Dynamixel XL430-W250-T servos. It incorporates a small fan in order to ensure proper cooling of the servo and camera during dynamic repeated motions of the tilt DOF. The nominal \u2018zero\u2019 position is of the head is shown below, along with the corresponding range of motion. DOF Range (deg) Min(deg) Max (deg) Pan 346 -234 112 Tilt 115 -25 90 ReSpeaker Microphone Array The ReSPeaker has 12 RGB LEDs that can be controlled programatically. By default they display sound intensity and direction of the microphone array. The ReSpeaker has 4 mems microphones mounted on a 64.61mm circle at 45 degree spacing. The drawing below shows the position and orientation of the microphone array relative to the head pan axis. Mounting Points The top of the head includes 3x M4 threaded mounting points as shown below. Runstop The runstop allows the user to pause the motion of the four primary DOF (base, lift, and arm) by tapping the illuminated button on the head. When the runstop is enabled, these DOF are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume. Lift The lift degree of freedom provides vertical translation of the arm. It is driven by a closed loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive. The \u2018shoulder\u2019 includes four mounting holes and a small delivery tray. NOTE : When using the shoulder mounting screws do not use M4 bolts longer than 7mm in length. Otherwise damage may occur. Item Notes A Delivery tray B Mounting holes Threaded M4. Length not to exceed 7mm. C Aruco Tag Size 40x40 mm D Developer ports USB2.0-A with 5V@500mA fused ; JST XHP-2, 12V@3A fused Pin 1: 12V Pin 2: GND Arm The arm comprises 5 telescoping carbon fiber links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion. The arm exhibits a small amount of play (lash) in the X, Y, Z, and theta directions which is a normal characteristic of its design. Despite this it can achieve good repeatability, in part because its gravity loading is fairly constant. The retracted arm and wrist combined are designed to fit within the footprint of the base. The arm is designed to have: Reach: 0.52m Wrist The wrist includes: Yaw DOF to allow for stowing of the tool 2 Aruco tags for calibration and visual localization of the tool Expansion port with Arduino expansion header USB-A connector Tool plate with dual sided mounting Dynamixel X-Series TTL bus Wrist Control Interface The wrist yaw degree-of-freedom uses a Dynamixel XL430 servo . Additional Dynamixel servos can be daisy chained off of this servo, allowing for one more additional degree-of-freedoms to be easily integrated onto the robot (such as the provided Stretch Gripper). Stretch comes with a XL430 compatible control cable preinstalled into this servo. If a different cable needs to be installed the servo cap can be removed as shown. Wrist Tool Plate The tool plate allows for mounting on the top or the bottom using the M2 bolt pattern. The mounting pattern is compatible with Robotis Dynamixel frames as well: FR12-H101K FR12-S102K FR12-S101K The tool plate includes a 'Zero indicator'. This mark indicates the forward position of the tool. It will point in the direction of the arm extension when the wrist yaw joint is at its zero position. In addition, the tool plate includes two index holes. These can be used to index a tool (e.g., Stretch Gripper) during installation. Compatible pins on the tool ensure that it is installed at the correct orientation. Wrist Yaw Range of Motion The wrist yaw DOF is calibrated so that the index hole faces forward at the 'zero' position. From this pose the wrist has a ROM of +256/-76 degrees as shown. Wrist Accelerometer The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The sensor is mounted inside the distal link of the arm as shown below. Wrist Expansion USB The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V. Wrist Expansion Header The wrist includes an expansion header that provides access to pins of the wrist Arduino board. The header connector can be accessed by removing the cap at the end of the arm. The header is wired to a Atmel SAMD21G18A-AUT ( datasheet ) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow: General purpose digital I/O Analog input In addition, the firmware can be configured for other pin functions, including: Serial SPI Serial I2C Serial UART The Stretch Firmware Manual covers this modification. The header pins utilize 3V3 TTL logic. They have limited nterface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded The pin mapping is: Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input Analog in (A0) The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN) . It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305 ). Pin 1 & 10 are indicated below. The expansion DIO schematic shown below. Wrist Mounts Gripper The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position. As shown, it includes mounting features on one side to allow for attachment of simple rigid tools such as hooks and pullers . Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 The attachment features are spaced at 9mm. The weight of the Stretch Compliant Gripper is 240g. Gripper Removal Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse. Unplug the Dynamixel cable from the back of the gripper. Remove the 4 screws holding the gripper to the bracket. Remove the gripper from the mounting bracket Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate. Robot Care Battery Maintenance Please review the Battery Maintenance Guide for proper care and charging of the Stretch batteries. Belt Tension A neoprene timing belt drives the arm up and down the lift. It may detension over long periods of time if it experiences sustained loading. In this case, slack will become visually apparent in the belt as the lift moves. The belt is very straightforward to re-tension. Please contact support@hello-robot.com for tensioning instructions. Keeping the Robot Clean The robot surfaces can be wiped down with an alcohol wipe or a moist rag from time to time in order to remove and debris or oils that accumulate on the shells or mast. The drive wheels can accumulate dust over time and begin to lose traction. They should be periodically wiped down as well. When possible, the Trunk cover for the base should be kept on in order to keep dust and debris out of the Trunk connectors. If the D435i camera requires cleaning use appropriate lens cleaning fluid and a microfiber cloth. Keeping the Robot Calibrated The robot comes pre-calibrated with a robot-specific URDF. This calibration allows the D435i depth sensor to accurately estimate where the robot wrist, and body, is in the depth image. The robot may become slightly uncalibrated over time for a variety of reasons: Normal wear and tear and loosening of joints of the robot The head structure is accidentally load and the structure becomes very slightly bent The wrist and should structure become accidentally highly loaded and become slightly bent The calibration accuracy can be checked using the provided ROS tools. If necessary, the user can recalibrate the robot. See the Stretch URDF Calibration Guide for more information. Transporting the Robot Stretch was designed to be easily transported in the back of a car, up a stair case, or around a building. For short trips, the robot can be simply rolled around by grabbing its mast. It may be picked up by its mast and carried up stairs as well. For safety, please use two people to lift the robot. For longer trips it is recommended to transport the robot in its original cardboard box with foam packaging. The metal protective cage that surrounds the head is only necessary if the robot might be shipped and the box will not remain upright. System Check It is useful to periodically run stretch_robot_system_check.py. This will check that the robot's hardware devices are present and within normal operating conditions. $ stretch_robot_system_check.py ---- Checking Devices ---- [Pass] : hello-wacc [Pass] : hello-motor-left-wheel [Pass] : hello-motor-arm [Pass] : hello-dynamixel-wrist [Pass] : hello-motor-right-wheel [Pass] : hello-motor-lift [Pass] : hello-pimu [Pass] : hello-respeaker [Pass] : hello-lrf [Pass] : hello-dynamixel-head ---- Checking Pimu ---- [Pass] Voltage = 12.8763639927 [Pass] Current = 3.25908634593 [Pass] Temperature = 36.3404559783 [Pass] Cliff-0 = -4.72064208984 [Pass] Cliff-1 = -8.56213378906 [Pass] Cliff-2 = 1.08505249023 [Pass] Cliff-3 = 5.68453979492 [Pass] IMU AZ = -9.80407142639 ---- Checking EndOfArm ---- [Dynamixel ID:013] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: wrist_yaw [Pass] Calibrated: wrist_yaw [Dynamixel ID:014] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: stretch_gripper [Pass] Calibrated: stretch_gripper ---- Checking Head ---- [Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: head_tilt [Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: head_pan ---- Checking Wacc ---- [Pass] AX = 9.4840593338 ---- Checking hello-motor-left-wheel ---- [Pass] Position = 43.9992256165 ---- Checking hello-motor-right-wheel ---- [Pass] Position = 15.1164712906 ---- Checking hello-motor-arm ---- [Pass] Position = 59.7719421387 [Pass] Position Calibrated = True ---- Checking hello-motor-lift ---- [Pass] Position = 83.7744064331 [Pass] Position Calibrated = True ---- Checking for Intel D435i ---- Bus 002 Device 016: ID 8086:0b3a Intel Corp. [Pass] : Device found Regulatory Compliance The Stretch Research Edition 1 (Stretch re2) is not certified for use as a consumer device in the U.S. Unless stated otherwise, the Stretch re2 is not subjected to compliance testing nor certified to meet any requirements, such as requirements for EMI, EMC, or ESD. Per FCC 47 CFR, Part 15, Subpart B, section 15.103(c) , we claim the Stretch Research Edition 1 as an exempted device, since it is a digital device used exclusively as industrial, commercial, or medical test equipment, where test equipment is equipment intended primarily for purposes of performing scientific investigations. OET BULLETIN NO. 62 , titled \"UNDERSTANDING THE FCC REGULATIONS FOR COMPUTERS AND OTHER DIGITAL DEVICES\" from December 1993 provides further clarification of the Section 15.103(c) exemption: \u201c Test equipment includes devices used for maintenance, research, evaluation, simulation and other analytical or scientific applications in areas such as industrial plants, public utilities, hospitals, universities, laboratories, automotive service centers and electronic repair shops.\u201d All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Hardware Guide"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#stretch-re2-hardware-guide","text":"This manual provides the engineering data and user guidance for working with the Hello Robot Stretch RE2 hardware.","title":"Stretch RE2 - Hardware Guide"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#disclaimer","text":"The Hello Robot Stretch is intended for use in the research of mobile manipulation applications by users experienced in the use and programming of research robots. This product is not intended for general use in the home by consumers, and lacks the required certifications for such use. Please see the section on Regulatory Compliance for further details.","title":"Disclaimer"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#functional-specification","text":"","title":"Functional Specification"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#body-plan","text":"","title":"Body Plan"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#hardware-architecture","text":"","title":"Hardware Architecture"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#robot-subsystems","text":"","title":"Robot Subsystems"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#base","text":"The base is a two wheel differential drive with a passive Mecanum wheel for a caster. It includes four cliff sensors to allow detection of stairs, thresholds, etc. Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm The base has 6 M4 threaded inserts available for mounting user accessories such as a tray. The mounting pattern is shown below. The inserts are recessed 1mm from the top of the base shell.","title":"Base"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#base-imu","text":"The base has a 9 DOF IMU using the 9 DOF FXOS8700 + FXAS21002 chipset. The IMU orientation is as shown below:","title":"Base IMU"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#trunk","text":"Development and charge ports are at the back of the base in the trunk. Item Notes A Vent Intake vent for computer fan B 6 Port USB Hub USB 3.0 , powered 5V/3A C Ethernet Connected to computer NIC D On/Off Robot power on / off. Switch is illuminated when on. E Charge Rated for upplied 12V/7A charger F HDMI Connected to computer HDMI G LED Light Bar Indicates battery voltage H 12V access plug Allows customer cable access to 12V Aux on Pimu PCBA","title":"Trunk"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#head","text":"The head provides the audio interface to the robot, a pan tilt depth camera, a runstop, as well as a developer interface to allow the addition of additional user hardware. Item Notes A Pan tilt depth camera Intel RealSense D435i Two Dynamixel XL430-W250-T servos B Speakers D Developer Interface Volume control, USB2.0-A with 5V@500mA fused JST XHP-2, 12V@3A fused Pin 1: 12V Pin 2: GND E Microphone array With programmable 12 RGB LED ring F Runstop","title":"Head"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#pan-tilt","text":"The head pan-tilt unit utilizes two Dynamixel XL430-W250-T servos. It incorporates a small fan in order to ensure proper cooling of the servo and camera during dynamic repeated motions of the tilt DOF. The nominal \u2018zero\u2019 position is of the head is shown below, along with the corresponding range of motion. DOF Range (deg) Min(deg) Max (deg) Pan 346 -234 112 Tilt 115 -25 90","title":"Pan Tilt"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#respeaker-microphone-array","text":"The ReSPeaker has 12 RGB LEDs that can be controlled programatically. By default they display sound intensity and direction of the microphone array. The ReSpeaker has 4 mems microphones mounted on a 64.61mm circle at 45 degree spacing. The drawing below shows the position and orientation of the microphone array relative to the head pan axis.","title":"ReSpeaker Microphone Array"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#mounting-points","text":"The top of the head includes 3x M4 threaded mounting points as shown below.","title":"Mounting Points"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#runstop","text":"The runstop allows the user to pause the motion of the four primary DOF (base, lift, and arm) by tapping the illuminated button on the head. When the runstop is enabled, these DOF are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume.","title":"Runstop"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#lift","text":"The lift degree of freedom provides vertical translation of the arm. It is driven by a closed loop stepper motor, providing smooth and precise motion through a low gear-ratio belt drive. The \u2018shoulder\u2019 includes four mounting holes and a small delivery tray. NOTE : When using the shoulder mounting screws do not use M4 bolts longer than 7mm in length. Otherwise damage may occur. Item Notes A Delivery tray B Mounting holes Threaded M4. Length not to exceed 7mm. C Aruco Tag Size 40x40 mm D Developer ports USB2.0-A with 5V@500mA fused ; JST XHP-2, 12V@3A fused Pin 1: 12V Pin 2: GND","title":"Lift"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#arm","text":"The arm comprises 5 telescoping carbon fiber links set on rollers. Its proprietary drive train is driven by a stepper motor with closed loop control and current sensing, allowing contact sensitivity during motion. The arm exhibits a small amount of play (lash) in the X, Y, Z, and theta directions which is a normal characteristic of its design. Despite this it can achieve good repeatability, in part because its gravity loading is fairly constant. The retracted arm and wrist combined are designed to fit within the footprint of the base. The arm is designed to have: Reach: 0.52m","title":"Arm"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist","text":"The wrist includes: Yaw DOF to allow for stowing of the tool 2 Aruco tags for calibration and visual localization of the tool Expansion port with Arduino expansion header USB-A connector Tool plate with dual sided mounting Dynamixel X-Series TTL bus","title":"Wrist"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-control-interface","text":"The wrist yaw degree-of-freedom uses a Dynamixel XL430 servo . Additional Dynamixel servos can be daisy chained off of this servo, allowing for one more additional degree-of-freedoms to be easily integrated onto the robot (such as the provided Stretch Gripper). Stretch comes with a XL430 compatible control cable preinstalled into this servo. If a different cable needs to be installed the servo cap can be removed as shown.","title":"Wrist Control Interface"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-tool-plate","text":"The tool plate allows for mounting on the top or the bottom using the M2 bolt pattern. The mounting pattern is compatible with Robotis Dynamixel frames as well: FR12-H101K FR12-S102K FR12-S101K The tool plate includes a 'Zero indicator'. This mark indicates the forward position of the tool. It will point in the direction of the arm extension when the wrist yaw joint is at its zero position. In addition, the tool plate includes two index holes. These can be used to index a tool (e.g., Stretch Gripper) during installation. Compatible pins on the tool ensure that it is installed at the correct orientation.","title":"Wrist Tool Plate"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-yaw-range-of-motion","text":"The wrist yaw DOF is calibrated so that the index hole faces forward at the 'zero' position. From this pose the wrist has a ROM of +256/-76 degrees as shown.","title":"Wrist Yaw Range of Motion"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-accelerometer","text":"The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The sensor is mounted inside the distal link of the arm as shown below.","title":"Wrist Accelerometer"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-expansion-usb","text":"The wrist includes a USB 2.0 A interface. This power to this USB port is fused to 500mA@5V.","title":"Wrist Expansion USB"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-expansion-header","text":"The wrist includes an expansion header that provides access to pins of the wrist Arduino board. The header connector can be accessed by removing the cap at the end of the arm. The header is wired to a Atmel SAMD21G18A-AUT ( datasheet ) microcontroller (same as Arduino Zero). The expansion header pins are configured at the factory to allow: General purpose digital I/O Analog input In addition, the firmware can be configured for other pin functions, including: Serial SPI Serial I2C Serial UART The Stretch Firmware Manual covers this modification. The header pins utilize 3V3 TTL logic. They have limited nterface protection (eg, ESD, over-voltage, shorts). It is possible to damage your robot if pin specifications are exceeded The pin mapping is: Pin Name Function Factory Firmware 1 DGND Digital ground 2 3V3 3.3V supply fused at 250mA. 3 E12V 12VDC fused at 500mA 4 SS DIO | SPI SS Digital out (D3) 5 SCK DIO | SPI SCK Digital out (D2) 6 MISO DIO | SPI MISO |UART TX Digital in (D0) 7 MOSI DIO | SPI MOSI | UART RX Digital in (D1) 8 SCL DIO | I2C SCL Not used 9 SS DIO | I2C SDA Not used 10 ANA0 Analog input Analog in (A0) The expansion DIO uses a 10 pin JST header B10B-PHDSS(LF)(SN) . It is compatible with a JST PHDR-10VS housing. JST provides pre-crimped wire compatible with this housing ( part APAPA22K305 ). Pin 1 & 10 are indicated below. The expansion DIO schematic shown below.","title":"Wrist Expansion Header"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#wrist-mounts","text":"","title":"Wrist Mounts"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#gripper","text":"The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position. As shown, it includes mounting features on one side to allow for attachment of simple rigid tools such as hooks and pullers . Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 The attachment features are spaced at 9mm. The weight of the Stretch Compliant Gripper is 240g.","title":"Gripper"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#gripper-removal","text":"Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse. Unplug the Dynamixel cable from the back of the gripper. Remove the 4 screws holding the gripper to the bracket. Remove the gripper from the mounting bracket Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate.","title":"Gripper Removal"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#robot-care","text":"","title":"Robot Care"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#battery-maintenance","text":"Please review the Battery Maintenance Guide for proper care and charging of the Stretch batteries.","title":"Battery Maintenance"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#belt-tension","text":"A neoprene timing belt drives the arm up and down the lift. It may detension over long periods of time if it experiences sustained loading. In this case, slack will become visually apparent in the belt as the lift moves. The belt is very straightforward to re-tension. Please contact support@hello-robot.com for tensioning instructions.","title":"Belt Tension"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#keeping-the-robot-clean","text":"The robot surfaces can be wiped down with an alcohol wipe or a moist rag from time to time in order to remove and debris or oils that accumulate on the shells or mast. The drive wheels can accumulate dust over time and begin to lose traction. They should be periodically wiped down as well. When possible, the Trunk cover for the base should be kept on in order to keep dust and debris out of the Trunk connectors. If the D435i camera requires cleaning use appropriate lens cleaning fluid and a microfiber cloth.","title":"Keeping the Robot Clean"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#keeping-the-robot-calibrated","text":"The robot comes pre-calibrated with a robot-specific URDF. This calibration allows the D435i depth sensor to accurately estimate where the robot wrist, and body, is in the depth image. The robot may become slightly uncalibrated over time for a variety of reasons: Normal wear and tear and loosening of joints of the robot The head structure is accidentally load and the structure becomes very slightly bent The wrist and should structure become accidentally highly loaded and become slightly bent The calibration accuracy can be checked using the provided ROS tools. If necessary, the user can recalibrate the robot. See the Stretch URDF Calibration Guide for more information.","title":"Keeping the Robot Calibrated"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#transporting-the-robot","text":"Stretch was designed to be easily transported in the back of a car, up a stair case, or around a building. For short trips, the robot can be simply rolled around by grabbing its mast. It may be picked up by its mast and carried up stairs as well. For safety, please use two people to lift the robot. For longer trips it is recommended to transport the robot in its original cardboard box with foam packaging. The metal protective cage that surrounds the head is only necessary if the robot might be shipped and the box will not remain upright.","title":"Transporting the Robot"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#system-check","text":"It is useful to periodically run stretch_robot_system_check.py. This will check that the robot's hardware devices are present and within normal operating conditions. $ stretch_robot_system_check.py ---- Checking Devices ---- [Pass] : hello-wacc [Pass] : hello-motor-left-wheel [Pass] : hello-motor-arm [Pass] : hello-dynamixel-wrist [Pass] : hello-motor-right-wheel [Pass] : hello-motor-lift [Pass] : hello-pimu [Pass] : hello-respeaker [Pass] : hello-lrf [Pass] : hello-dynamixel-head ---- Checking Pimu ---- [Pass] Voltage = 12.8763639927 [Pass] Current = 3.25908634593 [Pass] Temperature = 36.3404559783 [Pass] Cliff-0 = -4.72064208984 [Pass] Cliff-1 = -8.56213378906 [Pass] Cliff-2 = 1.08505249023 [Pass] Cliff-3 = 5.68453979492 [Pass] IMU AZ = -9.80407142639 ---- Checking EndOfArm ---- [Dynamixel ID:013] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: wrist_yaw [Pass] Calibrated: wrist_yaw [Dynamixel ID:014] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: stretch_gripper [Pass] Calibrated: stretch_gripper ---- Checking Head ---- [Dynamixel ID:012] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: head_tilt [Dynamixel ID:011] ping Succeeded. Dynamixel model number : 1060 [Pass] Ping of: head_pan ---- Checking Wacc ---- [Pass] AX = 9.4840593338 ---- Checking hello-motor-left-wheel ---- [Pass] Position = 43.9992256165 ---- Checking hello-motor-right-wheel ---- [Pass] Position = 15.1164712906 ---- Checking hello-motor-arm ---- [Pass] Position = 59.7719421387 [Pass] Position Calibrated = True ---- Checking hello-motor-lift ---- [Pass] Position = 83.7744064331 [Pass] Position Calibrated = True ---- Checking for Intel D435i ---- Bus 002 Device 016: ID 8086:0b3a Intel Corp. [Pass] : Device found","title":"System Check"},{"location":"stretch-hardware-guides/docs/hardware_guide_re2/#regulatory-compliance","text":"The Stretch Research Edition 1 (Stretch re2) is not certified for use as a consumer device in the U.S. Unless stated otherwise, the Stretch re2 is not subjected to compliance testing nor certified to meet any requirements, such as requirements for EMI, EMC, or ESD. Per FCC 47 CFR, Part 15, Subpart B, section 15.103(c) , we claim the Stretch Research Edition 1 as an exempted device, since it is a digital device used exclusively as industrial, commercial, or medical test equipment, where test equipment is equipment intended primarily for purposes of performing scientific investigations. OET BULLETIN NO. 62 , titled \"UNDERSTANDING THE FCC REGULATIONS FOR COMPUTERS AND OTHER DIGITAL DEVICES\" from December 1993 provides further clarification of the Section 15.103(c) exemption: \u201c Test equipment includes devices used for maintenance, research, evaluation, simulation and other analytical or scientific applications in areas such as industrial plants, public utilities, hospitals, universities, laboratories, automotive service centers and electronic repair shops.\u201d All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Regulatory Compliance"},{"location":"stretch-install/","text":"The stretch_install repository provides scripts required to install the Stretch software. Guides Guide Purpose Updating Software Updating the various components of Stretch's software + troubleshooting Adding a New User Creating a new Ubuntu user and setting it up with Stretch packages and robot configuration Creating a New ROS Workspace Creating and compiling a new ROS workspace, either ROS1's catkin or ROS2's ament Performing a Robot Install Installing a new operating system and setting it up with the default Ubuntu user and full software stack Configuring the DexWrist Changing a user's robot configuration to enable usage of the Dex Wrist Contributing Thank you for considering contributing to this repo! Please take a look at the contribution guide for more details. License All Hello Robot installation materials are released under the GNU General Public License v3.0 (GNU GPLv3). Details can be found in the LICENSE file.","title":"Overview"},{"location":"stretch-install/#guides","text":"Guide Purpose Updating Software Updating the various components of Stretch's software + troubleshooting Adding a New User Creating a new Ubuntu user and setting it up with Stretch packages and robot configuration Creating a New ROS Workspace Creating and compiling a new ROS workspace, either ROS1's catkin or ROS2's ament Performing a Robot Install Installing a new operating system and setting it up with the default Ubuntu user and full software stack Configuring the DexWrist Changing a user's robot configuration to enable usage of the Dex Wrist","title":"Guides"},{"location":"stretch-install/#contributing","text":"Thank you for considering contributing to this repo! Please take a look at the contribution guide for more details.","title":"Contributing"},{"location":"stretch-install/#license","text":"All Hello Robot installation materials are released under the GNU General Public License v3.0 (GNU GPLv3). Details can be found in the LICENSE file.","title":"License"},{"location":"stretch-install/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains installation software for use with the Stretch RE1 and RE2 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. This software is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link: https://www.gnu.org/licenses/gpl-3.0.html For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-install/docs/add_new_user/","text":"Adding a New User Why When multiple people are working with the robot, it can be helpful to create separate Ubuntu user accounts. Here's a few reasons you might add a new user: Your code and data is protected by password in your user account Your code is dependent on a specific version of Stretch's software and you don't want other users to accidentally upgrade it (NOTE: some of Stretch's software is installed per user, e.g. Stretch Body and Stretch ROS, whereas other parts of the software is installed per OS, e.g. ROS1 or ROS2) Logs from your experiments will not be mixed with logs from other users Finally, note that changes made in your user account can break another user account's set up. For example, if you configure a different gripper or end effector onto the robot with your account, other accounts will have outdated configurations for what gripper is attached to the robot. How Login to the \"hello-robot\" user. The \"hello-robot\" user account has administrator privileges. Go to Users system settings and unlock the administrator actions. Click \"Add User...\" and complete the subsequent form. Logout and the log back in as the new user. Open a terminal and execute the following to pull down the Stretch Install repository. cd ~ git clone https://github.com/hello-robot/stretch_install cd stretch_install git pull Execute the following to set up the new user account with Stretch packages and the robot's configuration data. ./stretch_new_user_install.sh Finally, reboot your robot and execute the following to confirm the new user account was set up successfully. stretch_robot_system_check.py Your new user account is now set up successfully!","title":"Adding New Users"},{"location":"stretch-install/docs/add_new_user/#adding-a-new-user","text":"","title":"Adding a New User"},{"location":"stretch-install/docs/add_new_user/#why","text":"When multiple people are working with the robot, it can be helpful to create separate Ubuntu user accounts. Here's a few reasons you might add a new user: Your code and data is protected by password in your user account Your code is dependent on a specific version of Stretch's software and you don't want other users to accidentally upgrade it (NOTE: some of Stretch's software is installed per user, e.g. Stretch Body and Stretch ROS, whereas other parts of the software is installed per OS, e.g. ROS1 or ROS2) Logs from your experiments will not be mixed with logs from other users Finally, note that changes made in your user account can break another user account's set up. For example, if you configure a different gripper or end effector onto the robot with your account, other accounts will have outdated configurations for what gripper is attached to the robot.","title":"Why"},{"location":"stretch-install/docs/add_new_user/#how","text":"Login to the \"hello-robot\" user. The \"hello-robot\" user account has administrator privileges. Go to Users system settings and unlock the administrator actions. Click \"Add User...\" and complete the subsequent form. Logout and the log back in as the new user. Open a terminal and execute the following to pull down the Stretch Install repository. cd ~ git clone https://github.com/hello-robot/stretch_install cd stretch_install git pull Execute the following to set up the new user account with Stretch packages and the robot's configuration data. ./stretch_new_user_install.sh Finally, reboot your robot and execute the following to confirm the new user account was set up successfully. stretch_robot_system_check.py Your new user account is now set up successfully!","title":"How"},{"location":"stretch-install/docs/configure_BIOS/","text":"Configuring the BIOS This documentation describes how to configure the BIOS of an Intel NUC for compatibility with the stretch installation procedure. Accessing the NUC BIOS Settings First plug in the NUC to a 19V DC power supply. Next power on the NUC using the power button on the fron of the NUC. When powered on, the NUC should display a welcome screen similar to the picture below: When this label becomes visible press 'F2' to enter into the BIOS configuration menu. The BIOS Settings page should look like the picture below: Select the 'Advanced' drop down menu near the top right of the screen, and then slect the option 'Boot' From the 'Boot' settings page select the 'Secure Boot' tab. Turn off 'Secure Boot' by toggling the checkbox labeled 'Secure Boot' to unchecked. Next Select the 'Power' tab From the power settings screen select the 'Power On' option from the 'After Power Failure' drop down selection. Next Select the Security tab Turn on UEFI third party drivers compatibility by toggling the checkbox labeled 'Allow UEFI Third Party Driver loaded' to checked. Now use the F10 key to save BIOS configuration changes and exit.","title":"Configuring the BIOS"},{"location":"stretch-install/docs/configure_BIOS/#configuring-the-bios","text":"This documentation describes how to configure the BIOS of an Intel NUC for compatibility with the stretch installation procedure.","title":"Configuring the BIOS"},{"location":"stretch-install/docs/configure_BIOS/#accessing-the-nuc-bios-settings","text":"First plug in the NUC to a 19V DC power supply. Next power on the NUC using the power button on the fron of the NUC. When powered on, the NUC should display a welcome screen similar to the picture below: When this label becomes visible press 'F2' to enter into the BIOS configuration menu. The BIOS Settings page should look like the picture below: Select the 'Advanced' drop down menu near the top right of the screen, and then slect the option 'Boot' From the 'Boot' settings page select the 'Secure Boot' tab. Turn off 'Secure Boot' by toggling the checkbox labeled 'Secure Boot' to unchecked. Next Select the 'Power' tab From the power settings screen select the 'Power On' option from the 'After Power Failure' drop down selection. Next Select the Security tab Turn on UEFI third party drivers compatibility by toggling the checkbox labeled 'Allow UEFI Third Party Driver loaded' to checked. Now use the F10 key to save BIOS configuration changes and exit.","title":"Accessing the NUC BIOS Settings"},{"location":"stretch-install/docs/contributing/","text":"Contributing to Stretch Install Thank you for considering contributing to this repository. Stretch Install houses bash scripts and tutorials that enable users to setup/configure their robots. This guide explains the layout of this repo and how best to make and test changes. Repo Layout README.md & LICENSE.md - includes info about the repo and a table of tutorials available stretch_new_*_install.sh - high level scripts meant to be run by the user factory/ - subscripts and assets not meant to be run by the user 18.04/ - subscripts and assets specific to performing a Ubuntu 18.04 software install stretch_initial_setup.sh - a bunch of checks and initial setup that are run before performing a robot install stretch_install_*.sh - helper scripts that install a specific set of packages stretch_create_*_workspace.sh - creates a ROS/ROS2 workspace stretch_ros*.repos - the ROS packages that are included and compiled in the ROS workspace by the stretch_create_*_workspace.sh script hello_robot_*.desktop - autostarts programs to run when the robot boots up <>.04/ - Ubuntu <>.04 software install related subscripts/assets. Similar in layout to 18.04/ docs/ - contains tutorials for using the scripts in this repo Once you're ready to make changes to this repo, you can fork it on Github . Contributing to the tutorials The tutorials in the docs/ folder are markdown files that get rendered into our https://docs.hello-robot.com site. If you edit them and file a pull request towards this repo, the changes to the tutorials will get reflected on the docs site. In order to live preview changes to these tutorials, first install mkdocs using: python3 -m pip install mkdocs mkdocs-material mkdocstrings == 0 .17.0 pytkdocs [ numpy-style ] jinja2 = 3 .0.3 Then, run the dev server using: python3 -m mkdocs serve Now you can make additions or changes to the source markdown files and see the changes reflected live in the browser. Contributing to the installation scripts If you are looking to change scripts/assets of an existing software installation (e.g. Ubuntu 18.04/20.04), look within the factory/<>.04/ directory and make changes to the appriopriate files. If you're looking to add support for a new Ubuntu distro (e.g. Ubuntu 19.04), create factory/19.04 with assets from a previous installation and tweak the scripts until they works correctly for the Ubuntu distro you are targeting. Then, edit the high level scripts (e.g. stretch_new_*_install.sh ) to call your distro's specific assets correctly. Ensuring that the tutorials in the docs/ work for your new distro is a good way to ensure that your factory/<>.04/ directory works correctly. Since bash scripts change behavior based on the underlying system, it can be helpful to use containers to create reproducible behaviors while you're developing support for the new distro. Multipass works well on Ubuntu systems. You can create a new container emulating any Ubuntu distro using the command: multipass launch -c 6 -d 30G -m 7G -n <container-name> 19 .04 Swap 19.04 in the above command with the distro you're targeting. The above command creates a containers with 30GB disk space, 7GB swap, 6 cores, and the name <container-name> . We've found that at least 30GB disk space is needed for the Ubuntu 18.04/20.04 installations. Then, you can access the shell of your new container using: multipass shell <container-name> Other helpful multipass subcommand include transfer , which allows you to transfer files to the container, and delete , which allows you to delete the container. See the multipass docs for more details. Filing a Pull Request Once your changes are committed to your fork, you can open a pull request towards the Stretch Install master branch. A member of Hello Robot's software team will review your new PR and get it merged and available for all Stretch users.","title":"Contributing to Stretch Install"},{"location":"stretch-install/docs/contributing/#contributing-to-stretch-install","text":"Thank you for considering contributing to this repository. Stretch Install houses bash scripts and tutorials that enable users to setup/configure their robots. This guide explains the layout of this repo and how best to make and test changes.","title":"Contributing to Stretch Install"},{"location":"stretch-install/docs/contributing/#repo-layout","text":"README.md & LICENSE.md - includes info about the repo and a table of tutorials available stretch_new_*_install.sh - high level scripts meant to be run by the user factory/ - subscripts and assets not meant to be run by the user 18.04/ - subscripts and assets specific to performing a Ubuntu 18.04 software install stretch_initial_setup.sh - a bunch of checks and initial setup that are run before performing a robot install stretch_install_*.sh - helper scripts that install a specific set of packages stretch_create_*_workspace.sh - creates a ROS/ROS2 workspace stretch_ros*.repos - the ROS packages that are included and compiled in the ROS workspace by the stretch_create_*_workspace.sh script hello_robot_*.desktop - autostarts programs to run when the robot boots up <>.04/ - Ubuntu <>.04 software install related subscripts/assets. Similar in layout to 18.04/ docs/ - contains tutorials for using the scripts in this repo Once you're ready to make changes to this repo, you can fork it on Github .","title":"Repo Layout"},{"location":"stretch-install/docs/contributing/#contributing-to-the-tutorials","text":"The tutorials in the docs/ folder are markdown files that get rendered into our https://docs.hello-robot.com site. If you edit them and file a pull request towards this repo, the changes to the tutorials will get reflected on the docs site. In order to live preview changes to these tutorials, first install mkdocs using: python3 -m pip install mkdocs mkdocs-material mkdocstrings == 0 .17.0 pytkdocs [ numpy-style ] jinja2 = 3 .0.3 Then, run the dev server using: python3 -m mkdocs serve Now you can make additions or changes to the source markdown files and see the changes reflected live in the browser.","title":"Contributing to the tutorials"},{"location":"stretch-install/docs/contributing/#contributing-to-the-installation-scripts","text":"If you are looking to change scripts/assets of an existing software installation (e.g. Ubuntu 18.04/20.04), look within the factory/<>.04/ directory and make changes to the appriopriate files. If you're looking to add support for a new Ubuntu distro (e.g. Ubuntu 19.04), create factory/19.04 with assets from a previous installation and tweak the scripts until they works correctly for the Ubuntu distro you are targeting. Then, edit the high level scripts (e.g. stretch_new_*_install.sh ) to call your distro's specific assets correctly. Ensuring that the tutorials in the docs/ work for your new distro is a good way to ensure that your factory/<>.04/ directory works correctly. Since bash scripts change behavior based on the underlying system, it can be helpful to use containers to create reproducible behaviors while you're developing support for the new distro. Multipass works well on Ubuntu systems. You can create a new container emulating any Ubuntu distro using the command: multipass launch -c 6 -d 30G -m 7G -n <container-name> 19 .04 Swap 19.04 in the above command with the distro you're targeting. The above command creates a containers with 30GB disk space, 7GB swap, 6 cores, and the name <container-name> . We've found that at least 30GB disk space is needed for the Ubuntu 18.04/20.04 installations. Then, you can access the shell of your new container using: multipass shell <container-name> Other helpful multipass subcommand include transfer , which allows you to transfer files to the container, and delete , which allows you to delete the container. See the multipass docs for more details.","title":"Contributing to the installation scripts"},{"location":"stretch-install/docs/contributing/#filing-a-pull-request","text":"Once your changes are committed to your fork, you can open a pull request towards the Stretch Install master branch. A member of Hello Robot's software team will review your new PR and get it merged and available for all Stretch users.","title":"Filing a Pull Request"},{"location":"stretch-install/docs/install_ubuntu_18.04/","text":"Ubuntu 18.04 Installation This guide describes how to perform an OS installation of Ubuntu 18.04 LTS onto Stretch. Ubuntu Image Download the 18.04.1 amd64 Ubuntu desktop image by clicking this link: http://old-releases.ubuntu.com/releases/18.04.1/ubuntu-18.04.1-desktop-amd64.iso Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive. Installation Insert a bootable drive into the USB hub in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu. From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\". From here, the monitor should show the grub bootloader and display a menu similar to what is shown below: From this menu select 'Install Ubuntu'. The Ubuntu 18.04 installer will be launched. At the first screen you will be prompted to select a language for the system. Select 'English' as shown below Next you will be prompted to select a keyboard layout. Select 'English(US)'. The next page will show a menu to select a wifi network if you are not already connected. It is suggested to use a wired connection if possible for a faster install; your connection status should be visible in the top right of the display. On the next page titled, 'Updates and other software', select 'Minimal Installation' under 'What apps would you like to install to start with?' Also, check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and, uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'. Erase & Reinstall vs Install Alongside On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 18.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase & reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB. Here's what the Erase & Reinstall option will look like, and an screenshot of the Install Alongside option is shown below. There will be a prompt to confirm you wish to create the appropriate partitions for the ubuntu install. If you've chosen the erase & reinstall option, ensure there is nothing on the hard drive you wish to save before selecting continue Next, select your timezone. Finally, enter the identifying information as written below replacing '1000' with the appropriate serial number for the robot. The robot's serial number can be found on the left wall of the robot's trunk. name: Hello Robot Inc. computer name: stretch-re1-1000 username: hello-robot password: xxxx Also select the 'Log in automatically' option. When finished the 'Who are you' page should look like the picture below. Ubuntu will now be installed. After the installation is completed, you will be prompted to remove the installation medium and restart. Remove the installation medium and turn off the robot. Ubuntu 18.04 is now installed successfully.","title":"Ubuntu 18.04 Installation"},{"location":"stretch-install/docs/install_ubuntu_18.04/#ubuntu-1804-installation","text":"This guide describes how to perform an OS installation of Ubuntu 18.04 LTS onto Stretch.","title":"Ubuntu 18.04 Installation"},{"location":"stretch-install/docs/install_ubuntu_18.04/#ubuntu-image","text":"Download the 18.04.1 amd64 Ubuntu desktop image by clicking this link: http://old-releases.ubuntu.com/releases/18.04.1/ubuntu-18.04.1-desktop-amd64.iso Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive.","title":"Ubuntu Image"},{"location":"stretch-install/docs/install_ubuntu_18.04/#installation","text":"Insert a bootable drive into the USB hub in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu. From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\". From here, the monitor should show the grub bootloader and display a menu similar to what is shown below: From this menu select 'Install Ubuntu'. The Ubuntu 18.04 installer will be launched. At the first screen you will be prompted to select a language for the system. Select 'English' as shown below Next you will be prompted to select a keyboard layout. Select 'English(US)'. The next page will show a menu to select a wifi network if you are not already connected. It is suggested to use a wired connection if possible for a faster install; your connection status should be visible in the top right of the display. On the next page titled, 'Updates and other software', select 'Minimal Installation' under 'What apps would you like to install to start with?' Also, check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and, uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'.","title":"Installation"},{"location":"stretch-install/docs/install_ubuntu_18.04/#erase-reinstall-vs-install-alongside","text":"On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 18.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase & reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB. Here's what the Erase & Reinstall option will look like, and an screenshot of the Install Alongside option is shown below. There will be a prompt to confirm you wish to create the appropriate partitions for the ubuntu install. If you've chosen the erase & reinstall option, ensure there is nothing on the hard drive you wish to save before selecting continue Next, select your timezone. Finally, enter the identifying information as written below replacing '1000' with the appropriate serial number for the robot. The robot's serial number can be found on the left wall of the robot's trunk. name: Hello Robot Inc. computer name: stretch-re1-1000 username: hello-robot password: xxxx Also select the 'Log in automatically' option. When finished the 'Who are you' page should look like the picture below. Ubuntu will now be installed. After the installation is completed, you will be prompted to remove the installation medium and restart. Remove the installation medium and turn off the robot. Ubuntu 18.04 is now installed successfully.","title":"Erase &amp; Reinstall vs Install Alongside"},{"location":"stretch-install/docs/install_ubuntu_20.04/","text":"Ubuntu 20.04 Installation This guide describes how to perform an OS installation of Ubuntu 20.04 LTS onto Stretch. Ubuntu Image Download the 20.04.4 amd64 Ubuntu desktop image . Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive. Installation Insert a bootable drive into the USB hub in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu. From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\". From here, the monitor should show the grub bootloader and display a menu similar to what is shown below: From this menu select 'Ubuntu'. A disk errors checker will start and then the Ubuntu 20.04 installer will be launched. At the first screen you will be prompted to select a language for the system. Select 'English' as shown below and click on the \"Install Ubuntu\". Next you will be prompted to select a keyboard layout. Select 'English(US)'. The next page will show a menu to select a Wifi network if you are not already connected. It is suggested to use a wired connection if possible for a faster install; your connection status should be visible in the top right of the display. On the next page titled, 'Updates and other software', select 'Minimal Installation' under 'What apps would you like to install to start with?' Also, check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and, uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'. Erase & Reinstall vs Install Alongside On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 20.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase & reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB. Here's what the Erase & Reinstall option will look like, and an screenshot of the Install Alongside option is shown below. There will be a prompt to confirm you wish to create the appropriate partitions for the ubuntu install. If you've chosen the erase & reinstall option, ensure there is nothing on the hard drive you wish to save before selecting continue Next, select your timezone. Finally, enter the identifying information as written below replacing '1000' with the appropriate serial number for the robot. The robot's serial number can be found on the left wall of the robot's trunk. name: Hello Robot Inc. computer name: stretch-re1-1000 username: hello-robot password: xxxx Also select the 'Log in automatically' option. When finished the 'Who are you' page should look like the picture below. Ubuntu will now be installed. After the installation is completed, you will be prompted to remove the installation medium and restart. Remove the installation medium and press ENTER to restart. Ubuntu 20.04 is now installed successfully.","title":"Ubuntu 20.04 Installation"},{"location":"stretch-install/docs/install_ubuntu_20.04/#ubuntu-2004-installation","text":"This guide describes how to perform an OS installation of Ubuntu 20.04 LTS onto Stretch.","title":"Ubuntu 20.04 Installation"},{"location":"stretch-install/docs/install_ubuntu_20.04/#ubuntu-image","text":"Download the 20.04.4 amd64 Ubuntu desktop image . Create a bootable drive with this Ubuntu image. There are many ways to do this, but the recommended way is to use Etcher on your personal machine. Open the Etcher software and follow it's instructions to create the bootable drive.","title":"Ubuntu Image"},{"location":"stretch-install/docs/install_ubuntu_20.04/#installation","text":"Insert a bootable drive into the USB hub in the robot's trunk, as well as a monitor and keyboard. Next, power on the robot and at the bios startup screen (shown below) press F10 when prompted to enter the boot menu. From the boot menu, select 'OS BOOTLOADER' or look for a similar option that mentions \"USB\", \"LIVE INSTALLATION\", or \"UBUNTU\". From here, the monitor should show the grub bootloader and display a menu similar to what is shown below: From this menu select 'Ubuntu'. A disk errors checker will start and then the Ubuntu 20.04 installer will be launched. At the first screen you will be prompted to select a language for the system. Select 'English' as shown below and click on the \"Install Ubuntu\". Next you will be prompted to select a keyboard layout. Select 'English(US)'. The next page will show a menu to select a Wifi network if you are not already connected. It is suggested to use a wired connection if possible for a faster install; your connection status should be visible in the top right of the display. On the next page titled, 'Updates and other software', select 'Minimal Installation' under 'What apps would you like to install to start with?' Also, check the box next to 'Download updates while installing Ubuntu' (this option will be unavailable if there is no interent connection) and, uncheck 'Install third-party software for graphics and Wi-Fi hardware and additional media formats'.","title":"Installation"},{"location":"stretch-install/docs/install_ubuntu_20.04/#erase-reinstall-vs-install-alongside","text":"On the next page titled 'Installation type', you may choose between 'Erase disk and reinstall Ubuntu' or 'Install Ubuntu 20.04 alongside Ubuntu XX.04'. If you've already backed up data from the previous partition, or the previous partition is corrupted, select the erase & reinstall option. If you'd like to preserve your previous Ubuntu partition, select the alongside option. If you choose the alongside option, another screen will allow you to change the size of each partition. It's recommended to give each partition at least 50GB. Here's what the Erase & Reinstall option will look like, and an screenshot of the Install Alongside option is shown below. There will be a prompt to confirm you wish to create the appropriate partitions for the ubuntu install. If you've chosen the erase & reinstall option, ensure there is nothing on the hard drive you wish to save before selecting continue Next, select your timezone. Finally, enter the identifying information as written below replacing '1000' with the appropriate serial number for the robot. The robot's serial number can be found on the left wall of the robot's trunk. name: Hello Robot Inc. computer name: stretch-re1-1000 username: hello-robot password: xxxx Also select the 'Log in automatically' option. When finished the 'Who are you' page should look like the picture below. Ubuntu will now be installed. After the installation is completed, you will be prompted to remove the installation medium and restart. Remove the installation medium and press ENTER to restart. Ubuntu 20.04 is now installed successfully.","title":"Erase &amp; Reinstall vs Install Alongside"},{"location":"stretch-install/docs/robot_install/","text":"Performing a Robot Installation Why A fresh robot-level install should only be done under the guidance of Hello Robot Support . This guide will lead you through a robot-level install, which can be used to: Erase the previous OS and set up Stretch with an entirely fresh software stack Erase a corrupted OS and set up Stretch with an entirely fresh software stack Upgrade Stretch by installing the 20.04 software stack alongside the previous OS Each OS installs on a separate partition on the hard drive. You can create as many robot-level installs (i.e. new partitions) as will fit in your robot's 500GB hard drive. Currently, there are two available versions of the software stack, one with Ubuntu 18.04 LTS and another with Ubuntu 20.04 LTS. Ubuntu 18.04 LTS shipped on robots until summer 2022, and included software for ROS Melodic and Python2. Ubuntu 20.04 LTS, the newest software stack, comes with ROS Noetic, Python3, and experimental ROS2 Galactic software. You can look at your robot's About page in system settings to identify which OS it is running. How There are a few steps to performing a new robot install: Backup robot configuration data Setup the BIOS (only necessary for NUCs not previously configured by Hello Robot) Install Ubuntu 18.04 or 20.04 Run the new robot installation script It typically takes ~2 hours to go through these steps and complete a new robot install. Back up robot configuration data It is a good idea to backup all valuable data beforehand. If your new robot install will replace a previous one, data from the previous robot install will be deleted. Even if your new robot install will live alongside the previous one(s), data from the previous robot install(s) can be lost. In particular, your new robot install will require the old install's robot calibration data. The steps to copy this material from an existing install is: Boot into the robot's original Ubuntu partition and plug in a USB key. The robot calibration data lives inside of a directory called stretch-re<y>-<xxxx> , where <y> is your robot model number (1 or 2), and <xxxx> is your robot's serial number. There's a few versions of this directory and you will need to decide which version to backup. Each Ubuntu user has a version of this directory located at /home/$USER/stretch_user/stretch-re<y>-<xxxx> . These user versions are updated when the user runs a URDF calibration, swaps out an end effector, updates Stretch parameters, and more. There's also a system version located at /etc/hello-robot/stretch-re<y>-<xxxx> , which is likely the oldest version since it was created at Hello Robot HQ. If you're not sure which version to backup, use the version at /etc/hello-robot/stretch-re<y>-<xxxx> for the next step. Copy the stretch-re<y>-<xxxx> directory, where <xxxx> is your robot's serial number, to a USB key. For example, if you're copying the system version, you can run a command similar to cp -r /etc/hello-robot/stretch-re<y>-<xxxx> /media/$USER/<USBKEY> from the command line, where <USBKEY> and <xxxx> is replaced with the mounted USB key's name and the robot's serial number, respectively. Or, you can open the file explorer to copy the directory. If your previous partition is corrupted or inaccessible, contact Hello Robot support and they will be able to supply an older version of the stretch-re<y>-<xxxx> directory. Setup the BIOS This step can be skipped if your robot had an existing software install on it. Otherwise, follow the guide to set up the BIOS . Install Ubuntu Choose between the following guides based on whether you're installing Ubuntu 18.04 or Ubuntu 20.04 (see above for info on what software ships with each OS). Within these guides, you'll have the choice of whether to replace the previous OS partition or to install alongside it. If you choose to install alongside it, you'll also be able to choose the size of each partition on the hard drive. Ubuntu 18.04 Installation guide . Ubuntu 20.04 Installation guide . After the Ubuntu install, the default hello-robot user account will be set up. Run the new robot installation script Login to the hello-robot user account on your new Ubuntu partition, open a terminal, and run: sudo apt update sudo apt install git zip Note : The system may not be able to run 'apt' immediately after a reboot as the OS may be running automatic updates in the background. Typically, waiting 10-20 minutes will allow you to use 'apt' again. Next, place the robot's calibration data in the home folder using the following steps: Plug in the USB key that contains the backed up calibration data. Copy the stretch-re<y>-<xxxx> directory, where <xxxx> is your robot's serial number, from the USB key into the home folder (i.e. /home/$USER/ ). For example, you can run a command similar to cp -r /media/$USER/<USBKEY>/stretch-re<y>-<xxxx> /home/$USER/ from the command line, where <USBKEY> and <xxxx> are replaced with your USB key's name and your robot's serial number, respectively. Or, you can open the file explorer to copy the directory. Next, pull down the Stretch Install repository and being the installation process: cd ~/ git clone https://github.com/hello-robot/stretch_install cd stretch_install git pull ./stretch_new_robot_install.sh Once the script has started, it will ask you for your robot's serial number, Y/N confirmation, and the password. Then, the script will typically take 20-30 minutes to complete on a wired connection. Once it finishes, it should print out something similar to: ############################################# DONE! COMPLETE THESE POST INSTALL STEPS: 1. Perform a FULL reboot by power cycling the robot [...] ############################################# If it has not printed out 'DONE', then the robot install did not complete successfully. Take a look at the troubleshooting section below for solutions to common issues, or contact Hello Robot support via email or the forum . Post install steps Next, we'll complete the post install steps. First, in order for the many changes to take effect, the robot will need a full reboot. The steps are: Ensure there's a clamp under the lift Shutdown the Ubuntu OS through the GUI or use sudo shutdown -h now in the terminal Turn the power switch in the robot's trunk to the off position (orange power LED becomes unlit) Ensure a keyboard/monitor is plugged into the robot. When the robot powers up, you can use the keyboard to decide which OS to boot into. Turn the power switch in the robot's trunk to the on position (orange power LED becomes lit) Boot into the new Ubuntu partition and log in if necessary Next, we'll ensure the robot's parameter YAML files are migrated to the new parameter management system (see https://forum.hello-robot.com/t/425/ for details). RE1_migrate_params.py Next, we'll ensure the robot's contact parameters are migrated to the new contact threshold system (see https://forum.hello-robot.com/t/476/ for details). RE1_migrate_contacts.py Next, we'll ensure the robot's firmware is upgraded to the latest available. Newer firmware unlocks new features (e.g. waypoint trajectory following, which is used in ROS2 to support MoveIt2) and fixes bugs. See the firmware releases for details. REx_firmware_updater.py --install Next, if your robot has the Stretch Dex Wrist , we'll configure the software to recognize it. Skip this step if you are not using the Dex Wrist. cd ~/stretch_install ./stretch_new_dex_wrist_install.sh REx_calibrate_guarded_contact.py --arm Finally, execute the following to confirm the new robot install was set up successfully. stretch_robot_system_check.py Your robot is now configured with a new robot install! In order to set up new users on this robot install, see the Adding a New User guide. Troubleshooting This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum . 'Expecting var HELLO_FLEET_ID to be undefined' error If you are seeing the following error: [...] Checking ~/.bashrc doesn't already define HELLO_FLEET_ID... Expecting var HELLO_FLEET_ID to be undefined. Check end of ~/.bashrc file, delete all lines in 'STRETCH BASHRC SETUP' section, and open a new terminal. Exiting. ############################################# FAILURE. INSTALLATION DID NOT COMPLETE. [...] You are performing a new robot install on a robot that has already gone through the robot install process. If this is intentional, you will need to manually delete lines that a previous robot install appended to the ~/.bashrc dotfile. Open the ~/.bashrc file in an editor and look near the end for a section that looks like: ###################### # STRETCH BASHRC SETUP ###################### export HELLO_FLEET_PATH=/home/ubuntu/stretch_user export HELLO_FLEET_ID=stretch-re1-2000 export PATH=${PATH}:~/.local/bin export LRS_LOG_LEVEL=None #Debug source /opt/ros/noetic/setup.bash #source /opt/ros/galactic/setup.bash source /home/ubuntu/catkin_ws/devel/setup.bash [...] Delete this section from the ~/.bashrc . Note that it's common for other programs (e.g. Conda, Ruby) to append to your ~/.bashrc as well, and deleting those lines accidentally can impede their functionality. Take care to only delete lines related to 'STRETCH BASHRC SETUP' section. Next, open a new terminal. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) automatically runs the commands in the ~/.bashrc dotfile when opened, so the new terminal won't be set up with the lines that were just deleted. Now you can run a new robot install and this error should gone. 'Expecting stretch-re1-xxxx to be present in the home folder' error If you are seeing the following error: [...] Checking robot calibration data in home folder... Expecting backed up version of stretch-re1-xxxx to be present in the the home folder. Exiting. ############################################# FAILURE. INSTALLATION DID NOT COMPLETE. [...] The install scripts exited before performing the robot install because it was unable to find the robot's calibration data folder, 'stretch-rey-xxxx'. Please ensure you have backed up your robot's calibration data to a USB key and copied the 'stretch-re1-xxxx' folder to the home folder of your new partition. See the Run the new robot installation script section for more details. Then, run the install scripts again and the error should be gone. 'Repo not up-to-date' error If you are seeing the following error: [...] Checking install repo is up-to-date... Repo not up-to-date. Please perform a 'git pull'. Exiting. ############################################# FAILURE. INSTALLATION DID NOT COMPLETE. [...] The version of Stretch Install being used is out of date. In a terminal, go to the Stretch Install folder (should be in the home folder: cd ~/stretch_install ), and perform a git pull to pull down the latest version. If the git pull fails, ensure Stretch Install has a clean working tree using git status . If you see any red files, save them if important, delete Stretch Install, and reclone it. 'Failed to fetch' error If you are seeing the following error: Install <some package> E: Failed to fetch <url to some .deb file> Connection failed [IP: <some IP address>] E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing? ############################################# FAILURE. INSTALLATION DID NOT COMPLETE. [...] Ubuntu's system package manager, Apt, has failed to contact the server that hosts some package that the install scripts need to download. Typically, these issues are transient and waiting some time before rerunning the install script will solve the issue. 'Firmware protocol mismatch' error If you are seeing the following error: ---------------- Firmware protocol mismatch on hello-<X>. Protocol on board is p<X>. Valid protocol is: p<X>. Disabling device. Please upgrade the firmware and/or version of Stretch Body. ---------------- Your version of Stretch Body does not align with the firmware installed with your robot. It's recommended that Stretch Body is first upgraded to the latest version available (but if you're intentionally running an older version, you can skip this step and the firmware updater will downgrade your firmware appriopriately). To upgrade Stretch Body, follow the instructions here . Next, run the firmware updater tool to automatically update the firmware to the required version for your software. REx_firmware_updater.py --install The firmware mismatch errors should now be gone. Homing Error If using stretch_robot_home.py does not result in the robot being homed, try running the command again. If this does not work, try shutting down the robot, turning off the robot with the power switch, waiting for a few seconds, and then powering it on again. Then, try stretch_robot_home.py again. ROS Launch File Fails ROS launch files have nondeterministic behavior. Sometimes they need to be run more than once for the nodes to start in a successful order that works. For example, a common symptom of a failed launch is the visualization of the robot's body appearing white and flat in RViz.","title":"Robot Install"},{"location":"stretch-install/docs/robot_install/#performing-a-robot-installation","text":"","title":"Performing a Robot Installation"},{"location":"stretch-install/docs/robot_install/#why","text":"A fresh robot-level install should only be done under the guidance of Hello Robot Support . This guide will lead you through a robot-level install, which can be used to: Erase the previous OS and set up Stretch with an entirely fresh software stack Erase a corrupted OS and set up Stretch with an entirely fresh software stack Upgrade Stretch by installing the 20.04 software stack alongside the previous OS Each OS installs on a separate partition on the hard drive. You can create as many robot-level installs (i.e. new partitions) as will fit in your robot's 500GB hard drive. Currently, there are two available versions of the software stack, one with Ubuntu 18.04 LTS and another with Ubuntu 20.04 LTS. Ubuntu 18.04 LTS shipped on robots until summer 2022, and included software for ROS Melodic and Python2. Ubuntu 20.04 LTS, the newest software stack, comes with ROS Noetic, Python3, and experimental ROS2 Galactic software. You can look at your robot's About page in system settings to identify which OS it is running.","title":"Why"},{"location":"stretch-install/docs/robot_install/#how","text":"There are a few steps to performing a new robot install: Backup robot configuration data Setup the BIOS (only necessary for NUCs not previously configured by Hello Robot) Install Ubuntu 18.04 or 20.04 Run the new robot installation script It typically takes ~2 hours to go through these steps and complete a new robot install.","title":"How"},{"location":"stretch-install/docs/robot_install/#back-up-robot-configuration-data","text":"It is a good idea to backup all valuable data beforehand. If your new robot install will replace a previous one, data from the previous robot install will be deleted. Even if your new robot install will live alongside the previous one(s), data from the previous robot install(s) can be lost. In particular, your new robot install will require the old install's robot calibration data. The steps to copy this material from an existing install is: Boot into the robot's original Ubuntu partition and plug in a USB key. The robot calibration data lives inside of a directory called stretch-re<y>-<xxxx> , where <y> is your robot model number (1 or 2), and <xxxx> is your robot's serial number. There's a few versions of this directory and you will need to decide which version to backup. Each Ubuntu user has a version of this directory located at /home/$USER/stretch_user/stretch-re<y>-<xxxx> . These user versions are updated when the user runs a URDF calibration, swaps out an end effector, updates Stretch parameters, and more. There's also a system version located at /etc/hello-robot/stretch-re<y>-<xxxx> , which is likely the oldest version since it was created at Hello Robot HQ. If you're not sure which version to backup, use the version at /etc/hello-robot/stretch-re<y>-<xxxx> for the next step. Copy the stretch-re<y>-<xxxx> directory, where <xxxx> is your robot's serial number, to a USB key. For example, if you're copying the system version, you can run a command similar to cp -r /etc/hello-robot/stretch-re<y>-<xxxx> /media/$USER/<USBKEY> from the command line, where <USBKEY> and <xxxx> is replaced with the mounted USB key's name and the robot's serial number, respectively. Or, you can open the file explorer to copy the directory. If your previous partition is corrupted or inaccessible, contact Hello Robot support and they will be able to supply an older version of the stretch-re<y>-<xxxx> directory.","title":"Back up robot configuration data"},{"location":"stretch-install/docs/robot_install/#setup-the-bios","text":"This step can be skipped if your robot had an existing software install on it. Otherwise, follow the guide to set up the BIOS .","title":"Setup the BIOS"},{"location":"stretch-install/docs/robot_install/#install-ubuntu","text":"Choose between the following guides based on whether you're installing Ubuntu 18.04 or Ubuntu 20.04 (see above for info on what software ships with each OS). Within these guides, you'll have the choice of whether to replace the previous OS partition or to install alongside it. If you choose to install alongside it, you'll also be able to choose the size of each partition on the hard drive. Ubuntu 18.04 Installation guide . Ubuntu 20.04 Installation guide . After the Ubuntu install, the default hello-robot user account will be set up.","title":"Install Ubuntu"},{"location":"stretch-install/docs/robot_install/#run-the-new-robot-installation-script","text":"Login to the hello-robot user account on your new Ubuntu partition, open a terminal, and run: sudo apt update sudo apt install git zip Note : The system may not be able to run 'apt' immediately after a reboot as the OS may be running automatic updates in the background. Typically, waiting 10-20 minutes will allow you to use 'apt' again. Next, place the robot's calibration data in the home folder using the following steps: Plug in the USB key that contains the backed up calibration data. Copy the stretch-re<y>-<xxxx> directory, where <xxxx> is your robot's serial number, from the USB key into the home folder (i.e. /home/$USER/ ). For example, you can run a command similar to cp -r /media/$USER/<USBKEY>/stretch-re<y>-<xxxx> /home/$USER/ from the command line, where <USBKEY> and <xxxx> are replaced with your USB key's name and your robot's serial number, respectively. Or, you can open the file explorer to copy the directory. Next, pull down the Stretch Install repository and being the installation process: cd ~/ git clone https://github.com/hello-robot/stretch_install cd stretch_install git pull ./stretch_new_robot_install.sh Once the script has started, it will ask you for your robot's serial number, Y/N confirmation, and the password. Then, the script will typically take 20-30 minutes to complete on a wired connection. Once it finishes, it should print out something similar to: ############################################# DONE! COMPLETE THESE POST INSTALL STEPS: 1. Perform a FULL reboot by power cycling the robot [...] ############################################# If it has not printed out 'DONE', then the robot install did not complete successfully. Take a look at the troubleshooting section below for solutions to common issues, or contact Hello Robot support via email or the forum .","title":"Run the new robot installation script"},{"location":"stretch-install/docs/robot_install/#post-install-steps","text":"Next, we'll complete the post install steps. First, in order for the many changes to take effect, the robot will need a full reboot. The steps are: Ensure there's a clamp under the lift Shutdown the Ubuntu OS through the GUI or use sudo shutdown -h now in the terminal Turn the power switch in the robot's trunk to the off position (orange power LED becomes unlit) Ensure a keyboard/monitor is plugged into the robot. When the robot powers up, you can use the keyboard to decide which OS to boot into. Turn the power switch in the robot's trunk to the on position (orange power LED becomes lit) Boot into the new Ubuntu partition and log in if necessary Next, we'll ensure the robot's parameter YAML files are migrated to the new parameter management system (see https://forum.hello-robot.com/t/425/ for details). RE1_migrate_params.py Next, we'll ensure the robot's contact parameters are migrated to the new contact threshold system (see https://forum.hello-robot.com/t/476/ for details). RE1_migrate_contacts.py Next, we'll ensure the robot's firmware is upgraded to the latest available. Newer firmware unlocks new features (e.g. waypoint trajectory following, which is used in ROS2 to support MoveIt2) and fixes bugs. See the firmware releases for details. REx_firmware_updater.py --install Next, if your robot has the Stretch Dex Wrist , we'll configure the software to recognize it. Skip this step if you are not using the Dex Wrist. cd ~/stretch_install ./stretch_new_dex_wrist_install.sh REx_calibrate_guarded_contact.py --arm Finally, execute the following to confirm the new robot install was set up successfully. stretch_robot_system_check.py Your robot is now configured with a new robot install! In order to set up new users on this robot install, see the Adding a New User guide.","title":"Post install steps"},{"location":"stretch-install/docs/robot_install/#troubleshooting","text":"This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum .","title":"Troubleshooting"},{"location":"stretch-install/docs/robot_install/#expecting-var-hello_fleet_id-to-be-undefined-error","text":"If you are seeing the following error: [...] Checking ~/.bashrc doesn't already define HELLO_FLEET_ID... Expecting var HELLO_FLEET_ID to be undefined. Check end of ~/.bashrc file, delete all lines in 'STRETCH BASHRC SETUP' section, and open a new terminal. Exiting. ############################################# FAILURE. INSTALLATION DID NOT COMPLETE. [...] You are performing a new robot install on a robot that has already gone through the robot install process. If this is intentional, you will need to manually delete lines that a previous robot install appended to the ~/.bashrc dotfile. Open the ~/.bashrc file in an editor and look near the end for a section that looks like: ###################### # STRETCH BASHRC SETUP ###################### export HELLO_FLEET_PATH=/home/ubuntu/stretch_user export HELLO_FLEET_ID=stretch-re1-2000 export PATH=${PATH}:~/.local/bin export LRS_LOG_LEVEL=None #Debug source /opt/ros/noetic/setup.bash #source /opt/ros/galactic/setup.bash source /home/ubuntu/catkin_ws/devel/setup.bash [...] Delete this section from the ~/.bashrc . Note that it's common for other programs (e.g. Conda, Ruby) to append to your ~/.bashrc as well, and deleting those lines accidentally can impede their functionality. Take care to only delete lines related to 'STRETCH BASHRC SETUP' section. Next, open a new terminal. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) automatically runs the commands in the ~/.bashrc dotfile when opened, so the new terminal won't be set up with the lines that were just deleted. Now you can run a new robot install and this error should gone.","title":"'Expecting var HELLO_FLEET_ID to be undefined' error"},{"location":"stretch-install/docs/robot_install/#expecting-stretch-re1-xxxx-to-be-present-in-the-home-folder-error","text":"If you are seeing the following error: [...] Checking robot calibration data in home folder... Expecting backed up version of stretch-re1-xxxx to be present in the the home folder. Exiting. ############################################# FAILURE. INSTALLATION DID NOT COMPLETE. [...] The install scripts exited before performing the robot install because it was unable to find the robot's calibration data folder, 'stretch-rey-xxxx'. Please ensure you have backed up your robot's calibration data to a USB key and copied the 'stretch-re1-xxxx' folder to the home folder of your new partition. See the Run the new robot installation script section for more details. Then, run the install scripts again and the error should be gone.","title":"'Expecting stretch-re1-xxxx to be present in the home folder' error"},{"location":"stretch-install/docs/robot_install/#repo-not-up-to-date-error","text":"If you are seeing the following error: [...] Checking install repo is up-to-date... Repo not up-to-date. Please perform a 'git pull'. Exiting. ############################################# FAILURE. INSTALLATION DID NOT COMPLETE. [...] The version of Stretch Install being used is out of date. In a terminal, go to the Stretch Install folder (should be in the home folder: cd ~/stretch_install ), and perform a git pull to pull down the latest version. If the git pull fails, ensure Stretch Install has a clean working tree using git status . If you see any red files, save them if important, delete Stretch Install, and reclone it.","title":"'Repo not up-to-date' error"},{"location":"stretch-install/docs/robot_install/#failed-to-fetch-error","text":"If you are seeing the following error: Install <some package> E: Failed to fetch <url to some .deb file> Connection failed [IP: <some IP address>] E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing? ############################################# FAILURE. INSTALLATION DID NOT COMPLETE. [...] Ubuntu's system package manager, Apt, has failed to contact the server that hosts some package that the install scripts need to download. Typically, these issues are transient and waiting some time before rerunning the install script will solve the issue.","title":"'Failed to fetch' error"},{"location":"stretch-install/docs/robot_install/#firmware-protocol-mismatch-error","text":"If you are seeing the following error: ---------------- Firmware protocol mismatch on hello-<X>. Protocol on board is p<X>. Valid protocol is: p<X>. Disabling device. Please upgrade the firmware and/or version of Stretch Body. ---------------- Your version of Stretch Body does not align with the firmware installed with your robot. It's recommended that Stretch Body is first upgraded to the latest version available (but if you're intentionally running an older version, you can skip this step and the firmware updater will downgrade your firmware appriopriately). To upgrade Stretch Body, follow the instructions here . Next, run the firmware updater tool to automatically update the firmware to the required version for your software. REx_firmware_updater.py --install The firmware mismatch errors should now be gone.","title":"'Firmware protocol mismatch' error"},{"location":"stretch-install/docs/robot_install/#homing-error","text":"If using stretch_robot_home.py does not result in the robot being homed, try running the command again. If this does not work, try shutting down the robot, turning off the robot with the power switch, waiting for a few seconds, and then powering it on again. Then, try stretch_robot_home.py again.","title":"Homing Error"},{"location":"stretch-install/docs/robot_install/#ros-launch-file-fails","text":"ROS launch files have nondeterministic behavior. Sometimes they need to be run more than once for the nodes to start in a successful order that works. For example, a common symptom of a failed launch is the visualization of the robot's body appearing white and flat in RViz.","title":"ROS Launch File Fails"},{"location":"stretch-install/docs/ros_workspace/","text":"Creating a new ROS Workspace Why ROS1 and ROS2 organize software by \"workspaces\", where ROS packages are developed, compiled, and made available to run from the command line. By default, a ROS1 workspace called catkin_ws is available in the home directory. If your robot is running Ubuntu 20.04, an additional ROS2 workspace called ament_ws is also available in the home directory. This guide will show you how to replace existing or create new ROS1/2 workspaces for developing ROS software. How Open a terminal and execute the following to pull down the Stretch Install repository. cd ~ git clone https://github.com/hello-robot/stretch_install cd stretch_install git pull Ubuntu 18.04 Run the following command to create a ROS1 workspace (replacing <optional-path-to-ws> for the -w flag with a filepath to the workspace. Not providing the flag defaults to ~/catkin_ws ). ./factory/18.04/stretch_create_catkin_workspace.sh -w <optional-path-to-ws> Ubuntu 20.04 Choose one of the following commands to create either or both ROS1/ROS2 workspaces (replacing <optional-path-to-ws> for the -w flag with a filepath to the workspace. Not providing the flag defaults to ~/catkin_ws or ~/ament_ws ). # Create a ROS1 workspace ./factory/20.04/stretch_create_catkin_workspace.sh -w <optional-path-to-ws> # Create a ROS2 workspace ./factory/20.04/stretch_create_ament_workspace.sh -w <optional-path-to-ws> Wrap up Close your current terminal and open a new one. The new terminal will have automatically activated the ROS workspace(s). Your new ROS workspace is now set up successfully! Troubleshooting This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum . 'Conflicting ROS version sourced' error If you are seeing the following error: ########################################### CREATING <ROS VERSION> WORKSPACE at <WS DIR> ########################################### [...] Ensuring correct version of ROS is sourced... Cannot create workspace while a conflicting ROS version is sourced. Exiting. The ROS workspace is not created because the check that a conflicting ROS version isn't already sourced has failed. For example, if you're creating an ROS2 Ament workspace, but ROS1 Noetic was previously sourced in the same environment, the check will error out since the new ROS2 workspace would fail to find its dependencies correctly in this environment. Sourcing a version of ROS typically happens using the following command: source /opt/ros/<ros version>/setup.bash . If you ran this command to source a conflicting version previously, simply open a new terminal and the new environment won't have the conflicting ROS version sourced. If you didn't run this command and you're still getting the error, it's likely because the command exists in the ~/.bashrc dotfile. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) runs the commands in the ~/.bashrc dotfile. Look at the bottom of this dotfile for this command, comment it out temporarily, and open a new terminal. This new shell environment should have no trouble creating the ROS workspace. 'ROS_DISTRO was set before' warning If you are seeing the following warning: ROS_DISTRO was set to '<ROS VERSION>' before. Please make sure that the environment does not mix paths from different distributions. Multiple versions of ROS are being sourced in the same environment. This is known to cause issues with the rosdep tool, and might cause issues elsewhere as well. If you haven't explicitly sourced conflicting versions by using the source /opt/ros/<ros version>/setup.bash (a variant on this command could look like source ~/<ws dir>/develop/setup.bash ) command twice, then it's likely that one or two versions of ROS are implicitly being sourced in the ~/.bashrc dofile. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) runs the commands in the ~/.bashrc dotfile. Look at the bottom of this dotfile for the source command and ensure conflicting versions aren't being sourced.","title":"Adding a ROS Workspace"},{"location":"stretch-install/docs/ros_workspace/#creating-a-new-ros-workspace","text":"","title":"Creating a new ROS Workspace"},{"location":"stretch-install/docs/ros_workspace/#why","text":"ROS1 and ROS2 organize software by \"workspaces\", where ROS packages are developed, compiled, and made available to run from the command line. By default, a ROS1 workspace called catkin_ws is available in the home directory. If your robot is running Ubuntu 20.04, an additional ROS2 workspace called ament_ws is also available in the home directory. This guide will show you how to replace existing or create new ROS1/2 workspaces for developing ROS software.","title":"Why"},{"location":"stretch-install/docs/ros_workspace/#how","text":"Open a terminal and execute the following to pull down the Stretch Install repository. cd ~ git clone https://github.com/hello-robot/stretch_install cd stretch_install git pull","title":"How"},{"location":"stretch-install/docs/ros_workspace/#ubuntu-1804","text":"Run the following command to create a ROS1 workspace (replacing <optional-path-to-ws> for the -w flag with a filepath to the workspace. Not providing the flag defaults to ~/catkin_ws ). ./factory/18.04/stretch_create_catkin_workspace.sh -w <optional-path-to-ws>","title":"Ubuntu 18.04"},{"location":"stretch-install/docs/ros_workspace/#ubuntu-2004","text":"Choose one of the following commands to create either or both ROS1/ROS2 workspaces (replacing <optional-path-to-ws> for the -w flag with a filepath to the workspace. Not providing the flag defaults to ~/catkin_ws or ~/ament_ws ). # Create a ROS1 workspace ./factory/20.04/stretch_create_catkin_workspace.sh -w <optional-path-to-ws> # Create a ROS2 workspace ./factory/20.04/stretch_create_ament_workspace.sh -w <optional-path-to-ws>","title":"Ubuntu 20.04"},{"location":"stretch-install/docs/ros_workspace/#wrap-up","text":"Close your current terminal and open a new one. The new terminal will have automatically activated the ROS workspace(s). Your new ROS workspace is now set up successfully!","title":"Wrap up"},{"location":"stretch-install/docs/ros_workspace/#troubleshooting","text":"This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum .","title":"Troubleshooting"},{"location":"stretch-install/docs/ros_workspace/#conflicting-ros-version-sourced-error","text":"If you are seeing the following error: ########################################### CREATING <ROS VERSION> WORKSPACE at <WS DIR> ########################################### [...] Ensuring correct version of ROS is sourced... Cannot create workspace while a conflicting ROS version is sourced. Exiting. The ROS workspace is not created because the check that a conflicting ROS version isn't already sourced has failed. For example, if you're creating an ROS2 Ament workspace, but ROS1 Noetic was previously sourced in the same environment, the check will error out since the new ROS2 workspace would fail to find its dependencies correctly in this environment. Sourcing a version of ROS typically happens using the following command: source /opt/ros/<ros version>/setup.bash . If you ran this command to source a conflicting version previously, simply open a new terminal and the new environment won't have the conflicting ROS version sourced. If you didn't run this command and you're still getting the error, it's likely because the command exists in the ~/.bashrc dotfile. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) runs the commands in the ~/.bashrc dotfile. Look at the bottom of this dotfile for this command, comment it out temporarily, and open a new terminal. This new shell environment should have no trouble creating the ROS workspace.","title":"'Conflicting ROS version sourced' error"},{"location":"stretch-install/docs/ros_workspace/#ros_distro-was-set-before-warning","text":"If you are seeing the following warning: ROS_DISTRO was set to '<ROS VERSION>' before. Please make sure that the environment does not mix paths from different distributions. Multiple versions of ROS are being sourced in the same environment. This is known to cause issues with the rosdep tool, and might cause issues elsewhere as well. If you haven't explicitly sourced conflicting versions by using the source /opt/ros/<ros version>/setup.bash (a variant on this command could look like source ~/<ws dir>/develop/setup.bash ) command twice, then it's likely that one or two versions of ROS are implicitly being sourced in the ~/.bashrc dofile. Every new bash shell (i.e. the terminal you open when searching for 'Terminal' in system applications) runs the commands in the ~/.bashrc dotfile. Look at the bottom of this dotfile for the source command and ensure conflicting versions aren't being sourced.","title":"'ROS_DISTRO was set before' warning"},{"location":"stretch-install/docs/updating_software/","text":"Updating Stretch Software Stretch's software is improved with new features and bug fixes with each update. In this guide, we cover when and how to update the various software components on your Stretch. When to Update We develop our software publicly on Github, allowing anyone to follow/propose the development of a code feature or bug fix. While we wholeheartedly welcome collaboration on Github, it is not necessary to be active on Github to follow our software releases. We announce every major release of software on our forum . These are stable releases with code that has been extensively tested on many Stretch robots. To be notified of new releases, create an account on the forum and click the bell icon in the top left of the announcements section . The forum is also available to report issues and ask questions about any of our software packages. How to Update Each Stretch is shipped with firmware, a Python SDK, and ROS packages developed specifically for Stretch. There are separate processes for updating each of these components. Stretch ROS Stretch ROS is the Robot Operating System (ROS) interface to the robot. Many robotics developers find ROS useful to bootstrap their robotics software developments. You may update it using the following commands: roscd stretch_core git pull cd ../../.. rosdep install -iy --from-paths src Stretch Python Modules There are a few Python modules that allow users to work with the robot in pure Python2/3. These modules are: Stretch Body is the Python SDK to the robot. It abstracts away the low level details of communication with the embedded devices and provides an intuitive API to working with the robot. Stretch Body Tools is a set of command line tools that builds on Stretch Body. They allow you to quickly home, stow, command joints, read sensors, and more on the robot. Stretch Tool Share is a repository of accessories (e.g. expo marker gripper, docking station) for Stretch, created by the community and Hello Robot. Stretch Factory is a library/set of command line tools that enable calibrations, firmware upgrading, and other factory related tasks. On Ubuntu 18.04, update them using: python -m pip install -U hello-robot-stretch-body hello-robot-stretch-body-tools hello-robot-stretch-tool-share hello-robot-stretch-factory On Ubuntu 20.04, update them using: python3 -m pip install -U hello-robot-stretch-body hello-robot-stretch-body-tools hello-robot-stretch-tool-share hello-robot-stretch-factory Stretch Firmware The firmware and the Python SDK (called Stretch Body) communicate on an established protocol. Therefore, it is important to maintain a protocol match between the different firmware and Stretch Body versions. Fortunately, there is a tool that handles this automatically. In the command line, run the following command: REx_firmware_updater.py --recommended This script will automatically determine what version is currently running on the robot and provide a recommendation for a next step. Follow the next steps provided by the firmware updater script. Ubuntu The operating system upon which Stretch is built is called Ubuntu. This operating system provides the underlying packages that power Stretch's software packages. Furthermore, users of Stretch depend on this operating system and the underlying packages to develop software on Stretch. Therefore, it is important to keep the OS and these underlying packages up to date. In the command line, run the following command: sudo apt update sudo apt upgrade sudo apt autoremove Apt is the package manager that handles updates for all Ubuntu packages. Verify updated successfully Finally, reboot your robot and execute the following to confirm that all software updated successfully. stretch_robot_system_check.py If you run into any errors, see the troubleshooting guide below or contact Hello Robot Support . Troubleshooting Param Migration Error If you see the following error: Please run tool REx_migrate_params.py before continuing. For more details, see https://forum.hello-robot.com/t/425 This error appears because the organization of Stretch's parameters has changed since Stretch Body v0.3 and requires a migration of these parameters to the new organization system. Executing the following command will automaticaly migrate your parameters over: REx_migrate_params.py To learn more about Stretch's parameter system, see this tutorial . Firmware Mismatch Error If you see the following error: ---------------- Firmware protocol mismatch on /dev/XXXX. Protocol on board is pX. Valid protocol is: pX. Disabling device. Please upgrade the firmware and/or version of Stretch Body. ---------------- This error appears because the low level Python SDK and the firmware cannot communicate to each other. There is a protocol mismatch preventing communication between the two. Simply run the following script and follow its recommendations to upgrade/downgrade the firmware as necessary to match the protocol level of Stretch Body. $ REx_firmware_updater.py --status","title":"Updating Software"},{"location":"stretch-install/docs/updating_software/#updating-stretch-software","text":"Stretch's software is improved with new features and bug fixes with each update. In this guide, we cover when and how to update the various software components on your Stretch.","title":"Updating Stretch Software"},{"location":"stretch-install/docs/updating_software/#when-to-update","text":"We develop our software publicly on Github, allowing anyone to follow/propose the development of a code feature or bug fix. While we wholeheartedly welcome collaboration on Github, it is not necessary to be active on Github to follow our software releases. We announce every major release of software on our forum . These are stable releases with code that has been extensively tested on many Stretch robots. To be notified of new releases, create an account on the forum and click the bell icon in the top left of the announcements section . The forum is also available to report issues and ask questions about any of our software packages.","title":"When to Update"},{"location":"stretch-install/docs/updating_software/#how-to-update","text":"Each Stretch is shipped with firmware, a Python SDK, and ROS packages developed specifically for Stretch. There are separate processes for updating each of these components.","title":"How to Update"},{"location":"stretch-install/docs/updating_software/#stretch-ros","text":"Stretch ROS is the Robot Operating System (ROS) interface to the robot. Many robotics developers find ROS useful to bootstrap their robotics software developments. You may update it using the following commands: roscd stretch_core git pull cd ../../.. rosdep install -iy --from-paths src","title":"Stretch ROS"},{"location":"stretch-install/docs/updating_software/#stretch-python-modules","text":"There are a few Python modules that allow users to work with the robot in pure Python2/3. These modules are: Stretch Body is the Python SDK to the robot. It abstracts away the low level details of communication with the embedded devices and provides an intuitive API to working with the robot. Stretch Body Tools is a set of command line tools that builds on Stretch Body. They allow you to quickly home, stow, command joints, read sensors, and more on the robot. Stretch Tool Share is a repository of accessories (e.g. expo marker gripper, docking station) for Stretch, created by the community and Hello Robot. Stretch Factory is a library/set of command line tools that enable calibrations, firmware upgrading, and other factory related tasks. On Ubuntu 18.04, update them using: python -m pip install -U hello-robot-stretch-body hello-robot-stretch-body-tools hello-robot-stretch-tool-share hello-robot-stretch-factory On Ubuntu 20.04, update them using: python3 -m pip install -U hello-robot-stretch-body hello-robot-stretch-body-tools hello-robot-stretch-tool-share hello-robot-stretch-factory","title":"Stretch Python Modules"},{"location":"stretch-install/docs/updating_software/#stretch-firmware","text":"The firmware and the Python SDK (called Stretch Body) communicate on an established protocol. Therefore, it is important to maintain a protocol match between the different firmware and Stretch Body versions. Fortunately, there is a tool that handles this automatically. In the command line, run the following command: REx_firmware_updater.py --recommended This script will automatically determine what version is currently running on the robot and provide a recommendation for a next step. Follow the next steps provided by the firmware updater script.","title":"Stretch Firmware"},{"location":"stretch-install/docs/updating_software/#ubuntu","text":"The operating system upon which Stretch is built is called Ubuntu. This operating system provides the underlying packages that power Stretch's software packages. Furthermore, users of Stretch depend on this operating system and the underlying packages to develop software on Stretch. Therefore, it is important to keep the OS and these underlying packages up to date. In the command line, run the following command: sudo apt update sudo apt upgrade sudo apt autoremove Apt is the package manager that handles updates for all Ubuntu packages.","title":"Ubuntu"},{"location":"stretch-install/docs/updating_software/#verify-updated-successfully","text":"Finally, reboot your robot and execute the following to confirm that all software updated successfully. stretch_robot_system_check.py If you run into any errors, see the troubleshooting guide below or contact Hello Robot Support .","title":"Verify updated successfully"},{"location":"stretch-install/docs/updating_software/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"stretch-install/docs/updating_software/#param-migration-error","text":"If you see the following error: Please run tool REx_migrate_params.py before continuing. For more details, see https://forum.hello-robot.com/t/425 This error appears because the organization of Stretch's parameters has changed since Stretch Body v0.3 and requires a migration of these parameters to the new organization system. Executing the following command will automaticaly migrate your parameters over: REx_migrate_params.py To learn more about Stretch's parameter system, see this tutorial .","title":"Param Migration Error"},{"location":"stretch-install/docs/updating_software/#firmware-mismatch-error","text":"If you see the following error: ---------------- Firmware protocol mismatch on /dev/XXXX. Protocol on board is pX. Valid protocol is: pX. Disabling device. Please upgrade the firmware and/or version of Stretch Body. ---------------- This error appears because the low level Python SDK and the firmware cannot communicate to each other. There is a protocol mismatch preventing communication between the two. Simply run the following script and follow its recommendations to upgrade/downgrade the firmware as necessary to match the protocol level of Stretch Body. $ REx_firmware_updater.py --status","title":"Firmware Mismatch Error"},{"location":"stretch-ros/","text":"Overview The stretch_ros repository holds ROS related code for the Stretch RE1 mobile manipulator from Hello Robot Inc. For an overview of the capabilities in this repository, we recommend you look at the following forum post . Please be aware that the code in this repository is currently under heavy development. Resource Description hello_helpers Miscellaneous helper code used across the stretch_ros repository stretch_calibration Creates and updates calibrated URDFs for the Stretch RE1 stretch_core Enables basic use of the Stretch RE1 from ROS stretch_deep_perception Demonstrations that use open deep learning models to perceive the world stretch_demos Demonstrations of simple autonomous manipulation stretch_description Generate and export URDFs stretch_funmap Demonstrations of Fast Unified Navigation, Manipulation And Planning (FUNMAP) stretch_gazebo Support for simulation of Stretch in the Gazebo simulator stretch_moveit_config Config files to use Stretch with the MoveIt Motion Planning Framework stretch_navigation Support for the ROS navigation stack, including move_base, gmapping, and AMCL Code Status & Development Plans We intend for the following high-level summary to provide guidance about the current state of the code and planned development activities. Directory Testing Status Notes hello_helpers GOOD stretch_calibration GOOD stretch_core GOOD stretch_deep_perception GOOD stretch_demos FAIR stretch_description GOOD stretch_funmap FAIR stretch_gazebo FAIR differs from stretch_core in its underlying controllers stretch_moveit_config FAIR does not support mobile base planning in ROS 1 stretch_navigation GOOD Licenses This software is intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc. For license details for this repository, see the LICENSE files found in the directories. A summary of the licenses follows: Directory License hello_helpers Apache 2.0 stretch_calibration GPLv3 stretch_core Apache 2.0 stretch_deep_perception Apache 2.0 stretch_demos Apache 2.0 stretch_description BSD 3-Clause Clear License stretch_funmap LGPLv3 stretch_gazebo Apache 2.0 stretch_moveit_config Apache 2.0 stretch_navigation Apache 2.0","title":"Overview"},{"location":"stretch-ros/#overview","text":"The stretch_ros repository holds ROS related code for the Stretch RE1 mobile manipulator from Hello Robot Inc. For an overview of the capabilities in this repository, we recommend you look at the following forum post . Please be aware that the code in this repository is currently under heavy development. Resource Description hello_helpers Miscellaneous helper code used across the stretch_ros repository stretch_calibration Creates and updates calibrated URDFs for the Stretch RE1 stretch_core Enables basic use of the Stretch RE1 from ROS stretch_deep_perception Demonstrations that use open deep learning models to perceive the world stretch_demos Demonstrations of simple autonomous manipulation stretch_description Generate and export URDFs stretch_funmap Demonstrations of Fast Unified Navigation, Manipulation And Planning (FUNMAP) stretch_gazebo Support for simulation of Stretch in the Gazebo simulator stretch_moveit_config Config files to use Stretch with the MoveIt Motion Planning Framework stretch_navigation Support for the ROS navigation stack, including move_base, gmapping, and AMCL","title":"Overview"},{"location":"stretch-ros/#code-status-development-plans","text":"We intend for the following high-level summary to provide guidance about the current state of the code and planned development activities. Directory Testing Status Notes hello_helpers GOOD stretch_calibration GOOD stretch_core GOOD stretch_deep_perception GOOD stretch_demos FAIR stretch_description GOOD stretch_funmap FAIR stretch_gazebo FAIR differs from stretch_core in its underlying controllers stretch_moveit_config FAIR does not support mobile base planning in ROS 1 stretch_navigation GOOD","title":"Code Status &amp; Development Plans"},{"location":"stretch-ros/#licenses","text":"This software is intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc. For license details for this repository, see the LICENSE files found in the directories. A summary of the licenses follows: Directory License hello_helpers Apache 2.0 stretch_calibration GPLv3 stretch_core Apache 2.0 stretch_deep_perception Apache 2.0 stretch_demos Apache 2.0 stretch_description BSD 3-Clause Clear License stretch_funmap LGPLv3 stretch_gazebo Apache 2.0 stretch_moveit_config Apache 2.0 stretch_navigation Apache 2.0","title":"Licenses"},{"location":"stretch-ros/hello_helpers/","text":"Overview hello_helpers mostly consists of the hello_helpers Python module. This module provides various Python files used across stretch_ros that have not attained sufficient status to stand on their own. Capabilities fit_plane.py : Fits planes to 3D data. hello_misc.py : Various functions, including a helpful Python object with which to create ROS nodes. hello_ros_viz.py : Various helper functions for vizualizations using RViz. Typical Usage import hello_helpers.fit_plane as fp import hello_helpers.hello_misc as hm import hello_helpers.hello_ros_viz as hr License For license information, please see the LICENSE files.","title":"hello_helpers"},{"location":"stretch-ros/hello_helpers/#overview","text":"hello_helpers mostly consists of the hello_helpers Python module. This module provides various Python files used across stretch_ros that have not attained sufficient status to stand on their own.","title":"Overview"},{"location":"stretch-ros/hello_helpers/#capabilities","text":"fit_plane.py : Fits planes to 3D data. hello_misc.py : Various functions, including a helpful Python object with which to create ROS nodes. hello_ros_viz.py : Various helper functions for vizualizations using RViz.","title":"Capabilities"},{"location":"stretch-ros/hello_helpers/#typical-usage","text":"import hello_helpers.fit_plane as fp import hello_helpers.hello_misc as hm import hello_helpers.hello_ros_viz as hr","title":"Typical Usage"},{"location":"stretch-ros/hello_helpers/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros/hello_helpers/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros/stretch_calibration/","text":"Overview stretch_calibration provides tools for calibrating and managing the URDF for the Stretch RE1 robot from Hello Robot Inc. The code's primary role is to generate a geometric model (i.e., a URDF) of the robot's body that corresponds well with views of the body from the robot's 3D camera (i.e., a Intel RealSense D435i). The code achieves this objective by adjusting the geometry of the model to predict where the 3D camera will see markers (i.e., ArUco markers) on the robot's body. Hello Robot Inc. uses this code to calibrate each robot prior to shipping. Users may wish to recalibrate their robots to compensate for changes over time or take advantage of improvements to the calibration code. In addition, after changing a tool, this code can be used to generate a new calibrated URDF that incorporates the tool without performing a new calibration optimization. Checking the Current Calibration with New Observations Make sure the basic joint limit calibration has been performed. stretch_robot_home.py Make sure the uncalibrated URDF is up to date. rosrun stretch_calibration update_uncalibrated_urdf.sh Collect new observations roslaunch stretch_calibration collect_check_head_calibration_data.launch Test how well the current calibrated model fits the new observations rosrun stretch_calibration check_head_calibration.sh The total_error printed on the command line should be less than 0.05. If it is not, an error will be printed on the command line. In RViz the white markers represent the locations for the ArUco markers predicted by the calibrated URDF. The colored markers represent the observed locations of the ArUco markers on the robot's body. For a good fit, the white markers will be close to the colored markers. Visually Inspecting the Current Calibration The following command will allow you to visually inspect a calibration with Rviz. You can use RViz to see how well the robot's 3D body model matches point clouds from the 3D camera. While visualizing the 3D model and point clouds in RViz, you can use keyboard commands in the terminal to move the head around, the lift up and down, and the arm in and out. The keyboard commands will be printed in the terminal. A good calibration should result in a close correspondence between the robot's 3D body model and the point cloud throughout the ranges of motion for the head, lift, and arm. You may notice higher error when the head is looking upward due to challenges associated with head tilt backlash. You might also notice higher error when the arm is fully extended, since small angular errors can result in larger positional errors at the robot's wrist. Test the current head calibration roslaunch stretch_calibration simple_test_head_calibration.launch Examples of Good and Bad Visual Fit In the images below, examples of good and bad fit between the point cloud and the geometric model are presented side by side. To make the distinction clear, the images have green and red circles indicating where the fit is either good or bad. Calibrate the Stretch RE1 Make sure the basic joint limit calibration has been performed. stretch_robot_home.py Make sure the uncalibrated URDF is up to date. rosrun stretch_calibration update_uncalibrated_urdf.sh Collect head calibration data Put the robot on a flat surface. Give it room to move its arm and good lighting. Then, have the robot collect data using the command below. While the robot is collecting data, do not block its view of its markers. roslaunch stretch_calibration collect_head_calibration_data.launch Process head calibration data Specify how much data to use and the quality of the fit YAML file with parameters: stretch_ros/stretch_calibration/config/head_calibration_options.yaml More data and higher quality fitting result in optimizations that take longer When quickly testing things out ~3 minutes without visualization data_to_use: use_very_little_data fit_quality: fastest_lowest_quality When calibrating the robot ~1 hour without visualization data_to_use: use_all_data fit_quality: slow_high_quality Perform the optimization to fit the model to the collected data Without visualization (faster) roslaunch stretch_calibration process_head_calibration_data.launch With visualization (slower) roslaunch stretch_calibration process_head_calibration_data_with_visualization.launch Inspect the fit of the most recent head calibration rosrun stretch_calibration visualize_most_recent_head_calibration.sh Start using the newest head calibration rosrun stretch_calibration update_with_most_recent_calibration.sh Test the current head calibration roslaunch stretch_calibration simple_test_head_calibration.launch Use RViz to visually inspect the calibrated model. The robot's 3D body model should look similar to the structure of your robot. You may refer to the section above to see examples of good and bad fit. Generate a New URDF After Changing the Tool If you change the Stretch RE1's tool attached to the wrist and want to generate a new URDF for it, you can do so with xacro files in the /stretch_ros/stretch_description/urdf/ directory. Specifically, you can edit stretch_description.xacro to include a xacro other than the default stretch_gripper.xacro. After changing the tool xacro you will need to generate a new URDF and also update this new URDF with the previously optimized calibration parameters. To do so, follow the directions below: In a terminal start roscore. This will enable the main script to proceed and terminate without pressing Ctrl-C. roscore Next, in a different terminal terminal run rosrun stretch_calibration update_urdf_after_xacro_change.sh This will update the uncalibrated URDF with the current xacro files and then create a calibrated URDF using the most recent calibration parameters. Revert to a Previous Calibration When a new calibration is performed, it is timestamped and added to the calibration directory under \"stretch_user/\". If you'd like to revert to a previous calibration, you may run the following command. It will move the most recent calibration files to a reversion directory and update the calibration in the stretch_description package from the remaining most recent calibration files. Revert to the previous head calibration rosrun stretch_calibration revert_to_previous_calibration.sh License stretch_calibration is licensed with the GPLv3. Please see the LICENSE file for details.","title":"stretch_calibration"},{"location":"stretch-ros/stretch_calibration/#overview","text":"stretch_calibration provides tools for calibrating and managing the URDF for the Stretch RE1 robot from Hello Robot Inc. The code's primary role is to generate a geometric model (i.e., a URDF) of the robot's body that corresponds well with views of the body from the robot's 3D camera (i.e., a Intel RealSense D435i). The code achieves this objective by adjusting the geometry of the model to predict where the 3D camera will see markers (i.e., ArUco markers) on the robot's body. Hello Robot Inc. uses this code to calibrate each robot prior to shipping. Users may wish to recalibrate their robots to compensate for changes over time or take advantage of improvements to the calibration code. In addition, after changing a tool, this code can be used to generate a new calibrated URDF that incorporates the tool without performing a new calibration optimization.","title":"Overview"},{"location":"stretch-ros/stretch_calibration/#checking-the-current-calibration-with-new-observations","text":"Make sure the basic joint limit calibration has been performed. stretch_robot_home.py Make sure the uncalibrated URDF is up to date. rosrun stretch_calibration update_uncalibrated_urdf.sh Collect new observations roslaunch stretch_calibration collect_check_head_calibration_data.launch Test how well the current calibrated model fits the new observations rosrun stretch_calibration check_head_calibration.sh The total_error printed on the command line should be less than 0.05. If it is not, an error will be printed on the command line. In RViz the white markers represent the locations for the ArUco markers predicted by the calibrated URDF. The colored markers represent the observed locations of the ArUco markers on the robot's body. For a good fit, the white markers will be close to the colored markers.","title":"Checking the Current Calibration with New Observations"},{"location":"stretch-ros/stretch_calibration/#visually-inspecting-the-current-calibration","text":"The following command will allow you to visually inspect a calibration with Rviz. You can use RViz to see how well the robot's 3D body model matches point clouds from the 3D camera. While visualizing the 3D model and point clouds in RViz, you can use keyboard commands in the terminal to move the head around, the lift up and down, and the arm in and out. The keyboard commands will be printed in the terminal. A good calibration should result in a close correspondence between the robot's 3D body model and the point cloud throughout the ranges of motion for the head, lift, and arm. You may notice higher error when the head is looking upward due to challenges associated with head tilt backlash. You might also notice higher error when the arm is fully extended, since small angular errors can result in larger positional errors at the robot's wrist. Test the current head calibration roslaunch stretch_calibration simple_test_head_calibration.launch","title":"Visually Inspecting the Current Calibration"},{"location":"stretch-ros/stretch_calibration/#examples-of-good-and-bad-visual-fit","text":"In the images below, examples of good and bad fit between the point cloud and the geometric model are presented side by side. To make the distinction clear, the images have green and red circles indicating where the fit is either good or bad.","title":"Examples of Good and Bad Visual Fit"},{"location":"stretch-ros/stretch_calibration/#calibrate-the-stretch-re1","text":"Make sure the basic joint limit calibration has been performed. stretch_robot_home.py Make sure the uncalibrated URDF is up to date. rosrun stretch_calibration update_uncalibrated_urdf.sh Collect head calibration data Put the robot on a flat surface. Give it room to move its arm and good lighting. Then, have the robot collect data using the command below. While the robot is collecting data, do not block its view of its markers. roslaunch stretch_calibration collect_head_calibration_data.launch Process head calibration data Specify how much data to use and the quality of the fit YAML file with parameters: stretch_ros/stretch_calibration/config/head_calibration_options.yaml More data and higher quality fitting result in optimizations that take longer When quickly testing things out ~3 minutes without visualization data_to_use: use_very_little_data fit_quality: fastest_lowest_quality When calibrating the robot ~1 hour without visualization data_to_use: use_all_data fit_quality: slow_high_quality Perform the optimization to fit the model to the collected data Without visualization (faster) roslaunch stretch_calibration process_head_calibration_data.launch With visualization (slower) roslaunch stretch_calibration process_head_calibration_data_with_visualization.launch Inspect the fit of the most recent head calibration rosrun stretch_calibration visualize_most_recent_head_calibration.sh Start using the newest head calibration rosrun stretch_calibration update_with_most_recent_calibration.sh Test the current head calibration roslaunch stretch_calibration simple_test_head_calibration.launch Use RViz to visually inspect the calibrated model. The robot's 3D body model should look similar to the structure of your robot. You may refer to the section above to see examples of good and bad fit.","title":"Calibrate the Stretch RE1"},{"location":"stretch-ros/stretch_calibration/#generate-a-new-urdf-after-changing-the-tool","text":"If you change the Stretch RE1's tool attached to the wrist and want to generate a new URDF for it, you can do so with xacro files in the /stretch_ros/stretch_description/urdf/ directory. Specifically, you can edit stretch_description.xacro to include a xacro other than the default stretch_gripper.xacro. After changing the tool xacro you will need to generate a new URDF and also update this new URDF with the previously optimized calibration parameters. To do so, follow the directions below: In a terminal start roscore. This will enable the main script to proceed and terminate without pressing Ctrl-C. roscore Next, in a different terminal terminal run rosrun stretch_calibration update_urdf_after_xacro_change.sh This will update the uncalibrated URDF with the current xacro files and then create a calibrated URDF using the most recent calibration parameters.","title":"Generate a New URDF After Changing the Tool"},{"location":"stretch-ros/stretch_calibration/#revert-to-a-previous-calibration","text":"When a new calibration is performed, it is timestamped and added to the calibration directory under \"stretch_user/\". If you'd like to revert to a previous calibration, you may run the following command. It will move the most recent calibration files to a reversion directory and update the calibration in the stretch_description package from the remaining most recent calibration files. Revert to the previous head calibration rosrun stretch_calibration revert_to_previous_calibration.sh","title":"Revert to a Previous Calibration"},{"location":"stretch-ros/stretch_calibration/#license","text":"stretch_calibration is licensed with the GPLv3. Please see the LICENSE file for details.","title":"License"},{"location":"stretch-ros/stretch_calibration/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation. The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link: https://www.gnu.org/licenses/gpl-3.0.html For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros/stretch_core/","text":"Overview stretch_core provides the core ROS interfaces to the Stretch RE1 mobile manipulator from Hello Robot Inc. It includes the following nodes: stretch_driver : node that communicates with the low-level Python library (stretch_body) to interface with the Stretch RE1 detect_aruco_markers : node that detects and estimates the pose of ArUco markers, including the markers on the robot's body d435i_ * : various nodes to help use the Stretch RE1's 3D camera keyboard_teleop : node that provides a keyboard interface to control the robot's joints License For license information, please see the LICENSE files.","title":"stretch_core"},{"location":"stretch-ros/stretch_core/#overview","text":"stretch_core provides the core ROS interfaces to the Stretch RE1 mobile manipulator from Hello Robot Inc. It includes the following nodes: stretch_driver : node that communicates with the low-level Python library (stretch_body) to interface with the Stretch RE1 detect_aruco_markers : node that detects and estimates the pose of ArUco markers, including the markers on the robot's body d435i_ * : various nodes to help use the Stretch RE1's 3D camera keyboard_teleop : node that provides a keyboard interface to control the robot's joints","title":"Overview"},{"location":"stretch-ros/stretch_core/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros/stretch_core/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros/stretch_deep_perception/","text":"Overview stretch_deep_perception provides demonstration code that uses open deep learning models to perceive the world. This code depends on the stretch_deep_perception_models repository, which should be installed under ~/stretch_user/ on your Stretch RE1 robot. Link to the stretch_deep_perception_models repository: https://github.com/hello-robot/stretch_deep_perception_models Getting Started Demos There are four demonstrations for you to try. Face Estimation Demo First, try running the face detection demonstration via the following command: roslaunch stretch_deep_perception stretch_detect_faces.launch RViz should show you the robot, the point cloud from the camera, and information about detected faces. If it detects a face, it should show a 3D planar model of the face and 3D facial landmarks. These deep learning models come from OpenCV and the Open Model Zoo (https://github.com/opencv/open_model_zoo). You can use the keyboard_teleop commands within the terminal that you ran roslaunch in order to move the robot's head around to see your face. i (tilt up) j (pan left) l (pan right) , (tilt down) Pan left and pan right are in terms of the robot's left and the robot's right. Now shut down everything that was launched by pressing q and Ctrl-C in the terminal. Object Detection Demo Second, try running the object detection demo, which uses the tiny YOLO v3 object detection network (https://pjreddie.com/darknet/yolo/). RViz will display planar detection regions. Detection class labels will be printed to the terminal. roslaunch stretch_deep_perception stretch_detect_objects.launch Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal. Body Landmark Detection Demo Third, try running the body landmark point detection demo. The deep learning model comes from the Open Model Zoo (https://github.com/opencv/open_model_zoo). RViz will display colored 3D points on body landmarks. The network also provides information to connect these landmarks, but this demo code does not currently use it. roslaunch stretch_deep_perception stretch_detect_body_landmarks.launch Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal. Nearest Mouth Detection Demo Finally, try running the nearest mouth detection demo. RViz will display a 3D frame of reference estimated for the nearest mouth detected by the robot. Sometimes the point cloud will make it difficult to see. Disabling the point cloud view in RViz will make it more visible. We have used this frame of reference to deliver food near a person's mouth. This has the potential to be useful for assistive feeding. However, use of this detector in this way could be risky. Please be very careful and aware that you are using it at your own risk. A less risky use of this detection is for object delivery. stretch_demos has a demonstration that delivers an object based on this frame of reference by holding out the object some distance from the mouth location and below the mouth location with respect to the world frame. This works well and is inspired by similar methods used with the robot EL-E at Georgia Tech [1]. roslaunch stretch_deep_perception stretch_detect_nearest_mouth.launch References [1] Hand It Over or Set It Down: A User Study of Object Delivery with an Assistive Mobile Manipulator, Young Sang Choi, Tiffany L. Chen, Advait Jain, Cressel Anderson, Jonathan D. Glass, and Charles C. Kemp, IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2009. http://pwp.gatech.edu/hrl/wp-content/uploads/sites/231/2016/05/roman2009_delivery.pdf License For license information, please see the LICENSE files.","title":"stretch_deep_perception"},{"location":"stretch-ros/stretch_deep_perception/#overview","text":"stretch_deep_perception provides demonstration code that uses open deep learning models to perceive the world. This code depends on the stretch_deep_perception_models repository, which should be installed under ~/stretch_user/ on your Stretch RE1 robot. Link to the stretch_deep_perception_models repository: https://github.com/hello-robot/stretch_deep_perception_models","title":"Overview"},{"location":"stretch-ros/stretch_deep_perception/#getting-started-demos","text":"There are four demonstrations for you to try.","title":"Getting Started Demos"},{"location":"stretch-ros/stretch_deep_perception/#face-estimation-demo","text":"First, try running the face detection demonstration via the following command: roslaunch stretch_deep_perception stretch_detect_faces.launch RViz should show you the robot, the point cloud from the camera, and information about detected faces. If it detects a face, it should show a 3D planar model of the face and 3D facial landmarks. These deep learning models come from OpenCV and the Open Model Zoo (https://github.com/opencv/open_model_zoo). You can use the keyboard_teleop commands within the terminal that you ran roslaunch in order to move the robot's head around to see your face. i (tilt up) j (pan left) l (pan right) , (tilt down) Pan left and pan right are in terms of the robot's left and the robot's right. Now shut down everything that was launched by pressing q and Ctrl-C in the terminal.","title":"Face Estimation Demo"},{"location":"stretch-ros/stretch_deep_perception/#object-detection-demo","text":"Second, try running the object detection demo, which uses the tiny YOLO v3 object detection network (https://pjreddie.com/darknet/yolo/). RViz will display planar detection regions. Detection class labels will be printed to the terminal. roslaunch stretch_deep_perception stretch_detect_objects.launch Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal.","title":"Object Detection Demo"},{"location":"stretch-ros/stretch_deep_perception/#body-landmark-detection-demo","text":"Third, try running the body landmark point detection demo. The deep learning model comes from the Open Model Zoo (https://github.com/opencv/open_model_zoo). RViz will display colored 3D points on body landmarks. The network also provides information to connect these landmarks, but this demo code does not currently use it. roslaunch stretch_deep_perception stretch_detect_body_landmarks.launch Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal.","title":"Body Landmark Detection Demo"},{"location":"stretch-ros/stretch_deep_perception/#nearest-mouth-detection-demo","text":"Finally, try running the nearest mouth detection demo. RViz will display a 3D frame of reference estimated for the nearest mouth detected by the robot. Sometimes the point cloud will make it difficult to see. Disabling the point cloud view in RViz will make it more visible. We have used this frame of reference to deliver food near a person's mouth. This has the potential to be useful for assistive feeding. However, use of this detector in this way could be risky. Please be very careful and aware that you are using it at your own risk. A less risky use of this detection is for object delivery. stretch_demos has a demonstration that delivers an object based on this frame of reference by holding out the object some distance from the mouth location and below the mouth location with respect to the world frame. This works well and is inspired by similar methods used with the robot EL-E at Georgia Tech [1]. roslaunch stretch_deep_perception stretch_detect_nearest_mouth.launch","title":"Nearest Mouth Detection Demo"},{"location":"stretch-ros/stretch_deep_perception/#references","text":"[1] Hand It Over or Set It Down: A User Study of Object Delivery with an Assistive Mobile Manipulator, Young Sang Choi, Tiffany L. Chen, Advait Jain, Cressel Anderson, Jonathan D. Glass, and Charles C. Kemp, IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2009. http://pwp.gatech.edu/hrl/wp-content/uploads/sites/231/2016/05/roman2009_delivery.pdf","title":"References"},{"location":"stretch-ros/stretch_deep_perception/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros/stretch_deep_perception/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros/stretch_demos/","text":"Overview stretch_demos provides simple demonstrations for the Stretch RE1 mobile manipulator from Hello Robot Inc. For an overview of the demos, we recommend you look at the following forum post: https://forum.hello-robot.com/t/autonomy-video-details Getting Started Demos Please be aware that these demonstrations typically do not perform careful collision avoidance. Instead, they expect to operate in freespace and detect contact through motor current if an obstacle gets in the way. Please be careful when trying out these demonstrations. Handover Object Demo First, place the robot near you so that it can freely move back and forth and reach near your body. Then, launch the handover object demo using the following command: roslaunch stretch_demos handover_object.launch For this demonstration, the robot will pan its head back and forth looking for a face. It will remember the 3D location of the mouth of the nearest face that it has detected. If you press \"y\" or \"Y\" on the keyboard in the terminal, the robot will move the grasp region of its gripper toward a handover location below and away from the mouth. The robot will restrict itself to Cartesian motion to do this. Specifically, it will move its mobile base backward and forward, its lift up and down, and its arm in and out. If you press \"y\" or \"Y\" again, it will retract its arm and then move to the most recent mouth location it has detected. At any time, you can also use the keyboard teleoperation commands in the terminal window. With this, you can adjust the gripper, including pointing it straight out and making it grasp an object to be handed over. Grasp Object Demo For this demonstration, the robot will look for the nearest elevated surface, look for an object on it, and then attempt to grasp the largest object using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. Once the robot is in position, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. Now that the robot is ready, launch the demo with the following command: roslaunch stretch_demos grasp_object.launch Then, press the key with \u2018 and \u201c on it while in the terminal to initiate a grasp attempt. While attempting the grasp the demo will save several images under the ./stretch_user/debug/ directory within various grasping related directories. You can view these images to see some of what the robot did to make its decisions. Clean Surface Demo For this demonstration, the robot will look for the nearest elevated surface, look for clear space on it, and then attempt to wipe the clear space using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. You should also place a soft cloth in the robot's gripper. Once the robot is in position with a cloth in its gripper, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. Now that the robot is ready, launch the demo with the following command: roslaunch stretch_demos clean_surface.launch Then, press the key with the / and ? on it while in the terminal to initiate a surface cleaning attempt. License For license information, please see the LICENSE files.","title":"stretch_demos"},{"location":"stretch-ros/stretch_demos/#overview","text":"stretch_demos provides simple demonstrations for the Stretch RE1 mobile manipulator from Hello Robot Inc. For an overview of the demos, we recommend you look at the following forum post: https://forum.hello-robot.com/t/autonomy-video-details","title":"Overview"},{"location":"stretch-ros/stretch_demos/#getting-started-demos","text":"Please be aware that these demonstrations typically do not perform careful collision avoidance. Instead, they expect to operate in freespace and detect contact through motor current if an obstacle gets in the way. Please be careful when trying out these demonstrations.","title":"Getting Started Demos"},{"location":"stretch-ros/stretch_demos/#handover-object-demo","text":"First, place the robot near you so that it can freely move back and forth and reach near your body. Then, launch the handover object demo using the following command: roslaunch stretch_demos handover_object.launch For this demonstration, the robot will pan its head back and forth looking for a face. It will remember the 3D location of the mouth of the nearest face that it has detected. If you press \"y\" or \"Y\" on the keyboard in the terminal, the robot will move the grasp region of its gripper toward a handover location below and away from the mouth. The robot will restrict itself to Cartesian motion to do this. Specifically, it will move its mobile base backward and forward, its lift up and down, and its arm in and out. If you press \"y\" or \"Y\" again, it will retract its arm and then move to the most recent mouth location it has detected. At any time, you can also use the keyboard teleoperation commands in the terminal window. With this, you can adjust the gripper, including pointing it straight out and making it grasp an object to be handed over.","title":"Handover Object Demo"},{"location":"stretch-ros/stretch_demos/#grasp-object-demo","text":"For this demonstration, the robot will look for the nearest elevated surface, look for an object on it, and then attempt to grasp the largest object using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. Once the robot is in position, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. Now that the robot is ready, launch the demo with the following command: roslaunch stretch_demos grasp_object.launch Then, press the key with \u2018 and \u201c on it while in the terminal to initiate a grasp attempt. While attempting the grasp the demo will save several images under the ./stretch_user/debug/ directory within various grasping related directories. You can view these images to see some of what the robot did to make its decisions.","title":"Grasp Object Demo"},{"location":"stretch-ros/stretch_demos/#clean-surface-demo","text":"For this demonstration, the robot will look for the nearest elevated surface, look for clear space on it, and then attempt to wipe the clear space using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. You should also place a soft cloth in the robot's gripper. Once the robot is in position with a cloth in its gripper, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. Now that the robot is ready, launch the demo with the following command: roslaunch stretch_demos clean_surface.launch Then, press the key with the / and ? on it while in the terminal to initiate a surface cleaning attempt.","title":"Clean Surface Demo"},{"location":"stretch-ros/stretch_demos/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros/stretch_demos/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros/stretch_description/","text":"Overview stretch_description provides materials for a URDF kinematic model of the Stretch RE1 mobile manipulator from Hello Robot Inc. Details The meshes directory contains STL mesh files representing the exterior geometry of various parts of the robot. The urdf directory contains xacro files representing various parts of the robot that are used to generate the robot's URDF. stretch_ros expects a URDF with the name stretch.urdf to reside within the urdf directory. The file stretch.urdf serves as the URDF for the robot and must be generated. Typically, it is a calibrated urdf file for the particular Stretch RE1 robot being used. To generate this file, please read the documentation within stretch_ros/stretch_calibration. The xacro_to_urdf.sh will usually only be indirectly run as part of various scripts and launch files within stretch_ros/stretch_calibration. Sometimes a stretch_uncalibrated.urdf file will reside with the urdf directory. This file is typically generated directly from the xacro files without any alterations. Exporting a URDF Sometimes a URDF is useful outside of ROS, such as for simulations and analysis. Running the export_urdf.sh script in the urdf directory will export a full URDF model of the robot based on stretch.urdf. The exported URDF will be found within an exported_urdf directory. It is also copied to a directory for your specific robot found under ~/stretch_user. The exported URDF includes meshes and controller calibration YAML files. The exported URDF can be visualized using stretch_urdf_show.py, which is part of the stretch_body Python code. Changing the Tool If you wish to remove the default gripper and add a different tool, you will typically edit /stretch_description/urdf/stretch_description.xacro. Specifically, you will replace the following line in order to include the xacro for the new tool and then follow directions within stretch_ros/stretch_calibration to generate a new calibrated urdf file (stretch.urdf) that includes the new tool. <xacro:include filename=\"stretch_gripper.xacro\" /> As an example we provide the xacro stretch_dry_erase_marker.xacro and its dependent mesh files with stretch_ros. Some of the tools found in the Stretch Body Tool Share include URDF data. To integrate these tools into the URDF for your Stretch >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_tool_share >>$ cd stretch_tool_share/<tool name> >>$ cp stretch_description/urdf/* ~/catkin_ws/src/stretch_ros/stretch_description/urdf/ >>$ cp stretch_description/meshes/* ~/catkin_ws/src/stretch_ros/stretch_description/meshes/ Next add the xacro for the particular tool to /stretch_description/urdf/stretch_description.xacro . Then you can generate and preview the uncalibrated URDF: >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp stretch.urdf stretch.urdf.bak >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh Now visualize the new tool >>$ roslaunch stretch_calibration simple_test_head_calibration.launch License and Patents Patents are pending that cover aspects of the Stretch RE1 mobile manipulator. For license information, please see the LICENSE files.","title":"stretch_description"},{"location":"stretch-ros/stretch_description/#overview","text":"stretch_description provides materials for a URDF kinematic model of the Stretch RE1 mobile manipulator from Hello Robot Inc.","title":"Overview"},{"location":"stretch-ros/stretch_description/#details","text":"The meshes directory contains STL mesh files representing the exterior geometry of various parts of the robot. The urdf directory contains xacro files representing various parts of the robot that are used to generate the robot's URDF. stretch_ros expects a URDF with the name stretch.urdf to reside within the urdf directory. The file stretch.urdf serves as the URDF for the robot and must be generated. Typically, it is a calibrated urdf file for the particular Stretch RE1 robot being used. To generate this file, please read the documentation within stretch_ros/stretch_calibration. The xacro_to_urdf.sh will usually only be indirectly run as part of various scripts and launch files within stretch_ros/stretch_calibration. Sometimes a stretch_uncalibrated.urdf file will reside with the urdf directory. This file is typically generated directly from the xacro files without any alterations.","title":"Details"},{"location":"stretch-ros/stretch_description/#exporting-a-urdf","text":"Sometimes a URDF is useful outside of ROS, such as for simulations and analysis. Running the export_urdf.sh script in the urdf directory will export a full URDF model of the robot based on stretch.urdf. The exported URDF will be found within an exported_urdf directory. It is also copied to a directory for your specific robot found under ~/stretch_user. The exported URDF includes meshes and controller calibration YAML files. The exported URDF can be visualized using stretch_urdf_show.py, which is part of the stretch_body Python code.","title":"Exporting a URDF"},{"location":"stretch-ros/stretch_description/#changing-the-tool","text":"If you wish to remove the default gripper and add a different tool, you will typically edit /stretch_description/urdf/stretch_description.xacro. Specifically, you will replace the following line in order to include the xacro for the new tool and then follow directions within stretch_ros/stretch_calibration to generate a new calibrated urdf file (stretch.urdf) that includes the new tool. <xacro:include filename=\"stretch_gripper.xacro\" /> As an example we provide the xacro stretch_dry_erase_marker.xacro and its dependent mesh files with stretch_ros. Some of the tools found in the Stretch Body Tool Share include URDF data. To integrate these tools into the URDF for your Stretch >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_tool_share >>$ cd stretch_tool_share/<tool name> >>$ cp stretch_description/urdf/* ~/catkin_ws/src/stretch_ros/stretch_description/urdf/ >>$ cp stretch_description/meshes/* ~/catkin_ws/src/stretch_ros/stretch_description/meshes/ Next add the xacro for the particular tool to /stretch_description/urdf/stretch_description.xacro . Then you can generate and preview the uncalibrated URDF: >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp stretch.urdf stretch.urdf.bak >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh Now visualize the new tool >>$ roslaunch stretch_calibration simple_test_head_calibration.launch","title":"Changing the Tool"},{"location":"stretch-ros/stretch_description/#license-and-patents","text":"Patents are pending that cover aspects of the Stretch RE1 mobile manipulator. For license information, please see the LICENSE files.","title":"License and Patents"},{"location":"stretch-ros/stretch_description/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"). The Contents consist of software and data used with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. The Clear BSD License Copyright (c) 2021 Hello Robot Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"LICENSE"},{"location":"stretch-ros/stretch_description/urdf/export_urdf_license_template/","text":"The following license applies to the entire contents of this directory (the \"Contents\") except where otherwise noted. The Contents consist of software and data used with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike-4.0-International (CC BY-NC-SA 4.0) license (the \"License\"); you may not use the Contents except in compliance with the License. You may obtain a copy of the License at https://creativecommons.org/licenses/by-nc-sa/4.0/ Unless required by applicable law or agreed to in writing, the Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Patents pending and trademark rights cover the Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\" The Contents may incorporate some parts of the \"RealSense Camera description package for Intel 3D D400 cameras\" released with an Apache 2.0 license and Copyright 2017 Intel Corporation. The details of the Apache 2.0 license can be found via the following link: https://www.apache.org/licenses/LICENSE-2.0 Specifically, the Contents may include the d435.dae mesh file and content generated by the _d435.urdf.xacro found within the GitHub repository available for download via the following link as of May 4, 2020: https://github.com/IntelRealSense/realsense-ros/tree/development/realsense2_description These specific materials are subject to the requirements of their original Apache 2.0 license. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"Export urdf license template"},{"location":"stretch-ros/stretch_funmap/","text":"Overview stretch_funmap is an implementation of Fast Unified Navigation, Manipulation And Planning (FUNMAP). FUNMAP provides navigation, manipulation, and planning capabilities for the Stretch RE1 mobile manipulator. stretch_funmap includes examples of efficient ways to take advantage of the Stretch RE1's unique properties. Previous commercially-available mobile manipulators have consisted of a serial manipulator (i.e., links connected by rotary joints) placed on a mobile base [1]. Widely used software (e.g., the Robot Operating System (ROS)) [1] typically expects a velocity-controlled mobile base that can be held still while the arm manipulates [3, 4]. In contrast, the Stretch RE1's mobile base is integral to manipulation and typically moves throughout a task. It can also perform high-fidelity position control with its mobile base. FUNMAP uses approximate geometric models and computer-vision algorithms to efficiently find plans that take advantage of its prismatic joints (e.g., telescoping arm) and Cartesian structure. In contrast to typical approaches that treat navigation (e.g., ROS Navigation Stack ) and manipulation (e.g., MoveIt! [5, 6]) separately, FUNMAP does both. Getting Started Demo First, make sure that your Stretch RE1 has clearance to rotate in place and will rotate without straining any cables connected to the trunk. Ideally, you should have the robot untethered. Next, run the following launch file: roslaunch stretch_funmap mapping.launch Now, you will take a head scan, which will involve the head panning around, the base rotating, and the head panning around again to overcome the blindspot due to the mast. While in the terminal in which you ran roslaunch, press the space bar to initiate the head scan. At this point, you should see a 3D map resulting from the head scan in RViz. You can rotate it around and look at it. It has been created by merging many 3D scans. If you have the robot untethered, you can now specify a navigation goal for the robot. If the robot finds a navigation plan to the goal, it will attempt to navigate to it. While navigating, it will look down with its 3D camera in an attempt to stop if it detects an obstacle. In RViz, press the \"2D Nav Goal\" button on the top bar with a magenta arrow icon. Specify a nearby navigation goal pose on the floor of the map by clicking and drawing a magenta arrow. For this to work, the navigation goal must be in a place that the robot can reach and that the robot has scanned well. For example, the robot will only navigate across floor regions that it has in its map. If the robot finds a path, you should see green lines connecting white spheres in RViz that display its plan as it attempts to navigate to the goal. Once the robot has reached the goal, you can take another head scan. While in the terminal, press the space bar to initiate another head scan. The robot should take the head scan and merge it with the previous scans. If all goes well, the merged 3D map will be visible in RViz. You can also have the robot automatically drive to a place that it thinks is a good place for it to take a head scan in order to map the environment. While in the terminal, press the key with \\ and | on it. This should work even if caps lock is enabled or the shift key is pressed. If the robot reached it's goal, then you can now take another head scan. While in the terminal, press the space bar to initiate another head scan. By repeating this process, you can create a 3D map of the environment. FUNMAP uses images to represent the environment with each pixel value representing the highest observed 3D point at a planar location. By default, all of the merged maps are saved to the following directory: ./stretch_user/debug/merged_maps/ You can see the image representations by looking at files with the following naming pattern: ./stretch_user/debug/merged_maps/merged_map_DATETIME_mhi_visualization.png You can also click on a reaching goal for the Stretch RE1 by clicking on \"Publish Point\" in Rviz and then selecting a 3D point on the map. FUNMAP will attempt to generate a navigation and manipulation plan to reach close to the selected 3D location with Stretch's gripper. In RViz, select a reach goal by clicking on \"Publish Point\" on the top bar with a red map location icon). Then, click on a 3D location on the map to specify a reaching target for the robot's gripper. That concludes the demonstration. Have fun with FUNMAP! More FUNMAP FUNMAP represents human environments with Max Height Images (MHIs). An MHI is an image for which each pixel typically represents the height of the robot\u2019s environment. Given a volume of interest (VOI) with its z-axis aligned with gravity, an MHI, I, maps locations to heights. Specifically, I(x,y)=z, where (x,y) represents a discretized planar location within the VOI and z represents the discretized height of the maximum occupied voxel within the VOI at that planar location (see Figure 1 above). The placement of the Stretch RE1 3D camera at a human head height enables it to capture MHIs that represent the horizontal surfaces with which humans frequently interact, such as table tops, countertops, and chairs. The orientation of its 3D camera enables the robot to quickly scan an environment to create a room-scale MHI by panning its head at a constant tilt angle. In contrast to other environment representations, such as point clouds and 3D mesh models, MHIs support fast, efficient operations through optimized image processing and are compatible with deep learning methods for images. Related representations have primarily been used for navigation, including for legged robots on rough terrain, but have not emphasized elevated surfaces in human environments nor incorporated manipulation [7, 8]. Object grasping systems for bin picking have used related representations, but have only considered small areas with objects and not incorporated navigation [9, 10]. During development, we have used FUNMAP to create MHIs for which each pixel represents a 6mm x 6mm region of the environment\u2019s floor and has a height resolution of less than 6mm per unit. This allows a single 2000 x 2000 pixel, 8-bit image to represent a 12m x 12m environment from 10cm below the estimated floor plane to 1.2m above the floor plane, which captures the great majority of open horizontal surfaces in human environments with which people interact. MHIs also have the potential to represent enclosed surfaces (e.g., surfaces in shelves and cabinets) and vertical surfaces (e.g., doors) by defining new VOIs with different orientations and heights, which is a capability that merits future exploration. For navigation and planning, FUNMAP uses fast distance transforms and morphological operators to efficiently create cost functions for robust optimization-based planning. For example, a high value of a distance transform at a floor location implies that the mobile base will be farther from obstacles. Similarly, a high value of a distance transform for a location on an elevated surface implies that the robot\u2019s end effector will be farther from obstacles. Figure 3: Left: The cyan lines represent achievable driving goals on the floor from which the robot can reach the target (red circle) on the table (dark blue). Right: When the target (red circle) is farther back near obstacles on the table (dark blue), the robot can reach the target from fewer locations (cyan lines). Figure 4: Left: Example of a piecewise linear navigation plan (green line segments and white spheres) being executed. Right: Example of a successfully executed plan to reach a target location on a table (light blue) while avoiding a wall with shelves (dark blue and purple). The Stretch RE1\u2019s slender links and Cartesian kinematics support rapid optimization of plans due to the simplified geometry of the robot\u2019s motions. FUNMAP uses piecewise linear paths on the cost image for fast navigation and manipulation planning. FUNMAP enables the Stretch RE1 to reach a 3D target position. FUNMAP models the linear motion of the telescoping arm as it extends to reach a target as a line in a manipulation cost image (see Figure 2). FUNMAP uses a navigation cost image and Djikstra\u2019s algorithm with a priority queue to efficiently estimate the cost of navigating to all floor locations (see Figure 3 left). FUNMAP then combines these results to find a minimum cost plan that increases the distance to obstacles and the manipulable workspace of the robot (see Figure 3 right). References [1] Bostelman, Roger, Tsai Hong, and Jeremy Marvel. \"Survey of research for performance measurement of mobile manipulators.\" Journal of Research of the National Institute of Standards and Technology 121, no. 3 (2016): 342-366. [2] Quigley, Morgan, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote, Jeremy Leibs, Rob Wheeler, and Andrew Y. Ng. \"ROS: an open-source Robot Operating System.\" In ICRA workshop on open source software, vol. 3, no. 3.2, p. 5. 2009. [3] Sachin Chitta, Eitan Marder-Eppstein, Wim Meeussen, Vijay Pradeep, Adolfo Rodr\u00edguez Tsouroukdissian, et al.. ros_control: A generic and simple control framework for ROS. The Journal of Open Source Software, 2017, 2 (20), pp.456 - 456. [4] Guimar\u00e3es, Rodrigo Longhi, Andr\u00e9 Schneider de Oliveira, Jo\u00e3o Alberto Fabro, Thiago Becker, and Vin\u00edcius Amilgar Brenner. \"ROS navigation: Concepts and tutorial.\" In Robot Operating System (ROS), pp. 121-160. Springer, Cham, 2016. [5] Chitta, Sachin, Ioan Sucan, and Steve Cousins. \"Moveit! [ros topics].\" IEEE Robotics & Automation Magazine 19, no. 1 (2012): 18-19. [6] Chitta, Sachin. \"MoveIt!: an introduction.\" In Robot Operating System (ROS), pp. 3-27. Springer, Cham, 2016. [7] Fankhauser, P\u00e9ter, and Marco Hutter. \"A universal grid map library: Implementation and use case for rough terrain navigation.\" In Robot Operating System (ROS), pp. 99-120. Springer, Cham, 2016. [8] Lu, David V., Dave Hershberger, and William D. Smart. \"Layered costmaps for context-sensitive navigation.\" In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 709-715. IEEE, 2014. [9] Zeng, Andy, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. \"Tossingbot: Learning to throw arbitrary objects with residual physics.\" arXiv preprint arXiv:1903.11239 (2019). [10] Zeng, Andy, Shuran Song, Kuan-Ting Yu, Elliott Donlon, Francois R. Hogan, Maria Bauza, Daolin Ma et al. \"Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching.\" In 2018 IEEE international conference on robotics and automation (ICRA), pp. 1-8. IEEE, 2018. License For license information, please see the LICENSE files.","title":"stretch_funmap"},{"location":"stretch-ros/stretch_funmap/#overview","text":"stretch_funmap is an implementation of Fast Unified Navigation, Manipulation And Planning (FUNMAP). FUNMAP provides navigation, manipulation, and planning capabilities for the Stretch RE1 mobile manipulator. stretch_funmap includes examples of efficient ways to take advantage of the Stretch RE1's unique properties. Previous commercially-available mobile manipulators have consisted of a serial manipulator (i.e., links connected by rotary joints) placed on a mobile base [1]. Widely used software (e.g., the Robot Operating System (ROS)) [1] typically expects a velocity-controlled mobile base that can be held still while the arm manipulates [3, 4]. In contrast, the Stretch RE1's mobile base is integral to manipulation and typically moves throughout a task. It can also perform high-fidelity position control with its mobile base. FUNMAP uses approximate geometric models and computer-vision algorithms to efficiently find plans that take advantage of its prismatic joints (e.g., telescoping arm) and Cartesian structure. In contrast to typical approaches that treat navigation (e.g., ROS Navigation Stack ) and manipulation (e.g., MoveIt! [5, 6]) separately, FUNMAP does both.","title":"Overview"},{"location":"stretch-ros/stretch_funmap/#getting-started-demo","text":"First, make sure that your Stretch RE1 has clearance to rotate in place and will rotate without straining any cables connected to the trunk. Ideally, you should have the robot untethered. Next, run the following launch file: roslaunch stretch_funmap mapping.launch Now, you will take a head scan, which will involve the head panning around, the base rotating, and the head panning around again to overcome the blindspot due to the mast. While in the terminal in which you ran roslaunch, press the space bar to initiate the head scan. At this point, you should see a 3D map resulting from the head scan in RViz. You can rotate it around and look at it. It has been created by merging many 3D scans. If you have the robot untethered, you can now specify a navigation goal for the robot. If the robot finds a navigation plan to the goal, it will attempt to navigate to it. While navigating, it will look down with its 3D camera in an attempt to stop if it detects an obstacle. In RViz, press the \"2D Nav Goal\" button on the top bar with a magenta arrow icon. Specify a nearby navigation goal pose on the floor of the map by clicking and drawing a magenta arrow. For this to work, the navigation goal must be in a place that the robot can reach and that the robot has scanned well. For example, the robot will only navigate across floor regions that it has in its map. If the robot finds a path, you should see green lines connecting white spheres in RViz that display its plan as it attempts to navigate to the goal. Once the robot has reached the goal, you can take another head scan. While in the terminal, press the space bar to initiate another head scan. The robot should take the head scan and merge it with the previous scans. If all goes well, the merged 3D map will be visible in RViz. You can also have the robot automatically drive to a place that it thinks is a good place for it to take a head scan in order to map the environment. While in the terminal, press the key with \\ and | on it. This should work even if caps lock is enabled or the shift key is pressed. If the robot reached it's goal, then you can now take another head scan. While in the terminal, press the space bar to initiate another head scan. By repeating this process, you can create a 3D map of the environment. FUNMAP uses images to represent the environment with each pixel value representing the highest observed 3D point at a planar location. By default, all of the merged maps are saved to the following directory: ./stretch_user/debug/merged_maps/ You can see the image representations by looking at files with the following naming pattern: ./stretch_user/debug/merged_maps/merged_map_DATETIME_mhi_visualization.png You can also click on a reaching goal for the Stretch RE1 by clicking on \"Publish Point\" in Rviz and then selecting a 3D point on the map. FUNMAP will attempt to generate a navigation and manipulation plan to reach close to the selected 3D location with Stretch's gripper. In RViz, select a reach goal by clicking on \"Publish Point\" on the top bar with a red map location icon). Then, click on a 3D location on the map to specify a reaching target for the robot's gripper. That concludes the demonstration. Have fun with FUNMAP!","title":"Getting Started Demo"},{"location":"stretch-ros/stretch_funmap/#more-funmap","text":"FUNMAP represents human environments with Max Height Images (MHIs). An MHI is an image for which each pixel typically represents the height of the robot\u2019s environment. Given a volume of interest (VOI) with its z-axis aligned with gravity, an MHI, I, maps locations to heights. Specifically, I(x,y)=z, where (x,y) represents a discretized planar location within the VOI and z represents the discretized height of the maximum occupied voxel within the VOI at that planar location (see Figure 1 above). The placement of the Stretch RE1 3D camera at a human head height enables it to capture MHIs that represent the horizontal surfaces with which humans frequently interact, such as table tops, countertops, and chairs. The orientation of its 3D camera enables the robot to quickly scan an environment to create a room-scale MHI by panning its head at a constant tilt angle. In contrast to other environment representations, such as point clouds and 3D mesh models, MHIs support fast, efficient operations through optimized image processing and are compatible with deep learning methods for images. Related representations have primarily been used for navigation, including for legged robots on rough terrain, but have not emphasized elevated surfaces in human environments nor incorporated manipulation [7, 8]. Object grasping systems for bin picking have used related representations, but have only considered small areas with objects and not incorporated navigation [9, 10]. During development, we have used FUNMAP to create MHIs for which each pixel represents a 6mm x 6mm region of the environment\u2019s floor and has a height resolution of less than 6mm per unit. This allows a single 2000 x 2000 pixel, 8-bit image to represent a 12m x 12m environment from 10cm below the estimated floor plane to 1.2m above the floor plane, which captures the great majority of open horizontal surfaces in human environments with which people interact. MHIs also have the potential to represent enclosed surfaces (e.g., surfaces in shelves and cabinets) and vertical surfaces (e.g., doors) by defining new VOIs with different orientations and heights, which is a capability that merits future exploration. For navigation and planning, FUNMAP uses fast distance transforms and morphological operators to efficiently create cost functions for robust optimization-based planning. For example, a high value of a distance transform at a floor location implies that the mobile base will be farther from obstacles. Similarly, a high value of a distance transform for a location on an elevated surface implies that the robot\u2019s end effector will be farther from obstacles. Figure 3: Left: The cyan lines represent achievable driving goals on the floor from which the robot can reach the target (red circle) on the table (dark blue). Right: When the target (red circle) is farther back near obstacles on the table (dark blue), the robot can reach the target from fewer locations (cyan lines). Figure 4: Left: Example of a piecewise linear navigation plan (green line segments and white spheres) being executed. Right: Example of a successfully executed plan to reach a target location on a table (light blue) while avoiding a wall with shelves (dark blue and purple). The Stretch RE1\u2019s slender links and Cartesian kinematics support rapid optimization of plans due to the simplified geometry of the robot\u2019s motions. FUNMAP uses piecewise linear paths on the cost image for fast navigation and manipulation planning. FUNMAP enables the Stretch RE1 to reach a 3D target position. FUNMAP models the linear motion of the telescoping arm as it extends to reach a target as a line in a manipulation cost image (see Figure 2). FUNMAP uses a navigation cost image and Djikstra\u2019s algorithm with a priority queue to efficiently estimate the cost of navigating to all floor locations (see Figure 3 left). FUNMAP then combines these results to find a minimum cost plan that increases the distance to obstacles and the manipulable workspace of the robot (see Figure 3 right).","title":"More FUNMAP"},{"location":"stretch-ros/stretch_funmap/#references","text":"[1] Bostelman, Roger, Tsai Hong, and Jeremy Marvel. \"Survey of research for performance measurement of mobile manipulators.\" Journal of Research of the National Institute of Standards and Technology 121, no. 3 (2016): 342-366. [2] Quigley, Morgan, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote, Jeremy Leibs, Rob Wheeler, and Andrew Y. Ng. \"ROS: an open-source Robot Operating System.\" In ICRA workshop on open source software, vol. 3, no. 3.2, p. 5. 2009. [3] Sachin Chitta, Eitan Marder-Eppstein, Wim Meeussen, Vijay Pradeep, Adolfo Rodr\u00edguez Tsouroukdissian, et al.. ros_control: A generic and simple control framework for ROS. The Journal of Open Source Software, 2017, 2 (20), pp.456 - 456. [4] Guimar\u00e3es, Rodrigo Longhi, Andr\u00e9 Schneider de Oliveira, Jo\u00e3o Alberto Fabro, Thiago Becker, and Vin\u00edcius Amilgar Brenner. \"ROS navigation: Concepts and tutorial.\" In Robot Operating System (ROS), pp. 121-160. Springer, Cham, 2016. [5] Chitta, Sachin, Ioan Sucan, and Steve Cousins. \"Moveit! [ros topics].\" IEEE Robotics & Automation Magazine 19, no. 1 (2012): 18-19. [6] Chitta, Sachin. \"MoveIt!: an introduction.\" In Robot Operating System (ROS), pp. 3-27. Springer, Cham, 2016. [7] Fankhauser, P\u00e9ter, and Marco Hutter. \"A universal grid map library: Implementation and use case for rough terrain navigation.\" In Robot Operating System (ROS), pp. 99-120. Springer, Cham, 2016. [8] Lu, David V., Dave Hershberger, and William D. Smart. \"Layered costmaps for context-sensitive navigation.\" In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 709-715. IEEE, 2014. [9] Zeng, Andy, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. \"Tossingbot: Learning to throw arbitrary objects with residual physics.\" arXiv preprint arXiv:1903.11239 (2019). [10] Zeng, Andy, Shuran Song, Kuan-Ting Yu, Elliott Donlon, Francois R. Hogan, Maria Bauza, Daolin Ma et al. \"Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching.\" In 2018 IEEE international conference on robotics and automation (ICRA), pp. 1-8. IEEE, 2018.","title":"References"},{"location":"stretch-ros/stretch_funmap/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros/stretch_funmap/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. This software is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License v3.0 (GNU LGPLv3) as published by the Free Software Foundation. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License v3.0 (GNU LGPLv3) for more details, which can be found via the following link: https://www.gnu.org/licenses/lgpl-3.0.en.html For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros/stretch_gazebo/","text":"Overview stretch_gazebo is an implementation of simulating a Stretch robot with Gazebo simulator. Details The urdf directory contains a xacro file that extends the capabilities of the original xacro files living in stretch_description package to include Gazebo functionality. The config directory contains rviz files and ros_control controller configuration files for various parts of the robot including: Base: diff_drive_controller/DiffDriveController Arm: position_controllers/JointTrajectoryController Gripper: position_controllers/JointTrajectoryController Head: position_controllers/JointTrajectoryController Joints: joint_state_controller/JointStateController The launch directory includes two files: gazebo.launch: Opens up an empty Gazebo world and spawns the robot loading all the controllers, including all the sensors except Cliff sensors and respeaker. teleop_keyboard.launch: Allows keyboard teleop in the terminal and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel , which the robot is taking velocity commands from. teleop_joy.launch: Spawns a joy and teleop_twist_joy instance and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel , which the robot is taking velocity commands from. Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless it is being pressed. For an Logitech F310 joystick this button is A. The script directory contains a single python file that publishes ground truth odometry of the robot from Gazebo. Setup These set up instructions will not be required on newly shipped robots. Follow these instructions if stretch_gazebo is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros and realsense_gazebo_plugin packages to your catkin workspace. Then install dependencies and build the packages, with the following set of commands: mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros git clone https://github.com/pal-robotics/realsense_gazebo_plugin cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make In order to use the built packages, make the packages discoverable by sourcing the ROS workspace: source ~/catkin_ws/devel/setup.bash . It is popular to add the sourcing command to your ~/.bashrc file, so that the ROS packages are discoverable in every new terminal that is opened. Running Demo # Terminal 1: roslaunch stretch_gazebo gazebo.launch rviz: = true # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller This will launch an Rviz instance that visualizes the sensors and an empty world in Gazebo with Stretch and load all the controllers. Although, the base will be able to move with the joystick commands, the joystick won't give joint commands to arm, head or gripper. To move these joints see the next section about Running Gazebo with MoveIt! and Stretch . Running Gazebo with MoveIt! and Stretch # Terminal 1: roslaunch stretch_gazebo gazebo.launch # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller # Terminal 3 roslaunch stretch_moveit_config demo_gazebo.launch This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm , stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to this moveit tutorial . A few notes to be kept in mind: Planning group can be changed via Planning Group drop down in Planning tab of Motion Planning Rviz plugin. Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning Rviz plugin. stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning Rviz plugin. Differences in Gazebo vs Stretch The simulated Stretch RE1 differs from the robot in the following ways. Gazebo Sensors vs Stretch Sensors Sensor Gazebo Stretch Notes LIDAR :heavy_check_mark: :heavy_check_mark: Base IMU :heavy_check_mark: :heavy_check_mark: Wrist Accelerometer :heavy_check_mark: :heavy_check_mark: Modeled as an IMU Realsense D435i :heavy_check_mark: :heavy_check_mark: Respeaker (Mic Array) :x: :heavy_check_mark: Cliff Sensors :x: :heavy_check_mark: Notes: Although there is no microphone in Gazebo, Respeaker can be represented with a ROS node that accesses compputer's microphone. Cliff sensors are not modeled but they can also be represented as 1D LIDAR sensors. See LIDAR definition in stretch_gazebo.urdf.xacro file. MoveIt Controllers vs stretch_core Actuators are defined as ros_control transmission objects in Gazebo using PositionJointInterfaces . MoveIt is configured to use three different action servers to control the body parts of stretch in Gazebo through the srdf file in stretch_moveit_config package. See the section above about MoveIt for details. Please note that this behavior is different than stretch_core as it works with a single Python interface to control all the joints. Uncalibrated XACRO vs Calibrated URDF We provide stretch_calibration to generate a calibrated URDF that is unique to each robot. The calibrated URDF is generated from the nominal description of Stretch RE1, the xacro files that live in stretch_description . The simulated Stretch RE1 is generated from the gazebo xacro description in the urdf directory and is not calibrated. License For license information, please see the LICENSE files.","title":"stretch_gazebo"},{"location":"stretch-ros/stretch_gazebo/#overview","text":"stretch_gazebo is an implementation of simulating a Stretch robot with Gazebo simulator.","title":"Overview"},{"location":"stretch-ros/stretch_gazebo/#details","text":"The urdf directory contains a xacro file that extends the capabilities of the original xacro files living in stretch_description package to include Gazebo functionality. The config directory contains rviz files and ros_control controller configuration files for various parts of the robot including: Base: diff_drive_controller/DiffDriveController Arm: position_controllers/JointTrajectoryController Gripper: position_controllers/JointTrajectoryController Head: position_controllers/JointTrajectoryController Joints: joint_state_controller/JointStateController The launch directory includes two files: gazebo.launch: Opens up an empty Gazebo world and spawns the robot loading all the controllers, including all the sensors except Cliff sensors and respeaker. teleop_keyboard.launch: Allows keyboard teleop in the terminal and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel , which the robot is taking velocity commands from. teleop_joy.launch: Spawns a joy and teleop_twist_joy instance and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel , which the robot is taking velocity commands from. Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless it is being pressed. For an Logitech F310 joystick this button is A. The script directory contains a single python file that publishes ground truth odometry of the robot from Gazebo.","title":"Details"},{"location":"stretch-ros/stretch_gazebo/#setup","text":"These set up instructions will not be required on newly shipped robots. Follow these instructions if stretch_gazebo is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros and realsense_gazebo_plugin packages to your catkin workspace. Then install dependencies and build the packages, with the following set of commands: mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros git clone https://github.com/pal-robotics/realsense_gazebo_plugin cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make In order to use the built packages, make the packages discoverable by sourcing the ROS workspace: source ~/catkin_ws/devel/setup.bash . It is popular to add the sourcing command to your ~/.bashrc file, so that the ROS packages are discoverable in every new terminal that is opened.","title":"Setup"},{"location":"stretch-ros/stretch_gazebo/#running-demo","text":"# Terminal 1: roslaunch stretch_gazebo gazebo.launch rviz: = true # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller This will launch an Rviz instance that visualizes the sensors and an empty world in Gazebo with Stretch and load all the controllers. Although, the base will be able to move with the joystick commands, the joystick won't give joint commands to arm, head or gripper. To move these joints see the next section about Running Gazebo with MoveIt! and Stretch .","title":"Running Demo"},{"location":"stretch-ros/stretch_gazebo/#running-gazebo-with-moveit-and-stretch","text":"# Terminal 1: roslaunch stretch_gazebo gazebo.launch # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller # Terminal 3 roslaunch stretch_moveit_config demo_gazebo.launch This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm , stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to this moveit tutorial . A few notes to be kept in mind: Planning group can be changed via Planning Group drop down in Planning tab of Motion Planning Rviz plugin. Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning Rviz plugin. stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning Rviz plugin.","title":"Running Gazebo with MoveIt! and Stretch"},{"location":"stretch-ros/stretch_gazebo/#differences-in-gazebo-vs-stretch","text":"The simulated Stretch RE1 differs from the robot in the following ways.","title":"Differences in Gazebo vs Stretch"},{"location":"stretch-ros/stretch_gazebo/#gazebo-sensors-vs-stretch-sensors","text":"Sensor Gazebo Stretch Notes LIDAR :heavy_check_mark: :heavy_check_mark: Base IMU :heavy_check_mark: :heavy_check_mark: Wrist Accelerometer :heavy_check_mark: :heavy_check_mark: Modeled as an IMU Realsense D435i :heavy_check_mark: :heavy_check_mark: Respeaker (Mic Array) :x: :heavy_check_mark: Cliff Sensors :x: :heavy_check_mark: Notes: Although there is no microphone in Gazebo, Respeaker can be represented with a ROS node that accesses compputer's microphone. Cliff sensors are not modeled but they can also be represented as 1D LIDAR sensors. See LIDAR definition in stretch_gazebo.urdf.xacro file.","title":"Gazebo Sensors vs Stretch Sensors"},{"location":"stretch-ros/stretch_gazebo/#moveit-controllers-vs-stretch_core","text":"Actuators are defined as ros_control transmission objects in Gazebo using PositionJointInterfaces . MoveIt is configured to use three different action servers to control the body parts of stretch in Gazebo through the srdf file in stretch_moveit_config package. See the section above about MoveIt for details. Please note that this behavior is different than stretch_core as it works with a single Python interface to control all the joints.","title":"MoveIt Controllers vs stretch_core"},{"location":"stretch-ros/stretch_gazebo/#uncalibrated-xacro-vs-calibrated-urdf","text":"We provide stretch_calibration to generate a calibrated URDF that is unique to each robot. The calibrated URDF is generated from the nominal description of Stretch RE1, the xacro files that live in stretch_description . The simulated Stretch RE1 is generated from the gazebo xacro description in the urdf directory and is not calibrated.","title":"Uncalibrated XACRO vs Calibrated URDF"},{"location":"stretch-ros/stretch_gazebo/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros/stretch_gazebo/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros/stretch_moveit_config/","text":"Stretch & MoveIt! MoveIt is the standard ROS manipulation platform, and this package is the configuration for working with Stretch with the MoveIt framework. Offline Demo To experiment with the planning capabilities of MoveIt on Stretch, you can run a demo without Stretch hardware. roslaunch stretch_moveit_config demo.launch This will allow you to move the robot around using interactive markers and create plans between poses. Hardware Integration There is no planned support to run MoveIt on Stretch hardware. Instead, support for running MoveIt 2 (the successor to MoveIt) on Stretch hardware is being developed in Stretch's ROS2 packages . The primary reason to support MoveIt 2 instead of MoveIt 1 is because MoveIt 2 introduces planning for differential drive bases, whereas MoveIt 1 does not have this ability. Manipulation with Stretch is more capable when the mobile base is included. Please keep an eye on Stretch's ROS2 packages and our forum to track the state of Stretch + MoveIt 2 support. License For license information, please see the LICENSE files.","title":"stretch_moveit_config"},{"location":"stretch-ros/stretch_moveit_config/#stretch-moveit","text":"MoveIt is the standard ROS manipulation platform, and this package is the configuration for working with Stretch with the MoveIt framework.","title":"Stretch &amp; MoveIt!"},{"location":"stretch-ros/stretch_moveit_config/#offline-demo","text":"To experiment with the planning capabilities of MoveIt on Stretch, you can run a demo without Stretch hardware. roslaunch stretch_moveit_config demo.launch This will allow you to move the robot around using interactive markers and create plans between poses.","title":"Offline Demo"},{"location":"stretch-ros/stretch_moveit_config/#hardware-integration","text":"There is no planned support to run MoveIt on Stretch hardware. Instead, support for running MoveIt 2 (the successor to MoveIt) on Stretch hardware is being developed in Stretch's ROS2 packages . The primary reason to support MoveIt 2 instead of MoveIt 1 is because MoveIt 2 introduces planning for differential drive bases, whereas MoveIt 1 does not have this ability. Manipulation with Stretch is more capable when the mobile base is included. Please keep an eye on Stretch's ROS2 packages and our forum to track the state of Stretch + MoveIt 2 support.","title":"Hardware Integration"},{"location":"stretch-ros/stretch_moveit_config/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros/stretch_moveit_config/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros/stretch_navigation/","text":"Overview stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive the stretch RE1 around a mapped space. Running this code will require the robot to be untethered. Setup These set up instructions are already performed on Stretch RE1 robots. Follow these instructions if stretch_navigation is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros to your catkin workspace. Then install dependencies and build the packages, with the following set of commands: mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make Quickstart The first step is to map the space that the robot will navigate in. The mapping.launch will enable you to do this. First run: roslaunch stretch_navigation mapping.launch Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to stretch_user/ . mkdir -p ~/stretch_user/maps rosrun map_server map_saver -f ${ HELLO_FLEET_PATH } /maps/<map_name> The <map_name> does not include an extension. Map_saver will save two files as <map_name>.pgm and <map_name>.yaml . Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location in the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place. It is also possible to send 2D Pose Estimates and Nav Goals programatically. In your own launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages in order to navigate the robot programatically. Running in Simulation To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the mapping_gazebo.launch and navigation_gazebo.launch launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch. roslaunch stretch_navigation mapping_gazebo.launch gazebo_world: = worlds/willowgarage.world Teleop using a Joystick Controller The mapping launch files, mapping.launch and mapping_gazebo.launch expose the ROS argument, \"teleop_type\". By default, this ROS arg is set to \"keyboard\", which launches keyboard teleop in the terminal. If the xbox controller that ships with Stretch RE1 is plugged into your computer, the following command will launch mapping with joystick teleop: roslaunch stretch_navigation mapping.launch teleop_type: = joystick Using ROS Remote Master If you have set up ROS Remote Master for untethered operation , you can use Rviz and teleop locally with the following commands: # On Robot roslaunch stretch_navigation mapping.launch rviz: = false teleop_type: = none # On your machine, Terminal 1: rviz -d ` rospack find stretch_navigation ` /rviz/mapping.launch # On your machine, Terminal 2: roslaunch stretch_core teleop_twist.launch teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller License For license information, please see the LICENSE files.","title":"stretch_navigation"},{"location":"stretch-ros/stretch_navigation/#overview","text":"stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive the stretch RE1 around a mapped space. Running this code will require the robot to be untethered.","title":"Overview"},{"location":"stretch-ros/stretch_navigation/#setup","text":"These set up instructions are already performed on Stretch RE1 robots. Follow these instructions if stretch_navigation is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros to your catkin workspace. Then install dependencies and build the packages, with the following set of commands: mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make","title":"Setup"},{"location":"stretch-ros/stretch_navigation/#quickstart","text":"The first step is to map the space that the robot will navigate in. The mapping.launch will enable you to do this. First run: roslaunch stretch_navigation mapping.launch Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to stretch_user/ . mkdir -p ~/stretch_user/maps rosrun map_server map_saver -f ${ HELLO_FLEET_PATH } /maps/<map_name> The <map_name> does not include an extension. Map_saver will save two files as <map_name>.pgm and <map_name>.yaml . Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location in the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place. It is also possible to send 2D Pose Estimates and Nav Goals programatically. In your own launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages in order to navigate the robot programatically.","title":"Quickstart"},{"location":"stretch-ros/stretch_navigation/#running-in-simulation","text":"To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the mapping_gazebo.launch and navigation_gazebo.launch launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch. roslaunch stretch_navigation mapping_gazebo.launch gazebo_world: = worlds/willowgarage.world","title":"Running in Simulation"},{"location":"stretch-ros/stretch_navigation/#teleop-using-a-joystick-controller","text":"The mapping launch files, mapping.launch and mapping_gazebo.launch expose the ROS argument, \"teleop_type\". By default, this ROS arg is set to \"keyboard\", which launches keyboard teleop in the terminal. If the xbox controller that ships with Stretch RE1 is plugged into your computer, the following command will launch mapping with joystick teleop: roslaunch stretch_navigation mapping.launch teleop_type: = joystick","title":"Teleop using a Joystick Controller"},{"location":"stretch-ros/stretch_navigation/#using-ros-remote-master","text":"If you have set up ROS Remote Master for untethered operation , you can use Rviz and teleop locally with the following commands: # On Robot roslaunch stretch_navigation mapping.launch rviz: = false teleop_type: = none # On your machine, Terminal 1: rviz -d ` rospack find stretch_navigation ` /rviz/mapping.launch # On your machine, Terminal 2: roslaunch stretch_core teleop_twist.launch teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller","title":"Using ROS Remote Master"},{"location":"stretch-ros/stretch_navigation/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros/stretch_navigation/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/","text":"ROS 2 Galactic Development Branch This is a development branch that we are using to port stretch_ros to ROS 2 Galactic, Python 3, and Ubuntu 20.04. We plan to begin shipping this version preinstalled on Stretch RE1 robots in the future. It is not in a usable state . It is also unstable, since we are actively conducting development in this branch. Since we have performed limited testing, you may encounter unexpected behaviors. Also, installation requires Ubuntu 20.04 on a second partition of your robot's hard drive. We are beginning to use this port internally at Hello Robot to test it, improve it, and add new capabilities. Ported to ROS 2 Stretch Description Stretch MoveIt Config Stretch Core (Driver Node) Known Issues No support for: Stretch Calibration Stretch Core (Everything Else) Stretch Dashboard Stretch Deep Perception Stretch Demos Stretch FUNMAP Stretch Gazebo Stretch Navigation Stretch OctoMap Stretch RTABMap The deep perception demos won't work with a default installation, since they require OpenCV compiled with OpenVINO. There is no support for the Respeaker Microphone Array. There is no support for the Dexterous Wrist. Directories The stretch_ros repository holds ROS related code for the Stretch RE1 mobile manipulator from Hello Robot Inc. For an overview of the capabilities in this repository, we recommend you look at the following forum post: https://forum.hello-robot.com/t/autonomy-video-details Resource Description hello_helpers Miscellaneous helper code used across the stretch_ros repository stretch_calibration Creates and updates calibrated URDFs for the Stretch RE1 stretch_core Enables basic use of the Stretch RE1 from ROS stretch_deep_perception Demonstrations that use open deep learning models to perceive the world stretch_demos Demonstrations of simple autonomous manipulation stretch_description Generate and export URDFs stretch_funmap Demonstrations of Fast Unified Navigation, Manipulation And Planning (FUNMAP) stretch_gazebo Support for simulation of Stretch in the Gazebo simulator stretch_moveit_config Config files to use Stretch with the MoveIt Motion Planning Framework stretch_navigation Support for the ROS navigation stack, including move_base, gmapping, and AMCL stretch_octomap Support for mapping using OctoMap: efficient probabilistic 3D Mapping based on Octrees stretch_rtabmap Support for mapping using Real-Time Appearance-Based Mapping (RTAB-Map) Licenses This software is intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc. For license details for this repository, see the LICENSE files found in the directories. A summary of the licenses follows: Directory License hello_helpers Apache 2.0 stretch_calibration GPLv3 stretch_core Apache 2.0 stretch_deep_perception Apache 2.0 stretch_demos Apache 2.0 stretch_description BSD 3-Clause Clear License stretch_funmap LGPLv3 stretch_gazebo Apache 2.0 stretch_moveit_config Apache 2.0 stretch_navigation Apache 2.0 stretch_octomap Apache 2.0 stretch_rtabmap Apache 2.0","title":"Overview"},{"location":"stretch-ros2/#ros-2-galactic-development-branch","text":"This is a development branch that we are using to port stretch_ros to ROS 2 Galactic, Python 3, and Ubuntu 20.04. We plan to begin shipping this version preinstalled on Stretch RE1 robots in the future. It is not in a usable state . It is also unstable, since we are actively conducting development in this branch. Since we have performed limited testing, you may encounter unexpected behaviors. Also, installation requires Ubuntu 20.04 on a second partition of your robot's hard drive. We are beginning to use this port internally at Hello Robot to test it, improve it, and add new capabilities.","title":"ROS 2 Galactic Development Branch"},{"location":"stretch-ros2/#ported-to-ros-2","text":"Stretch Description Stretch MoveIt Config Stretch Core (Driver Node)","title":"Ported to ROS 2"},{"location":"stretch-ros2/#known-issues","text":"No support for: Stretch Calibration Stretch Core (Everything Else) Stretch Dashboard Stretch Deep Perception Stretch Demos Stretch FUNMAP Stretch Gazebo Stretch Navigation Stretch OctoMap Stretch RTABMap The deep perception demos won't work with a default installation, since they require OpenCV compiled with OpenVINO. There is no support for the Respeaker Microphone Array. There is no support for the Dexterous Wrist.","title":"Known Issues"},{"location":"stretch-ros2/#directories","text":"The stretch_ros repository holds ROS related code for the Stretch RE1 mobile manipulator from Hello Robot Inc. For an overview of the capabilities in this repository, we recommend you look at the following forum post: https://forum.hello-robot.com/t/autonomy-video-details Resource Description hello_helpers Miscellaneous helper code used across the stretch_ros repository stretch_calibration Creates and updates calibrated URDFs for the Stretch RE1 stretch_core Enables basic use of the Stretch RE1 from ROS stretch_deep_perception Demonstrations that use open deep learning models to perceive the world stretch_demos Demonstrations of simple autonomous manipulation stretch_description Generate and export URDFs stretch_funmap Demonstrations of Fast Unified Navigation, Manipulation And Planning (FUNMAP) stretch_gazebo Support for simulation of Stretch in the Gazebo simulator stretch_moveit_config Config files to use Stretch with the MoveIt Motion Planning Framework stretch_navigation Support for the ROS navigation stack, including move_base, gmapping, and AMCL stretch_octomap Support for mapping using OctoMap: efficient probabilistic 3D Mapping based on Octrees stretch_rtabmap Support for mapping using Real-Time Appearance-Based Mapping (RTAB-Map)","title":"Directories"},{"location":"stretch-ros2/#licenses","text":"This software is intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc. For license details for this repository, see the LICENSE files found in the directories. A summary of the licenses follows: Directory License hello_helpers Apache 2.0 stretch_calibration GPLv3 stretch_core Apache 2.0 stretch_deep_perception Apache 2.0 stretch_demos Apache 2.0 stretch_description BSD 3-Clause Clear License stretch_funmap LGPLv3 stretch_gazebo Apache 2.0 stretch_moveit_config Apache 2.0 stretch_navigation Apache 2.0 stretch_octomap Apache 2.0 stretch_rtabmap Apache 2.0","title":"Licenses"},{"location":"stretch-ros2/install_noetic/","text":"Noetic Installation Instructions The code in this development branch and this installation guide are both under active development. Please proceed with caution. These instructions will guide you through the process of installing the Noetic version of stretch_ros onto a Stretch RE1 robot that has a factory installation of the Melodic version of stretch_ros. If you run into a problem, you may find a solution in the trouble shooting section at the end of this document. Install a partition with Ubuntu 20.04 Desktop Edition Download and write the Ubuntu 20.04 iso file to a USB key. Backup all of the critical files from the existing Ubuntu 18.04 partition on the robot. Make sure you have sufficient space on the robot\u2019s solid state drive (SSD) for a new partition with Ubuntu 20.04. The Stretch RE1 typically ships with a 500 GB SSD drive. The default ROS Melodic installation typically takes up less than 20 GB when shipped. However, the robot may be using much more space due to use. The default ROS Noetic installation typically takes up less than 20 GB, but you will want additional space while using the robot. Boot the robot with the USB key plugged into a USB port in the robot\u2019s trunk. The installer should detect the Ubuntu 18.04 installation and provide the option to install Ubuntu 20.04 alongside Ubuntu 18.04. This option will be selected by default. Proceed with the option to install Ubuntu 20.04 alonside Ubuntu 18.04. This will result in a new partition with Ubuntu 20.04. You will have the opportunity to decide how much space to devote to each partition. Copy materials from the robot's original Ubuntu 18.04 partition Boot into the robot's original Ubuntu 18.04 partition. Copy the /etc/hello-robot to a USB key. For example, you can run cp -r /etc/hello-robot /media/$USER/USBKEY from the command line where USBKEY is the mounted USB key. Or, you can open the file explorer to copy the directory. Make sure you look for /etc/hello-robot on the original Ubuntu 18.04 partition rather than the new partition. Boot into the robot's new Ubuntu 20.04 partition. Copy the folder of the form \"stretch-re1-xxxx\" found in the hello-robot directory into the home folder (i.e. /home/$USER/ ). For example, you can run a command similar to cp -r /media/$USER/USBKEY/hello-robot/stretch-re1-xxxx /home/$USER/ Or, you can use the visual file explorer. Run the setup script with the robot's new Ubuntu 20.04 installation Using a terminal, go to the home directory of the new Ubuntu 20.04 installation with cd . Then run the following commands. sudo apt install git cd $HOME git clone https://github.com/hello-robot/stretch_install.git -b dev/install_20.04 cd ./stretch_install/factory && ./stretch_install_nonfactory.sh You will need to provide input when prompted, including your robot's number and your password. The password request can come late in the process and freeze installation until you provide it. Check that the robot's Noetic ROS installation is working Shutdown and power off your robot. Then turn your robot back on. Once you're logged in, you can test your robot with the following commands. Go ahead and attempt to home your robot using stretch_robot_home.py . It should be calibrated afterward. Run stretch_robot_system_check.py to make sure that things are normal. Ideally, you will see all green and no red. It's not unusual for a cliff sensor to be red due to reporting a value outside of its ideal range. Make sure the game controller dongle is plugged in and run stretch_xbox_controller_teleop.py . Use the game controller to test out the motions of the robot. Run roslaunch stretch_core wheel_odometry_test.launch . You should see coherent visualizations of the robot's body, the laser range finder output, and the D435i point cloud output. You should be able to move the robot around using keyboard commands and see reasonable visualizations in RViz. You may delete the ./stretch-re1-xxxx directory and its contents that you copied over from the Ubuntu 18.04 partition, if you'd like. Recalibrate your robot The new Noetic ROS installation starts out by using the calibrated URDF that was created at the Hello Robot factory. We recommend that you recalibrate your robot by following the stretch_calibration instructions . This takes about 1.5 hours of robot time, but will result in a higher-quality model that matches the current state of the robot. For example, shipping can sometimes shift components a little. Noetic Installation Troubleshooting This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum. Firmware Mismatch Error If you are seeing the following error: ---------------- Firmware protocol mismatch on hello-. Protocol on board is pX. Valid protocol is: pX. Disabling device. Please upgrade the firmware and/or version of Stretch Body. ---------------- Your version of Stretch Body does not align with the firmware installed with your robot. Run the firmware updater tool to automatically update the firmware to the required version for your software. $ python -m pip install hello-robot-stretch-factory $ RE1_firmware_updater.py Homing Error If using stretch_robot_home.py does not result in the robot being calibrated, try running the command again. If this does not work, try shutting down the robot, turning off the robot with the power switch, waiting for a few seconds, and then powering it on again. Then, try stretch_robot_home.py again. For unknown reasons, one of our robots has needed to be homed, rebooted, and homed again after following this installation guide. ROS Launch File Fails The launch files have nondeterministic behavior. Sometimes they need to be run more than once for the nodes to start in a successful order that works. For example, a common symptom of a failed launch is the visualization of the robot's body appearing white and flat in RViz. Start Over by Reinstalling Ubuntu 20.04 You may decide to start over. If so, you can follow these steps to first delete your current Ubuntu 20.04 partition and then reinstall Ubuntu 20.04. Backup any materials you wish to keep in your Ubuntu 20.04 partition. Boot into Ubuntu 18.04, so that you can delete your Ubuntu 20.04 partition. While in Ubuntu 18.04, run GNOME Disks. You can find this disk management software by going to \"Show Applications\" and searching for \"Disks\". Use GNOME Disks to delete your Ubuntu 20.04 partition. For example, you can click on the partition to select it and then press the minus sign to start the process. Once you succeed in deleting the partition, it should be labeled as \"Free Space\". Shutdown your robot. Now proceed with the instructions above, starting with \"Install a partition with Ubuntu 20.04 Desktop Edition\"","title":"Install noetic"},{"location":"stretch-ros2/install_noetic/#noetic-installation-instructions","text":"The code in this development branch and this installation guide are both under active development. Please proceed with caution. These instructions will guide you through the process of installing the Noetic version of stretch_ros onto a Stretch RE1 robot that has a factory installation of the Melodic version of stretch_ros. If you run into a problem, you may find a solution in the trouble shooting section at the end of this document.","title":"Noetic Installation Instructions"},{"location":"stretch-ros2/install_noetic/#install-a-partition-with-ubuntu-2004-desktop-edition","text":"Download and write the Ubuntu 20.04 iso file to a USB key. Backup all of the critical files from the existing Ubuntu 18.04 partition on the robot. Make sure you have sufficient space on the robot\u2019s solid state drive (SSD) for a new partition with Ubuntu 20.04. The Stretch RE1 typically ships with a 500 GB SSD drive. The default ROS Melodic installation typically takes up less than 20 GB when shipped. However, the robot may be using much more space due to use. The default ROS Noetic installation typically takes up less than 20 GB, but you will want additional space while using the robot. Boot the robot with the USB key plugged into a USB port in the robot\u2019s trunk. The installer should detect the Ubuntu 18.04 installation and provide the option to install Ubuntu 20.04 alongside Ubuntu 18.04. This option will be selected by default. Proceed with the option to install Ubuntu 20.04 alonside Ubuntu 18.04. This will result in a new partition with Ubuntu 20.04. You will have the opportunity to decide how much space to devote to each partition.","title":"Install a partition with Ubuntu 20.04 Desktop Edition"},{"location":"stretch-ros2/install_noetic/#copy-materials-from-the-robots-original-ubuntu-1804-partition","text":"Boot into the robot's original Ubuntu 18.04 partition. Copy the /etc/hello-robot to a USB key. For example, you can run cp -r /etc/hello-robot /media/$USER/USBKEY from the command line where USBKEY is the mounted USB key. Or, you can open the file explorer to copy the directory. Make sure you look for /etc/hello-robot on the original Ubuntu 18.04 partition rather than the new partition. Boot into the robot's new Ubuntu 20.04 partition. Copy the folder of the form \"stretch-re1-xxxx\" found in the hello-robot directory into the home folder (i.e. /home/$USER/ ). For example, you can run a command similar to cp -r /media/$USER/USBKEY/hello-robot/stretch-re1-xxxx /home/$USER/ Or, you can use the visual file explorer.","title":"Copy materials from the robot's original Ubuntu 18.04 partition"},{"location":"stretch-ros2/install_noetic/#run-the-setup-script-with-the-robots-new-ubuntu-2004-installation","text":"Using a terminal, go to the home directory of the new Ubuntu 20.04 installation with cd . Then run the following commands. sudo apt install git cd $HOME git clone https://github.com/hello-robot/stretch_install.git -b dev/install_20.04 cd ./stretch_install/factory && ./stretch_install_nonfactory.sh You will need to provide input when prompted, including your robot's number and your password. The password request can come late in the process and freeze installation until you provide it.","title":"Run the setup script with the robot's new Ubuntu 20.04 installation"},{"location":"stretch-ros2/install_noetic/#check-that-the-robots-noetic-ros-installation-is-working","text":"Shutdown and power off your robot. Then turn your robot back on. Once you're logged in, you can test your robot with the following commands. Go ahead and attempt to home your robot using stretch_robot_home.py . It should be calibrated afterward. Run stretch_robot_system_check.py to make sure that things are normal. Ideally, you will see all green and no red. It's not unusual for a cliff sensor to be red due to reporting a value outside of its ideal range. Make sure the game controller dongle is plugged in and run stretch_xbox_controller_teleop.py . Use the game controller to test out the motions of the robot. Run roslaunch stretch_core wheel_odometry_test.launch . You should see coherent visualizations of the robot's body, the laser range finder output, and the D435i point cloud output. You should be able to move the robot around using keyboard commands and see reasonable visualizations in RViz. You may delete the ./stretch-re1-xxxx directory and its contents that you copied over from the Ubuntu 18.04 partition, if you'd like.","title":"Check that the robot's Noetic ROS installation is working"},{"location":"stretch-ros2/install_noetic/#recalibrate-your-robot","text":"The new Noetic ROS installation starts out by using the calibrated URDF that was created at the Hello Robot factory. We recommend that you recalibrate your robot by following the stretch_calibration instructions . This takes about 1.5 hours of robot time, but will result in a higher-quality model that matches the current state of the robot. For example, shipping can sometimes shift components a little.","title":"Recalibrate your robot"},{"location":"stretch-ros2/install_noetic/#noetic-installation-troubleshooting","text":"This section provides suggestions for common errors that occur during installation. If you become stuck and don't find an answer here, please email us or contact us through the forum.","title":"Noetic Installation Troubleshooting"},{"location":"stretch-ros2/install_noetic/#firmware-mismatch-error","text":"If you are seeing the following error: ---------------- Firmware protocol mismatch on hello-. Protocol on board is pX. Valid protocol is: pX. Disabling device. Please upgrade the firmware and/or version of Stretch Body. ---------------- Your version of Stretch Body does not align with the firmware installed with your robot. Run the firmware updater tool to automatically update the firmware to the required version for your software. $ python -m pip install hello-robot-stretch-factory $ RE1_firmware_updater.py","title":"Firmware Mismatch Error"},{"location":"stretch-ros2/install_noetic/#homing-error","text":"If using stretch_robot_home.py does not result in the robot being calibrated, try running the command again. If this does not work, try shutting down the robot, turning off the robot with the power switch, waiting for a few seconds, and then powering it on again. Then, try stretch_robot_home.py again. For unknown reasons, one of our robots has needed to be homed, rebooted, and homed again after following this installation guide.","title":"Homing Error"},{"location":"stretch-ros2/install_noetic/#ros-launch-file-fails","text":"The launch files have nondeterministic behavior. Sometimes they need to be run more than once for the nodes to start in a successful order that works. For example, a common symptom of a failed launch is the visualization of the robot's body appearing white and flat in RViz.","title":"ROS Launch File Fails"},{"location":"stretch-ros2/install_noetic/#start-over-by-reinstalling-ubuntu-2004","text":"You may decide to start over. If so, you can follow these steps to first delete your current Ubuntu 20.04 partition and then reinstall Ubuntu 20.04. Backup any materials you wish to keep in your Ubuntu 20.04 partition. Boot into Ubuntu 18.04, so that you can delete your Ubuntu 20.04 partition. While in Ubuntu 18.04, run GNOME Disks. You can find this disk management software by going to \"Show Applications\" and searching for \"Disks\". Use GNOME Disks to delete your Ubuntu 20.04 partition. For example, you can click on the partition to select it and then press the minus sign to start the process. Once you succeed in deleting the partition, it should be labeled as \"Free Space\". Shutdown your robot. Now proceed with the instructions above, starting with \"Install a partition with Ubuntu 20.04 Desktop Edition\"","title":"Start Over by Reinstalling Ubuntu 20.04"},{"location":"stretch-ros2/hello_helpers/","text":"Overview hello_helpers mostly consists of the hello_helpers Python module. This module provides various Python files used across stretch_ros that have not attained sufficient status to stand on their own. Ported to ROS 2 gripper_conversion.py : Used for converting measurements for the gripper Not Supported in ROS 2 Yet fit_plane.py : Fits planes to 3D data. hello_misc.py : Various functions, including a helpful Python object with which to create ROS nodes. hello_ros_viz.py : Various helper functions for visualizations using RViz. Typical Usage import hello_helpers.fit_plane as fp import hello_helpers.hello_misc as hm import hello_helpers.hello_ros_viz as hr License For license information, please see the LICENSE files.","title":"hello_helpers"},{"location":"stretch-ros2/hello_helpers/#overview","text":"hello_helpers mostly consists of the hello_helpers Python module. This module provides various Python files used across stretch_ros that have not attained sufficient status to stand on their own.","title":"Overview"},{"location":"stretch-ros2/hello_helpers/#ported-to-ros-2","text":"gripper_conversion.py : Used for converting measurements for the gripper","title":"Ported to ROS 2"},{"location":"stretch-ros2/hello_helpers/#not-supported-in-ros-2-yet","text":"fit_plane.py : Fits planes to 3D data. hello_misc.py : Various functions, including a helpful Python object with which to create ROS nodes. hello_ros_viz.py : Various helper functions for visualizations using RViz.","title":"Not Supported in ROS 2 Yet"},{"location":"stretch-ros2/hello_helpers/#typical-usage","text":"import hello_helpers.fit_plane as fp import hello_helpers.hello_misc as hm import hello_helpers.hello_ros_viz as hr","title":"Typical Usage"},{"location":"stretch-ros2/hello_helpers/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/hello_helpers/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_calibration/","text":"Overview stretch_calibration provides tools for calibrating and managing the URDF for the Stretch RE1 robot from Hello Robot Inc. The code's primary role is to generate a geometric model (i.e., a URDF) of the robot's body that corresponds well with views of the body from the robot's 3D camera (i.e., a Intel RealSense D435i). The code achieves this objective by adjusting the geometry of the model to predict where the 3D camera will see markers (i.e., ArUco markers) on the robot's body. Hello Robot Inc. uses this code to calibrate each robot prior to shipping. Users may wish to recalibrate their robots to compensate for changes over time or take advantage of improvements to the calibration code. In addition, after changing a tool, this code can be used to generate a new calibrated URDF that incorporates the tool without performing a new calibration optimization. Checking the Current Calibration with New Observations Make sure the basic joint limit calibration has been performed. stretch_robot_home.py Make sure the uncalibrated URDF is up to date. ros2 run stretch_calibration update_uncalibrated_urdf Collect new observations ros2 launch stretch_calibration collect_check_head_calibration_data.launch.py Test how well the current calibrated model fits the new observations ros2 run stretch_calibration check_head_calibration The total_error printed on the command line should be less than 0.05. If it is not, an error will be printed on the command line. In RViz the white markers represent the locations for the ArUco markers predicted by the calibrated URDF. The colored markers represent the observed locations of the ArUco markers on the robot's body. For a good fit, the white markers will be close to the colored markers. Visually Inspecting the Current Calibration The following command will allow you to visually inspect a calibration with Rviz. You can use RViz to see how well the robot's 3D body model matches point clouds from the 3D camera. While visualizing the 3D model and point clouds in RViz, you can use keyboard commands in the terminal to move the head around, the lift up and down, and the arm in and out. The keyboard commands will be printed in the terminal. A good calibration should result in a close correspondence between the robot's 3D body model and the point cloud throughout the ranges of motion for the head, lift, and arm. You may notice higher error when the head is looking upward due to challenges associated with head tilt backlash. You might also notice higher error when the arm is fully extended, since small angular errors can result in larger positional errors at the robot's wrist. Test the current head calibration ros2 launch stretch_calibration simple_test_head_calibration.launch.py Examples of Good and Bad Visual Fit In the images below, examples of good and bad fit between the point cloud and the geometric model are presented side by side. To make the distinction clear, the images have green and red circles indicating where the fit is either good or bad. Calibrate the Stretch RE1 Make sure the basic joint limit calibration has been performed. stretch_robot_home.py Make sure the uncalibrated URDF is up to date. ros2 run stretch_calibration update_uncalibrated_urdf Collect head calibration data Put the robot on a flat surface. Give it room to move its arm and good lighting. Then, have the robot collect data using the command below. While the robot is collecting data, do not block its view of its markers. ros2 launch stretch_calibration collect_head_calibration_data.launch.py Process head calibration data Specify how much data to use and the quality of the fit YAML file with parameters: stretch_ros/stretch_calibration/config/head_calibration_options.yaml More data and higher quality fitting result in optimizations that take longer When quickly testing things out ~3 minutes without visualization data_to_use: use_very_little_data fit_quality: fastest_lowest_quality When calibrating the robot ~1 hour without visualization data_to_use: use_all_data fit_quality: slow_high_quality Perform the optimization to fit the model to the collected data Without visualization (faster) ros2 launch stretch_calibration process_head_calibration_data.launch.py With visualization (slower) ros2 launch stretch_calibration process_head_calibration_data_with_visualization.launch.py Inspect the fit of the most recent head calibration ros2 run stretch_calibration visualize_most_recent_head_calibration Start using the newest head calibration ros2 run stretch_calibration update_with_most_recent_calibration Test the current head calibration ros2 launch stretch_calibration simple_test_head_calibration.launch.py Use RViz to visually inspect the calibrated model. The robot's 3D body model should look similar to the structure of your robot. You may refer to the section above to see examples of good and bad fit. Generate a New URDF After Changing the Tool If you change the Stretch RE1's tool attached to the wrist and want to generate a new URDF for it, you can do so with xacro files in the /stretch_ros/stretch_description/urdf/ directory. Specifically, you can edit stretch_description.xacro to include a xacro other than the default stretch_gripper.xacro. After changing the tool xacro you will need to generate a new URDF and also update this new URDF with the previously optimized calibration parameters. To do so, follow the directions below: In a terminal run ros2 run stretch_calibration update_urdf_after_xacro_change This will update the uncalibrated URDF with the current xacro files and then create a calibrated URDF using the most recent calibration parameters. Revert to a Previous Calibration When a new calibration is performed, it is timestamped and added to the calibration directory under \"stretch_user/\". If you'd like to revert to a previous calibration, you may run the following command. It will move the most recent calibration files to a reversion directory and update the calibration in the stretch_description package from the remaining most recent calibration files. Revert to the previous head calibration ros2 run stretch_calibration revert_to_previous_calibration License stretch_calibration is licensed with the GPLv3. Please see the LICENSE file for details.","title":"stretch_calibration"},{"location":"stretch-ros2/stretch_calibration/#overview","text":"stretch_calibration provides tools for calibrating and managing the URDF for the Stretch RE1 robot from Hello Robot Inc. The code's primary role is to generate a geometric model (i.e., a URDF) of the robot's body that corresponds well with views of the body from the robot's 3D camera (i.e., a Intel RealSense D435i). The code achieves this objective by adjusting the geometry of the model to predict where the 3D camera will see markers (i.e., ArUco markers) on the robot's body. Hello Robot Inc. uses this code to calibrate each robot prior to shipping. Users may wish to recalibrate their robots to compensate for changes over time or take advantage of improvements to the calibration code. In addition, after changing a tool, this code can be used to generate a new calibrated URDF that incorporates the tool without performing a new calibration optimization.","title":"Overview"},{"location":"stretch-ros2/stretch_calibration/#checking-the-current-calibration-with-new-observations","text":"Make sure the basic joint limit calibration has been performed. stretch_robot_home.py Make sure the uncalibrated URDF is up to date. ros2 run stretch_calibration update_uncalibrated_urdf Collect new observations ros2 launch stretch_calibration collect_check_head_calibration_data.launch.py Test how well the current calibrated model fits the new observations ros2 run stretch_calibration check_head_calibration The total_error printed on the command line should be less than 0.05. If it is not, an error will be printed on the command line. In RViz the white markers represent the locations for the ArUco markers predicted by the calibrated URDF. The colored markers represent the observed locations of the ArUco markers on the robot's body. For a good fit, the white markers will be close to the colored markers.","title":"Checking the Current Calibration with New Observations"},{"location":"stretch-ros2/stretch_calibration/#visually-inspecting-the-current-calibration","text":"The following command will allow you to visually inspect a calibration with Rviz. You can use RViz to see how well the robot's 3D body model matches point clouds from the 3D camera. While visualizing the 3D model and point clouds in RViz, you can use keyboard commands in the terminal to move the head around, the lift up and down, and the arm in and out. The keyboard commands will be printed in the terminal. A good calibration should result in a close correspondence between the robot's 3D body model and the point cloud throughout the ranges of motion for the head, lift, and arm. You may notice higher error when the head is looking upward due to challenges associated with head tilt backlash. You might also notice higher error when the arm is fully extended, since small angular errors can result in larger positional errors at the robot's wrist. Test the current head calibration ros2 launch stretch_calibration simple_test_head_calibration.launch.py","title":"Visually Inspecting the Current Calibration"},{"location":"stretch-ros2/stretch_calibration/#examples-of-good-and-bad-visual-fit","text":"In the images below, examples of good and bad fit between the point cloud and the geometric model are presented side by side. To make the distinction clear, the images have green and red circles indicating where the fit is either good or bad.","title":"Examples of Good and Bad Visual Fit"},{"location":"stretch-ros2/stretch_calibration/#calibrate-the-stretch-re1","text":"Make sure the basic joint limit calibration has been performed. stretch_robot_home.py Make sure the uncalibrated URDF is up to date. ros2 run stretch_calibration update_uncalibrated_urdf Collect head calibration data Put the robot on a flat surface. Give it room to move its arm and good lighting. Then, have the robot collect data using the command below. While the robot is collecting data, do not block its view of its markers. ros2 launch stretch_calibration collect_head_calibration_data.launch.py Process head calibration data Specify how much data to use and the quality of the fit YAML file with parameters: stretch_ros/stretch_calibration/config/head_calibration_options.yaml More data and higher quality fitting result in optimizations that take longer When quickly testing things out ~3 minutes without visualization data_to_use: use_very_little_data fit_quality: fastest_lowest_quality When calibrating the robot ~1 hour without visualization data_to_use: use_all_data fit_quality: slow_high_quality Perform the optimization to fit the model to the collected data Without visualization (faster) ros2 launch stretch_calibration process_head_calibration_data.launch.py With visualization (slower) ros2 launch stretch_calibration process_head_calibration_data_with_visualization.launch.py Inspect the fit of the most recent head calibration ros2 run stretch_calibration visualize_most_recent_head_calibration Start using the newest head calibration ros2 run stretch_calibration update_with_most_recent_calibration Test the current head calibration ros2 launch stretch_calibration simple_test_head_calibration.launch.py Use RViz to visually inspect the calibrated model. The robot's 3D body model should look similar to the structure of your robot. You may refer to the section above to see examples of good and bad fit.","title":"Calibrate the Stretch RE1"},{"location":"stretch-ros2/stretch_calibration/#generate-a-new-urdf-after-changing-the-tool","text":"If you change the Stretch RE1's tool attached to the wrist and want to generate a new URDF for it, you can do so with xacro files in the /stretch_ros/stretch_description/urdf/ directory. Specifically, you can edit stretch_description.xacro to include a xacro other than the default stretch_gripper.xacro. After changing the tool xacro you will need to generate a new URDF and also update this new URDF with the previously optimized calibration parameters. To do so, follow the directions below: In a terminal run ros2 run stretch_calibration update_urdf_after_xacro_change This will update the uncalibrated URDF with the current xacro files and then create a calibrated URDF using the most recent calibration parameters.","title":"Generate a New URDF After Changing the Tool"},{"location":"stretch-ros2/stretch_calibration/#revert-to-a-previous-calibration","text":"When a new calibration is performed, it is timestamped and added to the calibration directory under \"stretch_user/\". If you'd like to revert to a previous calibration, you may run the following command. It will move the most recent calibration files to a reversion directory and update the calibration in the stretch_description package from the remaining most recent calibration files. Revert to the previous head calibration ros2 run stretch_calibration revert_to_previous_calibration","title":"Revert to a Previous Calibration"},{"location":"stretch-ros2/stretch_calibration/#license","text":"stretch_calibration is licensed with the GPLv3. Please see the LICENSE file for details.","title":"License"},{"location":"stretch-ros2/stretch_calibration/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation. The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link: https://www.gnu.org/licenses/gpl-3.0.html For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_core/","text":"Overview stretch_core provides the core ROS interfaces to the Stretch RE1 mobile manipulator from Hello Robot Inc. It includes the following nodes: stretch_driver : node that communicates with the low-level Python library (stretch_body) to interface with the Stretch RE1 detect_aruco_markers : node that detects and estimates the pose of ArUco markers, including the markers on the robot's body d435i_ * : various nodes to help use the Stretch RE1's 3D camera keyboard_teleop : node that provides a keyboard interface to control the robot's joints Testing Colcon is used to run the integration tests in the /test folder. The command to run the entire suite of tests is: $ cd ~/ament_ws $ colcon test --packages-select stretch_core Here are description of each test suite: test_services.py : tests the ROS2 services within the stretch_driver node test_action_manipulation_mode.py : tests the ROS2 FollowJointTrajectory action server's manipulation mode test_flake8.py and test_pep257.py : linters that identify unrecommended code style License For license information, please see the LICENSE files.","title":"stretch_core"},{"location":"stretch-ros2/stretch_core/#overview","text":"stretch_core provides the core ROS interfaces to the Stretch RE1 mobile manipulator from Hello Robot Inc. It includes the following nodes: stretch_driver : node that communicates with the low-level Python library (stretch_body) to interface with the Stretch RE1 detect_aruco_markers : node that detects and estimates the pose of ArUco markers, including the markers on the robot's body d435i_ * : various nodes to help use the Stretch RE1's 3D camera keyboard_teleop : node that provides a keyboard interface to control the robot's joints","title":"Overview"},{"location":"stretch-ros2/stretch_core/#testing","text":"Colcon is used to run the integration tests in the /test folder. The command to run the entire suite of tests is: $ cd ~/ament_ws $ colcon test --packages-select stretch_core Here are description of each test suite: test_services.py : tests the ROS2 services within the stretch_driver node test_action_manipulation_mode.py : tests the ROS2 FollowJointTrajectory action server's manipulation mode test_flake8.py and test_pep257.py : linters that identify unrecommended code style","title":"Testing"},{"location":"stretch-ros2/stretch_core/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/stretch_core/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_dashboard/","text":"Stretch Dashboard Gear icon used for calibration Next button used for changing modes Battery shows voltage/current as a tooltip To Run rosrun stretch_dashboard dashboard Note : The first time you run it you may need to add the --force-discover arg to the end of the above command.","title":"stretch_dashboard"},{"location":"stretch-ros2/stretch_dashboard/#stretch-dashboard","text":"Gear icon used for calibration Next button used for changing modes Battery shows voltage/current as a tooltip","title":"Stretch Dashboard"},{"location":"stretch-ros2/stretch_dashboard/#to-run","text":"rosrun stretch_dashboard dashboard Note : The first time you run it you may need to add the --force-discover arg to the end of the above command.","title":"To Run"},{"location":"stretch-ros2/stretch_deep_perception/","text":"Overview stretch_deep_perception provides demonstration code that uses open deep learning models to perceive the world. This code depends on the stretch_deep_perception_models repository, which should be installed under ~/stretch_user/ on your Stretch RE1 robot. Link to the stretch_deep_perception_models repository: https://github.com/hello-robot/stretch_deep_perception_models Getting Started Demos There are four demonstrations for you to try. Face Estimation Demo First, try running the face detection demonstration via the following command: roslaunch stretch_deep_perception stretch_detect_faces.launch RViz should show you the robot, the point cloud from the camera, and information about detected faces. If it detects a face, it should show a 3D planar model of the face and 3D facial landmarks. These deep learning models come from OpenCV and the Open Model Zoo (https://github.com/opencv/open_model_zoo). You can use the keyboard_teleop commands within the terminal that you ran roslaunch in order to move the robot's head around to see your face. i (tilt up) j (pan left) l (pan right) , (tilt down) Pan left and pan right are in terms of the robot's left and the robot's right. Now shut down everything that was launched by pressing q and Ctrl-C in the terminal. Object Detection Demo Second, try running the object detection demo, which uses the tiny YOLO v3 object detection network (https://pjreddie.com/darknet/yolo/). RViz will display planar detection regions. Detection class labels will be printed to the terminal. roslaunch stretch_deep_perception stretch_detect_objects.launch Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal. Body Landmark Detection Demo Third, try running the body landmark point detection demo. The deep learning model comes from the Open Model Zoo (https://github.com/opencv/open_model_zoo). RViz will display colored 3D points on body landmarks. The network also provides information to connect these landmarks, but this demo code does not currently use it. roslaunch stretch_deep_perception stretch_detect_body_landmarks.launch Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal. Nearest Mouth Detection Demo Finally, try running the nearest mouth detection demo. RViz will display a 3D frame of reference estimated for the nearest mouth detected by the robot. Sometimes the point cloud will make it difficult to see. Disabling the point cloud view in RViz will make it more visible. We have used this frame of reference to deliver food near a person's mouth. This has the potential to be useful for assistive feeding. However, use of this detector in this way could be risky. Please be very careful and aware that you are using it at your own risk. A less risky use of this detection is for object delivery. stretch_demos has a demonstration that delivers an object based on this frame of reference by holding out the object some distance from the mouth location and below the mouth location with respect to the world frame. This works well and is inspired by similar methods used with the robot EL-E at Georgia Tech [1]. roslaunch stretch_deep_perception stretch_detect_nearest_mouth.launch References [1] Hand It Over or Set It Down: A User Study of Object Delivery with an Assistive Mobile Manipulator, Young Sang Choi, Tiffany L. Chen, Advait Jain, Cressel Anderson, Jonathan D. Glass, and Charles C. Kemp, IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2009. http://pwp.gatech.edu/hrl/wp-content/uploads/sites/231/2016/05/roman2009_delivery.pdf License For license information, please see the LICENSE files.","title":"stretch_deep_perception"},{"location":"stretch-ros2/stretch_deep_perception/#overview","text":"stretch_deep_perception provides demonstration code that uses open deep learning models to perceive the world. This code depends on the stretch_deep_perception_models repository, which should be installed under ~/stretch_user/ on your Stretch RE1 robot. Link to the stretch_deep_perception_models repository: https://github.com/hello-robot/stretch_deep_perception_models","title":"Overview"},{"location":"stretch-ros2/stretch_deep_perception/#getting-started-demos","text":"There are four demonstrations for you to try.","title":"Getting Started Demos"},{"location":"stretch-ros2/stretch_deep_perception/#face-estimation-demo","text":"First, try running the face detection demonstration via the following command: roslaunch stretch_deep_perception stretch_detect_faces.launch RViz should show you the robot, the point cloud from the camera, and information about detected faces. If it detects a face, it should show a 3D planar model of the face and 3D facial landmarks. These deep learning models come from OpenCV and the Open Model Zoo (https://github.com/opencv/open_model_zoo). You can use the keyboard_teleop commands within the terminal that you ran roslaunch in order to move the robot's head around to see your face. i (tilt up) j (pan left) l (pan right) , (tilt down) Pan left and pan right are in terms of the robot's left and the robot's right. Now shut down everything that was launched by pressing q and Ctrl-C in the terminal.","title":"Face Estimation Demo"},{"location":"stretch-ros2/stretch_deep_perception/#object-detection-demo","text":"Second, try running the object detection demo, which uses the tiny YOLO v3 object detection network (https://pjreddie.com/darknet/yolo/). RViz will display planar detection regions. Detection class labels will be printed to the terminal. roslaunch stretch_deep_perception stretch_detect_objects.launch Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal.","title":"Object Detection Demo"},{"location":"stretch-ros2/stretch_deep_perception/#body-landmark-detection-demo","text":"Third, try running the body landmark point detection demo. The deep learning model comes from the Open Model Zoo (https://github.com/opencv/open_model_zoo). RViz will display colored 3D points on body landmarks. The network also provides information to connect these landmarks, but this demo code does not currently use it. roslaunch stretch_deep_perception stretch_detect_body_landmarks.launch Once you're ready for the next demo, shut down everything that was launched by pressing q and Ctrl-C in the terminal.","title":"Body Landmark Detection Demo"},{"location":"stretch-ros2/stretch_deep_perception/#nearest-mouth-detection-demo","text":"Finally, try running the nearest mouth detection demo. RViz will display a 3D frame of reference estimated for the nearest mouth detected by the robot. Sometimes the point cloud will make it difficult to see. Disabling the point cloud view in RViz will make it more visible. We have used this frame of reference to deliver food near a person's mouth. This has the potential to be useful for assistive feeding. However, use of this detector in this way could be risky. Please be very careful and aware that you are using it at your own risk. A less risky use of this detection is for object delivery. stretch_demos has a demonstration that delivers an object based on this frame of reference by holding out the object some distance from the mouth location and below the mouth location with respect to the world frame. This works well and is inspired by similar methods used with the robot EL-E at Georgia Tech [1]. roslaunch stretch_deep_perception stretch_detect_nearest_mouth.launch","title":"Nearest Mouth Detection Demo"},{"location":"stretch-ros2/stretch_deep_perception/#references","text":"[1] Hand It Over or Set It Down: A User Study of Object Delivery with an Assistive Mobile Manipulator, Young Sang Choi, Tiffany L. Chen, Advait Jain, Cressel Anderson, Jonathan D. Glass, and Charles C. Kemp, IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2009. http://pwp.gatech.edu/hrl/wp-content/uploads/sites/231/2016/05/roman2009_delivery.pdf","title":"References"},{"location":"stretch-ros2/stretch_deep_perception/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/stretch_deep_perception/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_demos/","text":"Overview stretch_demos provides simple demonstrations for the Stretch RE1 mobile manipulator from Hello Robot Inc. For an overview of the demos, we recommend you look at the following forum post: https://forum.hello-robot.com/t/autonomy-video-details Getting Started Demos Please be aware that these demonstrations typically do not perform careful collision avoidance. Instead, they expect to operate in freespace and detect contact through motor current if an obstacle gets in the way. Please be careful when trying out these demonstrations. Handover Object Demo First, place the robot near you so that it can freely move back and forth and reach near your body. Then, launch the handover object demo using the following command: roslaunch stretch_demos handover_object.launch For this demonstration, the robot will pan its head back and forth looking for a face. It will remember the 3D location of the mouth of the nearest face that it has detected. If you press \"y\" or \"Y\" on the keyboard in the terminal, the robot will move the grasp region of its gripper toward a handover location below and away from the mouth. The robot will restrict itself to Cartesian motion to do this. Specifically, it will move its mobile base backward and forward, its lift up and down, and its arm in and out. If you press \"y\" or \"Y\" again, it will retract its arm and then move to the most recent mouth location it has detected. At any time, you can also use the keyboard teleoperation commands in the terminal window. With this, you can adjust the gripper, including pointing it straight out and making it grasp an object to be handed over. Grasp Object Demo For this demonstration, the robot will look for the nearest elevated surface, look for an object on it, and then attempt to grasp the largest object using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. Once the robot is in position, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. Now that the robot is ready, launch the demo with the following command: roslaunch stretch_demos grasp_object.launch Then, press the key with \u2018 and \u201c on it while in the terminal to initiate a grasp attempt. While attempting the grasp the demo will save several images under the ./stretch_user/debug/ directory within various grasping related directories. You can view these images to see some of what the robot did to make its decisions. Clean Surface Demo For this demonstration, the robot will look for the nearest elevated surface, look for clear space on it, and then attempt to wipe the clear space using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. You should also place a soft cloth in the robot's gripper. Once the robot is in position with a cloth in its gripper, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. Now that the robot is ready, launch the demo with the following command: roslaunch stretch_demos clean_surface.launch Then, press the key with the / and ? on it while in the terminal to initiate a surface cleaning attempt. License For license information, please see the LICENSE files.","title":"stretch_demos"},{"location":"stretch-ros2/stretch_demos/#overview","text":"stretch_demos provides simple demonstrations for the Stretch RE1 mobile manipulator from Hello Robot Inc. For an overview of the demos, we recommend you look at the following forum post: https://forum.hello-robot.com/t/autonomy-video-details","title":"Overview"},{"location":"stretch-ros2/stretch_demos/#getting-started-demos","text":"Please be aware that these demonstrations typically do not perform careful collision avoidance. Instead, they expect to operate in freespace and detect contact through motor current if an obstacle gets in the way. Please be careful when trying out these demonstrations.","title":"Getting Started Demos"},{"location":"stretch-ros2/stretch_demos/#handover-object-demo","text":"First, place the robot near you so that it can freely move back and forth and reach near your body. Then, launch the handover object demo using the following command: roslaunch stretch_demos handover_object.launch For this demonstration, the robot will pan its head back and forth looking for a face. It will remember the 3D location of the mouth of the nearest face that it has detected. If you press \"y\" or \"Y\" on the keyboard in the terminal, the robot will move the grasp region of its gripper toward a handover location below and away from the mouth. The robot will restrict itself to Cartesian motion to do this. Specifically, it will move its mobile base backward and forward, its lift up and down, and its arm in and out. If you press \"y\" or \"Y\" again, it will retract its arm and then move to the most recent mouth location it has detected. At any time, you can also use the keyboard teleoperation commands in the terminal window. With this, you can adjust the gripper, including pointing it straight out and making it grasp an object to be handed over.","title":"Handover Object Demo"},{"location":"stretch-ros2/stretch_demos/#grasp-object-demo","text":"For this demonstration, the robot will look for the nearest elevated surface, look for an object on it, and then attempt to grasp the largest object using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. Once the robot is in position, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. Now that the robot is ready, launch the demo with the following command: roslaunch stretch_demos grasp_object.launch Then, press the key with \u2018 and \u201c on it while in the terminal to initiate a grasp attempt. While attempting the grasp the demo will save several images under the ./stretch_user/debug/ directory within various grasping related directories. You can view these images to see some of what the robot did to make its decisions.","title":"Grasp Object Demo"},{"location":"stretch-ros2/stretch_demos/#clean-surface-demo","text":"For this demonstration, the robot will look for the nearest elevated surface, look for clear space on it, and then attempt to wipe the clear space using Cartesian motions. Prior to running the demo, you should move the robot so that its workspace will be able to move its gripper over the surface while performing Cartesian motions. You should also place a soft cloth in the robot's gripper. Once the robot is in position with a cloth in its gripper, retract and lower the arm so that the robot can clearly see the surface when looking out in the direction of its arm. Now that the robot is ready, launch the demo with the following command: roslaunch stretch_demos clean_surface.launch Then, press the key with the / and ? on it while in the terminal to initiate a surface cleaning attempt.","title":"Clean Surface Demo"},{"location":"stretch-ros2/stretch_demos/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/stretch_demos/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_description/","text":"Overview stretch_description provides materials for a URDF kinematic model of the Stretch RE1 mobile manipulator from Hello Robot Inc. Quick View ros2 launch stretch_description display.launch.py Details The meshes directory contains STL mesh files representing the exterior geometry of various parts of the robot. The urdf directory contains xacro files representing various parts of the robot that are used to generate the robot's URDF. TODO: Everything below this point must be updated once stretch_calibration is updated stretch_ros expects a URDF with the name stretch.urdf to reside within the urdf directory. The file stretch.urdf serves as the URDF for the robot and must be generated. Typically, it is a calibrated urdf file for the particular Stretch RE1 robot being used. To generate this file, please read the documentation within stretch_ros/stretch_calibration. The xacro_to_urdf.sh will usually only be indirectly run as part of various scripts and launch files within stretch_ros/stretch_calibration. Sometimes a stretch_uncalibrated.urdf file will reside with the urdf directory. This file is typically generated directly from the xacro files without any alterations. Exporting a URDF Sometimes a URDF is useful outside of ROS, such as for simulations and analysis. Running the export_urdf.sh script in the urdf directory will export a full URDF model of the robot based on stretch.urdf. The exported URDF will be found within an exported_urdf directory. It is also copied to a directory for your specific robot found under ~/stretch_user. The exported URDF includes meshes and controller calibration YAML files. The exported URDF can be visualized using stretch_urdf_show.py, which is part of the stretch_body Python code. Changing the Tool If you wish to remove the default gripper and add a different tool, you will typically edit /stretch_description/urdf/stretch_description.xacro. Specifically, you will replace the following line in order to include the xacro for the new tool and then follow directions within stretch_ros/stretch_calibration to generate a new calibrated urdf file (stretch.urdf) that includes the new tool. <xacro:include filename=\"stretch_gripper.xacro\" /> As an example we provide the xacro stretch_dry_erase_marker.xacro and its dependent mesh files with stretch_ros. Some of the tools found in the Stretch Body Tool Share include URDF data. To integrate these tools into the URDF for your Stretch >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_tool_share >>$ cd stretch_tool_share/<tool name> >>$ cp stretch_description/urdf/* ~/catkin_ws/src/stretch_ros/stretch_description/urdf/ >>$ cp stretch_description/meshes/* ~/catkin_ws/src/stretch_ros/stretch_description/meshes/ Next add the xacro for the particular tool to /stretch_description/urdf/stretch_description.xacro . Then you can generate and preview the uncalibrated URDF: >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp stretch.urdf stretch.urdf.bak >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh Now visualize the new tool >>$ roslaunch stretch_calibration simple_test_head_calibration.launch License and Patents Patents are pending that cover aspects of the Stretch RE1 mobile manipulator. For license information, please see the LICENSE files.","title":"stretch_description"},{"location":"stretch-ros2/stretch_description/#overview","text":"stretch_description provides materials for a URDF kinematic model of the Stretch RE1 mobile manipulator from Hello Robot Inc.","title":"Overview"},{"location":"stretch-ros2/stretch_description/#quick-view","text":"ros2 launch stretch_description display.launch.py","title":"Quick View"},{"location":"stretch-ros2/stretch_description/#details","text":"The meshes directory contains STL mesh files representing the exterior geometry of various parts of the robot. The urdf directory contains xacro files representing various parts of the robot that are used to generate the robot's URDF. TODO: Everything below this point must be updated once stretch_calibration is updated stretch_ros expects a URDF with the name stretch.urdf to reside within the urdf directory. The file stretch.urdf serves as the URDF for the robot and must be generated. Typically, it is a calibrated urdf file for the particular Stretch RE1 robot being used. To generate this file, please read the documentation within stretch_ros/stretch_calibration. The xacro_to_urdf.sh will usually only be indirectly run as part of various scripts and launch files within stretch_ros/stretch_calibration. Sometimes a stretch_uncalibrated.urdf file will reside with the urdf directory. This file is typically generated directly from the xacro files without any alterations.","title":"Details"},{"location":"stretch-ros2/stretch_description/#exporting-a-urdf","text":"Sometimes a URDF is useful outside of ROS, such as for simulations and analysis. Running the export_urdf.sh script in the urdf directory will export a full URDF model of the robot based on stretch.urdf. The exported URDF will be found within an exported_urdf directory. It is also copied to a directory for your specific robot found under ~/stretch_user. The exported URDF includes meshes and controller calibration YAML files. The exported URDF can be visualized using stretch_urdf_show.py, which is part of the stretch_body Python code.","title":"Exporting a URDF"},{"location":"stretch-ros2/stretch_description/#changing-the-tool","text":"If you wish to remove the default gripper and add a different tool, you will typically edit /stretch_description/urdf/stretch_description.xacro. Specifically, you will replace the following line in order to include the xacro for the new tool and then follow directions within stretch_ros/stretch_calibration to generate a new calibrated urdf file (stretch.urdf) that includes the new tool. <xacro:include filename=\"stretch_gripper.xacro\" /> As an example we provide the xacro stretch_dry_erase_marker.xacro and its dependent mesh files with stretch_ros. Some of the tools found in the Stretch Body Tool Share include URDF data. To integrate these tools into the URDF for your Stretch >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_tool_share >>$ cd stretch_tool_share/<tool name> >>$ cp stretch_description/urdf/* ~/catkin_ws/src/stretch_ros/stretch_description/urdf/ >>$ cp stretch_description/meshes/* ~/catkin_ws/src/stretch_ros/stretch_description/meshes/ Next add the xacro for the particular tool to /stretch_description/urdf/stretch_description.xacro . Then you can generate and preview the uncalibrated URDF: >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp stretch.urdf stretch.urdf.bak >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh Now visualize the new tool >>$ roslaunch stretch_calibration simple_test_head_calibration.launch","title":"Changing the Tool"},{"location":"stretch-ros2/stretch_description/#license-and-patents","text":"Patents are pending that cover aspects of the Stretch RE1 mobile manipulator. For license information, please see the LICENSE files.","title":"License and Patents"},{"location":"stretch-ros2/stretch_description/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"). The Contents consist of software and data used with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. The Clear BSD License Copyright (c) 2021 Hello Robot Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"LICENSE"},{"location":"stretch-ros2/stretch_description/urdf/export_urdf_license_template/","text":"The following license applies to the entire contents of this directory (the \"Contents\") except where otherwise noted. The Contents consist of software and data used with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike-4.0-International (CC BY-NC-SA 4.0) license (the \"License\"); you may not use the Contents except in compliance with the License. You may obtain a copy of the License at https://creativecommons.org/licenses/by-nc-sa/4.0/ Unless required by applicable law or agreed to in writing, the Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Patents pending and trademark rights cover the Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\" The Contents may incorporate some parts of the \"RealSense Camera description package for Intel 3D D400 cameras\" released with an Apache 2.0 license and Copyright 2017 Intel Corporation. The details of the Apache 2.0 license can be found via the following link: https://www.apache.org/licenses/LICENSE-2.0 Specifically, the Contents may include the d435.dae mesh file and content generated by the _d435.urdf.xacro found within the GitHub repository available for download via the following link as of May 4, 2020: https://github.com/IntelRealSense/realsense-ros/tree/development/realsense2_description These specific materials are subject to the requirements of their original Apache 2.0 license. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"Export urdf license template"},{"location":"stretch-ros2/stretch_funmap/","text":"Overview stretch_funmap is an implementation of Fast Unified Navigation, Manipulation And Planning (FUNMAP). FUNMAP provides navigation, manipulation, and planning capabilities for the Stretch RE1 mobile manipulator. stretch_funmap includes examples of efficient ways to take advantage of the Stretch RE1's unique properties. Previous commercially-available mobile manipulators have consisted of a serial manipulator (i.e., links connected by rotary joints) placed on a mobile base [1]. Widely used software (e.g., the Robot Operating System (ROS)) [1] typically expects a velocity-controlled mobile base that can be held still while the arm manipulates [3, 4]. In contrast, the Stretch RE1's mobile base is integral to manipulation and typically moves throughout a task. It can also perform high-fidelity position control with its mobile base. FUNMAP uses approximate geometric models and computer-vision algorithms to efficiently find plans that take advantage of its prismatic joints (e.g., telescoping arm) and Cartesian structure. In contrast to typical approaches that treat navigation (e.g., ROS Navigation Stack ) and manipulation (e.g., MoveIt! [5, 6]) separately, FUNMAP does both. Getting Started Demo First, make sure that your Stretch RE1 has clearance to rotate in place and will rotate without straining any cables connected to the trunk. Ideally, you should have the robot untethered. Next, run the following launch file: roslaunch stretch_funmap mapping.launch Now, you will take a head scan, which will involve the head panning around, the base rotating, and the head panning around again to overcome the blindspot due to the mast. While in the terminal in which you ran roslaunch, press the space bar to initiate the head scan. At this point, you should see a 3D map resulting from the head scan in RViz. You can rotate it around and look at it. It has been created by merging many 3D scans. If you have the robot untethered, you can now specify a navigation goal for the robot. If the robot finds a navigation plan to the goal, it will attempt to navigate to it. While navigating, it will look down with its 3D camera in an attempt to stop if it detects an obstacle. In RViz, press the \"2D Nav Goal\" button on the top bar with a magenta arrow icon. Specify a nearby navigation goal pose on the floor of the map by clicking and drawing a magenta arrow. For this to work, the navigation goal must be in a place that the robot can reach and that the robot has scanned well. For example, the robot will only navigate across floor regions that it has in its map. If the robot finds a path, you should see green lines connecting white spheres in RViz that display its plan as it attempts to navigate to the goal. Once the robot has reached the goal, you can take another head scan. While in the terminal, press the space bar to initiate another head scan. The robot should take the head scan and merge it with the previous scans. If all goes well, the merged 3D map will be visible in RViz. You can also have the robot automatically drive to a place that it thinks is a good place for it to take a head scan in order to map the environment. While in the terminal, press the key with \\ and | on it. This should work even if caps lock is enabled or the shift key is pressed. If the robot reached it's goal, then you can now take another head scan. While in the terminal, press the space bar to initiate another head scan. By repeating this process, you can create a 3D map of the environment. FUNMAP uses images to represent the environment with each pixel value representing the highest observed 3D point at a planar location. By default, all of the merged maps are saved to the following directory: ./stretch_user/debug/merged_maps/ You can see the image representations by looking at files with the following naming pattern: ./stretch_user/debug/merged_maps/merged_map_DATETIME_mhi_visualization.png You can also click on a reaching goal for the Stretch RE1 by clicking on \"Publish Point\" in Rviz and then selecting a 3D point on the map. FUNMAP will attempt to generate a navigation and manipulation plan to reach close to the selected 3D location with Stretch's gripper. In RViz, select a reach goal by clicking on \"Publish Point\" on the top bar with a red map location icon). Then, click on a 3D location on the map to specify a reaching target for the robot's gripper. That concludes the demonstration. Have fun with FUNMAP! More FUNMAP FUNMAP represents human environments with Max Height Images (MHIs). An MHI is an image for which each pixel typically represents the height of the robot\u2019s environment. Given a volume of interest (VOI) with its z-axis aligned with gravity, an MHI, I, maps locations to heights. Specifically, I(x,y)=z, where (x,y) represents a discretized planar location within the VOI and z represents the discretized height of the maximum occupied voxel within the VOI at that planar location (see Figure 1 above). The placement of the Stretch RE1 3D camera at a human head height enables it to capture MHIs that represent the horizontal surfaces with which humans frequently interact, such as table tops, countertops, and chairs. The orientation of its 3D camera enables the robot to quickly scan an environment to create a room-scale MHI by panning its head at a constant tilt angle. In contrast to other environment representations, such as point clouds and 3D mesh models, MHIs support fast, efficient operations through optimized image processing and are compatible with deep learning methods for images. Related representations have primarily been used for navigation, including for legged robots on rough terrain, but have not emphasized elevated surfaces in human environments nor incorporated manipulation [7, 8]. Object grasping systems for bin picking have used related representations, but have only considered small areas with objects and not incorporated navigation [9, 10]. During development, we have used FUNMAP to create MHIs for which each pixel represents a 6mm x 6mm region of the environment\u2019s floor and has a height resolution of less than 6mm per unit. This allows a single 2000 x 2000 pixel, 8-bit image to represent a 12m x 12m environment from 10cm below the estimated floor plane to 1.2m above the floor plane, which captures the great majority of open horizontal surfaces in human environments with which people interact. MHIs also have the potential to represent enclosed surfaces (e.g., surfaces in shelves and cabinets) and vertical surfaces (e.g., doors) by defining new VOIs with different orientations and heights, which is a capability that merits future exploration. For navigation and planning, FUNMAP uses fast distance transforms and morphological operators to efficiently create cost functions for robust optimization-based planning. For example, a high value of a distance transform at a floor location implies that the mobile base will be farther from obstacles. Similarly, a high value of a distance transform for a location on an elevated surface implies that the robot\u2019s end effector will be farther from obstacles. Figure 3: Left: The cyan lines represent achievable driving goals on the floor from which the robot can reach the target (red circle) on the table (dark blue). Right: When the target (red circle) is farther back near obstacles on the table (dark blue), the robot can reach the target from fewer locations (cyan lines). Figure 4: Left: Example of a piecewise linear navigation plan (green line segments and white spheres) being executed. Right: Example of a successfully executed plan to reach a target location on a table (light blue) while avoiding a wall with shelves (dark blue and purple). The Stretch RE1\u2019s slender links and Cartesian kinematics support rapid optimization of plans due to the simplified geometry of the robot\u2019s motions. FUNMAP uses piecewise linear paths on the cost image for fast navigation and manipulation planning. FUNMAP enables the Stretch RE1 to reach a 3D target position. FUNMAP models the linear motion of the telescoping arm as it extends to reach a target as a line in a manipulation cost image (see Figure 2). FUNMAP uses a navigation cost image and Djikstra\u2019s algorithm with a priority queue to efficiently estimate the cost of navigating to all floor locations (see Figure 3 left). FUNMAP then combines these results to find a minimum cost plan that increases the distance to obstacles and the manipulable workspace of the robot (see Figure 3 right). References [1] Bostelman, Roger, Tsai Hong, and Jeremy Marvel. \"Survey of research for performance measurement of mobile manipulators.\" Journal of Research of the National Institute of Standards and Technology 121, no. 3 (2016): 342-366. [2] Quigley, Morgan, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote, Jeremy Leibs, Rob Wheeler, and Andrew Y. Ng. \"ROS: an open-source Robot Operating System.\" In ICRA workshop on open source software, vol. 3, no. 3.2, p. 5. 2009. [3] Sachin Chitta, Eitan Marder-Eppstein, Wim Meeussen, Vijay Pradeep, Adolfo Rodr\u00edguez Tsouroukdissian, et al.. ros_control: A generic and simple control framework for ROS. The Journal of Open Source Software, 2017, 2 (20), pp.456 - 456. [4] Guimar\u00e3es, Rodrigo Longhi, Andr\u00e9 Schneider de Oliveira, Jo\u00e3o Alberto Fabro, Thiago Becker, and Vin\u00edcius Amilgar Brenner. \"ROS navigation: Concepts and tutorial.\" In Robot Operating System (ROS), pp. 121-160. Springer, Cham, 2016. [5] Chitta, Sachin, Ioan Sucan, and Steve Cousins. \"Moveit! [ros topics].\" IEEE Robotics & Automation Magazine 19, no. 1 (2012): 18-19. [6] Chitta, Sachin. \"MoveIt!: an introduction.\" In Robot Operating System (ROS), pp. 3-27. Springer, Cham, 2016. [7] Fankhauser, P\u00e9ter, and Marco Hutter. \"A universal grid map library: Implementation and use case for rough terrain navigation.\" In Robot Operating System (ROS), pp. 99-120. Springer, Cham, 2016. [8] Lu, David V., Dave Hershberger, and William D. Smart. \"Layered costmaps for context-sensitive navigation.\" In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 709-715. IEEE, 2014. [9] Zeng, Andy, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. \"Tossingbot: Learning to throw arbitrary objects with residual physics.\" arXiv preprint arXiv:1903.11239 (2019). [10] Zeng, Andy, Shuran Song, Kuan-Ting Yu, Elliott Donlon, Francois R. Hogan, Maria Bauza, Daolin Ma et al. \"Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching.\" In 2018 IEEE international conference on robotics and automation (ICRA), pp. 1-8. IEEE, 2018. License For license information, please see the LICENSE files.","title":"stretch_funmap"},{"location":"stretch-ros2/stretch_funmap/#overview","text":"stretch_funmap is an implementation of Fast Unified Navigation, Manipulation And Planning (FUNMAP). FUNMAP provides navigation, manipulation, and planning capabilities for the Stretch RE1 mobile manipulator. stretch_funmap includes examples of efficient ways to take advantage of the Stretch RE1's unique properties. Previous commercially-available mobile manipulators have consisted of a serial manipulator (i.e., links connected by rotary joints) placed on a mobile base [1]. Widely used software (e.g., the Robot Operating System (ROS)) [1] typically expects a velocity-controlled mobile base that can be held still while the arm manipulates [3, 4]. In contrast, the Stretch RE1's mobile base is integral to manipulation and typically moves throughout a task. It can also perform high-fidelity position control with its mobile base. FUNMAP uses approximate geometric models and computer-vision algorithms to efficiently find plans that take advantage of its prismatic joints (e.g., telescoping arm) and Cartesian structure. In contrast to typical approaches that treat navigation (e.g., ROS Navigation Stack ) and manipulation (e.g., MoveIt! [5, 6]) separately, FUNMAP does both.","title":"Overview"},{"location":"stretch-ros2/stretch_funmap/#getting-started-demo","text":"First, make sure that your Stretch RE1 has clearance to rotate in place and will rotate without straining any cables connected to the trunk. Ideally, you should have the robot untethered. Next, run the following launch file: roslaunch stretch_funmap mapping.launch Now, you will take a head scan, which will involve the head panning around, the base rotating, and the head panning around again to overcome the blindspot due to the mast. While in the terminal in which you ran roslaunch, press the space bar to initiate the head scan. At this point, you should see a 3D map resulting from the head scan in RViz. You can rotate it around and look at it. It has been created by merging many 3D scans. If you have the robot untethered, you can now specify a navigation goal for the robot. If the robot finds a navigation plan to the goal, it will attempt to navigate to it. While navigating, it will look down with its 3D camera in an attempt to stop if it detects an obstacle. In RViz, press the \"2D Nav Goal\" button on the top bar with a magenta arrow icon. Specify a nearby navigation goal pose on the floor of the map by clicking and drawing a magenta arrow. For this to work, the navigation goal must be in a place that the robot can reach and that the robot has scanned well. For example, the robot will only navigate across floor regions that it has in its map. If the robot finds a path, you should see green lines connecting white spheres in RViz that display its plan as it attempts to navigate to the goal. Once the robot has reached the goal, you can take another head scan. While in the terminal, press the space bar to initiate another head scan. The robot should take the head scan and merge it with the previous scans. If all goes well, the merged 3D map will be visible in RViz. You can also have the robot automatically drive to a place that it thinks is a good place for it to take a head scan in order to map the environment. While in the terminal, press the key with \\ and | on it. This should work even if caps lock is enabled or the shift key is pressed. If the robot reached it's goal, then you can now take another head scan. While in the terminal, press the space bar to initiate another head scan. By repeating this process, you can create a 3D map of the environment. FUNMAP uses images to represent the environment with each pixel value representing the highest observed 3D point at a planar location. By default, all of the merged maps are saved to the following directory: ./stretch_user/debug/merged_maps/ You can see the image representations by looking at files with the following naming pattern: ./stretch_user/debug/merged_maps/merged_map_DATETIME_mhi_visualization.png You can also click on a reaching goal for the Stretch RE1 by clicking on \"Publish Point\" in Rviz and then selecting a 3D point on the map. FUNMAP will attempt to generate a navigation and manipulation plan to reach close to the selected 3D location with Stretch's gripper. In RViz, select a reach goal by clicking on \"Publish Point\" on the top bar with a red map location icon). Then, click on a 3D location on the map to specify a reaching target for the robot's gripper. That concludes the demonstration. Have fun with FUNMAP!","title":"Getting Started Demo"},{"location":"stretch-ros2/stretch_funmap/#more-funmap","text":"FUNMAP represents human environments with Max Height Images (MHIs). An MHI is an image for which each pixel typically represents the height of the robot\u2019s environment. Given a volume of interest (VOI) with its z-axis aligned with gravity, an MHI, I, maps locations to heights. Specifically, I(x,y)=z, where (x,y) represents a discretized planar location within the VOI and z represents the discretized height of the maximum occupied voxel within the VOI at that planar location (see Figure 1 above). The placement of the Stretch RE1 3D camera at a human head height enables it to capture MHIs that represent the horizontal surfaces with which humans frequently interact, such as table tops, countertops, and chairs. The orientation of its 3D camera enables the robot to quickly scan an environment to create a room-scale MHI by panning its head at a constant tilt angle. In contrast to other environment representations, such as point clouds and 3D mesh models, MHIs support fast, efficient operations through optimized image processing and are compatible with deep learning methods for images. Related representations have primarily been used for navigation, including for legged robots on rough terrain, but have not emphasized elevated surfaces in human environments nor incorporated manipulation [7, 8]. Object grasping systems for bin picking have used related representations, but have only considered small areas with objects and not incorporated navigation [9, 10]. During development, we have used FUNMAP to create MHIs for which each pixel represents a 6mm x 6mm region of the environment\u2019s floor and has a height resolution of less than 6mm per unit. This allows a single 2000 x 2000 pixel, 8-bit image to represent a 12m x 12m environment from 10cm below the estimated floor plane to 1.2m above the floor plane, which captures the great majority of open horizontal surfaces in human environments with which people interact. MHIs also have the potential to represent enclosed surfaces (e.g., surfaces in shelves and cabinets) and vertical surfaces (e.g., doors) by defining new VOIs with different orientations and heights, which is a capability that merits future exploration. For navigation and planning, FUNMAP uses fast distance transforms and morphological operators to efficiently create cost functions for robust optimization-based planning. For example, a high value of a distance transform at a floor location implies that the mobile base will be farther from obstacles. Similarly, a high value of a distance transform for a location on an elevated surface implies that the robot\u2019s end effector will be farther from obstacles. Figure 3: Left: The cyan lines represent achievable driving goals on the floor from which the robot can reach the target (red circle) on the table (dark blue). Right: When the target (red circle) is farther back near obstacles on the table (dark blue), the robot can reach the target from fewer locations (cyan lines). Figure 4: Left: Example of a piecewise linear navigation plan (green line segments and white spheres) being executed. Right: Example of a successfully executed plan to reach a target location on a table (light blue) while avoiding a wall with shelves (dark blue and purple). The Stretch RE1\u2019s slender links and Cartesian kinematics support rapid optimization of plans due to the simplified geometry of the robot\u2019s motions. FUNMAP uses piecewise linear paths on the cost image for fast navigation and manipulation planning. FUNMAP enables the Stretch RE1 to reach a 3D target position. FUNMAP models the linear motion of the telescoping arm as it extends to reach a target as a line in a manipulation cost image (see Figure 2). FUNMAP uses a navigation cost image and Djikstra\u2019s algorithm with a priority queue to efficiently estimate the cost of navigating to all floor locations (see Figure 3 left). FUNMAP then combines these results to find a minimum cost plan that increases the distance to obstacles and the manipulable workspace of the robot (see Figure 3 right).","title":"More FUNMAP"},{"location":"stretch-ros2/stretch_funmap/#references","text":"[1] Bostelman, Roger, Tsai Hong, and Jeremy Marvel. \"Survey of research for performance measurement of mobile manipulators.\" Journal of Research of the National Institute of Standards and Technology 121, no. 3 (2016): 342-366. [2] Quigley, Morgan, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote, Jeremy Leibs, Rob Wheeler, and Andrew Y. Ng. \"ROS: an open-source Robot Operating System.\" In ICRA workshop on open source software, vol. 3, no. 3.2, p. 5. 2009. [3] Sachin Chitta, Eitan Marder-Eppstein, Wim Meeussen, Vijay Pradeep, Adolfo Rodr\u00edguez Tsouroukdissian, et al.. ros_control: A generic and simple control framework for ROS. The Journal of Open Source Software, 2017, 2 (20), pp.456 - 456. [4] Guimar\u00e3es, Rodrigo Longhi, Andr\u00e9 Schneider de Oliveira, Jo\u00e3o Alberto Fabro, Thiago Becker, and Vin\u00edcius Amilgar Brenner. \"ROS navigation: Concepts and tutorial.\" In Robot Operating System (ROS), pp. 121-160. Springer, Cham, 2016. [5] Chitta, Sachin, Ioan Sucan, and Steve Cousins. \"Moveit! [ros topics].\" IEEE Robotics & Automation Magazine 19, no. 1 (2012): 18-19. [6] Chitta, Sachin. \"MoveIt!: an introduction.\" In Robot Operating System (ROS), pp. 3-27. Springer, Cham, 2016. [7] Fankhauser, P\u00e9ter, and Marco Hutter. \"A universal grid map library: Implementation and use case for rough terrain navigation.\" In Robot Operating System (ROS), pp. 99-120. Springer, Cham, 2016. [8] Lu, David V., Dave Hershberger, and William D. Smart. \"Layered costmaps for context-sensitive navigation.\" In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 709-715. IEEE, 2014. [9] Zeng, Andy, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. \"Tossingbot: Learning to throw arbitrary objects with residual physics.\" arXiv preprint arXiv:1903.11239 (2019). [10] Zeng, Andy, Shuran Song, Kuan-Ting Yu, Elliott Donlon, Francois R. Hogan, Maria Bauza, Daolin Ma et al. \"Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching.\" In 2018 IEEE international conference on robotics and automation (ICRA), pp. 1-8. IEEE, 2018.","title":"References"},{"location":"stretch-ros2/stretch_funmap/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/stretch_funmap/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. This software is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License v3.0 (GNU LGPLv3) as published by the Free Software Foundation. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License v3.0 (GNU LGPLv3) for more details, which can be found via the following link: https://www.gnu.org/licenses/lgpl-3.0.en.html For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_gazebo/","text":"Overview stretch_gazebo is an implementation of simulating a Stretch robot with Gazebo simulator. Details The urdf directory contains a xacro file that extends the capabilities of the original xacro files living in stretch_description package to include Gazebo functionality. The config directory contains rviz files and ros_control controller configuration files for various parts of the robot including: Base: diff_drive_controller/DiffDriveController Arm: position_controllers/JointTrajectoryController Gripper: position_controllers/JointTrajectoryController Head: position_controllers/JointTrajectoryController Joints: joint_state_controller/JointStateController The launch directory includes two files: gazebo.launch: Opens up an empty Gazebo world and spawns the robot loading all the controllers, including all the sensors except Cliff sensors and respeaker. teleop_keyboard.launch: Allows keyboard teleop in the terminal and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel , which the robot is taking velocity commands from. teleop_joy.launch: Spawns a joy and teleop_twist_joy instance and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel , which the robot is taking velocity commands from. Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless it is being pressed. For an Logitech F310 joystick this button is A. The script directory contains a single python file that publishes ground truth odometry of the robot from Gazebo. Setup These set up instructions will not be required on newly shipped robots. Follow these instructions if stretch_gazebo is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros and realsense_gazebo_plugin packages to your catkin workspace. Then install dependencies and build the packages, with the following set of commands: mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros -b dev/noetic git clone https://github.com/pal-robotics/realsense_gazebo_plugin cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make Running Demo # Terminal 1: roslaunch stretch_gazebo gazebo.launch rviz: = true # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller This will launch an Rviz instance that visualizes the sensors and an empty world in Gazebo with Stretch and load all the controllers. Although, the base will be able to move with the joystick commands, the joystick won't give joint commands to arm, head or gripper. To move these joints see the next section about Running Gazebo with MoveIt! and Stretch . Running Gazebo with MoveIt! and Stretch # Terminal 1: roslaunch stretch_gazebo gazebo.launch # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller # Terminal 3 roslaunch stretch_moveit_config demo_gazebo.launch This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm , stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to this moveit tutorial . A few notes to be kept in mind: Planning group can be changed via Planning Group drop down in Planning tab of Motion Planning Rviz plugin. Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning Rviz plugin. stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning Rviz plugin. Differences in Gazebo vs Stretch The simulated Stretch RE1 differs from the robot in the following ways. Gazebo Sensors vs Stretch Sensors Sensor Gazebo Stretch Notes LIDAR :heavy_check_mark: :heavy_check_mark: Base IMU :heavy_check_mark: :heavy_check_mark: Wrist Accelerometer :heavy_check_mark: :heavy_check_mark: Modeled as an IMU Realsense D435i :heavy_check_mark: :heavy_check_mark: Respeaker (Mic Array) :x: :heavy_check_mark: Cliff Sensors :x: :heavy_check_mark: Notes: Although there is no microphone in Gazebo, Respeaker can be represented with a ROS node that accesses compputer's microphone. Cliff sensors are not modeled but they can also be represented as 1D LIDAR sensors. See LIDAR definition in stretch_gazebo.urdf.xacro file. MoveIt Controllers vs stretch_core Actuators are defined as ros_control transmission objects in Gazebo using PositionJointInterfaces . MoveIt is configured to use three different action servers to control the body parts of stretch in Gazebo through the srdf file in stretch_moveit_config package. See the section above about MoveIt for details. Please note that this behavior is different than stretch_core as it works with a single Python interface to control all the joints. Uncalibrated XACRO vs Calibrated URDF We provide stretch_calibration to generate a calibrated URDF that is unique to each robot. The calibrated URDF is generated from the nominal description of Stretch RE1, the xacro files that live in stretch_description . The simulated Stretch RE1 is generated from the gazebo xacro description in the urdf directory and is not calibrated. License For license information, please see the LICENSE files.","title":"stretch_gazebo"},{"location":"stretch-ros2/stretch_gazebo/#overview","text":"stretch_gazebo is an implementation of simulating a Stretch robot with Gazebo simulator.","title":"Overview"},{"location":"stretch-ros2/stretch_gazebo/#details","text":"The urdf directory contains a xacro file that extends the capabilities of the original xacro files living in stretch_description package to include Gazebo functionality. The config directory contains rviz files and ros_control controller configuration files for various parts of the robot including: Base: diff_drive_controller/DiffDriveController Arm: position_controllers/JointTrajectoryController Gripper: position_controllers/JointTrajectoryController Head: position_controllers/JointTrajectoryController Joints: joint_state_controller/JointStateController The launch directory includes two files: gazebo.launch: Opens up an empty Gazebo world and spawns the robot loading all the controllers, including all the sensors except Cliff sensors and respeaker. teleop_keyboard.launch: Allows keyboard teleop in the terminal and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel , which the robot is taking velocity commands from. teleop_joy.launch: Spawns a joy and teleop_twist_joy instance and remaps cmd_vel topics to /stretch_diff_drive_controller/cmd_vel , which the robot is taking velocity commands from. Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless it is being pressed. For an Logitech F310 joystick this button is A. The script directory contains a single python file that publishes ground truth odometry of the robot from Gazebo.","title":"Details"},{"location":"stretch-ros2/stretch_gazebo/#setup","text":"These set up instructions will not be required on newly shipped robots. Follow these instructions if stretch_gazebo is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros and realsense_gazebo_plugin packages to your catkin workspace. Then install dependencies and build the packages, with the following set of commands: mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros -b dev/noetic git clone https://github.com/pal-robotics/realsense_gazebo_plugin cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make","title":"Setup"},{"location":"stretch-ros2/stretch_gazebo/#running-demo","text":"# Terminal 1: roslaunch stretch_gazebo gazebo.launch rviz: = true # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller This will launch an Rviz instance that visualizes the sensors and an empty world in Gazebo with Stretch and load all the controllers. Although, the base will be able to move with the joystick commands, the joystick won't give joint commands to arm, head or gripper. To move these joints see the next section about Running Gazebo with MoveIt! and Stretch .","title":"Running Demo"},{"location":"stretch-ros2/stretch_gazebo/#running-gazebo-with-moveit-and-stretch","text":"# Terminal 1: roslaunch stretch_gazebo gazebo.launch # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller # Terminal 3 roslaunch stretch_moveit_config demo_gazebo.launch This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm , stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to this moveit tutorial . A few notes to be kept in mind: Planning group can be changed via Planning Group drop down in Planning tab of Motion Planning Rviz plugin. Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning Rviz plugin. stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning Rviz plugin.","title":"Running Gazebo with MoveIt! and Stretch"},{"location":"stretch-ros2/stretch_gazebo/#differences-in-gazebo-vs-stretch","text":"The simulated Stretch RE1 differs from the robot in the following ways.","title":"Differences in Gazebo vs Stretch"},{"location":"stretch-ros2/stretch_gazebo/#gazebo-sensors-vs-stretch-sensors","text":"Sensor Gazebo Stretch Notes LIDAR :heavy_check_mark: :heavy_check_mark: Base IMU :heavy_check_mark: :heavy_check_mark: Wrist Accelerometer :heavy_check_mark: :heavy_check_mark: Modeled as an IMU Realsense D435i :heavy_check_mark: :heavy_check_mark: Respeaker (Mic Array) :x: :heavy_check_mark: Cliff Sensors :x: :heavy_check_mark: Notes: Although there is no microphone in Gazebo, Respeaker can be represented with a ROS node that accesses compputer's microphone. Cliff sensors are not modeled but they can also be represented as 1D LIDAR sensors. See LIDAR definition in stretch_gazebo.urdf.xacro file.","title":"Gazebo Sensors vs Stretch Sensors"},{"location":"stretch-ros2/stretch_gazebo/#moveit-controllers-vs-stretch_core","text":"Actuators are defined as ros_control transmission objects in Gazebo using PositionJointInterfaces . MoveIt is configured to use three different action servers to control the body parts of stretch in Gazebo through the srdf file in stretch_moveit_config package. See the section above about MoveIt for details. Please note that this behavior is different than stretch_core as it works with a single Python interface to control all the joints.","title":"MoveIt Controllers vs stretch_core"},{"location":"stretch-ros2/stretch_gazebo/#uncalibrated-xacro-vs-calibrated-urdf","text":"We provide stretch_calibration to generate a calibrated URDF that is unique to each robot. The calibrated URDF is generated from the nominal description of Stretch RE1, the xacro files that live in stretch_description . The simulated Stretch RE1 is generated from the gazebo xacro description in the urdf directory and is not calibrated.","title":"Uncalibrated XACRO vs Calibrated URDF"},{"location":"stretch-ros2/stretch_gazebo/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/stretch_gazebo/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_moveit_config/","text":"Overview stretch_moveit2 configures MoveIt2 for Stretch RE1. The MoveIt Motion Planning Framework makes whole body planning, manipulation, 3D perception, control/navigation, and more available on Stretch. Drag & Drop Rviz Planning Demo with Simulated Robot To experiment with the planning capabilities of MoveIt2 on Stretch, you can run a demo without Stretch hardware. The fake robot will be backed by ros2_control controllers that provide MoveIt2 with the action server required for planning, however, the 'execution' button in Rviz will not be able to execute plans that include mobile base motion. ros2 launch stretch_moveit_config demo.launch.py This will allow you to move the robot around using interactive markers and create plans between poses. Hardware Integration TBD License For license information, please see the LICENSE files.","title":"stretch_moveit_config"},{"location":"stretch-ros2/stretch_moveit_config/#overview","text":"stretch_moveit2 configures MoveIt2 for Stretch RE1. The MoveIt Motion Planning Framework makes whole body planning, manipulation, 3D perception, control/navigation, and more available on Stretch.","title":"Overview"},{"location":"stretch-ros2/stretch_moveit_config/#drag-drop-rviz-planning","text":"","title":"Drag &amp; Drop Rviz Planning"},{"location":"stretch-ros2/stretch_moveit_config/#demo-with-simulated-robot","text":"To experiment with the planning capabilities of MoveIt2 on Stretch, you can run a demo without Stretch hardware. The fake robot will be backed by ros2_control controllers that provide MoveIt2 with the action server required for planning, however, the 'execution' button in Rviz will not be able to execute plans that include mobile base motion. ros2 launch stretch_moveit_config demo.launch.py This will allow you to move the robot around using interactive markers and create plans between poses.","title":"Demo with Simulated Robot"},{"location":"stretch-ros2/stretch_moveit_config/#hardware-integration","text":"TBD","title":"Hardware Integration"},{"location":"stretch-ros2/stretch_moveit_config/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/stretch_moveit_config/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_moveit_config/demo/","text":"Overview MoveIt 2 is a whole-body motion planning framework for mobile manipulators that allows planning pose and joint goals in environments with and without obstacles. Stretch being a mobile manipulator is uniquely well-suited to utilize the planning capabilities of MoveIt 2 in different scenarios. Motivation Stretch has a kinematically simple 3 DoF arm (+2 with DexWrist) that is suitable for pick and place tasks of varied objects. Its mobile base provides it with 2 additional degrees of freedom that afford it more manipulability and also the ability to move around freely in its environment. To fully utilize these capabilities, we need a planner that can plan for both the arm and the mobile base at the same time. With MoveIt 2 and ROS 2, it is now possible to achieve this, empowering users to plan more complicated robot trajectories in difficult and uncertain environments. Demo with Stretch Robot Installing ROS 2 on Ubuntu 20.04 By default, Stretch RE1 ships with Ubuntu 18.04, ROS Melodic, and Python2 packages. The following steps will install a second operating system with Ubuntu 20.04, ROS Noetic, ROS 2 Galactic (where MoveIt 2 is supported), and Python3 packages. If you already have an Ubuntu 20.04 partition on the robot set up with the correct ROS workspaces, skip to the next section. If your robot is an RE1 and you have not upgraded the software stack recently, follow the steps in this link to perform a fresh robot installation. We recommend backing up important data before you begin. Installing MoveIt 2 and Its Dependencies If you performed the above steps successfully, you don't need to follow the steps in this section as the install scripts take care of satisfying all dependencies you need to run MoveIt 2 on your robot. However, if you already had a stable ROS 2 installation running but were missing the most up to date workspaces, follow the steps in this link to update your ROS workspaces. We recommend backing up important data before you begin as this step replaces your existing workspace with a new one. Planning with MoveIt 2 Using RViz Before we proceed, it's always a good idea to home the robot first by running the following script so that we have the correct joint positions being published on the /joint_states topic. This is necessary for planning trajectories on Stretch with MoveIt. stretch_robot_home.py The easiest way to run MoveIt 2 on your robot is through RViz. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To launch RViz with MoveIt 2, run the following command. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py Follow instructions in this tutorial to plan and execute trajectories using the interactive markers in RViz. Use the interactive markers to drag joints to desired positions or go to the manipulation tab in the Motion Planning pane to fine-tune joint values using the sliders. Next, click the 'Plan' button to plan the trajectory. If the plan is valid, you should be able to execute the trajectory by clicking the 'Execute' button. Below we see Stretch raising its arm without any obstacle in the way. To plan with obstacles, you can insert objects like a box, cyclinder or sphere, in the planning scene to plan trajectories around the object. This can be done by adding an object using the Scene Objects tab in the Motion Planning pane. Below we see Stretch raising its arm with a flat cuboid obstacle in the way. The mobile base allows Stretch to move forward and then back again while raising the arm to avoid the obstacle. Planning with MoveIt 2 Using the MoveGroup C++ API If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. Execute the launch file again and go through the comments in the node to understand what's going on. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py Follow instructions in this tutorial to plan and execute trajectories using the MoveGroup C++ API. Read the comments in the code for a breakdown of the node.","title":"Demo"},{"location":"stretch-ros2/stretch_moveit_config/demo/#overview","text":"MoveIt 2 is a whole-body motion planning framework for mobile manipulators that allows planning pose and joint goals in environments with and without obstacles. Stretch being a mobile manipulator is uniquely well-suited to utilize the planning capabilities of MoveIt 2 in different scenarios.","title":"Overview"},{"location":"stretch-ros2/stretch_moveit_config/demo/#motivation","text":"Stretch has a kinematically simple 3 DoF arm (+2 with DexWrist) that is suitable for pick and place tasks of varied objects. Its mobile base provides it with 2 additional degrees of freedom that afford it more manipulability and also the ability to move around freely in its environment. To fully utilize these capabilities, we need a planner that can plan for both the arm and the mobile base at the same time. With MoveIt 2 and ROS 2, it is now possible to achieve this, empowering users to plan more complicated robot trajectories in difficult and uncertain environments.","title":"Motivation"},{"location":"stretch-ros2/stretch_moveit_config/demo/#demo-with-stretch-robot","text":"","title":"Demo with Stretch Robot"},{"location":"stretch-ros2/stretch_moveit_config/demo/#installing-ros-2-on-ubuntu-2004","text":"By default, Stretch RE1 ships with Ubuntu 18.04, ROS Melodic, and Python2 packages. The following steps will install a second operating system with Ubuntu 20.04, ROS Noetic, ROS 2 Galactic (where MoveIt 2 is supported), and Python3 packages. If you already have an Ubuntu 20.04 partition on the robot set up with the correct ROS workspaces, skip to the next section. If your robot is an RE1 and you have not upgraded the software stack recently, follow the steps in this link to perform a fresh robot installation. We recommend backing up important data before you begin.","title":"Installing ROS 2 on Ubuntu 20.04"},{"location":"stretch-ros2/stretch_moveit_config/demo/#installing-moveit-2-and-its-dependencies","text":"If you performed the above steps successfully, you don't need to follow the steps in this section as the install scripts take care of satisfying all dependencies you need to run MoveIt 2 on your robot. However, if you already had a stable ROS 2 installation running but were missing the most up to date workspaces, follow the steps in this link to update your ROS workspaces. We recommend backing up important data before you begin as this step replaces your existing workspace with a new one.","title":"Installing MoveIt 2 and Its Dependencies"},{"location":"stretch-ros2/stretch_moveit_config/demo/#planning-with-moveit-2-using-rviz","text":"Before we proceed, it's always a good idea to home the robot first by running the following script so that we have the correct joint positions being published on the /joint_states topic. This is necessary for planning trajectories on Stretch with MoveIt. stretch_robot_home.py The easiest way to run MoveIt 2 on your robot is through RViz. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To launch RViz with MoveIt 2, run the following command. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py Follow instructions in this tutorial to plan and execute trajectories using the interactive markers in RViz. Use the interactive markers to drag joints to desired positions or go to the manipulation tab in the Motion Planning pane to fine-tune joint values using the sliders. Next, click the 'Plan' button to plan the trajectory. If the plan is valid, you should be able to execute the trajectory by clicking the 'Execute' button. Below we see Stretch raising its arm without any obstacle in the way. To plan with obstacles, you can insert objects like a box, cyclinder or sphere, in the planning scene to plan trajectories around the object. This can be done by adding an object using the Scene Objects tab in the Motion Planning pane. Below we see Stretch raising its arm with a flat cuboid obstacle in the way. The mobile base allows Stretch to move forward and then back again while raising the arm to avoid the obstacle.","title":"Planning with MoveIt 2 Using RViz"},{"location":"stretch-ros2/stretch_moveit_config/demo/#planning-with-moveit-2-using-the-movegroup-c-api","text":"If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. Execute the launch file again and go through the comments in the node to understand what's going on. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py Follow instructions in this tutorial to plan and execute trajectories using the MoveGroup C++ API. Read the comments in the code for a breakdown of the node.","title":"Planning with MoveIt 2 Using the MoveGroup C++ API"},{"location":"stretch-ros2/stretch_navigation/","text":"Overview stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive the stretch RE1 around a mapped space. Running this code will require the robot to be untethered. Setup These set up instructions are already performed on Stretch RE1 robots. Follow these instructions if stretch_navigation is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros to your catkin workspace. Then install dependencies and build the packages, with the following set of commands: mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make Quickstart The first step is to map the space that the robot will navigate in. The mapping.launch will enable you to do this. First run: roslaunch stretch_navigation mapping.launch Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to stretch_user/ . mkdir -p ~/stretch_user/maps rosrun map_server map_saver -f ${ HELLO_FLEET_PATH } /maps/<map_name> The <map_name> does not include an extension. Map_saver will save two files as <map_name>.pgm and <map_name>.yaml . Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location in the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place. It is also possible to send 2D Pose Estimates and Nav Goals programatically. In your own launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages in order to navigate the robot programatically. Running in Simulation To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the mapping_gazebo.launch and navigation_gazebo.launch launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch. roslaunch stretch_navigation mapping_gazebo.launch gazebo_world: = worlds/willowgarage.world Teleop using a Joystick Controller The mapping launch files, mapping.launch and mapping_gazebo.launch expose the ROS argument, \"teleop_type\". By default, this ROS arg is set to \"keyboard\", which launches keyboard teleop in the terminal. If the xbox controller that ships with Stretch RE1 is plugged into your computer, the following command will launch mapping with joystick teleop: roslaunch stretch_navigation mapping.launch teleop_type: = joystick Using ROS Remote Master If you have set up ROS Remote Master for untethered operation , you can use Rviz and teleop locally with the following commands: # On Robot roslaunch stretch_navigation mapping.launch rviz: = false teleop_type: = none # On your machine, Terminal 1: rviz -d ` rospack find stretch_navigation ` /rviz/mapping.launch # On your machine, Terminal 2: roslaunch stretch_core teleop_twist.launch teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller License For license information, please see the LICENSE files.","title":"stretch_navigation"},{"location":"stretch-ros2/stretch_navigation/#overview","text":"stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive the stretch RE1 around a mapped space. Running this code will require the robot to be untethered.","title":"Overview"},{"location":"stretch-ros2/stretch_navigation/#setup","text":"These set up instructions are already performed on Stretch RE1 robots. Follow these instructions if stretch_navigation is not present in your ROS workspace or you are simulating Stretch on external hardware. Clone stretch_ros to your catkin workspace. Then install dependencies and build the packages, with the following set of commands: mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make","title":"Setup"},{"location":"stretch-ros2/stretch_navigation/#quickstart","text":"The first step is to map the space that the robot will navigate in. The mapping.launch will enable you to do this. First run: roslaunch stretch_navigation mapping.launch Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to stretch_user/ . mkdir -p ~/stretch_user/maps rosrun map_server map_saver -f ${ HELLO_FLEET_PATH } /maps/<map_name> The <map_name> does not include an extension. Map_saver will save two files as <map_name>.pgm and <map_name>.yaml . Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location in the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place. It is also possible to send 2D Pose Estimates and Nav Goals programatically. In your own launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages in order to navigate the robot programatically.","title":"Quickstart"},{"location":"stretch-ros2/stretch_navigation/#running-in-simulation","text":"To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the mapping_gazebo.launch and navigation_gazebo.launch launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch. roslaunch stretch_navigation mapping_gazebo.launch gazebo_world: = worlds/willowgarage.world","title":"Running in Simulation"},{"location":"stretch-ros2/stretch_navigation/#teleop-using-a-joystick-controller","text":"The mapping launch files, mapping.launch and mapping_gazebo.launch expose the ROS argument, \"teleop_type\". By default, this ROS arg is set to \"keyboard\", which launches keyboard teleop in the terminal. If the xbox controller that ships with Stretch RE1 is plugged into your computer, the following command will launch mapping with joystick teleop: roslaunch stretch_navigation mapping.launch teleop_type: = joystick","title":"Teleop using a Joystick Controller"},{"location":"stretch-ros2/stretch_navigation/#using-ros-remote-master","text":"If you have set up ROS Remote Master for untethered operation , you can use Rviz and teleop locally with the following commands: # On Robot roslaunch stretch_navigation mapping.launch rviz: = false teleop_type: = none # On your machine, Terminal 1: rviz -d ` rospack find stretch_navigation ` /rviz/mapping.launch # On your machine, Terminal 2: roslaunch stretch_core teleop_twist.launch teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller","title":"Using ROS Remote Master"},{"location":"stretch-ros2/stretch_navigation/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/stretch_navigation/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_octomap/","text":"Overview This package is in active development. Proceed with caution. stretch_octomap provides mapping of Stretch's environment into an Octree representation. License For license information, please see the LICENSE files.","title":"stretch_octomap"},{"location":"stretch-ros2/stretch_octomap/#overview","text":"This package is in active development. Proceed with caution. stretch_octomap provides mapping of Stretch's environment into an Octree representation.","title":"Overview"},{"location":"stretch-ros2/stretch_octomap/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/stretch_octomap/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-ros2/stretch_rtabmap/","text":"Overview This package is in active development. Proceed with caution. stretch_rtabmap provides RTAB-MAPPiNG. This package utilizes rtab-map, move_base, and AMCL to map and drive the stretch RE1 around a space. Running this code will require the robot to be untethered. Setup Use rosdep to install the required packages. cd ~/catkin_ws/src git clone https://github.com/NateWright/stretch_ros.git git clone https://github.com/pal-robotics/realsense_gazebo_plugin cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make Running Demo Gazebo roslaunch stretch_rtabmap gazebo.launch roslaunch stretch_rtabmap start_rtab.launch sim: = true localization: = false move_base_config: = 2d roslaunch stretch_rtabmap rviz_rtab.launch mapping: = true Stretch RE1 roslaunch stretch_rtabmap start_rtab.launch sim: = false localization: = false move_base_config: = 2d roslaunch stretch_rtabmap rviz_rtab.launch mapping: = true Code Status & Development Plans Move_base_config Gazebo Stretch RE1 2d okay Good 2d_unkown Not Implemented Not Implemented 3d Testing Testing 3d_unkown Not Implemented Not Implemented License For license information, please see the LICENSE files.","title":"stretch_rtabmap"},{"location":"stretch-ros2/stretch_rtabmap/#overview","text":"This package is in active development. Proceed with caution. stretch_rtabmap provides RTAB-MAPPiNG. This package utilizes rtab-map, move_base, and AMCL to map and drive the stretch RE1 around a space. Running this code will require the robot to be untethered.","title":"Overview"},{"location":"stretch-ros2/stretch_rtabmap/#setup","text":"Use rosdep to install the required packages. cd ~/catkin_ws/src git clone https://github.com/NateWright/stretch_ros.git git clone https://github.com/pal-robotics/realsense_gazebo_plugin cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make","title":"Setup"},{"location":"stretch-ros2/stretch_rtabmap/#running-demo","text":"","title":"Running Demo"},{"location":"stretch-ros2/stretch_rtabmap/#gazebo","text":"roslaunch stretch_rtabmap gazebo.launch roslaunch stretch_rtabmap start_rtab.launch sim: = true localization: = false move_base_config: = 2d roslaunch stretch_rtabmap rviz_rtab.launch mapping: = true","title":"Gazebo"},{"location":"stretch-ros2/stretch_rtabmap/#stretch-re1","text":"roslaunch stretch_rtabmap start_rtab.launch sim: = false localization: = false move_base_config: = 2d roslaunch stretch_rtabmap rviz_rtab.launch mapping: = true","title":"Stretch RE1"},{"location":"stretch-ros2/stretch_rtabmap/#code-status-development-plans","text":"Move_base_config Gazebo Stretch RE1 2d okay Good 2d_unkown Not Implemented Not Implemented 3d Testing Testing 3d_unkown Not Implemented Not Implemented","title":"Code Status &amp; Development Plans"},{"location":"stretch-ros2/stretch_rtabmap/#license","text":"For license information, please see the LICENSE files.","title":"License"},{"location":"stretch-ros2/stretch_rtabmap/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-tool-share/","text":"Overview We designed Stretch's hardware to be easily extended. You can make your own tool and attach it to the wrist to creatively expand what the Stretch RE1 can do. Your tool can also use Dynamixel X-series servos from Robotis via the provided TTL bus. In this repository, we provide examples of tools that we've created. We've released them with a permissive Apache 2.0 license, so you're free to use them as you wish. We hope they'll inspire you to create your own tools. We also include URDF and mesh files for many of the tools in their stretch_description folder. See the Stretch ROS documentation for guidance on integrating these tools into your robot model. We'd love it if you shared your creations with the community. We recommend you create a GitHub repository similar to this one for your own tools and then post an announcement to the forum to let people know about it. Tool Stretch Dex Wrist Stretch Teleop Kit Stretch Docking Station Wrist USB Board Camera Stretch 2 Model Phone Holder V1 ReactorX Wrist V1 Dry Erase Holder V1 Swiffer Mount V1 Tray Cup Holder V1 Puller V1 Stretch RE1 Arm Stretch RE1 Head Easy-Access Card Holder Arm-Mounted Tray Button Pusher Licenses The contents in this repository that represent parts of the Stretch RE1 robot, such as its head, arm, wrist, and default gripper, are covered by the CC BY-NC-SA 4.0 license. Please note that the Stretch RE1 robot and its default gripper are also covered by pending patents. Please see the ROBOT_LICENSE file for details. Other contents in this repository created by Hello Robot Inc. that specifically pertain to the tools that attach to the Stretch RE1 as accessories are covered by the Apache 2.0 license. Please see the TOOL_LICENSE file for details. The contents of this repository are intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc.","title":"Overview"},{"location":"stretch-tool-share/#overview","text":"We designed Stretch's hardware to be easily extended. You can make your own tool and attach it to the wrist to creatively expand what the Stretch RE1 can do. Your tool can also use Dynamixel X-series servos from Robotis via the provided TTL bus. In this repository, we provide examples of tools that we've created. We've released them with a permissive Apache 2.0 license, so you're free to use them as you wish. We hope they'll inspire you to create your own tools. We also include URDF and mesh files for many of the tools in their stretch_description folder. See the Stretch ROS documentation for guidance on integrating these tools into your robot model. We'd love it if you shared your creations with the community. We recommend you create a GitHub repository similar to this one for your own tools and then post an announcement to the forum to let people know about it. Tool Stretch Dex Wrist Stretch Teleop Kit Stretch Docking Station Wrist USB Board Camera Stretch 2 Model Phone Holder V1 ReactorX Wrist V1 Dry Erase Holder V1 Swiffer Mount V1 Tray Cup Holder V1 Puller V1 Stretch RE1 Arm Stretch RE1 Head Easy-Access Card Holder Arm-Mounted Tray Button Pusher","title":"Overview"},{"location":"stretch-tool-share/#licenses","text":"The contents in this repository that represent parts of the Stretch RE1 robot, such as its head, arm, wrist, and default gripper, are covered by the CC BY-NC-SA 4.0 license. Please note that the Stretch RE1 robot and its default gripper are also covered by pending patents. Please see the ROBOT_LICENSE file for details. Other contents in this repository created by Hello Robot Inc. that specifically pertain to the tools that attach to the Stretch RE1 as accessories are covered by the Apache 2.0 license. Please see the TOOL_LICENSE file for details. The contents of this repository are intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc.","title":"Licenses"},{"location":"stretch-tool-share/ROBOT_LICENSE/","text":"The following license applies to the entire contents of this directory that represent parts of the Stretch RE1 robot (the \"Robot Contents\"), such as the robot's head, arm, wrist, and default gripper. The Robot Contents consists of software and data for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Robot Contents are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike-4.0-International (CC BY-NC-SA 4.0) license (the \"License\"); you may not use the Robot Contents except in compliance with the License. You may obtain a copy of the License at https://creativecommons.org/licenses/by-nc-sa/4.0/ Unless required by applicable law or agreed to in writing, the Robot Contents distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Patents pending and trademark rights cover the Robot Contents. As stated by the detailed License, \"Patent and trademark rights are not licensed under this Public License.\" For further information about the Robot Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"ROBOT LICENSE"},{"location":"stretch-tool-share/TOOL_LICENSE/","text":"The following license applies to the contents (the \"Tool Contents\") in this repository specific to tools created by Hello Robot Inc. that attach to the Stretch RE1 robot. This license explicitly excludes any contents covered by the license found in the ROBOT_LICENSE.md file. The Tool Contents does not include any part of the Stretch RE1 robot, such as its arm, wrist, or default gripper. The Tool Contents consists of software and data related to tools intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Tool Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Tool Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Tool Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Tool Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"TOOL LICENSE"},{"location":"stretch-tool-share/python/LICENSE/","text":"The following license applies to the entire contents of this directory (the \"Contents\"), which contains software for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents include free software and other kinds of works: you can redistribute them and/or modify them under the terms of the GNU General Public License v3.0 (GNU GPLv3) as published by the Free Software Foundation. The Contents are distributed in the hope that they will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License v3.0 (GNU GPLv3) for more details, which can be found via the following link: https://www.gnu.org/licenses/gpl-3.0.html For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/","text":"Arm-Mounted Tray Created by : Hello Robot Inc Overview This tray conveniently clips onto Stretch's first arm link to provide a surface for carrying objects, including two hooks in the front and a cup holder to assist a variety of tasks. Parts List Item Qty Vendor Tray 1 PLA 3D Printer Arm Clips 2 PLA 3D Printer M4x6mm Socket Head Screw 4 McMaster-Carr M4 Nut 4 McMaster-Carr Assembly instructions View 3D assembly Install one M4 screw through each tray hole, and secure each with an M4 nut set in the tray clips. Ensure the hooks on the tray clips are facing the flat side of the tray, away from the cupholder. Press the tray onto the first arm link until it snaps into place.","title":"Arm Mounted Tray"},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/#arm-mounted-tray","text":"Created by : Hello Robot Inc","title":"Arm-Mounted Tray"},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/#overview","text":"This tray conveniently clips onto Stretch's first arm link to provide a surface for carrying objects, including two hooks in the front and a cup holder to assist a variety of tasks.","title":"Overview"},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/#parts-list","text":"Item Qty Vendor Tray 1 PLA 3D Printer Arm Clips 2 PLA 3D Printer M4x6mm Socket Head Screw 4 McMaster-Carr M4 Nut 4 McMaster-Carr","title":"Parts List"},{"location":"stretch-tool-share/tool_share/arm_mounted_tray_V1/#assembly-instructions","text":"View 3D assembly Install one M4 screw through each tray hole, and secure each with an M4 nut set in the tray clips. Ensure the hooks on the tray clips are facing the flat side of the tray, away from the cupholder. Press the tray onto the first arm link until it snaps into place.","title":"Assembly instructions"},{"location":"stretch-tool-share/tool_share/button_pusher_V2/","text":"Button Pusher Create by : Hello Robot Inc. and Henry Evans A simple part to help Stretch push buttons or anything else you can imagine. The concave shape is designed to avoid blocking the gripper teleop camera, allowing for high-percision and easy use. We found adding a layer of Dycem non-slip material helped with a more secure grip on the tool. Foam or cardboard blocks under each gripper pad may help stabilize as well. Parts List Item Qty Vendor button_pusher_V2.stl 1 PLA 3D printer Dycem 1 Various","title":"Index"},{"location":"stretch-tool-share/tool_share/button_pusher_V2/#button-pusher","text":"Create by : Hello Robot Inc. and Henry Evans A simple part to help Stretch push buttons or anything else you can imagine. The concave shape is designed to avoid blocking the gripper teleop camera, allowing for high-percision and easy use. We found adding a layer of Dycem non-slip material helped with a more secure grip on the tool. Foam or cardboard blocks under each gripper pad may help stabilize as well.","title":"Button Pusher"},{"location":"stretch-tool-share/tool_share/button_pusher_V2/#parts-list","text":"Item Qty Vendor button_pusher_V2.stl 1 PLA 3D printer Dycem 1 Various","title":"Parts List"},{"location":"stretch-tool-share/tool_share/card_holder_V1.5/","text":"Easy-Access Playing Card Holder Created by : Hello Robot Inc A spinning card holder that Stretch can manipulate to position cards in an optimal position for grasping or to signal which card an opponent should take. Parts List Item Qty Vendor Gear Wheel 1 PLA 3D printer A Frame 1 PLA 3D printer Base 1 PLA 3D printer Pull Tab 1 PLA 3D printer Foam Block* 1 Recycled Material *We used extra packing foam, but styrofoam or anything similar should work! Assembly instructions Cut an approximately 6 inch diameter circle out of the foam. Slice around the perimiter about 2 inches into the foam to create a slot to hold cards. Gently press the spikes on the front of the gear wheel into the center of the foam circle. Push the axel on the A frame through the hole in the back of the wheel gear and into the foam until the back of the wheel gear meets the front of the A frame. With the foam and gear wheel secured onto the A frame, push the frame into the openings on the base. Slide the pull tab into the slot on the A Frame. The teeth on the pull tab should interlock with the teeth on the wheel gear so pulling and pushing the tab spins the wheel.","title":"Card Holder"},{"location":"stretch-tool-share/tool_share/card_holder_V1.5/#easy-access-playing-card-holder","text":"Created by : Hello Robot Inc A spinning card holder that Stretch can manipulate to position cards in an optimal position for grasping or to signal which card an opponent should take.","title":"Easy-Access Playing Card Holder"},{"location":"stretch-tool-share/tool_share/card_holder_V1.5/#parts-list","text":"Item Qty Vendor Gear Wheel 1 PLA 3D printer A Frame 1 PLA 3D printer Base 1 PLA 3D printer Pull Tab 1 PLA 3D printer Foam Block* 1 Recycled Material *We used extra packing foam, but styrofoam or anything similar should work!","title":"Parts List"},{"location":"stretch-tool-share/tool_share/card_holder_V1.5/#assembly-instructions","text":"Cut an approximately 6 inch diameter circle out of the foam. Slice around the perimiter about 2 inches into the foam to create a slot to hold cards. Gently press the spikes on the front of the gear wheel into the center of the foam circle. Push the axel on the A frame through the hole in the back of the wheel gear and into the foam until the back of the wheel gear meets the front of the A frame. With the foam and gear wheel secured onto the A frame, push the frame into the openings on the base. Slide the pull tab into the slot on the A Frame. The teeth on the pull tab should interlock with the teeth on the wheel gear so pulling and pushing the tab spins the wheel.","title":"Assembly instructions"},{"location":"stretch-tool-share/tool_share/dry_erase_holder_V1/","text":"Dry Erase Holder Created by : Hello Robot Inc This tool allows Stretch to hold a dry erase marker. It is spring loaded, allowing for compliant interaction between the marker and a white board. The tool can be integrated into your robot URDF by integrating its stretch_description as described in the Stretch ROS documentation . Parts List Item Qty Vendor Expo Dry Erase 1 Amazon M5x50mm Hex Head Bolt 1 McMaster-Carr M5 Nut 2 McMaster-Carr wrist_end_cap_5mm 1 PLA 3D Printer dry_erase_bushing_block 1 PLA 3D Printer Size 30 Rubber Band 2 McMaster-Carr 3/4\" Shaft Collar 1 McMaster-Carr Assembly instructions View 3D assembly Install the bolt into the dry_erase_bushing block and secure from below with an M5 nut. Attach the dry erase bushing block to the tool plate, securing from below with the wrist_end_cap_5mm and an M5 nut. Orient the block so the marker points forward. Attach the shaft collar to your dry erase marker, approximately 8mm from the back of the marker. Slide the marker into the bushing block. Loop a rubber band around the back of the marker and over to one of the pegs on the side of the bushing block. Repeat with the other peg. The marker should now easily spring back when pushed against. You're ready to write!","title":"Dry Erase Holder"},{"location":"stretch-tool-share/tool_share/dry_erase_holder_V1/#dry-erase-holder","text":"Created by : Hello Robot Inc This tool allows Stretch to hold a dry erase marker. It is spring loaded, allowing for compliant interaction between the marker and a white board. The tool can be integrated into your robot URDF by integrating its stretch_description as described in the Stretch ROS documentation .","title":"Dry Erase Holder"},{"location":"stretch-tool-share/tool_share/dry_erase_holder_V1/#parts-list","text":"Item Qty Vendor Expo Dry Erase 1 Amazon M5x50mm Hex Head Bolt 1 McMaster-Carr M5 Nut 2 McMaster-Carr wrist_end_cap_5mm 1 PLA 3D Printer dry_erase_bushing_block 1 PLA 3D Printer Size 30 Rubber Band 2 McMaster-Carr 3/4\" Shaft Collar 1 McMaster-Carr","title":"Parts List"},{"location":"stretch-tool-share/tool_share/dry_erase_holder_V1/#assembly-instructions","text":"View 3D assembly Install the bolt into the dry_erase_bushing block and secure from below with an M5 nut. Attach the dry erase bushing block to the tool plate, securing from below with the wrist_end_cap_5mm and an M5 nut. Orient the block so the marker points forward. Attach the shaft collar to your dry erase marker, approximately 8mm from the back of the marker. Slide the marker into the bushing block. Loop a rubber band around the back of the marker and over to one of the pegs on the side of the bushing block. Repeat with the other peg. The marker should now easily spring back when pushed against. You're ready to write!","title":"Assembly instructions"},{"location":"stretch-tool-share/tool_share/phone_holder_V1/","text":"Phone Holder Create by : Hello Robot Inc This tool allows you to attach a smart phone to your Stretch. It uses an off the shelf phone holder with and a photography standard 1/4-20 attachment post. Parts List Item Qty Vendor 1/4-20 x 3\" BHCS 1 McMaster-Carr 1/4-20 Nut 2 McMaster-Carr wrist_mount_spacer.STL 2 PLA 3D printer Ulanzi ST-01 Phone Tripod Mount 1 Amazon Assembly instructions View 3D assembly Install the two wrist_mount spacers onto wrist Install the long bolt and lock into place using the first nut. Friction will hold this in a fixed orientation. Install the second nut and then the phone mount, screwing onto the bolt Lock the phone mount into place, using the second nut as a jam nut If you want to adjust the phone orientation relative to the wrist yaw, loosen up the jam nut, adjust, then tighten.","title":"Phone Holder"},{"location":"stretch-tool-share/tool_share/phone_holder_V1/#phone-holder","text":"Create by : Hello Robot Inc This tool allows you to attach a smart phone to your Stretch. It uses an off the shelf phone holder with and a photography standard 1/4-20 attachment post.","title":"Phone Holder"},{"location":"stretch-tool-share/tool_share/phone_holder_V1/#parts-list","text":"Item Qty Vendor 1/4-20 x 3\" BHCS 1 McMaster-Carr 1/4-20 Nut 2 McMaster-Carr wrist_mount_spacer.STL 2 PLA 3D printer Ulanzi ST-01 Phone Tripod Mount 1 Amazon","title":"Parts List"},{"location":"stretch-tool-share/tool_share/phone_holder_V1/#assembly-instructions","text":"View 3D assembly Install the two wrist_mount spacers onto wrist Install the long bolt and lock into place using the first nut. Friction will hold this in a fixed orientation. Install the second nut and then the phone mount, screwing onto the bolt Lock the phone mount into place, using the second nut as a jam nut If you want to adjust the phone orientation relative to the wrist yaw, loosen up the jam nut, adjust, then tighten.","title":"Assembly instructions"},{"location":"stretch-tool-share/tool_share/puller_v1/","text":"Puller Create by : Hello Robot Inc This is a a simple 'puller' attachment for the Stretch Compliant Gripper. We've used it to pull open many common drawers, cabinet doors, and even a mini-fridge door. You can also use it to push things closed, such as drawers. You can think of it as a circular hook used to pull things or a finger used to push things. It attaches to the 6-32 stud on the side of the gripper. By turning the gripper sideways during manipulation, the hook can drop over the drawer handle, allowing the arm to retract and pull the door open. Parts List Item Qty Vendor 6-32 x 0.5\" BHCS 1 McMaster-Carr 6-32 x 1\" aluminum threaded standoff 1 McMaster-Carr Puller_V1.STL 1 PLA 3D printer Assembly instructions View 3D assembly Screw the standoff on to the gripper's threaded post. Secure tightly and add a drop of light duty Loctite if desired. Attach the plastic pull to the standoff using the BHCS.","title":"Puller"},{"location":"stretch-tool-share/tool_share/puller_v1/#puller","text":"Create by : Hello Robot Inc This is a a simple 'puller' attachment for the Stretch Compliant Gripper. We've used it to pull open many common drawers, cabinet doors, and even a mini-fridge door. You can also use it to push things closed, such as drawers. You can think of it as a circular hook used to pull things or a finger used to push things. It attaches to the 6-32 stud on the side of the gripper. By turning the gripper sideways during manipulation, the hook can drop over the drawer handle, allowing the arm to retract and pull the door open.","title":"Puller"},{"location":"stretch-tool-share/tool_share/puller_v1/#parts-list","text":"Item Qty Vendor 6-32 x 0.5\" BHCS 1 McMaster-Carr 6-32 x 1\" aluminum threaded standoff 1 McMaster-Carr Puller_V1.STL 1 PLA 3D printer","title":"Parts List"},{"location":"stretch-tool-share/tool_share/puller_v1/#assembly-instructions","text":"View 3D assembly Screw the standoff on to the gripper's threaded post. Secure tightly and add a drop of light duty Loctite if desired. Attach the plastic pull to the standoff using the BHCS.","title":"Assembly instructions"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/","text":"Reactor X Wrist Created by : Hello Robot Inc Here we describe how to modify an Interbotx ReactorX 150 Dynamixel arm to add a Pitch-Roll wrist plus parallel jaw gripper to Stretch, giving you a Yaw-Pitch-Roll wrist! The ReactorX 150 comes standard with Dynamixel XM430-W350-T servos for the shoulder joints and XL430-W250-T servos for its wrist and gripper. We're going to disassemble the arm and then rebuild it so that the Pitch and Roll DOF use the stronger XM430-W350-T servos. View the 3D assembly Parts List Item Qty Vendor ReactorX 150 Robot Arm 1 Trossen Robotics Robotis FR12-S101K Frame 1 Robotis M2x4mm SHCS 10 McMaster Carr Wrist Build Start with a ReactorX 150 Detach the the wrist unit from the arm. This includes the final pitch / roll /gripper DOFs with the XL430 servos. Remove the XM430 servos from the shoulder of the arm. The XM430s have the metal horns as shown. No replace the pitch and roll servos with the XM430s. Pay attention to how the cables route and which ports they plug into so you can recable the wrist correctly. To attach the two side plates to the pitch servo (above, right), you'll want to use the shorter M2x4mm screws. The ones that came off of the XL430s are too long and will cause the servo to bind up if used. Recable the servos and attach the pitch and roll joints to the gripper. You're done! Other Modifications The FR12-S101K frame doesn't allow a X-Series TTL connector to pass through by default. You will need to drill out one of the 8mm holes on a drill press to 10mm. This will allow the cable routing to go through the center of Wrist Yaw rotation (recommended) Dynamixel Configuration The Dynamixel servos that come with the Reactor arm need to be reprogrammed with the correct IDs and baudrate. Out of the box, the Reactor servos use a baudrate of 1Mbps while Stretch requires 115200Kbps. To reconfigure the servos: Download and install the Dynamixel Wizard 2.0 from Robotis Attach your 3 DOF gripper-wrist to a PC using a U2D2 adapter from Robotis Launch the Wizard and click Scan. It should identify the three servos. Select each servo from the left hand drop down menu and set its baudrate and ID. Hit save. By convention we use Dynamixel IDs of: Wrist PITCH: 14 Wrist ROLL: 15 Gripper: 16 Assembly instructions Now attach your wrist to the Stretch tool plate using the Robotis Frame and their provided hardware, as shown above. Code You're ready to start using your wrist. Once it is plugged into the robot you will want to: Install Stretch Tool Share >>$ pip install -U hello-robot-stretch-tool-share Backup User YAML $ cd $HELLO_FLEET_PATH/$HELLO_FLEET_ID $ cp stretch_user_params.yaml stretch_user_params.yaml.bak Update User YAML robot : tool : tool_reactor_wrist params : [ 'stretch_tool_share.reactorx_wrist_v1.params' ] lift : i_feedforward : 0.75 hello-motor-lift : gains : i_safety_feedforward : 0.75 Set the Baud Rate The default YAML for the Reactor wrist is set to 115200. Your robot may be running at 57600. You can check the current baud settings by: >>$ stretch_params.py | grep baud stretch_body.robot_params.nominal_params param.end_of_arm.baud 115200 stretch_body.robot_params.nominal_params param.head.baud 115200 stretch_body.robot_params.nominal_params param.head_pan.baud 115200 stretch_body.robot_params.nominal_params param.head_tilt.baud 115200 stretch_body.robot_params.nominal_params param.stretch_gripper.baud 115200 stretch_body.robot_params.nominal_params param.tool_none.baud 115200 stretch_body.robot_params.nominal_params param.tool_stretch_gripper.baud 115200 stretch_body.robot_params.nominal_params param.wrist_yaw.baud 115200 You can change the baud for a joint (eg ID 15) >>$ REx_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 15 115200 Ensure that all baud rates are at 115200 for the end-of-arm (IDs 13, 14, 15, 16) Jog the Wrist Try out the jog tool >>$ reactor_wrist_jog.py Important Notes With the release of the Stretch Dex Wrist, the Reactor Wrist build is not being actively supported by Hello Robot. Parameters describing the range of motion of the wrist pitch, roll, and gripper were most recently updated by Kavya Puthuveetil from RCHI Lab @ CMU for an independent research project using the Reactor X Wrist in September 2022. As a result, these values may not match the convention used for other end of arm tools in Stretch Tool Share created by Hello Robot. We provide visualizations of the updated range of motion and zero position of each joint here for your reference. Wrist Pitch Pitch Angle = 0 \u2192 Pitch Angle = \u03c0/2 Wrist Roll Roll Angle = 0 \u2192 Roll Angle = \u03c0 Roll Angle = 0 \u2192 Roll Angle = -\u03c0 Gripper Gripper Angle = \u03c0/2 \u2192 Gripper Angle = 0 Example: All Joints Zeroed Pitch, Roll, Gripper Angle = 0 Modifying Reactor X Wrist Parameters To customize the range of motion or zero position of any of the gripper joints for your own application, you can modify the 'range_t' and 'zero_t' parameters, respectively, defined in stretch_tool_share/python/stretch_tool_share/reactorx_wrist_v1/params.py . Note that your changes will not be applied unless you override the system version of the Python package hello-robot-stretch-tool-share with a local install. To do this, execute the file stretch_tool_share/python/local_install.sh by navigating to the stretch_tool_share/python directory and running: ./ local_install.sh","title":"ReactorX Wrist"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#reactor-x-wrist","text":"Created by : Hello Robot Inc Here we describe how to modify an Interbotx ReactorX 150 Dynamixel arm to add a Pitch-Roll wrist plus parallel jaw gripper to Stretch, giving you a Yaw-Pitch-Roll wrist! The ReactorX 150 comes standard with Dynamixel XM430-W350-T servos for the shoulder joints and XL430-W250-T servos for its wrist and gripper. We're going to disassemble the arm and then rebuild it so that the Pitch and Roll DOF use the stronger XM430-W350-T servos. View the 3D assembly","title":"Reactor X Wrist"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#parts-list","text":"Item Qty Vendor ReactorX 150 Robot Arm 1 Trossen Robotics Robotis FR12-S101K Frame 1 Robotis M2x4mm SHCS 10 McMaster Carr","title":"Parts List"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#wrist-build","text":"Start with a ReactorX 150 Detach the the wrist unit from the arm. This includes the final pitch / roll /gripper DOFs with the XL430 servos. Remove the XM430 servos from the shoulder of the arm. The XM430s have the metal horns as shown. No replace the pitch and roll servos with the XM430s. Pay attention to how the cables route and which ports they plug into so you can recable the wrist correctly. To attach the two side plates to the pitch servo (above, right), you'll want to use the shorter M2x4mm screws. The ones that came off of the XL430s are too long and will cause the servo to bind up if used. Recable the servos and attach the pitch and roll joints to the gripper. You're done!","title":"Wrist Build"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#other-modifications","text":"The FR12-S101K frame doesn't allow a X-Series TTL connector to pass through by default. You will need to drill out one of the 8mm holes on a drill press to 10mm. This will allow the cable routing to go through the center of Wrist Yaw rotation (recommended)","title":"Other Modifications"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#dynamixel-configuration","text":"The Dynamixel servos that come with the Reactor arm need to be reprogrammed with the correct IDs and baudrate. Out of the box, the Reactor servos use a baudrate of 1Mbps while Stretch requires 115200Kbps. To reconfigure the servos: Download and install the Dynamixel Wizard 2.0 from Robotis Attach your 3 DOF gripper-wrist to a PC using a U2D2 adapter from Robotis Launch the Wizard and click Scan. It should identify the three servos. Select each servo from the left hand drop down menu and set its baudrate and ID. Hit save. By convention we use Dynamixel IDs of: Wrist PITCH: 14 Wrist ROLL: 15 Gripper: 16","title":"Dynamixel Configuration"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#assembly-instructions","text":"Now attach your wrist to the Stretch tool plate using the Robotis Frame and their provided hardware, as shown above.","title":"Assembly instructions"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#code","text":"You're ready to start using your wrist. Once it is plugged into the robot you will want to:","title":"Code"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#install-stretch-tool-share","text":">>$ pip install -U hello-robot-stretch-tool-share","title":"Install Stretch Tool Share"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#backup-user-yaml","text":"$ cd $HELLO_FLEET_PATH/$HELLO_FLEET_ID $ cp stretch_user_params.yaml stretch_user_params.yaml.bak","title":"Backup User YAML"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#update-user-yaml","text":"robot : tool : tool_reactor_wrist params : [ 'stretch_tool_share.reactorx_wrist_v1.params' ] lift : i_feedforward : 0.75 hello-motor-lift : gains : i_safety_feedforward : 0.75","title":"Update User YAML"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#set-the-baud-rate","text":"The default YAML for the Reactor wrist is set to 115200. Your robot may be running at 57600. You can check the current baud settings by: >>$ stretch_params.py | grep baud stretch_body.robot_params.nominal_params param.end_of_arm.baud 115200 stretch_body.robot_params.nominal_params param.head.baud 115200 stretch_body.robot_params.nominal_params param.head_pan.baud 115200 stretch_body.robot_params.nominal_params param.head_tilt.baud 115200 stretch_body.robot_params.nominal_params param.stretch_gripper.baud 115200 stretch_body.robot_params.nominal_params param.tool_none.baud 115200 stretch_body.robot_params.nominal_params param.tool_stretch_gripper.baud 115200 stretch_body.robot_params.nominal_params param.wrist_yaw.baud 115200 You can change the baud for a joint (eg ID 15) >>$ REx_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 15 115200 Ensure that all baud rates are at 115200 for the end-of-arm (IDs 13, 14, 15, 16)","title":"Set the Baud Rate"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#jog-the-wrist","text":"Try out the jog tool >>$ reactor_wrist_jog.py","title":"Jog the Wrist"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#important-notes","text":"With the release of the Stretch Dex Wrist, the Reactor Wrist build is not being actively supported by Hello Robot. Parameters describing the range of motion of the wrist pitch, roll, and gripper were most recently updated by Kavya Puthuveetil from RCHI Lab @ CMU for an independent research project using the Reactor X Wrist in September 2022. As a result, these values may not match the convention used for other end of arm tools in Stretch Tool Share created by Hello Robot. We provide visualizations of the updated range of motion and zero position of each joint here for your reference.","title":"Important Notes"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#wrist-pitch","text":"Pitch Angle = 0 \u2192 Pitch Angle = \u03c0/2","title":"Wrist Pitch"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#wrist-roll","text":"Roll Angle = 0 \u2192 Roll Angle = \u03c0 Roll Angle = 0 \u2192 Roll Angle = -\u03c0","title":"Wrist Roll"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#gripper","text":"Gripper Angle = \u03c0/2 \u2192 Gripper Angle = 0","title":"Gripper"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#example-all-joints-zeroed","text":"Pitch, Roll, Gripper Angle = 0","title":"Example: All Joints Zeroed"},{"location":"stretch-tool-share/tool_share/reactorx_wrist_V1/#modifying-reactor-x-wrist-parameters","text":"To customize the range of motion or zero position of any of the gripper joints for your own application, you can modify the 'range_t' and 'zero_t' parameters, respectively, defined in stretch_tool_share/python/stretch_tool_share/reactorx_wrist_v1/params.py . Note that your changes will not be applied unless you override the system version of the Python package hello-robot-stretch-tool-share with a local install. To do this, execute the file stretch_tool_share/python/local_install.sh by navigating to the stretch_tool_share/python directory and running: ./ local_install.sh","title":"Modifying Reactor X Wrist Parameters"},{"location":"stretch-tool-share/tool_share/stretch_2_STEP/","text":"Stretch 2 Model Created by : Hello Robot Inc This is a STEP model of the Stretch 2 robot (Nina batch). It includes a poseable SolidWorks SLDASM of the mated STEP files for both the standard Stretch 2 and the DexWrist Stretch 2 . It can assist the design of your own tools for your Stretch. Files CAD","title":"Index"},{"location":"stretch-tool-share/tool_share/stretch_2_STEP/#stretch-2-model","text":"Created by : Hello Robot Inc This is a STEP model of the Stretch 2 robot (Nina batch). It includes a poseable SolidWorks SLDASM of the mated STEP files for both the standard Stretch 2 and the DexWrist Stretch 2 . It can assist the design of your own tools for your Stretch. Files CAD","title":"Stretch 2 Model"},{"location":"stretch-tool-share/tool_share/stretch_RE1_arm/","text":"Stretch RE1 Arm Model Created by : Hello Robot Inc This is an STL model of the Stretch RE1 arm (retracted) including the end-of-arm interface. It can assist the design of your own tools for your Stretch. Files Stretch_RE1_Arm.STL","title":"Stretch RE1 Arm"},{"location":"stretch-tool-share/tool_share/stretch_RE1_arm/#stretch-re1-arm-model","text":"Created by : Hello Robot Inc This is an STL model of the Stretch RE1 arm (retracted) including the end-of-arm interface. It can assist the design of your own tools for your Stretch. Files Stretch_RE1_Arm.STL","title":"Stretch RE1 Arm Model"},{"location":"stretch-tool-share/tool_share/stretch_RE1_head/","text":"Stretch RE1 Head Model Created by : Hello Robot Inc These are STL models of the Stretch RE1 head shells. They can assist the design of your own tools to attach to Stretch's head. These shells can be printed in PLA using an FDM printer. The heat set inserts use McMaster Carr part number 94180A321. Files Stretch_RE1_Head_Left.STL Stretch_RE1_Head_Right.STL Stretch_RE1_Head_Camera.STL","title":"Stretch RE1 Head"},{"location":"stretch-tool-share/tool_share/stretch_RE1_head/#stretch-re1-head-model","text":"Created by : Hello Robot Inc These are STL models of the Stretch RE1 head shells. They can assist the design of your own tools to attach to Stretch's head. These shells can be printed in PLA using an FDM printer. The heat set inserts use McMaster Carr part number 94180A321. Files Stretch_RE1_Head_Left.STL Stretch_RE1_Head_Right.STL Stretch_RE1_Head_Camera.STL","title":"Stretch RE1 Head Model"},{"location":"stretch-tool-share/tool_share/stretch_dex_wrist/","text":"Stretch Dex Wrist Created by : Hello Robot Inc The Stretch Dex Wrist is commercially available from Hello Robot. The following hardware guides are available: RE1 DexWrist Hardware Guide RE2 DexWrist Hardware Guide Additional resources available here include: Gazebo Support URDF","title":"Stretch Dex Wrist"},{"location":"stretch-tool-share/tool_share/stretch_dex_wrist/#stretch-dex-wrist","text":"Created by : Hello Robot Inc The Stretch Dex Wrist is commercially available from Hello Robot. The following hardware guides are available: RE1 DexWrist Hardware Guide RE2 DexWrist Hardware Guide Additional resources available here include: Gazebo Support URDF","title":"Stretch Dex Wrist"},{"location":"stretch-tool-share/tool_share/stretch_dex_wrist/gazebo_support/","text":"Stretch Dex Wrist - Gazebo Support Created by : Hello Robot Inc The Stretch RE1 robot with the Dex Wrist can also be simulated with Gazebo simulator. The information on Stretch robot's Gazebo implementation can be found here stretch_gazebo . To add the Dex Wrist to Stretch with Gazebo support: cd ~/catkin_ws/src/stretch_ros/ git pull cd ~/repos git clone https://github.com/hello-robot/stretch_tool_share cd stretch_tool_share/tool_share/stretch_dex_wrist cp stretch_description/urdf/stretch_dex_wrist.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp stretch_description/urdf/stretch_description.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp stretch_description/meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes cp gazebo_support/stretch_gazebo.urdf.xacro ~/catkin_ws/src/stretch_ros/stretch_gazebo/urdf cp gazebo_support/dex_wrist.yaml ~/catkin_ws/src/stretch_ros/stretch_gazebo/config cp gazebo_support/cp gazebo.launch ~/catkin_ws/src/stretch_ros/stretch_gazebo/launch During the Gazebo simulation with Dex Wrist, the wrist's Roll and Pitch can be controlled using the spawned \"stretch_dex_wrist_controller\" of type position_controllers/JointTrajectoryController Note : Still there is no planned support to run MoveIt with Stretch Dex Wrist in Gazebo.","title":"Index"},{"location":"stretch-tool-share/tool_share/stretch_dex_wrist/gazebo_support/#stretch-dex-wrist-gazebo-support","text":"Created by : Hello Robot Inc The Stretch RE1 robot with the Dex Wrist can also be simulated with Gazebo simulator. The information on Stretch robot's Gazebo implementation can be found here stretch_gazebo . To add the Dex Wrist to Stretch with Gazebo support: cd ~/catkin_ws/src/stretch_ros/ git pull cd ~/repos git clone https://github.com/hello-robot/stretch_tool_share cd stretch_tool_share/tool_share/stretch_dex_wrist cp stretch_description/urdf/stretch_dex_wrist.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp stretch_description/urdf/stretch_description.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp stretch_description/meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes cp gazebo_support/stretch_gazebo.urdf.xacro ~/catkin_ws/src/stretch_ros/stretch_gazebo/urdf cp gazebo_support/dex_wrist.yaml ~/catkin_ws/src/stretch_ros/stretch_gazebo/config cp gazebo_support/cp gazebo.launch ~/catkin_ws/src/stretch_ros/stretch_gazebo/launch During the Gazebo simulation with Dex Wrist, the wrist's Roll and Pitch can be controlled using the spawned \"stretch_dex_wrist_controller\" of type position_controllers/JointTrajectoryController Note : Still there is no planned support to run MoveIt with Stretch Dex Wrist in Gazebo.","title":"Stretch Dex Wrist - Gazebo Support"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/","text":"Stretch Docking Station Created by : Hello Robot Inc Overview The Stretch Docking Station provides automated charging of Stretch. A 110mm Aruco tag on the docking station allows the robot to accomplish visually guided docking. Hardware Coming soon. Assembly Coming soon. Software Coming soon. ROS Support URDF The Stretch Docking Station, can be included in Gazebo and RViz. The URDF data is available on the Stretch Tool Share . x Gazebo Coming soon.","title":"Stretch Docking Station"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#stretch-docking-station","text":"Created by : Hello Robot Inc","title":"Stretch Docking Station"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#overview","text":"The Stretch Docking Station provides automated charging of Stretch. A 110mm Aruco tag on the docking station allows the robot to accomplish visually guided docking.","title":"Overview"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#hardware","text":"Coming soon.","title":"Hardware"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#assembly","text":"Coming soon.","title":"Assembly"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#software","text":"Coming soon.","title":"Software"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#ros-support","text":"","title":"ROS Support"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#urdf","text":"The Stretch Docking Station, can be included in Gazebo and RViz. The URDF data is available on the Stretch Tool Share . x","title":"URDF"},{"location":"stretch-tool-share/tool_share/stretch_docking_station/#gazebo","text":"Coming soon.","title":"Gazebo"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/","text":"Stretch Teleop Kit Created by : Hello Robot Inc Overview The Stretch Teleop Kit allows for improved remote teleoperation of Stretch. It adds two fish-eye USB cameras to Stretch. One is added to the robot's gripper for a better view while manipulating. The other is added to the robot's head and points downward, providing a better view while navigating. Hardware The Stretch Teleop Kit uses two Spinel UC20MPE_F185 USB cameras that provide a 185 degree FOV and 2MP resolution. These board cameras are mounted in 3D printed shells and attached to existing mount points of the Stretch. Assembly Hello Robot has provided the STL files, BOM, and assembly instructions necessary to build your own Stretch Teleop Kit. Alternatively, the kit is available for sale by Hello Robot . * Note if building your own kit: : The Spinel cameras come with a custom USB to JST ZH cable. We recommend using a custom length cable however in order to improve the cable routing of your system. Please contact Hello Robot for details. 3D Printed Parts 3DP-808_Teleop_Camera_Mount_Front.STL 3DP-809_Head_Teleop_Mount_Back.STL 3DP-810_Head_Teleop_Mount.STL 3DP-811_Gripper_Teleop_Mount_Back.STL 3DP-812_Gripper_Teleop_Mount_Bottom.STL Software Teleoperation Interface The Stretch Teleop Kit includes the open-source teleoperation interface (beta) that utilizes these cameras as shown below. ROS Support URDF The Stretch RE1 URDF can be augmented with these two cameras as well. The URDF information is found here . To add the Teleop Kit to your URDF: cd ~/catkin_ws/src/stretch_ros/ git pull cd ~/repos git clone https://github.com/hello-robot/stretch_tool_share cd ~/repos/stretch_tool_share/tool_share/stretch_teleop_kit/stretch_description cp urdf/stretch_teleop_kit.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp urdf/stretch_description.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp meshes/*teleop*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes Gazebo The Stretch RE1 robot with the Teleop Kit can also be simulated with Gazebo simulator. The information on Stretch robot's Gazebo implementation can be found here stretch_gazebo . To add Teleop kit to the Stretch Gazebo implementation: cd ~/catkin_ws/src/stretch_ros/ git pull cd ~/repos git clone https://github.com/hello-robot/stretch_tool_share cd ~/repos/stretch_tool_share/tool_share/stretch_teleop_kit/stretch_description cp urdf/stretch_teleop_kit.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp urdf/stretch_gazebo.urdf.xacro ~/catkin_ws/src/stretch_ros/stretch_gazebo/urdf/stretch_gazebo.urdf.xacro cp meshes/*teleop*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes During Gazebo simulation the two Teleop camera's video streams would be published to the topics \"teleop/gripper_camera\" and \"teleop/head_camera\".","title":"Stretch Teleop Kit"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#stretch-teleop-kit","text":"Created by : Hello Robot Inc","title":"Stretch Teleop Kit"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#overview","text":"The Stretch Teleop Kit allows for improved remote teleoperation of Stretch. It adds two fish-eye USB cameras to Stretch. One is added to the robot's gripper for a better view while manipulating. The other is added to the robot's head and points downward, providing a better view while navigating.","title":"Overview"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#hardware","text":"The Stretch Teleop Kit uses two Spinel UC20MPE_F185 USB cameras that provide a 185 degree FOV and 2MP resolution. These board cameras are mounted in 3D printed shells and attached to existing mount points of the Stretch.","title":"Hardware"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#assembly","text":"Hello Robot has provided the STL files, BOM, and assembly instructions necessary to build your own Stretch Teleop Kit. Alternatively, the kit is available for sale by Hello Robot . * Note if building your own kit: : The Spinel cameras come with a custom USB to JST ZH cable. We recommend using a custom length cable however in order to improve the cable routing of your system. Please contact Hello Robot for details. 3D Printed Parts 3DP-808_Teleop_Camera_Mount_Front.STL 3DP-809_Head_Teleop_Mount_Back.STL 3DP-810_Head_Teleop_Mount.STL 3DP-811_Gripper_Teleop_Mount_Back.STL 3DP-812_Gripper_Teleop_Mount_Bottom.STL","title":"Assembly"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#software","text":"","title":"Software"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#teleoperation-interface","text":"The Stretch Teleop Kit includes the open-source teleoperation interface (beta) that utilizes these cameras as shown below.","title":"Teleoperation Interface"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#ros-support","text":"","title":"ROS Support"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#urdf","text":"The Stretch RE1 URDF can be augmented with these two cameras as well. The URDF information is found here . To add the Teleop Kit to your URDF: cd ~/catkin_ws/src/stretch_ros/ git pull cd ~/repos git clone https://github.com/hello-robot/stretch_tool_share cd ~/repos/stretch_tool_share/tool_share/stretch_teleop_kit/stretch_description cp urdf/stretch_teleop_kit.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp urdf/stretch_description.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp meshes/*teleop*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes","title":"URDF"},{"location":"stretch-tool-share/tool_share/stretch_teleop_kit/#gazebo","text":"The Stretch RE1 robot with the Teleop Kit can also be simulated with Gazebo simulator. The information on Stretch robot's Gazebo implementation can be found here stretch_gazebo . To add Teleop kit to the Stretch Gazebo implementation: cd ~/catkin_ws/src/stretch_ros/ git pull cd ~/repos git clone https://github.com/hello-robot/stretch_tool_share cd ~/repos/stretch_tool_share/tool_share/stretch_teleop_kit/stretch_description cp urdf/stretch_teleop_kit.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp urdf/stretch_gazebo.urdf.xacro ~/catkin_ws/src/stretch_ros/stretch_gazebo/urdf/stretch_gazebo.urdf.xacro cp meshes/*teleop*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes During Gazebo simulation the two Teleop camera's video streams would be published to the topics \"teleop/gripper_camera\" and \"teleop/head_camera\".","title":"Gazebo"},{"location":"stretch-tool-share/tool_share/swiffer_mount_V1/","text":"Swiffer Mount Created by : Hello Robot Inc This tool allows a Swiffer duster to your Stretch. The designs uses a clamp to hold the Swiffer handle in place, allowing the height and orientation of the Swiffer to be adjusted by loosening the clamp. Parts List Item Qty Vendor M5x50mm BHCS 3 McMaster-Carr M5 nut 4 McMaster-Carr swiffer_mount.STL 1 PLA 3D printer swiffer_mount_clamp.STL 1 PLA 3D printer wrist_end_cap_5mm.STL 1 PLA 3D printer Swiffer Sweeper Cleaner Dry and Wet Mop Kit 1 Amazon Assembly instructions View 3D Assembly Attach the M5 bolt through the swiffer_mount_primary part and lock into place by tightening the M5 nut Attach swiffer_mount_primary to the tool plate using the wrist_end_cap_5mm and a second M5 nut. Friction holds the assembly in place against the tool plate. Attach the swiffer_mount_clamp part using two M5 bolts and nuts, clamping the shaft of the Swiffer handle in place. Adjust the length and orientation of the Swiffer and clamp in place. Clean away!","title":"Swiffer Mount"},{"location":"stretch-tool-share/tool_share/swiffer_mount_V1/#swiffer-mount","text":"Created by : Hello Robot Inc This tool allows a Swiffer duster to your Stretch. The designs uses a clamp to hold the Swiffer handle in place, allowing the height and orientation of the Swiffer to be adjusted by loosening the clamp.","title":"Swiffer Mount"},{"location":"stretch-tool-share/tool_share/swiffer_mount_V1/#parts-list","text":"Item Qty Vendor M5x50mm BHCS 3 McMaster-Carr M5 nut 4 McMaster-Carr swiffer_mount.STL 1 PLA 3D printer swiffer_mount_clamp.STL 1 PLA 3D printer wrist_end_cap_5mm.STL 1 PLA 3D printer Swiffer Sweeper Cleaner Dry and Wet Mop Kit 1 Amazon","title":"Parts List"},{"location":"stretch-tool-share/tool_share/swiffer_mount_V1/#assembly-instructions","text":"View 3D Assembly Attach the M5 bolt through the swiffer_mount_primary part and lock into place by tightening the M5 nut Attach swiffer_mount_primary to the tool plate using the wrist_end_cap_5mm and a second M5 nut. Friction holds the assembly in place against the tool plate. Attach the swiffer_mount_clamp part using two M5 bolts and nuts, clamping the shaft of the Swiffer handle in place. Adjust the length and orientation of the Swiffer and clamp in place. Clean away!","title":"Assembly instructions"},{"location":"stretch-tool-share/tool_share/tray_cup_holder_V1/","text":"Tray Cup Holder Created by : Hello Robot Inc This is a tray and cup holder attachment for your Stretch. The tray stows nicely within the robot footprint during navigation and can un-stow upon delivery to a person. The 3D printed attachment clips easily onto the bottom of an off the shelf tray. Parts List Item Qty Vendor M5 Nut 2 McMaster-Carr M5x50mm Hex Head Bolt 1 McMaster-Carr wrist_end_cap_5mm.STL 1 PLA 3D printer tray_clip.STL 1 PLA 3D printer AUTUT Universal Car Cup Holder 1 Amazon Assembly instructions View 3D assembly Install the M5 bolt into the tray clip and secure it rigidly to the part using the M5 nut. Clip the tray onto the tray clip. Drop the tray clip assembly on to the tool plate from above. Secure it to the tool plate using the wrist_end_cap_5mm and M5 nut.","title":"Tray Cup Holder"},{"location":"stretch-tool-share/tool_share/tray_cup_holder_V1/#tray-cup-holder","text":"Created by : Hello Robot Inc This is a tray and cup holder attachment for your Stretch. The tray stows nicely within the robot footprint during navigation and can un-stow upon delivery to a person. The 3D printed attachment clips easily onto the bottom of an off the shelf tray.","title":"Tray Cup Holder"},{"location":"stretch-tool-share/tool_share/tray_cup_holder_V1/#parts-list","text":"Item Qty Vendor M5 Nut 2 McMaster-Carr M5x50mm Hex Head Bolt 1 McMaster-Carr wrist_end_cap_5mm.STL 1 PLA 3D printer tray_clip.STL 1 PLA 3D printer AUTUT Universal Car Cup Holder 1 Amazon","title":"Parts List"},{"location":"stretch-tool-share/tool_share/tray_cup_holder_V1/#assembly-instructions","text":"View 3D assembly Install the M5 bolt into the tray clip and secure it rigidly to the part using the M5 nut. Clip the tray onto the tray clip. Drop the tray clip assembly on to the tool plate from above. Secure it to the tool plate using the wrist_end_cap_5mm and M5 nut.","title":"Assembly instructions"},{"location":"stretch-tool-share/tool_share/wrist_USB_board_camera/","text":"Wrist USB Board Camera Create by : Hello Robot Inc This design allows you to attach a USB board camera to the wrist yaw joint of Stretch. The wrist mounted camera can pan left-right, making it perfect for simple remote inspection tasks. The camera can be integrated into your robot URDF by integrating its stretch_description as described in the Stretch ROS documentation . Parts List Item Qty Vendor M2x8mm SHCS 8 McMaster-Carr M2x6mm Thread Forming Screw 5 McMaster-Carr Board_Camera_Ball_Shell.STL 1 PLA 3D printer Board_Camera_Ball_Cover.STL 1 PLA 3D printer ELP 2MP USB Board Camera (Suggested) 1 Amazon / Spinel Assembly instructions View 3D assembly Attach the Board_Camera_Ball_Shell to the wrist tool plate using the 8 M2 bolts Attach the camera to the Board_Camera_Ball_Shell using 4 self-threading screws Attach the USB cable and route it down and through the wrist yaw passage Attach the Boad_Camera_Ball_Cover using 1 self-threading screw Plug the USB cable into the USB-A port on the wrist NOTE: The USB cable that comes with the camera uses a JST-PH connector. You may want to make your own cable with a custom length using a right-angle USB-A cable. This may require a JST-PH crimp tool. NOTE: A variety of USB board cameras are available from ELP and Spinel, among others. The shells may need to be modified to accommodate variations in the mechanical packaging of these cameras.","title":"Wrist USB Camera"},{"location":"stretch-tool-share/tool_share/wrist_USB_board_camera/#wrist-usb-board-camera","text":"Create by : Hello Robot Inc This design allows you to attach a USB board camera to the wrist yaw joint of Stretch. The wrist mounted camera can pan left-right, making it perfect for simple remote inspection tasks. The camera can be integrated into your robot URDF by integrating its stretch_description as described in the Stretch ROS documentation .","title":"Wrist USB Board Camera"},{"location":"stretch-tool-share/tool_share/wrist_USB_board_camera/#parts-list","text":"Item Qty Vendor M2x8mm SHCS 8 McMaster-Carr M2x6mm Thread Forming Screw 5 McMaster-Carr Board_Camera_Ball_Shell.STL 1 PLA 3D printer Board_Camera_Ball_Cover.STL 1 PLA 3D printer ELP 2MP USB Board Camera (Suggested) 1 Amazon / Spinel","title":"Parts List"},{"location":"stretch-tool-share/tool_share/wrist_USB_board_camera/#assembly-instructions","text":"View 3D assembly Attach the Board_Camera_Ball_Shell to the wrist tool plate using the 8 M2 bolts Attach the camera to the Board_Camera_Ball_Shell using 4 self-threading screws Attach the USB cable and route it down and through the wrist yaw passage Attach the Boad_Camera_Ball_Cover using 1 self-threading screw Plug the USB cable into the USB-A port on the wrist NOTE: The USB cable that comes with the camera uses a JST-PH connector. You may want to make your own cable with a custom length using a right-angle USB-A cable. This may require a JST-PH crimp tool. NOTE: A variety of USB board cameras are available from ELP and Spinel, among others. The shells may need to be modified to accommodate variations in the mechanical packaging of these cameras.","title":"Assembly instructions"},{"location":"stretch-tutorials/","text":"Overview The Stretch Tutorials repository provides tutorials on programming Stretch robots. The tutorials are organized into the following tracks. Tutorial Track Description Getting to Know Stretch Everything a new user of Stretch needs to get started Stretch Body Python SDK Learn how to program Stretch using its low level Python interface ROS Learn how to program Stretch using its ROS interface ROS 2 (Beta) Learn how to program Stretch using its ROS2 interface Stretch Tool Share Learn how to update the end of arm tool hardware All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Overview"},{"location":"stretch-tutorials/#overview","text":"The Stretch Tutorials repository provides tutorials on programming Stretch robots. The tutorials are organized into the following tracks. Tutorial Track Description Getting to Know Stretch Everything a new user of Stretch needs to get started Stretch Body Python SDK Learn how to program Stretch using its low level Python interface ROS Learn how to program Stretch using its ROS interface ROS 2 (Beta) Learn how to program Stretch using its ROS2 interface Stretch Tool Share Learn how to update the end of arm tool hardware All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Overview"},{"location":"stretch-tutorials/tutorial_template/","text":"Motivation The aim of this document is to be the starting point for generating new ROS tutorials and to maintain consistency in structure and tone across the documentation. It starts off by formalizing the key objectives and then goes on to templatize the structure and tone that we should adopt going forward. Objectives Be inclusive of all levels of understanding - start from the basics and link relevant external content to be concise yet informative Reinforce that getting started with Stretch is a breeze - use easy to understand vocab and a friendly tone to encourage readers Encourage users to read other tutorials - wherever possible, link other tutorials from the documentation to convey completeness and complexity Have a clear flow - start with the theory, show with GIFs what to expect, and then breakdown the code What follows can be considered the template. Major Topic In this tutorial, we will work with Stretch to explore the main theme of tutorial using primary module and also learn how to achieve secondory theme . If you want to know more about any previously covered topic on Stretch and how to get it up and running, we recommend visiting the previous tutorials on link to topic and link to topic . Motivation for the problem the topic solves . The great thing about Stretch is that it comes preloaded with software that makes it a breeze to achieve theme of the tutorial . By the end of this tutorial, we will have a clear idea about how first minor topic works and how to use it to achieve second minor topic with Stretch. Let's jump in! First Minor Topic Title PyTorch is an open source end-to-end machine learning framework that makes many pretrained production quality neural networks available for general use. In this tutorial we will use the YOLOv5s model trained on the COCO dataset. YOLOv5 is a popular object detection model that divides a supplied image into a grid and detects objects in each cell of the grid recursively. The YOLOv5s model that we have deployed on Stretch has been pretrained on the COCO dataset which allows Stretch to detect a wide range of day to day objects. However, that\u2019s not all, in this demo we want to go a step further and use this extremely versatile object detection model to extract useful information about the scene. Second Minor Topic Title Now, let\u2019s use what we have learned so far to upgrade the collision avoidance demo in a way that Stretch is able to scan an entire room autonomously without bumping into things or people. To account for dynamic obstacles getting too close to the robot, we will define a keepout distance of 0.4 m - detections below this value stop the robot. To keep Stretch from getting too close to static obstacles, we will define another variable called turning distance of 0.75 m - frontal detections below this value make Stretch turn to the left until it sees a clear path ahead. Building up on this, let's implement a simple logic for obstacle avoidance. The logic can be broken down into three steps: 1. If the minimum value from the frontal scans is greater than 0.75 m, then continue to move forward 2. If the minimum value from the frontal scans is less than 0.75 m, then turn to the right until this is no longer true 3. If the minimum value from the overall scans is less than 0.4 m, then stop the robot Third Minor Topic Title If a tutorial covers more than two minor topics, it might be a good idea to break it down into multiple tutorials Warning Running this tutorial on Stretch might result in harm to robot, humans or the surrounding environment . Please ensure these conditions . We recommend taking these actions to ensure safe operation. See It In Action Go ahead and execute the following commands to run the demo and visualize the result in RViz: Terminal 1: Enter first command here Terminal 2: Enter second command here Enter GIF to show robot behavior Enter GIF to show robot and sensor visualization in RViz Code Breakdown Now, let's jump into the code to see how things work under the hood. Follow along link to code to have a look at the entire script. We make use of two separate Python classes for this demo. The FrameListener class is derived from the Node class and is the place where we compute the TF transformations. For an explantion of this class, you can refer to the TF listener tutorial. class FrameListener ( Node ): The AlignToAruco class is where we command Stretch to the pose goal. This class is derived from the FrameListener class so that they can both share the node instance. class AlignToAruco ( FrameListener ): The constructor initializes the Joint trajectory action client. It also initializes the attribute called offset that determines the end distance between the marker and the robot. def __init__ ( self , node , offset = 0.75 ): self . trans_base = TransformStamped () self . trans_camera = TransformStamped () self . joint_state = JointState () self . offset = offset self . node = node self . trajectory_client = ActionClient ( self . node , FollowJointTrajectory , '/stretch_controller/follow_joint_trajectory' ) server_reached = self . trajectory_client . wait_for_server ( timeout_sec = 60.0 ) if not server_reached : self . node . get_logger () . error ( 'Unable to connect to arm action server. Timeout exceeded.' ) sys . exit () The apply_to_image() method passes the stream of RGB images from the realsense camera to the YOLOv5s model and returns detections in the form of a dictionary consisting of class_id, label, confidence and bouding box coordinates. The last part is exactly what we need for further computations. def apply_to_image ( self , rgb_image , draw_output = False ): results = self . model ( rgb_image ) ... if draw_output : output_image = rgb_image . copy () for detection_dict in results : self . draw_detection ( output_image , detection_dict ) return results , output_image Motivate users to play with the code or continue exploring more topics . Now go ahead and experiment with a few more pretrained models using PyTorch or OpenVINO on Stretch. If you are feeling extra motivated, try creating your own neural networks and training them. Stretch is ready to deploy them!","title":"Motivation"},{"location":"stretch-tutorials/tutorial_template/#motivation","text":"The aim of this document is to be the starting point for generating new ROS tutorials and to maintain consistency in structure and tone across the documentation. It starts off by formalizing the key objectives and then goes on to templatize the structure and tone that we should adopt going forward.","title":"Motivation"},{"location":"stretch-tutorials/tutorial_template/#objectives","text":"Be inclusive of all levels of understanding - start from the basics and link relevant external content to be concise yet informative Reinforce that getting started with Stretch is a breeze - use easy to understand vocab and a friendly tone to encourage readers Encourage users to read other tutorials - wherever possible, link other tutorials from the documentation to convey completeness and complexity Have a clear flow - start with the theory, show with GIFs what to expect, and then breakdown the code What follows can be considered the template.","title":"Objectives"},{"location":"stretch-tutorials/tutorial_template/#major-topic","text":"In this tutorial, we will work with Stretch to explore the main theme of tutorial using primary module and also learn how to achieve secondory theme . If you want to know more about any previously covered topic on Stretch and how to get it up and running, we recommend visiting the previous tutorials on link to topic and link to topic . Motivation for the problem the topic solves . The great thing about Stretch is that it comes preloaded with software that makes it a breeze to achieve theme of the tutorial . By the end of this tutorial, we will have a clear idea about how first minor topic works and how to use it to achieve second minor topic with Stretch. Let's jump in!","title":"Major Topic"},{"location":"stretch-tutorials/tutorial_template/#first-minor-topic-title","text":"PyTorch is an open source end-to-end machine learning framework that makes many pretrained production quality neural networks available for general use. In this tutorial we will use the YOLOv5s model trained on the COCO dataset. YOLOv5 is a popular object detection model that divides a supplied image into a grid and detects objects in each cell of the grid recursively. The YOLOv5s model that we have deployed on Stretch has been pretrained on the COCO dataset which allows Stretch to detect a wide range of day to day objects. However, that\u2019s not all, in this demo we want to go a step further and use this extremely versatile object detection model to extract useful information about the scene.","title":"First Minor Topic Title"},{"location":"stretch-tutorials/tutorial_template/#second-minor-topic-title","text":"Now, let\u2019s use what we have learned so far to upgrade the collision avoidance demo in a way that Stretch is able to scan an entire room autonomously without bumping into things or people. To account for dynamic obstacles getting too close to the robot, we will define a keepout distance of 0.4 m - detections below this value stop the robot. To keep Stretch from getting too close to static obstacles, we will define another variable called turning distance of 0.75 m - frontal detections below this value make Stretch turn to the left until it sees a clear path ahead. Building up on this, let's implement a simple logic for obstacle avoidance. The logic can be broken down into three steps: 1. If the minimum value from the frontal scans is greater than 0.75 m, then continue to move forward 2. If the minimum value from the frontal scans is less than 0.75 m, then turn to the right until this is no longer true 3. If the minimum value from the overall scans is less than 0.4 m, then stop the robot","title":"Second Minor Topic Title"},{"location":"stretch-tutorials/tutorial_template/#third-minor-topic-title","text":"If a tutorial covers more than two minor topics, it might be a good idea to break it down into multiple tutorials","title":"Third Minor Topic Title"},{"location":"stretch-tutorials/tutorial_template/#warning","text":"Running this tutorial on Stretch might result in harm to robot, humans or the surrounding environment . Please ensure these conditions . We recommend taking these actions to ensure safe operation.","title":"Warning"},{"location":"stretch-tutorials/tutorial_template/#see-it-in-action","text":"Go ahead and execute the following commands to run the demo and visualize the result in RViz: Terminal 1: Enter first command here Terminal 2: Enter second command here Enter GIF to show robot behavior Enter GIF to show robot and sensor visualization in RViz","title":"See It In Action"},{"location":"stretch-tutorials/tutorial_template/#code-breakdown","text":"Now, let's jump into the code to see how things work under the hood. Follow along link to code to have a look at the entire script. We make use of two separate Python classes for this demo. The FrameListener class is derived from the Node class and is the place where we compute the TF transformations. For an explantion of this class, you can refer to the TF listener tutorial. class FrameListener ( Node ): The AlignToAruco class is where we command Stretch to the pose goal. This class is derived from the FrameListener class so that they can both share the node instance. class AlignToAruco ( FrameListener ): The constructor initializes the Joint trajectory action client. It also initializes the attribute called offset that determines the end distance between the marker and the robot. def __init__ ( self , node , offset = 0.75 ): self . trans_base = TransformStamped () self . trans_camera = TransformStamped () self . joint_state = JointState () self . offset = offset self . node = node self . trajectory_client = ActionClient ( self . node , FollowJointTrajectory , '/stretch_controller/follow_joint_trajectory' ) server_reached = self . trajectory_client . wait_for_server ( timeout_sec = 60.0 ) if not server_reached : self . node . get_logger () . error ( 'Unable to connect to arm action server. Timeout exceeded.' ) sys . exit () The apply_to_image() method passes the stream of RGB images from the realsense camera to the YOLOv5s model and returns detections in the form of a dictionary consisting of class_id, label, confidence and bouding box coordinates. The last part is exactly what we need for further computations. def apply_to_image ( self , rgb_image , draw_output = False ): results = self . model ( rgb_image ) ... if draw_output : output_image = rgb_image . copy () for detection_dict in results : self . draw_detection ( output_image , detection_dict ) return results , output_image Motivate users to play with the code or continue exploring more topics . Now go ahead and experiment with a few more pretrained models using PyTorch or OpenVINO on Stretch. If you are feeling extra motivated, try creating your own neural networks and training them. Stretch is ready to deploy them!","title":"Code Breakdown"},{"location":"stretch-tutorials/getting_started/","text":"Tutorial Track: Getting to Know Stretch New to Stretch? Welcome! Please take the time to get to know your robot by going through these tutorials in order. What Version of Robot Do I Have? Stretch RE1 and Stretch 2 are very similar. One quick way to tell the difference is to look at the robot's hostname: >>$ hostname stretch-re2-2001 Another way is to look for the distinctive pink stripe on the base of Stretch 2: Basics Tutorial Description 1 Safety Guide Guide to safe operation of the Stretch 2 Quick Start RE1 Unboxing Stretch RE1 and getting started 2 Quick Start Stretch 2 Unboxing Stretch 2 and getting started 3 Best Practices Best practices to keep Stretch charged and healthy 4 Troubleshooting Solutions to common issues Advanced Tutorial Description 1 Untethered Operation Setting up your network to work with Stretch untethered 2 Updating Software How to periodically update the Stretch software All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Overview"},{"location":"stretch-tutorials/getting_started/#tutorial-track-getting-to-know-stretch","text":"New to Stretch? Welcome! Please take the time to get to know your robot by going through these tutorials in order.","title":"Tutorial Track: Getting to Know Stretch"},{"location":"stretch-tutorials/getting_started/#what-version-of-robot-do-i-have","text":"Stretch RE1 and Stretch 2 are very similar. One quick way to tell the difference is to look at the robot's hostname: >>$ hostname stretch-re2-2001 Another way is to look for the distinctive pink stripe on the base of Stretch 2:","title":"What Version of Robot Do I Have?"},{"location":"stretch-tutorials/getting_started/#basics","text":"Tutorial Description 1 Safety Guide Guide to safe operation of the Stretch 2 Quick Start RE1 Unboxing Stretch RE1 and getting started 2 Quick Start Stretch 2 Unboxing Stretch 2 and getting started 3 Best Practices Best practices to keep Stretch charged and healthy 4 Troubleshooting Solutions to common issues","title":"Basics"},{"location":"stretch-tutorials/getting_started/#advanced","text":"Tutorial Description 1 Untethered Operation Setting up your network to work with Stretch untethered 2 Updating Software How to periodically update the Stretch software All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Advanced"},{"location":"stretch-tutorials/getting_started/best_practices/","text":"Stretch Best Practices Keeping the Robot Charged Keeping Stretch charged is important to the long-term health of its batteries - and to the success of any project you develop on Stretch. Please read carefully the following guides and adopt their best-practices for battery health. Stretch RE1 Stretch 2 RE1 Battery Maintenance Guide Stretch 2 Battery Maintenance Guide Keeping the Robot Healthy While Stretch can tolerate a fair amount of misuse, the following best practices can help it achieve a long and healthy life. Video Description Stretch Best Practices - Powered Off Video How to work with Stretch when its power is off Stretch Best Practices - Powered On Video How to work with Stretch when its power is on All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Best Practices"},{"location":"stretch-tutorials/getting_started/best_practices/#stretch-best-practices","text":"","title":"Stretch Best Practices"},{"location":"stretch-tutorials/getting_started/best_practices/#keeping-the-robot-charged","text":"Keeping Stretch charged is important to the long-term health of its batteries - and to the success of any project you develop on Stretch. Please read carefully the following guides and adopt their best-practices for battery health. Stretch RE1 Stretch 2 RE1 Battery Maintenance Guide Stretch 2 Battery Maintenance Guide","title":"Keeping the Robot Charged"},{"location":"stretch-tutorials/getting_started/best_practices/#keeping-the-robot-healthy","text":"While Stretch can tolerate a fair amount of misuse, the following best practices can help it achieve a long and healthy life. Video Description Stretch Best Practices - Powered Off Video How to work with Stretch when its power is off Stretch Best Practices - Powered On Video How to work with Stretch when its power is on All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Keeping the Robot Healthy"},{"location":"stretch-tutorials/getting_started/command_line_tools/","text":"Command Line Tools The Stretch robot comes with a set of command line tools that are helpful for introspection during general use or while troubleshooting issues. This page provides an overview of these tools. If you like, visit the stretch_body repository to have a look under the hood. You can execute these commands from anywhere in the terminal. We recommend you execute these commands as we follow each one of them. You can also find the description for the utility each tool provides by passing the optional '-h' flag along with the tool name in the terminal. For example, from anywhere in the terminal execute: stretch_about.py -h System Information stretch_about.py This tool displays the model and serial number information as an image. stretch_about_text.py This tool displays the model and serial number information as text. stretch_version.sh This script prints the version information for various software packages on the robot. stretch_params.py This tool prints the Stretch parameters to the console. stretch_robot_monitor.py This tool runs the Robot Monitor and prints to the console. stretch_robot_urdf_visualizer.py This tool allows you to visualize robot URDF. Introspection stretch_robot_system_check.py This tool checks that all robot hardware is present and is reporting sane values. stretch_robot_battery_check.py This is a tool to print the battery state to the console. stretch_hardware_echo.py This tool echoes the robot and computer hardware details to the console. stretch_robot_dynamixel_reboot.py This tool reboots all Dynamixel servos on the robot. stretch_pimu_scope.py This tool allows you to visualize Pimu (Power+IMU) board data with an oscilloscope. Pass the '-h' flag along with the command to see optional arguments. stretch_wacc_scope.py This is a tool to visualize Wacc (Wrist+Accel) board data with an oscilloscope. Pass the '-h' flag along with the command to see optional arguments. stretch_realsense_visualizer.py This is a tool to test the Realsense D435i Camera. Pass the '-h' flag along with the command to see optional arguments. stretch_respeaker_test.py This tool allows you to record and playback audio via Respeaker. stretch_audio_test.py This tool allows you to test the audio system. Homing Joints stretch_robot_home.py This tool calibrates the robot by finding zeros for all robot joints. stretch_gripper_home.py This tool calibrates the gripper position by closing until the motion stops. stretch_wrist_yaw_home.py This tool calibrates the wrist_yaw position by moving to both hardstops. stretch_arm_home.py This tool calibrates arm position by moving to hardstop. stretch_lift_home.py This tool calibrates the lift position by moving to the upper hardstop. Jogging Joints stretch_robot_jog.py This tool prints all robot data to the console. stretch_gripper_jog.py This tool allows you to jog the griper from the keyboard. stretch_wrist_yaw_jog.py This tool allows you to jog the wrist_yaw joint from the keyboard. stretch_arm_jog.py This tool allows you to jog the arm motion from the keyboard. stretch_lift_jog.py This tool allows you to jog the lift motion from the keyboard. stretch_base_jog.py This tool allows you to jog the base motion from the keyboard. stretch_head_jog.py This tool allows you to jog the head from the keyboard. Jogging Modules stretch_wacc_jog.py This tool allows you to command and query the Wacc (Wrist Accelerometer) board from the keyboard. stretch_pimu_jog.py This tool allows you to command and query the Pimu (Power+IMU) board from the keyboard. stretch_rp_lidar_jog.py This is a tool to control the RP-Lidar. Pass the '-h' flag along with the command to see optional arguments. stretch_trajectory_jog.py This tool allows you to test out splined trajectories on the various joint from a GUI or text menu. Pass the '-h' flag along with the command to see optional arguments. Teleoperation stretch_robot_stow.py This tool moves the robot to stow position. stretch_robot_keyboard_teleop.py This tool allows you to control the robot base, lift, arm, head, and tool from the keyboard. stretch_xbox_controller_teleop.py This tool allows you to jog the robot from an Xbox Controller.","title":"Command Line Tools"},{"location":"stretch-tutorials/getting_started/command_line_tools/#command-line-tools","text":"The Stretch robot comes with a set of command line tools that are helpful for introspection during general use or while troubleshooting issues. This page provides an overview of these tools. If you like, visit the stretch_body repository to have a look under the hood. You can execute these commands from anywhere in the terminal. We recommend you execute these commands as we follow each one of them. You can also find the description for the utility each tool provides by passing the optional '-h' flag along with the tool name in the terminal. For example, from anywhere in the terminal execute: stretch_about.py -h","title":"Command Line Tools"},{"location":"stretch-tutorials/getting_started/command_line_tools/#system-information","text":"","title":"System Information"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_aboutpy","text":"This tool displays the model and serial number information as an image.","title":"stretch_about.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_about_textpy","text":"This tool displays the model and serial number information as text.","title":"stretch_about_text.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_versionsh","text":"This script prints the version information for various software packages on the robot.","title":"stretch_version.sh"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_paramspy","text":"This tool prints the Stretch parameters to the console.","title":"stretch_params.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_monitorpy","text":"This tool runs the Robot Monitor and prints to the console.","title":"stretch_robot_monitor.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_urdf_visualizerpy","text":"This tool allows you to visualize robot URDF.","title":"stretch_robot_urdf_visualizer.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#introspection","text":"","title":"Introspection"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_system_checkpy","text":"This tool checks that all robot hardware is present and is reporting sane values.","title":"stretch_robot_system_check.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_battery_checkpy","text":"This is a tool to print the battery state to the console.","title":"stretch_robot_battery_check.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_hardware_echopy","text":"This tool echoes the robot and computer hardware details to the console.","title":"stretch_hardware_echo.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_dynamixel_rebootpy","text":"This tool reboots all Dynamixel servos on the robot.","title":"stretch_robot_dynamixel_reboot.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_pimu_scopepy","text":"This tool allows you to visualize Pimu (Power+IMU) board data with an oscilloscope. Pass the '-h' flag along with the command to see optional arguments.","title":"stretch_pimu_scope.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_wacc_scopepy","text":"This is a tool to visualize Wacc (Wrist+Accel) board data with an oscilloscope. Pass the '-h' flag along with the command to see optional arguments.","title":"stretch_wacc_scope.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_realsense_visualizerpy","text":"This is a tool to test the Realsense D435i Camera. Pass the '-h' flag along with the command to see optional arguments.","title":"stretch_realsense_visualizer.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_respeaker_testpy","text":"This tool allows you to record and playback audio via Respeaker.","title":"stretch_respeaker_test.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_audio_testpy","text":"This tool allows you to test the audio system.","title":"stretch_audio_test.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#homing-joints","text":"","title":"Homing Joints"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_homepy","text":"This tool calibrates the robot by finding zeros for all robot joints.","title":"stretch_robot_home.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_gripper_homepy","text":"This tool calibrates the gripper position by closing until the motion stops.","title":"stretch_gripper_home.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_wrist_yaw_homepy","text":"This tool calibrates the wrist_yaw position by moving to both hardstops.","title":"stretch_wrist_yaw_home.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_arm_homepy","text":"This tool calibrates arm position by moving to hardstop.","title":"stretch_arm_home.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_lift_homepy","text":"This tool calibrates the lift position by moving to the upper hardstop.","title":"stretch_lift_home.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#jogging-joints","text":"","title":"Jogging Joints"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_jogpy","text":"This tool prints all robot data to the console.","title":"stretch_robot_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_gripper_jogpy","text":"This tool allows you to jog the griper from the keyboard.","title":"stretch_gripper_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_wrist_yaw_jogpy","text":"This tool allows you to jog the wrist_yaw joint from the keyboard.","title":"stretch_wrist_yaw_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_arm_jogpy","text":"This tool allows you to jog the arm motion from the keyboard.","title":"stretch_arm_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_lift_jogpy","text":"This tool allows you to jog the lift motion from the keyboard.","title":"stretch_lift_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_base_jogpy","text":"This tool allows you to jog the base motion from the keyboard.","title":"stretch_base_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_head_jogpy","text":"This tool allows you to jog the head from the keyboard.","title":"stretch_head_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#jogging-modules","text":"","title":"Jogging Modules"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_wacc_jogpy","text":"This tool allows you to command and query the Wacc (Wrist Accelerometer) board from the keyboard.","title":"stretch_wacc_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_pimu_jogpy","text":"This tool allows you to command and query the Pimu (Power+IMU) board from the keyboard.","title":"stretch_pimu_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_rp_lidar_jogpy","text":"This is a tool to control the RP-Lidar. Pass the '-h' flag along with the command to see optional arguments.","title":"stretch_rp_lidar_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_trajectory_jogpy","text":"This tool allows you to test out splined trajectories on the various joint from a GUI or text menu. Pass the '-h' flag along with the command to see optional arguments.","title":"stretch_trajectory_jog.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#teleoperation","text":"","title":"Teleoperation"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_stowpy","text":"This tool moves the robot to stow position.","title":"stretch_robot_stow.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_robot_keyboard_teleoppy","text":"This tool allows you to control the robot base, lift, arm, head, and tool from the keyboard.","title":"stretch_robot_keyboard_teleop.py"},{"location":"stretch-tutorials/getting_started/command_line_tools/#stretch_xbox_controller_teleoppy","text":"This tool allows you to jog the robot from an Xbox Controller.","title":"stretch_xbox_controller_teleop.py"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/","text":"Stretch RE1: Quick Start Guide Congratulations on your Stretch RE1! This guide will get you started with your new robot. Safety Stretch has the potential to cause harm if not properly used. All users should review the Stretch Safety Guide before operating the robot. Unboxing Please watch the Stretch Unboxing Video . Robot Tour A few items you'll want to know about before getting started. Power The entire robot powers up and down with the On/Off switch. When powering down, we recommend selecting 'Power Off' from the Ubuntu Desktop before hitting the Off switch The provided battery charger can be plugged and unplugged at any time during operation. Stretch uses the following charger modes: Mode Function STANDBY Charger not charging the robot 12V AGM Charging while robot is powered down SUPPLY 1) Power the robot during tethered use 2) Repair damaged batteries. REPAIR Repair damaged batteries. Please review the Battery Maintenance Guide for proper care and charging of Stretch batteries. Runstop The illuminated button on the head is its Runstop. Just tap it, you'll hear a beep and it will start flashing. This will pause the motion of the primary robot joints during operation. This can be useful if the robot makes an unsafe motion, or if you just want to free up the robot motors while you roll it around. To allow motion once again, hold the button down for two seconds. After the beep, the motion can resume. Safe Handling Like any robot, it is possible to break Stretch if you're not careful. Use common sense when applying forces to its joints, transporting it, etc. The Stretch Unpowered Best Practices Video provides a quick overview of how to work with the robot. Things that won't hurt the robot : Manually push and pull the arm (when the motor isn't holding a position). Manually raise and lower the lift (when the motor isn't holding a position). Manually tilt and roll the base around (when the motors aren't holding a position). Pick up and carry Stretch (while holding it by the mast, two people for safety). Things to be mindful of : Manually moving the head and wrist. They will move but they want to go at their own speed. The arm will slowly descend when the robot is powered off. If the arm is retracted it may rest the tool on the base. If desired to hold the arm up when un-powered, the provided 'clip-clamp' can be clipped onto the mast below the shoulder to support it. Things that can hurt the robot : Driving the wrist and gripper into the base. When the arm and wrist are stowed it is possible to collide the two. Getting the gripper stuck on something and then driving the arm, lift, or base. Laying the robot down with its weight on its camera. Trying to ride on the robot, getting it wet, etc. (eg, common sense) Hello World Demo Stretch comes ready to run out of the box. The Xbox Teleoperation demo will let you quickly test out the robot's capabilities by teleoperating it with an Xbox Controller. Note You will find the USB Dongle already plugged into the USB port of the base trunk. To start the demo after unboxing: Remove the 'trunk' cover and power on the robot Wait for about 45 seconds. You will hear the Ubuntu startup sound, followed by two beeps (indicating the demo is running). Hit the Connect button on the controller. The upper two LEDs of the ring will illuminate. Hit the Home Robot button. Stretch will go through its homing calibration routine. Warning Make sure the space around the robot is clear before running the Home function You're ready to go! A few things to try: Hit the Stow Robot button. The robot will assume the stow pose. Practice driving the robot around. Pull the Fast Base trigger while driving. When stowed, it will make Stretch drive faster Manually stop the arm or lift from moving to make it stop upon contact. Try picking up your cellphone from the floor Try grasping a cup from a countertop Try delivering an object to a person If you're done, hold down the Shutdown PC button for 2 seconds. This will cause the PC to turn off. You can then power down the robot. Or proceed to the next step... Now that you're familiar with the robot, take a minute to watch the Stretch Powered Best Practices Video . Get Plugged In Let's get plugged in. Remove the 'trunk' cover and power on the robot if it's not already on. Plug in a mouse, keyboard and HDMI monitor to the robot trunk Plug in the battery charger Place the charger in SUPPLY mode Log in to the robot computer. The default user credentials came in the box with the robot. Start Coding Python is the easiest way to begin writing code for the robot. This section will give you a quick look at Stretch Body, which is the low-level Python interface to the robot. Detailed information on the Stretch Body Interface can be found here. Note Stretch is configured to run the Xbox Controller demo in the background at startup. To run your own code you'll need to kill this process so that it doesn't contend with your code. pkill -f stretch_xbox* While you're at it, disable this autoboot feature. You can always turn it back on later. Search for 'Startup' from Ubuntu Activities. Uncheck the box for 'hello_robot_xbox_teleop' Now open up a Terminal. From the command line, first, verify that that all of the hardware is present and happy stretch_robot_system_check.py You may see a few joints reported in red because they haven't yet been calibrated. If so, home the robot stretch_robot_home.py Once the robot has homed, let's write some quick test code: ipython Now let's move the robot around using the Robot API. Try typing in these interactive commands at the iPython prompt: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . stow () robot . arm . move_to ( 0.25 ) robot . push_command () robot . arm . move_to ( 0.0 ) robot . push_command () robot . lift . move_to ( 0.4 ) robot . push_command () robot . pretty_print () robot . lift . pretty_print () robot . head . pose ( 'tool' ) robot . head . pose ( 'ahead' ) robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) robot . end_of_arm . move_to ( 'stretch_gripper' , - 50 ) robot . stow () robot . stop () Change Credentials Finally, we recommend that you change the login credentials for the default user, hello-robot. sudo passwd hello-robot If you'd like to set up a new user account, check out the Stretch Installation Guide . In a lab setting, it's useful for lab members to have their own user accounts to run experiments. Power Down The recommended power-down procedure is Place a clamp on the mast below the shoulder to prevent dropping Shutdown the computer from the Desktop When the laser range finder has stopped spinning, turn off the main power switch Attach the charger Place the charger in 12V AGM mode Join the Community Forum Join the Hello Robot Community . We'd welcome hearing your feedback as you get to know your robot. Hello Robot support monitors the forum closely and will quickly get back to you on any questions or issues you post. Further Exploration Encounter any issues while getting started? Please let us know at support@hello-robot.com. Also, take a minute to review the Stretch Troubleshooting Guide We recommend next exploring the ROS-based demos that ship with Stretch. These are found in the stretch_ros repository . That's it. Happy coding! All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Quick Start Stretch RE1"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#stretch-re1-quick-start-guide","text":"Congratulations on your Stretch RE1! This guide will get you started with your new robot.","title":"Stretch RE1: Quick Start Guide"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#safety","text":"Stretch has the potential to cause harm if not properly used. All users should review the Stretch Safety Guide before operating the robot.","title":"Safety"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#unboxing","text":"Please watch the Stretch Unboxing Video .","title":"Unboxing"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#robot-tour","text":"A few items you'll want to know about before getting started.","title":"Robot Tour"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#power","text":"The entire robot powers up and down with the On/Off switch. When powering down, we recommend selecting 'Power Off' from the Ubuntu Desktop before hitting the Off switch The provided battery charger can be plugged and unplugged at any time during operation. Stretch uses the following charger modes: Mode Function STANDBY Charger not charging the robot 12V AGM Charging while robot is powered down SUPPLY 1) Power the robot during tethered use 2) Repair damaged batteries. REPAIR Repair damaged batteries. Please review the Battery Maintenance Guide for proper care and charging of Stretch batteries.","title":"Power"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#runstop","text":"The illuminated button on the head is its Runstop. Just tap it, you'll hear a beep and it will start flashing. This will pause the motion of the primary robot joints during operation. This can be useful if the robot makes an unsafe motion, or if you just want to free up the robot motors while you roll it around. To allow motion once again, hold the button down for two seconds. After the beep, the motion can resume.","title":"Runstop"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#safe-handling","text":"Like any robot, it is possible to break Stretch if you're not careful. Use common sense when applying forces to its joints, transporting it, etc. The Stretch Unpowered Best Practices Video provides a quick overview of how to work with the robot. Things that won't hurt the robot : Manually push and pull the arm (when the motor isn't holding a position). Manually raise and lower the lift (when the motor isn't holding a position). Manually tilt and roll the base around (when the motors aren't holding a position). Pick up and carry Stretch (while holding it by the mast, two people for safety). Things to be mindful of : Manually moving the head and wrist. They will move but they want to go at their own speed. The arm will slowly descend when the robot is powered off. If the arm is retracted it may rest the tool on the base. If desired to hold the arm up when un-powered, the provided 'clip-clamp' can be clipped onto the mast below the shoulder to support it. Things that can hurt the robot : Driving the wrist and gripper into the base. When the arm and wrist are stowed it is possible to collide the two. Getting the gripper stuck on something and then driving the arm, lift, or base. Laying the robot down with its weight on its camera. Trying to ride on the robot, getting it wet, etc. (eg, common sense)","title":"Safe Handling"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#hello-world-demo","text":"Stretch comes ready to run out of the box. The Xbox Teleoperation demo will let you quickly test out the robot's capabilities by teleoperating it with an Xbox Controller. Note You will find the USB Dongle already plugged into the USB port of the base trunk. To start the demo after unboxing: Remove the 'trunk' cover and power on the robot Wait for about 45 seconds. You will hear the Ubuntu startup sound, followed by two beeps (indicating the demo is running). Hit the Connect button on the controller. The upper two LEDs of the ring will illuminate. Hit the Home Robot button. Stretch will go through its homing calibration routine. Warning Make sure the space around the robot is clear before running the Home function You're ready to go! A few things to try: Hit the Stow Robot button. The robot will assume the stow pose. Practice driving the robot around. Pull the Fast Base trigger while driving. When stowed, it will make Stretch drive faster Manually stop the arm or lift from moving to make it stop upon contact. Try picking up your cellphone from the floor Try grasping a cup from a countertop Try delivering an object to a person If you're done, hold down the Shutdown PC button for 2 seconds. This will cause the PC to turn off. You can then power down the robot. Or proceed to the next step... Now that you're familiar with the robot, take a minute to watch the Stretch Powered Best Practices Video .","title":"Hello World Demo"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#get-plugged-in","text":"Let's get plugged in. Remove the 'trunk' cover and power on the robot if it's not already on. Plug in a mouse, keyboard and HDMI monitor to the robot trunk Plug in the battery charger Place the charger in SUPPLY mode Log in to the robot computer. The default user credentials came in the box with the robot.","title":"Get Plugged In"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#start-coding","text":"Python is the easiest way to begin writing code for the robot. This section will give you a quick look at Stretch Body, which is the low-level Python interface to the robot. Detailed information on the Stretch Body Interface can be found here. Note Stretch is configured to run the Xbox Controller demo in the background at startup. To run your own code you'll need to kill this process so that it doesn't contend with your code. pkill -f stretch_xbox* While you're at it, disable this autoboot feature. You can always turn it back on later. Search for 'Startup' from Ubuntu Activities. Uncheck the box for 'hello_robot_xbox_teleop' Now open up a Terminal. From the command line, first, verify that that all of the hardware is present and happy stretch_robot_system_check.py You may see a few joints reported in red because they haven't yet been calibrated. If so, home the robot stretch_robot_home.py Once the robot has homed, let's write some quick test code: ipython Now let's move the robot around using the Robot API. Try typing in these interactive commands at the iPython prompt: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . stow () robot . arm . move_to ( 0.25 ) robot . push_command () robot . arm . move_to ( 0.0 ) robot . push_command () robot . lift . move_to ( 0.4 ) robot . push_command () robot . pretty_print () robot . lift . pretty_print () robot . head . pose ( 'tool' ) robot . head . pose ( 'ahead' ) robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) robot . end_of_arm . move_to ( 'stretch_gripper' , - 50 ) robot . stow () robot . stop ()","title":"Start Coding"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#change-credentials","text":"Finally, we recommend that you change the login credentials for the default user, hello-robot. sudo passwd hello-robot If you'd like to set up a new user account, check out the Stretch Installation Guide . In a lab setting, it's useful for lab members to have their own user accounts to run experiments.","title":"Change Credentials"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#power-down","text":"The recommended power-down procedure is Place a clamp on the mast below the shoulder to prevent dropping Shutdown the computer from the Desktop When the laser range finder has stopped spinning, turn off the main power switch Attach the charger Place the charger in 12V AGM mode","title":"Power Down"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#join-the-community-forum","text":"Join the Hello Robot Community . We'd welcome hearing your feedback as you get to know your robot. Hello Robot support monitors the forum closely and will quickly get back to you on any questions or issues you post.","title":"Join the Community Forum"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re1/#further-exploration","text":"Encounter any issues while getting started? Please let us know at support@hello-robot.com. Also, take a minute to review the Stretch Troubleshooting Guide We recommend next exploring the ROS-based demos that ship with Stretch. These are found in the stretch_ros repository . That's it. Happy coding! All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Further Exploration"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/","text":"Stretch 2: Quick Start Guide Congratulations on your Stretch 2! This guide will get you started with your new robot. Safety Stretch has the potential to cause harm if not properly used. All users should review the Stretch Safety Guide before operating the robot. Unboxing Please watch the Stretch Unboxing Video . Please note that the unboxing instructions for a Stretch 2 are the same as for a Stretch RE1. Robot Tour A few items you'll want to know about before getting started. Power The entire robot powers up and down with the On/Off switch. When powering down, we recommend selecting 'Power Off' from the Ubuntu Desktop prior to hitting the Off switch Runstop The illuminated button on the head is its Runstop. Just tap it, you'll hear a beep and it will start flashing. This will pause the motion of the primary robot joints during operation. This can be useful if the robot makes an unsafe motion, or if you just want to free up the robot motors while you roll it around. To allow motion once again, hold the button down for two seconds. After the beep, the motion can resume. LED Lightbar The LED lightbar in the base provides a simple way to quickly ascertain the robot's state. At all times its color indicates the battery voltage. More information on the voltage display is available in the Battery Maintenance Guide The lightbar will also flash as follows: Mode Flashing Normal Operation None Runstopped Rapid flash at 1 Hz Charger plugged in Slow strobe at 0.5 Hz Hello World Demo Stretch comes ready to run out of the box. The Xbox Teleoperation demo will let you quickly test out the robot's capabilities by teleoperating it with an Xbox Controller. Note : You will find the USB Dongle already plugged into the USB port of the base trunk. To start the demo after unboxing and turning the power on: Wait for about 45 seconds. You will hear the Ubuntu startup sound, followed by two beeps (indicating the demo is running). Hit the Connect button on the controller. The upper two LEDs of the ring will illuminate. Hit the Home Robot button. Stretch will go through its homing calibration routine. Note : make sure the space around the robot is clear before running the Home function You're ready to go! A few things to try: Hit the Stow Robot button. The robot will assume the stow pose. Practice driving the robot around. Pull the Fast Base trigger while driving. When stowed, it will make Stretch drive faster Manually stop the arm or lift from moving to make it stop upon contact. Try picking up your cellphone from the floor Try grasping a cup from a countertop Try delivering an object to a person If you're done, let's power down. First, attach the clip-clamp just below the shoulder as shown. Hold down the Shutdown PC button on the Xbox controller for 2 seconds. This will cause the PC to turn off. You can then power down the robot with the On/Off switch. Now that you're familiar with the robot, take a minute to watch the Stretch Powered Best Practices Video . Please note that the best practice instructions for a Stretch 2 are the same as for a Stretch RE1. Safe Handling Like any robot, it is possible to break Stretch if you're not careful. Use common sense when applying forces to its joints, transporting it, etc. The Stretch Unpowered Best Practices Video provides a quick overview of how to work with the robot. Please note that the best practice instructions for a Stretch 2 are the same as for a Stretch RE1. Things that won't hurt the robot : Manually push and pull the arm (when the motor isn't holding a position). Manually raise and lower the lift (when the motor isn't holding a position). Manually tilt and roll the base around (when the motors aren't holding a position). Pick up and carry Stretch (while holding it by the mast, two people for safety). Things to be mindful of : Manually moving the head and wrist. They will move but they want to go at their own speed. The lift will slowly descend when the robot is powered off. If the arm is retracted it may come to rest the tool on the base. If desired to hold the arm up when un-powered, the provided 'clip-clamp' can be clipped onto the mast below the shoulder to support it. Note The Stretch 2 lift descends faster than the Stretch RE1. For Stretch 2 we recommend always attaching the clip-clamp before powering down the NUC computer Things that can hurt the robot : Driving the wrist and gripper into the base. When the arm and wrist are stowed it is possible to collide the two. Getting the gripper stuck on something and then driving the arm, lift, or base. Laying the robot down with its weight on its camera. Trying to ride on the robot, getting it wet, etc. (eg, common sense) Charging the Battery The provided battery charger can be plugged and unplugged at any time during operation. Stretch uses the following charger modes: Mode Function STANDBY Charger not charging the robot 12V AGM Charging while robot is powered down SUPPLY 1) Power the robot during tethered use 2) Repair damaged batteries. REPAIR Repair damaged batteries. Please review the Battery Maintenance Guide for proper care and charging of Stretch batteries. Get Plugged In Let's get plugged in. Power up the robot Plug in a mouse, keyboard and HDMI monitor to the robot trunk Plug in the battery charger Place the charger in SUPPLY mode Log in to the robot computer. The default user credentials came in the box with the robot. Start Coding Python is the easiest way to begin writing code for the robot. This section will give you a quick look at Stretch Body, which is the low-level Python interface to the robot. Detailed information on the Stretch Body Interface can be found here. Note Stretch is configured to run the Xbox Controller demo in the background at startup. To run your own code you'll need to kill this process so that it doesn't contend with your code. pkill -f stretch_xbox* While you're at it, disable this autoboot feature. You can always turn it back on later. Search for 'Startup' from Ubuntu Activities. Uncheck the box for 'hello_robot_xbox_teleop' Now open up a Terminal. From the command line, first, verify that all of the hardware is present and happy stretch_robot_system_check.py You may see a few joints reported in red because they haven't yet been calibrated. If so, home the robot stretch_robot_home.py Once the robot has homed, let's write some quick test code: ipython Now let's move the robot around using the Stretch Body Robot API . Try typing in these interactive commands in the iPython prompt: >> $ ipython ... import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . stow () robot . arm . move_to ( 0.25 ) robot . push_command () robot . arm . move_to ( 0.0 ) robot . push_command () robot . lift . move_to ( 0.4 ) robot . push_command () robot . pretty_print () robot . lift . pretty_print () robot . head . pose ( 'tool' ) robot . head . pose ( 'ahead' ) robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) robot . end_of_arm . move_to ( 'stretch_gripper' , - 50 ) robot . stow () robot . stop () Change Credentials Finally, we recommend that you change the login credentials for the default user, hello-robot. sudo passwd hello-robot If you'd like to set up a new user account, check out the Stretch Installation Guide . In a lab setting, it's useful for lab members to have their own user accounts to run experiments. Power Down The recommended power-down procedure is Place a clamp on the mast below the shoulder to prevent a slow drop (if this is a concern) Shutdown the computer from the Desktop When the laser range finder has stopped spinning, turn off the main power switch Attach the charger Place the charger in 12V AGM mode Join the Community Forum Join the Hello Robot Community . We'd welcome hearing your feedback as you get to know your robot. Hello Robot support monitors the forum closely and will quickly get back to you on any questions or issues you post. Further Exploration Encounter any issues while getting started? Please let us know at support@hello-robot.com. Also, take a minute to review the Stretch Troubleshooting Guide We recommend next exploring the ROS-based demos that ship with Stretch. These are found in the stretch_ros repository . That's it. Happy coding! All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Quick Start Stretch 2"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#stretch-2-quick-start-guide","text":"Congratulations on your Stretch 2! This guide will get you started with your new robot.","title":"Stretch 2: Quick Start Guide"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#safety","text":"Stretch has the potential to cause harm if not properly used. All users should review the Stretch Safety Guide before operating the robot.","title":"Safety"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#unboxing","text":"Please watch the Stretch Unboxing Video . Please note that the unboxing instructions for a Stretch 2 are the same as for a Stretch RE1.","title":"Unboxing"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#robot-tour","text":"A few items you'll want to know about before getting started.","title":"Robot Tour"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#power","text":"The entire robot powers up and down with the On/Off switch. When powering down, we recommend selecting 'Power Off' from the Ubuntu Desktop prior to hitting the Off switch","title":"Power"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#runstop","text":"The illuminated button on the head is its Runstop. Just tap it, you'll hear a beep and it will start flashing. This will pause the motion of the primary robot joints during operation. This can be useful if the robot makes an unsafe motion, or if you just want to free up the robot motors while you roll it around. To allow motion once again, hold the button down for two seconds. After the beep, the motion can resume.","title":"Runstop"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#led-lightbar","text":"The LED lightbar in the base provides a simple way to quickly ascertain the robot's state. At all times its color indicates the battery voltage. More information on the voltage display is available in the Battery Maintenance Guide The lightbar will also flash as follows: Mode Flashing Normal Operation None Runstopped Rapid flash at 1 Hz Charger plugged in Slow strobe at 0.5 Hz","title":"LED Lightbar"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#hello-world-demo","text":"Stretch comes ready to run out of the box. The Xbox Teleoperation demo will let you quickly test out the robot's capabilities by teleoperating it with an Xbox Controller. Note : You will find the USB Dongle already plugged into the USB port of the base trunk. To start the demo after unboxing and turning the power on: Wait for about 45 seconds. You will hear the Ubuntu startup sound, followed by two beeps (indicating the demo is running). Hit the Connect button on the controller. The upper two LEDs of the ring will illuminate. Hit the Home Robot button. Stretch will go through its homing calibration routine. Note : make sure the space around the robot is clear before running the Home function You're ready to go! A few things to try: Hit the Stow Robot button. The robot will assume the stow pose. Practice driving the robot around. Pull the Fast Base trigger while driving. When stowed, it will make Stretch drive faster Manually stop the arm or lift from moving to make it stop upon contact. Try picking up your cellphone from the floor Try grasping a cup from a countertop Try delivering an object to a person If you're done, let's power down. First, attach the clip-clamp just below the shoulder as shown. Hold down the Shutdown PC button on the Xbox controller for 2 seconds. This will cause the PC to turn off. You can then power down the robot with the On/Off switch. Now that you're familiar with the robot, take a minute to watch the Stretch Powered Best Practices Video . Please note that the best practice instructions for a Stretch 2 are the same as for a Stretch RE1.","title":"Hello World Demo"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#safe-handling","text":"Like any robot, it is possible to break Stretch if you're not careful. Use common sense when applying forces to its joints, transporting it, etc. The Stretch Unpowered Best Practices Video provides a quick overview of how to work with the robot. Please note that the best practice instructions for a Stretch 2 are the same as for a Stretch RE1. Things that won't hurt the robot : Manually push and pull the arm (when the motor isn't holding a position). Manually raise and lower the lift (when the motor isn't holding a position). Manually tilt and roll the base around (when the motors aren't holding a position). Pick up and carry Stretch (while holding it by the mast, two people for safety). Things to be mindful of : Manually moving the head and wrist. They will move but they want to go at their own speed. The lift will slowly descend when the robot is powered off. If the arm is retracted it may come to rest the tool on the base. If desired to hold the arm up when un-powered, the provided 'clip-clamp' can be clipped onto the mast below the shoulder to support it. Note The Stretch 2 lift descends faster than the Stretch RE1. For Stretch 2 we recommend always attaching the clip-clamp before powering down the NUC computer Things that can hurt the robot : Driving the wrist and gripper into the base. When the arm and wrist are stowed it is possible to collide the two. Getting the gripper stuck on something and then driving the arm, lift, or base. Laying the robot down with its weight on its camera. Trying to ride on the robot, getting it wet, etc. (eg, common sense)","title":"Safe Handling"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#charging-the-battery","text":"The provided battery charger can be plugged and unplugged at any time during operation. Stretch uses the following charger modes: Mode Function STANDBY Charger not charging the robot 12V AGM Charging while robot is powered down SUPPLY 1) Power the robot during tethered use 2) Repair damaged batteries. REPAIR Repair damaged batteries. Please review the Battery Maintenance Guide for proper care and charging of Stretch batteries.","title":"Charging the Battery"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#get-plugged-in","text":"Let's get plugged in. Power up the robot Plug in a mouse, keyboard and HDMI monitor to the robot trunk Plug in the battery charger Place the charger in SUPPLY mode Log in to the robot computer. The default user credentials came in the box with the robot.","title":"Get Plugged In"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#start-coding","text":"Python is the easiest way to begin writing code for the robot. This section will give you a quick look at Stretch Body, which is the low-level Python interface to the robot. Detailed information on the Stretch Body Interface can be found here. Note Stretch is configured to run the Xbox Controller demo in the background at startup. To run your own code you'll need to kill this process so that it doesn't contend with your code. pkill -f stretch_xbox* While you're at it, disable this autoboot feature. You can always turn it back on later. Search for 'Startup' from Ubuntu Activities. Uncheck the box for 'hello_robot_xbox_teleop' Now open up a Terminal. From the command line, first, verify that all of the hardware is present and happy stretch_robot_system_check.py You may see a few joints reported in red because they haven't yet been calibrated. If so, home the robot stretch_robot_home.py Once the robot has homed, let's write some quick test code: ipython Now let's move the robot around using the Stretch Body Robot API . Try typing in these interactive commands in the iPython prompt: >> $ ipython ... import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . stow () robot . arm . move_to ( 0.25 ) robot . push_command () robot . arm . move_to ( 0.0 ) robot . push_command () robot . lift . move_to ( 0.4 ) robot . push_command () robot . pretty_print () robot . lift . pretty_print () robot . head . pose ( 'tool' ) robot . head . pose ( 'ahead' ) robot . end_of_arm . move_to ( 'wrist_yaw' , 0 ) robot . end_of_arm . move_to ( 'stretch_gripper' , 50 ) robot . end_of_arm . move_to ( 'stretch_gripper' , - 50 ) robot . stow () robot . stop ()","title":"Start Coding"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#change-credentials","text":"Finally, we recommend that you change the login credentials for the default user, hello-robot. sudo passwd hello-robot If you'd like to set up a new user account, check out the Stretch Installation Guide . In a lab setting, it's useful for lab members to have their own user accounts to run experiments.","title":"Change Credentials"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#power-down","text":"The recommended power-down procedure is Place a clamp on the mast below the shoulder to prevent a slow drop (if this is a concern) Shutdown the computer from the Desktop When the laser range finder has stopped spinning, turn off the main power switch Attach the charger Place the charger in 12V AGM mode","title":"Power Down"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#join-the-community-forum","text":"Join the Hello Robot Community . We'd welcome hearing your feedback as you get to know your robot. Hello Robot support monitors the forum closely and will quickly get back to you on any questions or issues you post.","title":"Join the Community Forum"},{"location":"stretch-tutorials/getting_started/quick_start_guide_re2/#further-exploration","text":"Encounter any issues while getting started? Please let us know at support@hello-robot.com. Also, take a minute to review the Stretch Troubleshooting Guide We recommend next exploring the ROS-based demos that ship with Stretch. These are found in the stretch_ros repository . That's it. Happy coding! All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Further Exploration"},{"location":"stretch-tutorials/getting_started/safety_guide/","text":"Robot Safety Guide The Stretch robots are potentially dangerous machines with safety hazards. If improperly used they can cause injury or death. All users must carefully read the following safety information before using the robot. Anyone near the robot who has not read this safety information must be closely supervised at all times and made aware that the robot could be dangerous. Only use the robot after inspecting the surrounding environment for potential hazards. Intended Use The Stretch robots are intended for use by researchers to conduct research in controlled indoor environments. This product is not intended for other uses and lacks the required certifications for other uses, such as use in a home environment by consumers. Safety Hazards As described later in this document, we have designed Stretch to be safer than previous commercially-available human-scale mobile manipulators, so that researchers can explore the future of mobile manipulation. For example, we have made it smaller and lighter weight with backdrivable torque-sensing joints that can stop when they detect contact. Nonetheless, Stretch is a research robot that can be dangerous. Researchers must use Stretch carefully to avoid damage, injury, or death. Here, we list several safety hazards that researchers must consider before and while using Stretch. Stretch Can Put People And Animals At Risk As described in more detail later, Stretch can put people and animals at risk. People and animals near the robot must be closely supervised at all times. At all times, an experienced researcher must carefully monitor the robot and be prepared to stop it. Any people near the robot must be made aware that the robot could be dangerous. Before any use of the robot near people or animals, researchers must carefully assess and minimize risks. Researchers who use the robot near children, animals, vulnerable adults, or other people do so at their own risk. Researchers must take appropriate precautions and obtain the required approvals from their organizations. Stretch Can Topple Onto A Person The robot may drive off stairs, push or pull itself over with its telescoping arm, fall over while attempting to traverse a threshold or encounter obstacles that cause it to fall on or otherwise collide with people, causing injury. Operate the robot only on flat surfaces away from stairs or other obstacles that may cause it to topple, and do not allow the robot to push or pull itself over. Stretch Should Not Be Lifted By A Single Person Stretch with the standard gripper weighs about 23 kg (50.5 lb), so two or more people should lift and carry the robot. A single person can move the robot around by enabling the runstop button, tilting it over, and rolling it on flat ground. At least two people should lift and carry the robot when needed. Stretch Can Cause Lacerations The robot's wrist and tool have sharp edges that can cause lacerations or punctures to the skin or the eyes. Operate the robot away from eyes and other sensitive body parts. Stretch Can Trap, Crush, Or Pinch Body Parts The robot has moving joints that can trap, crush or pinch hands, fingers, or other body parts. The robot could also injure a person or animal by driving over a body part. Keep body parts away from the trap, crush, and pinch points during robot motion, including underneath the wheels. Stretch Can Entrap Loose Clothing Or Hair The robot's shoulder and telescoping arm have rollers that can pull in and entrap loose clothing or hair. Keep loose clothing and long hair away from the robot's shoulder and telescoping arm when either is in motion. Stretch Has Flammable Components The robot has polyurethane covers that are flammable and must be kept away from potential ignition sources, such as open flames and hot surfaces. The robot\u2019s head, shoulder, and mobile base have polyurethane covers. Keep the robot away from potential ignition sources and always have a working fire extinguisher nearby. Stretch Is An Electrical Device Stretch has batteries, electronics, wires, and other electrical components throughout its body. It also provides uncovered connectors that provide power. While the robot has fuses to reduce electrical risks, users must be careful. Keep the robot dry and away from liquids, avoid electrical shocks, ensure power cables and wires are in good condition, be careful with the robot\u2019s connectors, and generally exercise caution while working with this electrical device. Stretch Can Perform Dangerous Activities Stretch is a versatile robot capable of performing many actions, including actions that would be dangerous to people. For example, if a dangerous object is held by or affixed to the robot, such as a knife, a heavy object, or breakable glass, the robot can become very dangerous. Likewise, the robot is capable of physically altering the environment in ways that would be dangerous, such as turning a knob that releases gas from a gas stove. Users must be cautious while using the robot to ensure it interacts safely with people and the surrounding environment. Stretch Is An Open Platform That Can Be Made More Dangerous Stretch is an open platform with user-modifiable and user-extensible hardware and software. User changes to the hardware or software can entail serious risks. For example, when shipped, the robot has conservative settings that restrict its speed and the forces it applies to reduce the risks associated with the robot. By modifying the robot, users could enable the robot to move at unsafe speeds and apply unsafe forces. As another example, improper electrical connections could result in a fire. Researchers who choose to modify or extend the robot\u2019s hardware or software do so at their own risk and should be careful to understand the implications of their modifications or extensions. Changes to the robot could result in dangerous situations that cause injury or death. Additional Risks The most important aspects of safety with Stretch are to use good judgment and common sense. Additional important considerations follow: If the robot appears to be damaged, stop the robot immediately. Always be ready to stop the robot. Do not operate the robot unless an experienced user is present and attentive. Be aware that the robot can move in unexpected ways. Do not put fingers or other objects into the channel that runs along the length of the mast. A belt moves within this channel. Keep an eye on cords, rugs, and any other floor hazards as the robot drives. Keep the robot at least 3 meters from ledges, curbs, stairs, and any other toppling hazard. Do not operate the robot outdoors. Do not attempt to ride the robot. Do not have the robot hold sharp objects. Do not attempt to service the robot without supervision by Hello Robot. Other Problems Will Likely Occur \u201c Anticipate potential problems and hazards. Always imagine what might happen if the robot malfunctions or behaves in a way different from the desired action. Be vigilant.\u201d - PR2 User Manual by Willow Garage from October 5, 2012 Stretch is a complex device that includes many mechanical, electrical, and computational systems that have been designed to work together. Be prepared for something to go wrong. For example, a motor control board might fail, software might not operate as anticipated, an unexpected process might still be running on the robot, or the batteries for the Xbox-style controller or the robot itself might run out. Safety Features We have considered safety from the outset in the design of Stretch. Runstop : The illuminated runstop button on Stretch\u2019s head can be used to pause the operation of the four primary joints (base, lift, and arm) of the robot when it is in motion. Lightweight design: The overall mass of Stretch with the standard gripper is 23Kg (50.5lb), and the majority of the mass is in the base. The carbon fiber arm and aluminum mast make for a remarkably lightweight upper body. While this reduces the risk of crushing, crushing injuries can still occur and should be carefully monitored. Gravity friendly : Due to Stretch\u2019s design, its actuators don't have to counteract gravity on a large lever arm. As a result, the motors and gearboxes are lower torque and lower weight than a conventional mobile manipulator with a comparable reach, avoiding the often dangerously strong shoulder joints of typical robot arms. Low gear ratio : The primary joints of Stretch (base, lift, and arm) have low gear-ratios (approx 5:1), allowing for backdriving of joints when powered off. A low gear-ratio also reduces the effective inertia of each joint, limiting the impacted force during undesired contact with people and the environment. Contact Sensitivity : The four primary joints of Stretch (base, lift, and arm) have contact sensitivity. We measure motor currents to estimate contact forces. Because Stretch is a low gear-ratio robot, current sensing provides a fairly sensitive measure of contact forces. Firmware limits : Motor torques are limited at the lowest level of the firmware to configured bounds. Velocity limits : Fast motions of the base are restricted when the arm is up high and the tool is outside the base footprint. This limits the likelihood of toppling or snagging the tool during base motion. Tilt detection : The robot can detect when its body is tilted beyond a safe threshold. The robot can be configured to trigger a runstop event during an over-tilt event. Safety Markings Stretch has the following safety markings: Top of the shoulder, indicating potential pinch point between rollers and mast. Top of the base, indicating potential pinch point between arm and base. Runstop The runstop allows the user to pause the motion of the four primary actuators (base, lift, and arm) by tapping the illuminated button on the head. An experienced operator should always keep the runstop within reach, allowing them to stop the motion of the robot if it is deemed unsafe. Warning The runstop is not equivalent to an Emergency Stop found on industrial equipment and no safety guarantees are made by its function. When the runstop is enabled, these actuators are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume. The runstop logic is: Action Runstop State Button Illumination Robot startup Motion enabled Solid Tap runstop button Motion disabled Flashing at 1Hz Hold down runstop button for >2s Motion enabled Solid Safety Hazard Details Sharp Edges The Stretch robot is a piece of laboratory equipment. As such, its structure has moderately sharp edges and corners that can be unsafe. These edges can get snagged during motion, or they may cause lacerations when sufficient force is applied to a person. Care should be taken when grasping or otherwise making contact with Stretch that a sharp corner or edge is not contacted. Toppling Stretch is a relatively lightweight robot. In some kinematic configurations, a high center of gravity can make it prone to toppling. Toppling can occur when: The mobile base is moving at a moderate or fast speed and hits a bump, threshold, or other change in floor property. The arm is raised high and pushes or pulls on the environment with sufficient force. The robot drives over a drop-off such as a stair or a curb. Warning While Stretch has cliff sensors, they do not currently inhibit motion of the base. During typical use, the robot will not attempt to stop itself at a cliff, and can fall down stairs and hurt itself or a person. Pinch Points Pinch points around the robot's head, gripper, and wrist can cause discomfort and care should be taken when handling these joints as they move. The shoulder, which travels up and down on the lift, has a series of rollers that ride along the mast. While the shoulder shells can prevent large objects from getting pinched by the rollers, small and thin objects can be pulled into and crushed. The telescoping arm, which extends and retracts, has rollers that ride along the telescoping elements. While the arm link cuffs can reduce the chance of large objects getting pinched, small and thin objects, such as hair, can be pulled in. Extra care should be taken with long hair, clothing, and small fingers around the shoulder rollers. Crush Points The lift degree of freedom is the strongest joint on the robot and as such can apply potentially unsafe forces to a person. The lift, while in motion, may trap or crush objects between the \u2018shoulder\u2019 and another surface. As such, best practices for lift safety should always be used when using the lift degree of freedom. The lift has a max theoretical strength of nearly 200N of linear force. In practice, this force is limited by the lift\u2019s Guarded Move function, which places the lift in Safety Mode when the actuator forces exceed a threshold. The diagrams below show the potential crush points at the top and bottom of the lift range of motion. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Safety Guide"},{"location":"stretch-tutorials/getting_started/safety_guide/#robot-safety-guide","text":"The Stretch robots are potentially dangerous machines with safety hazards. If improperly used they can cause injury or death. All users must carefully read the following safety information before using the robot. Anyone near the robot who has not read this safety information must be closely supervised at all times and made aware that the robot could be dangerous. Only use the robot after inspecting the surrounding environment for potential hazards.","title":"Robot Safety Guide"},{"location":"stretch-tutorials/getting_started/safety_guide/#intended-use","text":"The Stretch robots are intended for use by researchers to conduct research in controlled indoor environments. This product is not intended for other uses and lacks the required certifications for other uses, such as use in a home environment by consumers.","title":"Intended Use"},{"location":"stretch-tutorials/getting_started/safety_guide/#safety-hazards","text":"As described later in this document, we have designed Stretch to be safer than previous commercially-available human-scale mobile manipulators, so that researchers can explore the future of mobile manipulation. For example, we have made it smaller and lighter weight with backdrivable torque-sensing joints that can stop when they detect contact. Nonetheless, Stretch is a research robot that can be dangerous. Researchers must use Stretch carefully to avoid damage, injury, or death. Here, we list several safety hazards that researchers must consider before and while using Stretch.","title":"Safety Hazards"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-put-people-and-animals-at-risk","text":"As described in more detail later, Stretch can put people and animals at risk. People and animals near the robot must be closely supervised at all times. At all times, an experienced researcher must carefully monitor the robot and be prepared to stop it. Any people near the robot must be made aware that the robot could be dangerous. Before any use of the robot near people or animals, researchers must carefully assess and minimize risks. Researchers who use the robot near children, animals, vulnerable adults, or other people do so at their own risk. Researchers must take appropriate precautions and obtain the required approvals from their organizations.","title":"Stretch Can Put People And Animals At Risk"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-topple-onto-a-person","text":"The robot may drive off stairs, push or pull itself over with its telescoping arm, fall over while attempting to traverse a threshold or encounter obstacles that cause it to fall on or otherwise collide with people, causing injury. Operate the robot only on flat surfaces away from stairs or other obstacles that may cause it to topple, and do not allow the robot to push or pull itself over.","title":"Stretch Can Topple Onto A Person"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-should-not-be-lifted-by-a-single-person","text":"Stretch with the standard gripper weighs about 23 kg (50.5 lb), so two or more people should lift and carry the robot. A single person can move the robot around by enabling the runstop button, tilting it over, and rolling it on flat ground. At least two people should lift and carry the robot when needed.","title":"Stretch Should Not Be Lifted By A Single Person"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-cause-lacerations","text":"The robot's wrist and tool have sharp edges that can cause lacerations or punctures to the skin or the eyes. Operate the robot away from eyes and other sensitive body parts.","title":"Stretch Can Cause Lacerations"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-trap-crush-or-pinch-body-parts","text":"The robot has moving joints that can trap, crush or pinch hands, fingers, or other body parts. The robot could also injure a person or animal by driving over a body part. Keep body parts away from the trap, crush, and pinch points during robot motion, including underneath the wheels.","title":"Stretch Can Trap, Crush, Or Pinch Body Parts"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-entrap-loose-clothing-or-hair","text":"The robot's shoulder and telescoping arm have rollers that can pull in and entrap loose clothing or hair. Keep loose clothing and long hair away from the robot's shoulder and telescoping arm when either is in motion.","title":"Stretch Can Entrap Loose Clothing Or Hair"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-has-flammable-components","text":"The robot has polyurethane covers that are flammable and must be kept away from potential ignition sources, such as open flames and hot surfaces. The robot\u2019s head, shoulder, and mobile base have polyurethane covers. Keep the robot away from potential ignition sources and always have a working fire extinguisher nearby.","title":"Stretch Has Flammable Components"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-is-an-electrical-device","text":"Stretch has batteries, electronics, wires, and other electrical components throughout its body. It also provides uncovered connectors that provide power. While the robot has fuses to reduce electrical risks, users must be careful. Keep the robot dry and away from liquids, avoid electrical shocks, ensure power cables and wires are in good condition, be careful with the robot\u2019s connectors, and generally exercise caution while working with this electrical device.","title":"Stretch Is An Electrical Device"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-can-perform-dangerous-activities","text":"Stretch is a versatile robot capable of performing many actions, including actions that would be dangerous to people. For example, if a dangerous object is held by or affixed to the robot, such as a knife, a heavy object, or breakable glass, the robot can become very dangerous. Likewise, the robot is capable of physically altering the environment in ways that would be dangerous, such as turning a knob that releases gas from a gas stove. Users must be cautious while using the robot to ensure it interacts safely with people and the surrounding environment.","title":"Stretch Can Perform Dangerous Activities"},{"location":"stretch-tutorials/getting_started/safety_guide/#stretch-is-an-open-platform-that-can-be-made-more-dangerous","text":"Stretch is an open platform with user-modifiable and user-extensible hardware and software. User changes to the hardware or software can entail serious risks. For example, when shipped, the robot has conservative settings that restrict its speed and the forces it applies to reduce the risks associated with the robot. By modifying the robot, users could enable the robot to move at unsafe speeds and apply unsafe forces. As another example, improper electrical connections could result in a fire. Researchers who choose to modify or extend the robot\u2019s hardware or software do so at their own risk and should be careful to understand the implications of their modifications or extensions. Changes to the robot could result in dangerous situations that cause injury or death.","title":"Stretch Is An Open Platform That Can Be Made More Dangerous"},{"location":"stretch-tutorials/getting_started/safety_guide/#additional-risks","text":"The most important aspects of safety with Stretch are to use good judgment and common sense. Additional important considerations follow: If the robot appears to be damaged, stop the robot immediately. Always be ready to stop the robot. Do not operate the robot unless an experienced user is present and attentive. Be aware that the robot can move in unexpected ways. Do not put fingers or other objects into the channel that runs along the length of the mast. A belt moves within this channel. Keep an eye on cords, rugs, and any other floor hazards as the robot drives. Keep the robot at least 3 meters from ledges, curbs, stairs, and any other toppling hazard. Do not operate the robot outdoors. Do not attempt to ride the robot. Do not have the robot hold sharp objects. Do not attempt to service the robot without supervision by Hello Robot.","title":"Additional Risks"},{"location":"stretch-tutorials/getting_started/safety_guide/#other-problems-will-likely-occur","text":"\u201c Anticipate potential problems and hazards. Always imagine what might happen if the robot malfunctions or behaves in a way different from the desired action. Be vigilant.\u201d - PR2 User Manual by Willow Garage from October 5, 2012 Stretch is a complex device that includes many mechanical, electrical, and computational systems that have been designed to work together. Be prepared for something to go wrong. For example, a motor control board might fail, software might not operate as anticipated, an unexpected process might still be running on the robot, or the batteries for the Xbox-style controller or the robot itself might run out.","title":"Other Problems Will Likely Occur"},{"location":"stretch-tutorials/getting_started/safety_guide/#safety-features","text":"We have considered safety from the outset in the design of Stretch. Runstop : The illuminated runstop button on Stretch\u2019s head can be used to pause the operation of the four primary joints (base, lift, and arm) of the robot when it is in motion. Lightweight design: The overall mass of Stretch with the standard gripper is 23Kg (50.5lb), and the majority of the mass is in the base. The carbon fiber arm and aluminum mast make for a remarkably lightweight upper body. While this reduces the risk of crushing, crushing injuries can still occur and should be carefully monitored. Gravity friendly : Due to Stretch\u2019s design, its actuators don't have to counteract gravity on a large lever arm. As a result, the motors and gearboxes are lower torque and lower weight than a conventional mobile manipulator with a comparable reach, avoiding the often dangerously strong shoulder joints of typical robot arms. Low gear ratio : The primary joints of Stretch (base, lift, and arm) have low gear-ratios (approx 5:1), allowing for backdriving of joints when powered off. A low gear-ratio also reduces the effective inertia of each joint, limiting the impacted force during undesired contact with people and the environment. Contact Sensitivity : The four primary joints of Stretch (base, lift, and arm) have contact sensitivity. We measure motor currents to estimate contact forces. Because Stretch is a low gear-ratio robot, current sensing provides a fairly sensitive measure of contact forces. Firmware limits : Motor torques are limited at the lowest level of the firmware to configured bounds. Velocity limits : Fast motions of the base are restricted when the arm is up high and the tool is outside the base footprint. This limits the likelihood of toppling or snagging the tool during base motion. Tilt detection : The robot can detect when its body is tilted beyond a safe threshold. The robot can be configured to trigger a runstop event during an over-tilt event.","title":"Safety Features"},{"location":"stretch-tutorials/getting_started/safety_guide/#safety-markings","text":"Stretch has the following safety markings: Top of the shoulder, indicating potential pinch point between rollers and mast. Top of the base, indicating potential pinch point between arm and base.","title":"Safety Markings"},{"location":"stretch-tutorials/getting_started/safety_guide/#runstop","text":"The runstop allows the user to pause the motion of the four primary actuators (base, lift, and arm) by tapping the illuminated button on the head. An experienced operator should always keep the runstop within reach, allowing them to stop the motion of the robot if it is deemed unsafe. Warning The runstop is not equivalent to an Emergency Stop found on industrial equipment and no safety guarantees are made by its function. When the runstop is enabled, these actuators are in a \u2018Safety Mode\u2019 that inhibits the motion controller at the firmware level. Disabling the runstop allows normal operation to resume. The runstop logic is: Action Runstop State Button Illumination Robot startup Motion enabled Solid Tap runstop button Motion disabled Flashing at 1Hz Hold down runstop button for >2s Motion enabled Solid","title":"Runstop"},{"location":"stretch-tutorials/getting_started/safety_guide/#safety-hazard-details","text":"","title":"Safety Hazard Details"},{"location":"stretch-tutorials/getting_started/safety_guide/#sharp-edges","text":"The Stretch robot is a piece of laboratory equipment. As such, its structure has moderately sharp edges and corners that can be unsafe. These edges can get snagged during motion, or they may cause lacerations when sufficient force is applied to a person. Care should be taken when grasping or otherwise making contact with Stretch that a sharp corner or edge is not contacted.","title":"Sharp Edges"},{"location":"stretch-tutorials/getting_started/safety_guide/#toppling","text":"Stretch is a relatively lightweight robot. In some kinematic configurations, a high center of gravity can make it prone to toppling. Toppling can occur when: The mobile base is moving at a moderate or fast speed and hits a bump, threshold, or other change in floor property. The arm is raised high and pushes or pulls on the environment with sufficient force. The robot drives over a drop-off such as a stair or a curb. Warning While Stretch has cliff sensors, they do not currently inhibit motion of the base. During typical use, the robot will not attempt to stop itself at a cliff, and can fall down stairs and hurt itself or a person.","title":"Toppling"},{"location":"stretch-tutorials/getting_started/safety_guide/#pinch-points","text":"Pinch points around the robot's head, gripper, and wrist can cause discomfort and care should be taken when handling these joints as they move. The shoulder, which travels up and down on the lift, has a series of rollers that ride along the mast. While the shoulder shells can prevent large objects from getting pinched by the rollers, small and thin objects can be pulled into and crushed. The telescoping arm, which extends and retracts, has rollers that ride along the telescoping elements. While the arm link cuffs can reduce the chance of large objects getting pinched, small and thin objects, such as hair, can be pulled in. Extra care should be taken with long hair, clothing, and small fingers around the shoulder rollers.","title":"Pinch Points"},{"location":"stretch-tutorials/getting_started/safety_guide/#crush-points","text":"The lift degree of freedom is the strongest joint on the robot and as such can apply potentially unsafe forces to a person. The lift, while in motion, may trap or crush objects between the \u2018shoulder\u2019 and another surface. As such, best practices for lift safety should always be used when using the lift degree of freedom. The lift has a max theoretical strength of nearly 200N of linear force. In practice, this force is limited by the lift\u2019s Guarded Move function, which places the lift in Safety Mode when the actuator forces exceed a threshold. The diagrams below show the potential crush points at the top and bottom of the lift range of motion. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Crush Points"},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/","text":"Stretch Troubleshooting Guide This guide covers common issues and ways to resolve them. Please check the Hello Robot Forum for additional topics not covered here. Xbox teleoperation is not working The provided Easy SMX wireless controller can accidentally be placed in the wrong mode. The mode is indicated by the round illuminated ring (shown as Connect below). The top 2 LEDs only should be illuminated. If a different LED pattern is shown then the button mapping expected by stretch_xbox_controller_teleop.py will be incorrect. To set the controller into the correct mode: Hold the center button down for 5s. It will switch modes. Release. Repeat until the top half of the ring (upper two lights) is illuminated. In addition, check that the provided USB dongle is plugged into the robot USB port in its trunk. Battery is not staying charged Stretch RE1 Stretch 2 Please review the troubleshooting section of the RE1 Battery Maintenance Guide . Please review the troubleshooting section of the Stretch 2 Battery Maintenance Guide . RPC Transport Errors (Stretch doesn't respond to commands) If more than one instance of Stretch Body's Robot class is instantiated at a time, Stretch Body will report communication errors and will not always execute motion commands as expected. This is because the Robot class manages communications with the robot hardware and it doesn't support multiple writes to the USB devices. These errors can appear as Transport RX Error on RPC_ACK_SEND_BLOCK_MORE False 0 102 ---- Debug Exception --------------- New RPC ------------------------- Framer sent RPC_START_NEW_RPC ... or as IOError ( None ) : None ... To check if an instance of Robot is already instantiated, you may use the Unix top command to monitor active processes. You may use the Unix pkill command to end the background instance of Robot. pkill -9 python As shipped, Stretch launches stretch_xbox_controller_teleop.py upon boot. It is necessary to turn off this automatic launch feature, otherwise, your own Robot instance will conflict with this script. Additionally, if you are logged into multiple accounts, a Robot instance may be active in another user account. To turn it off, search for 'Startup' from Ubuntu Activities. Uncheck the box for 'hello_robot_xbox_teleop'. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Troubleshooting"},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/#stretch-troubleshooting-guide","text":"This guide covers common issues and ways to resolve them. Please check the Hello Robot Forum for additional topics not covered here.","title":"Stretch Troubleshooting Guide"},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/#xbox-teleoperation-is-not-working","text":"The provided Easy SMX wireless controller can accidentally be placed in the wrong mode. The mode is indicated by the round illuminated ring (shown as Connect below). The top 2 LEDs only should be illuminated. If a different LED pattern is shown then the button mapping expected by stretch_xbox_controller_teleop.py will be incorrect. To set the controller into the correct mode: Hold the center button down for 5s. It will switch modes. Release. Repeat until the top half of the ring (upper two lights) is illuminated. In addition, check that the provided USB dongle is plugged into the robot USB port in its trunk.","title":"Xbox teleoperation is not working"},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/#battery-is-not-staying-charged","text":"Stretch RE1 Stretch 2 Please review the troubleshooting section of the RE1 Battery Maintenance Guide . Please review the troubleshooting section of the Stretch 2 Battery Maintenance Guide .","title":"Battery is not staying charged"},{"location":"stretch-tutorials/getting_started/troubleshooting_guide/#rpc-transport-errors-stretch-doesnt-respond-to-commands","text":"If more than one instance of Stretch Body's Robot class is instantiated at a time, Stretch Body will report communication errors and will not always execute motion commands as expected. This is because the Robot class manages communications with the robot hardware and it doesn't support multiple writes to the USB devices. These errors can appear as Transport RX Error on RPC_ACK_SEND_BLOCK_MORE False 0 102 ---- Debug Exception --------------- New RPC ------------------------- Framer sent RPC_START_NEW_RPC ... or as IOError ( None ) : None ... To check if an instance of Robot is already instantiated, you may use the Unix top command to monitor active processes. You may use the Unix pkill command to end the background instance of Robot. pkill -9 python As shipped, Stretch launches stretch_xbox_controller_teleop.py upon boot. It is necessary to turn off this automatic launch feature, otherwise, your own Robot instance will conflict with this script. Additionally, if you are logged into multiple accounts, a Robot instance may be active in another user account. To turn it off, search for 'Startup' from Ubuntu Activities. Uncheck the box for 'hello_robot_xbox_teleop'. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"RPC Transport Errors (Stretch doesn't respond to commands)"},{"location":"stretch-tutorials/getting_started/untethered_operation/","text":"Untethered Operation As a mobile manipulator, Stretch can only go so far when tethered to the monitor, keyboard, and mouse setup. This guide will explain three methods of setting up the Stretch for untethered usage. These methods typically require a wireless network, but it is possible to set up any of these methods without a wireless network by setting up a hotspot . Remote Desktop Requirements This is the recommended approach if you are running Windows or MacOS. This method requires a Virtual Network Computing (VNC) package. Using any of the free or paid options available for Windows, MacOS, and Chrome will be fine since they all use the Remote Frame Buffer (RFB) protocol to communicate with the robot. If you're using Ubuntu, Remmina Remote Desktop Client will be installed by default. How To While Stretch is tethered to the monitor, keyboard, and mouse setup, first verify that the robot is connected to the wireless network then install Vino VNC server using the following command: sudo apt install vino Go to System Settings. Select the Sharing tab and turn it on then, turn on Screen Sharing and choose a password. If you plan to connect to the robot from a Windows or MacOS machine, then open a terminal and run the following command. sudo gsettings set org.gnome.Vino require-encryption false Finally, we need the robot's IP address, username, and password. Open a terminal and run ifconfig , which will print out the network information of the machine. In the wireless section (typically named wlp2s0), look for something that looks like \"inet 10.0.0.15\". The four numbers represent the IP address of the robot on the local network. The robot's default username and password are printed on papers that came in the tools box alongside the robot. VNC will only function properly with an external display attached to the robot. Using a dummy HDMI dongle when operating the robot untethered via VNC is recommended. One possible dummy HDMI dongle can be found on Amazon here . On your computer, connect to the same wireless network as the robot and open the VNC package being used. Using the robot's IP address and username, initialize a new connection to the robot. The robot's desktop will open in a new window. SSH & X Server Requirements This is the recommended approach if you are running a Unix-based operating system, like Ubuntu or Arch Linux. This method requires both SSH and X Server to be installed. While most Unix-based operating systems have both installed by default, MacOS will only have SSH installed and Windows has neither installed by default. It is possible to install these tools for MacOS or Windows. How To While the Remote Desktop approach is easy to set up, graphics and interaction with the remote desktop are often slow. In this method, we will use SSH and X Server to accomplish the same a bit faster. SSH stands for Secure Shell, enabling one to remotely use the terminal (shell) of another machine. X Server is used on many Unix variants to render the Windowed GUI of applications. With SSH and X Server, it is possible to render a Windowed GUI of an application running on the robot on your computer's screen. The first step is to identify the robot's IP address on the local network. While Stretch is tethered to the monitor, keyboard, and mouse, verify that the robot is connected to a wireless network. Then, open a terminal and run ifconfig , which will print out the network information of the machine. In the wireless section (typically named wlp2s0), look for something that looks like \"inet 10.0.0.15\". The four numbers represent the IP address of the robot on the local network. Using any other machine on the same local network, I can SSH into the robot using this IP address. Take note of the username and password of the robot. The default combo is printed on papers that came in the tools box alongside the robot. To SSH into the robot, run the following. It will require the password and may ask you to add the robot to the known hosts. ssh -X username@ip-address Now that you're SSH-ed into the robot, you can disconnect any wires from the robot. You can accomplish any of the same tasks through the terminal. For example, you can type in ipython and interact with the robot using Stretch Body, as explained in the Quick Start Guide . Furthermore, Windowed GUI applications that would have been displayed on the monitor will now display on your SSH-ed machine. For example, we can open Rviz to visualize what the robot is seeing. Open two terminals and SSH into the robot as explained above. In the first, run roslaunch stretch_core stretch_driver.launch . You should see some information print out in the terminal. In the second, run rviz . A window will pop up and information about the robot can be visualized by clicking on Add -> RobotModel and Add -> By Topic -> /Scan . Additional information on how to use ROS tools can be found in ROS's tutorials or in our Stretch ROS guides . Moving files to/from the robot wirelessly It's common to need to move files to/from the robot wirelessly and a tool similar to SSH can help with this: Secure Copy (SCP). To send the files from your computer to the robot, run: scp ./filename username@ip-address:~/path/to/put/it/ To copy the files from the robot to your computer, run the reverse: scp username@ip-address:/path/to/filename ~/path/to/put/it/ This works for copying directories and their contents as well. ROS Remote Master Requirements This is the recommended approach if you are running Ubuntu 16.04/18.04/20.04 with ROS kinetic/melodic/noetic installed on your computer. This method will utilize the local installation of ROS tools, such as Rviz, rostopic, and rosservice, while retrieving data from the robot. How To If you are developing ROS code to test on Stretch and you already have ROS installed on your Ubuntu computer, then there is an easier way of using Rviz than the method described in SSH & X Server . In the ROS world, this concept is known as \"remote master\". First, identify your robot's and computer's IP address on the network (e.g. using ifconfig ). These are robot-ip-address and computer-ip-address respectively. Next, run the following on the robot: export ROS_IP = robot-ip-address export ROS_MASTER_URI = http://robot-ip-address:11311/ Next, start the ROS launch files on the robot as you normally would. Finally, on your computer, run: export ROS_IP = computer-ip-address export ROS_MASTER_URI = http://robot-ip-address:11311 If you use ROS Remote Master often, you can export these environment variables in your bashrc . Tools like rostopic and rosservice can now be used on your computer as you would have on the robot. For example, you can use rostopic list on your computer to print out the topics available on the robot. Additional information can be found in the ROS Multiple Machines Tutorial . Visualizing remotely with RViz If you'd like to visualize the robot model on your computer using Rviz, you'll need to set up a ROS workspace with the Stretch Description package. First, copy over the ~/stretch_user directory from the robot to your computer (e.g. using Secure Copy ). Second, clone Stretch Install , and checkout the noetic branch if you are running ROS Noetic on the robot. Finally, run the stretch_create_ros_workspace.sh script. A ROS Workspace with the Stretch ROS packages is now set up on your computer. Furthermore, Stretch Description has been set up with your robot's calibrated URDF. We can now use remote master and Rviz to visualize what the robot is seeing on your computer. Open two terminals. First, SSH into the robot and run roslaunch stretch_core stretch_driver.launch . You should see some information print out in the terminal. In the second, run rviz . A window will pop up and information about the robot can be visualized by clicking on Add -> RobotModel and Add -> By Topic -> /Scan . Additional information on how to use Rviz can be found in ROS's tutorials or our Stretch ROS guides . Additional Ideas Although the methods described above will enable you to wirelessly control the robot, there are several ways to improve the usability and security of your wireless connection. These ideas are listed here. Hotspot Often the trouble with wirelessly controlling the robot is the network. If your network is using industrial security like 2-factor authentication, there may be trouble connecting the robot to the network. If the network is servicing a large number of users, the connection may feel sluggish. The alternative is to skip the network by connecting directly to the robot. After starting a hotspot on the robot, you can follow instructions for any of the methods described above to control the robot. The trade-off is that while connected to the robot's hotspot, you will be unable to connect to the internet. To set up the robot's hotspot, visit the Ubuntu Wifi Settings page in the robot. Click on the hamburger menu in the top right and select \"Enable hotspot\". From your local machine, connect to the robot's hotspot and save the credentials. To change the hotspot's password or enable the hotspot automatically whenever the robot boots, see the following Stackoverflow post . VS Code Remote Development It is possible to simultaneously develop code on the robot while running wireless experiments using the Remote Development Extension provided by the VS Code IDE. If you're already using the VS Code IDE , navigate to the Extensions tab and search for Remote Development Extension by Microsoft . After installing, click on a green button in the bottom left of the screen and then select \"Remote-SSH: Connect to Host\". Setting this up for the first time will require you to know the robot's IP address and username. Add a new host with the information. While connecting, VS Code will ask you for the password of the robot. Once you are connected, you can open any folder and edit the code remotely. Combined with the method explained in SSH & X Server , this is a powerful method of iteratively developing code while testing it. Static IP Address Routers that serve wireless networks often dynamically assign IP addressess to machines that connect to the network. This means that your robot's IP address may have changed since the last time you turned it on. Since it becomes a pain to connect to the monitor, keyboard, and mouse setup every time to run ifconfig , many users prefer to assign the robot a static IP address. If you control the router, visit the router's settings page to set up the robot's static IP address. It is common at universities and companies to have staff dedicated to the management of the network. This staff will often be able to set up a static IP address for the robot. Public Key Authentication The method of SSH described in SSH & X Server uses basic password authentication when connecting. There is a better and more secure method of SSH-ing into the robot called Public Key Authentication. This method will allow multiple developers to SSH into the robot without having to share the robot's admin password. The first step is to generate public and private keys on your computer. Linux and MacOS machines can simply open the terminal and run: ssh-keygen -t ed25519 -f <key_filepath_without_extension> -C \"<some comment>\" It will prompt you to enter a password. If you do, you'll need it to use the private key when you SSH into the robot. Next, we give the robot the public key. Linux and MacOS machines can run: ssh-copy-id -i <key_filepath_without_extension> username@ip-address This requires you to know the username and IP address of the robot. Instructions on how to find this information are found in the SSH & X Server section. You may now SSH into the robot as normal, and no prompt for the robot's password will appear. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Untethered Operation"},{"location":"stretch-tutorials/getting_started/untethered_operation/#untethered-operation","text":"As a mobile manipulator, Stretch can only go so far when tethered to the monitor, keyboard, and mouse setup. This guide will explain three methods of setting up the Stretch for untethered usage. These methods typically require a wireless network, but it is possible to set up any of these methods without a wireless network by setting up a hotspot .","title":"Untethered Operation"},{"location":"stretch-tutorials/getting_started/untethered_operation/#remote-desktop","text":"","title":"Remote Desktop"},{"location":"stretch-tutorials/getting_started/untethered_operation/#requirements","text":"This is the recommended approach if you are running Windows or MacOS. This method requires a Virtual Network Computing (VNC) package. Using any of the free or paid options available for Windows, MacOS, and Chrome will be fine since they all use the Remote Frame Buffer (RFB) protocol to communicate with the robot. If you're using Ubuntu, Remmina Remote Desktop Client will be installed by default.","title":"Requirements"},{"location":"stretch-tutorials/getting_started/untethered_operation/#how-to","text":"While Stretch is tethered to the monitor, keyboard, and mouse setup, first verify that the robot is connected to the wireless network then install Vino VNC server using the following command: sudo apt install vino Go to System Settings. Select the Sharing tab and turn it on then, turn on Screen Sharing and choose a password. If you plan to connect to the robot from a Windows or MacOS machine, then open a terminal and run the following command. sudo gsettings set org.gnome.Vino require-encryption false Finally, we need the robot's IP address, username, and password. Open a terminal and run ifconfig , which will print out the network information of the machine. In the wireless section (typically named wlp2s0), look for something that looks like \"inet 10.0.0.15\". The four numbers represent the IP address of the robot on the local network. The robot's default username and password are printed on papers that came in the tools box alongside the robot. VNC will only function properly with an external display attached to the robot. Using a dummy HDMI dongle when operating the robot untethered via VNC is recommended. One possible dummy HDMI dongle can be found on Amazon here . On your computer, connect to the same wireless network as the robot and open the VNC package being used. Using the robot's IP address and username, initialize a new connection to the robot. The robot's desktop will open in a new window.","title":"How To"},{"location":"stretch-tutorials/getting_started/untethered_operation/#ssh-x-server","text":"","title":"SSH &amp; X Server"},{"location":"stretch-tutorials/getting_started/untethered_operation/#requirements_1","text":"This is the recommended approach if you are running a Unix-based operating system, like Ubuntu or Arch Linux. This method requires both SSH and X Server to be installed. While most Unix-based operating systems have both installed by default, MacOS will only have SSH installed and Windows has neither installed by default. It is possible to install these tools for MacOS or Windows.","title":"Requirements"},{"location":"stretch-tutorials/getting_started/untethered_operation/#how-to_1","text":"While the Remote Desktop approach is easy to set up, graphics and interaction with the remote desktop are often slow. In this method, we will use SSH and X Server to accomplish the same a bit faster. SSH stands for Secure Shell, enabling one to remotely use the terminal (shell) of another machine. X Server is used on many Unix variants to render the Windowed GUI of applications. With SSH and X Server, it is possible to render a Windowed GUI of an application running on the robot on your computer's screen. The first step is to identify the robot's IP address on the local network. While Stretch is tethered to the monitor, keyboard, and mouse, verify that the robot is connected to a wireless network. Then, open a terminal and run ifconfig , which will print out the network information of the machine. In the wireless section (typically named wlp2s0), look for something that looks like \"inet 10.0.0.15\". The four numbers represent the IP address of the robot on the local network. Using any other machine on the same local network, I can SSH into the robot using this IP address. Take note of the username and password of the robot. The default combo is printed on papers that came in the tools box alongside the robot. To SSH into the robot, run the following. It will require the password and may ask you to add the robot to the known hosts. ssh -X username@ip-address Now that you're SSH-ed into the robot, you can disconnect any wires from the robot. You can accomplish any of the same tasks through the terminal. For example, you can type in ipython and interact with the robot using Stretch Body, as explained in the Quick Start Guide . Furthermore, Windowed GUI applications that would have been displayed on the monitor will now display on your SSH-ed machine. For example, we can open Rviz to visualize what the robot is seeing. Open two terminals and SSH into the robot as explained above. In the first, run roslaunch stretch_core stretch_driver.launch . You should see some information print out in the terminal. In the second, run rviz . A window will pop up and information about the robot can be visualized by clicking on Add -> RobotModel and Add -> By Topic -> /Scan . Additional information on how to use ROS tools can be found in ROS's tutorials or in our Stretch ROS guides .","title":"How To"},{"location":"stretch-tutorials/getting_started/untethered_operation/#moving-files-tofrom-the-robot-wirelessly","text":"It's common to need to move files to/from the robot wirelessly and a tool similar to SSH can help with this: Secure Copy (SCP). To send the files from your computer to the robot, run: scp ./filename username@ip-address:~/path/to/put/it/ To copy the files from the robot to your computer, run the reverse: scp username@ip-address:/path/to/filename ~/path/to/put/it/ This works for copying directories and their contents as well.","title":"Moving files to/from the robot wirelessly"},{"location":"stretch-tutorials/getting_started/untethered_operation/#ros-remote-master","text":"","title":"ROS Remote Master"},{"location":"stretch-tutorials/getting_started/untethered_operation/#requirements_2","text":"This is the recommended approach if you are running Ubuntu 16.04/18.04/20.04 with ROS kinetic/melodic/noetic installed on your computer. This method will utilize the local installation of ROS tools, such as Rviz, rostopic, and rosservice, while retrieving data from the robot.","title":"Requirements"},{"location":"stretch-tutorials/getting_started/untethered_operation/#how-to_2","text":"If you are developing ROS code to test on Stretch and you already have ROS installed on your Ubuntu computer, then there is an easier way of using Rviz than the method described in SSH & X Server . In the ROS world, this concept is known as \"remote master\". First, identify your robot's and computer's IP address on the network (e.g. using ifconfig ). These are robot-ip-address and computer-ip-address respectively. Next, run the following on the robot: export ROS_IP = robot-ip-address export ROS_MASTER_URI = http://robot-ip-address:11311/ Next, start the ROS launch files on the robot as you normally would. Finally, on your computer, run: export ROS_IP = computer-ip-address export ROS_MASTER_URI = http://robot-ip-address:11311 If you use ROS Remote Master often, you can export these environment variables in your bashrc . Tools like rostopic and rosservice can now be used on your computer as you would have on the robot. For example, you can use rostopic list on your computer to print out the topics available on the robot. Additional information can be found in the ROS Multiple Machines Tutorial .","title":"How To"},{"location":"stretch-tutorials/getting_started/untethered_operation/#visualizing-remotely-with-rviz","text":"If you'd like to visualize the robot model on your computer using Rviz, you'll need to set up a ROS workspace with the Stretch Description package. First, copy over the ~/stretch_user directory from the robot to your computer (e.g. using Secure Copy ). Second, clone Stretch Install , and checkout the noetic branch if you are running ROS Noetic on the robot. Finally, run the stretch_create_ros_workspace.sh script. A ROS Workspace with the Stretch ROS packages is now set up on your computer. Furthermore, Stretch Description has been set up with your robot's calibrated URDF. We can now use remote master and Rviz to visualize what the robot is seeing on your computer. Open two terminals. First, SSH into the robot and run roslaunch stretch_core stretch_driver.launch . You should see some information print out in the terminal. In the second, run rviz . A window will pop up and information about the robot can be visualized by clicking on Add -> RobotModel and Add -> By Topic -> /Scan . Additional information on how to use Rviz can be found in ROS's tutorials or our Stretch ROS guides .","title":"Visualizing remotely with RViz"},{"location":"stretch-tutorials/getting_started/untethered_operation/#additional-ideas","text":"Although the methods described above will enable you to wirelessly control the robot, there are several ways to improve the usability and security of your wireless connection. These ideas are listed here.","title":"Additional Ideas"},{"location":"stretch-tutorials/getting_started/untethered_operation/#hotspot","text":"Often the trouble with wirelessly controlling the robot is the network. If your network is using industrial security like 2-factor authentication, there may be trouble connecting the robot to the network. If the network is servicing a large number of users, the connection may feel sluggish. The alternative is to skip the network by connecting directly to the robot. After starting a hotspot on the robot, you can follow instructions for any of the methods described above to control the robot. The trade-off is that while connected to the robot's hotspot, you will be unable to connect to the internet. To set up the robot's hotspot, visit the Ubuntu Wifi Settings page in the robot. Click on the hamburger menu in the top right and select \"Enable hotspot\". From your local machine, connect to the robot's hotspot and save the credentials. To change the hotspot's password or enable the hotspot automatically whenever the robot boots, see the following Stackoverflow post .","title":"Hotspot"},{"location":"stretch-tutorials/getting_started/untethered_operation/#vs-code-remote-development","text":"It is possible to simultaneously develop code on the robot while running wireless experiments using the Remote Development Extension provided by the VS Code IDE. If you're already using the VS Code IDE , navigate to the Extensions tab and search for Remote Development Extension by Microsoft . After installing, click on a green button in the bottom left of the screen and then select \"Remote-SSH: Connect to Host\". Setting this up for the first time will require you to know the robot's IP address and username. Add a new host with the information. While connecting, VS Code will ask you for the password of the robot. Once you are connected, you can open any folder and edit the code remotely. Combined with the method explained in SSH & X Server , this is a powerful method of iteratively developing code while testing it.","title":"VS Code Remote Development"},{"location":"stretch-tutorials/getting_started/untethered_operation/#static-ip-address","text":"Routers that serve wireless networks often dynamically assign IP addressess to machines that connect to the network. This means that your robot's IP address may have changed since the last time you turned it on. Since it becomes a pain to connect to the monitor, keyboard, and mouse setup every time to run ifconfig , many users prefer to assign the robot a static IP address. If you control the router, visit the router's settings page to set up the robot's static IP address. It is common at universities and companies to have staff dedicated to the management of the network. This staff will often be able to set up a static IP address for the robot.","title":"Static IP Address"},{"location":"stretch-tutorials/getting_started/untethered_operation/#public-key-authentication","text":"The method of SSH described in SSH & X Server uses basic password authentication when connecting. There is a better and more secure method of SSH-ing into the robot called Public Key Authentication. This method will allow multiple developers to SSH into the robot without having to share the robot's admin password. The first step is to generate public and private keys on your computer. Linux and MacOS machines can simply open the terminal and run: ssh-keygen -t ed25519 -f <key_filepath_without_extension> -C \"<some comment>\" It will prompt you to enter a password. If you do, you'll need it to use the private key when you SSH into the robot. Next, we give the robot the public key. Linux and MacOS machines can run: ssh-copy-id -i <key_filepath_without_extension> username@ip-address This requires you to know the username and IP address of the robot. Instructions on how to find this information are found in the SSH & X Server section. You may now SSH into the robot as normal, and no prompt for the robot's password will appear. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Public Key Authentication"},{"location":"stretch-tutorials/getting_started/updating_software/","text":"Updating Stretch Software Stretch's software is improved with new features and bug fixes with each update. In this guide, we cover when and how to update the various software components on your Stretch. When to Update We develop our software publicly on GitHub, allowing anyone to follow and propose the development of a code feature or bug fix. While we wholeheartedly welcome collaboration on GitHub, it is not necessary to be active on GitHub to follow our software releases. We announce every major release of software on our forum . These are stable releases with code that has been extensively tested on many Stretch robots. To be notified of new releases, create an account on the forum and click the bell icon in the top left of the announcements section . The forum is also available to report issues and ask questions about any of our software packages. How to Update Each Stretch is shipped with firmware, a Python SDK, and ROS packages developed specifically for Stretch. At the moment, there are three separate processes for updating each of these components. Stretch ROS Stretch ROS is the Robot Operating System (ROS) interface to the robot. Many robotics developers find ROS useful to bootstrap their robotics software developments. Depending on whether you want to set up a ROS or ROS 2 workspace, the easiest way to download the most recent updates in the stretch_ros and stretch_ros2 code repositories, while resolving all source-built dependencies at the same time, is by following the instructions in the Creating a New ROS Workspace section in the stretch_install repo. Warning Before you proceed, please ensure that all your personal files in the catkin or ament workspace have been backed up safely. This is important because executing the following set of commands deletes your existing workspace and replaces it with a fresh one. To download the stretch_install repo, execute: cd ~/ git clone https://github.com/hello-robot/stretch_install.git cd stretch_install git pull To replace the ROS Melodic catkin_ws in Ubuntu 18.04, execute: ./factory/18.04/stretch_create_catkin_workspace.sh -w <optional-path-to-ws> To replace the ROS Noetic catkin_ws in Ubuntu 20.04, execute: ./factory/20.04/stretch_create_catkin_workspace.sh -w <optional-path-to-ws> To replace the ROS 2 Galactic ament_ws in Ubuntu 20.04, execute: ./factory/20.04/stretch_create_ament_workspace.sh -w <optional-path-to-ws> Stretch Body Python SDK Stretch Body is the Python SDK for the robot. It abstracts away the low-level details of communication with the embedded devices and provides an intuitive API for working with the robot. You may update it using the following commands depending on the Python version. If you are using Python2, execute: pip install -U hello-robot-stretch-body pip install -U hello-robot-stretch-body-tools pip install -U hello-robot-stretch-factory pip3 install -U hello_robot_stretch_body_tools_py3 For Python3, execute: python3 -m pip -q install --no-warn-script-location hello-robot-stretch-body python3 -m pip -q install --no-warn-script-location hello-robot-stretch-body-tools python3 -m pip -q install --no-warn-script-location hello-robot-stretch-factory python3 -m pip -q install --no-warn-script-location hello-robot-stretch-tool-share Stretch Firmware The firmware and the Python SDK (called Stretch Body) communicate on an established protocol. Therefore, it is important to maintain a protocol match between the different firmware and Stretch Body versions. Fortunately, there is a script that handles this automatically. In the command line, run the following command: REx_firmware_updater.py --status This script will automatically determine what version is currently running on the robot and provide a recommendation for the next step. Follow the next steps provided by the firmware updater script. Ubuntu The operating system upon which Stretch is built is called Ubuntu. This operating system provides the underlying packages that power Stretch's software packages. Furthermore, users of Stretch depend on this operating system and the underlying packages to develop software on Stretch. Therefore, it is important to keep the OS and these underlying packages up to date. In the command line, run the following command: sudo apt update sudo apt upgrade Apt is the package manager that handles updates for all Ubuntu packages. Troubleshooting Firmware Mismatch Error When working with Stretch Body, if you see the following error: ---------------- Firmware protocol mismatch on /dev/XXXX. Protocol on board is pX. Valid protocol is: pX. Disabling device. Please upgrade the firmware and/or version of Stretch Body. ---------------- This error appears because the low-level Python SDK and the firmware cannot communicate with each other. There is a protocol mismatch preventing communication between the two. Simply run the following script and follow its recommendations to upgrade/downgrade the firmware as necessary to match the protocol level of Stretch Body. REx_firmware_updater.py --status All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Updating Software"},{"location":"stretch-tutorials/getting_started/updating_software/#updating-stretch-software","text":"Stretch's software is improved with new features and bug fixes with each update. In this guide, we cover when and how to update the various software components on your Stretch.","title":"Updating Stretch Software"},{"location":"stretch-tutorials/getting_started/updating_software/#when-to-update","text":"We develop our software publicly on GitHub, allowing anyone to follow and propose the development of a code feature or bug fix. While we wholeheartedly welcome collaboration on GitHub, it is not necessary to be active on GitHub to follow our software releases. We announce every major release of software on our forum . These are stable releases with code that has been extensively tested on many Stretch robots. To be notified of new releases, create an account on the forum and click the bell icon in the top left of the announcements section . The forum is also available to report issues and ask questions about any of our software packages.","title":"When to Update"},{"location":"stretch-tutorials/getting_started/updating_software/#how-to-update","text":"Each Stretch is shipped with firmware, a Python SDK, and ROS packages developed specifically for Stretch. At the moment, there are three separate processes for updating each of these components.","title":"How to Update"},{"location":"stretch-tutorials/getting_started/updating_software/#stretch-ros","text":"Stretch ROS is the Robot Operating System (ROS) interface to the robot. Many robotics developers find ROS useful to bootstrap their robotics software developments. Depending on whether you want to set up a ROS or ROS 2 workspace, the easiest way to download the most recent updates in the stretch_ros and stretch_ros2 code repositories, while resolving all source-built dependencies at the same time, is by following the instructions in the Creating a New ROS Workspace section in the stretch_install repo. Warning Before you proceed, please ensure that all your personal files in the catkin or ament workspace have been backed up safely. This is important because executing the following set of commands deletes your existing workspace and replaces it with a fresh one. To download the stretch_install repo, execute: cd ~/ git clone https://github.com/hello-robot/stretch_install.git cd stretch_install git pull To replace the ROS Melodic catkin_ws in Ubuntu 18.04, execute: ./factory/18.04/stretch_create_catkin_workspace.sh -w <optional-path-to-ws> To replace the ROS Noetic catkin_ws in Ubuntu 20.04, execute: ./factory/20.04/stretch_create_catkin_workspace.sh -w <optional-path-to-ws> To replace the ROS 2 Galactic ament_ws in Ubuntu 20.04, execute: ./factory/20.04/stretch_create_ament_workspace.sh -w <optional-path-to-ws>","title":"Stretch ROS"},{"location":"stretch-tutorials/getting_started/updating_software/#stretch-body-python-sdk","text":"Stretch Body is the Python SDK for the robot. It abstracts away the low-level details of communication with the embedded devices and provides an intuitive API for working with the robot. You may update it using the following commands depending on the Python version. If you are using Python2, execute: pip install -U hello-robot-stretch-body pip install -U hello-robot-stretch-body-tools pip install -U hello-robot-stretch-factory pip3 install -U hello_robot_stretch_body_tools_py3 For Python3, execute: python3 -m pip -q install --no-warn-script-location hello-robot-stretch-body python3 -m pip -q install --no-warn-script-location hello-robot-stretch-body-tools python3 -m pip -q install --no-warn-script-location hello-robot-stretch-factory python3 -m pip -q install --no-warn-script-location hello-robot-stretch-tool-share","title":"Stretch Body Python SDK"},{"location":"stretch-tutorials/getting_started/updating_software/#stretch-firmware","text":"The firmware and the Python SDK (called Stretch Body) communicate on an established protocol. Therefore, it is important to maintain a protocol match between the different firmware and Stretch Body versions. Fortunately, there is a script that handles this automatically. In the command line, run the following command: REx_firmware_updater.py --status This script will automatically determine what version is currently running on the robot and provide a recommendation for the next step. Follow the next steps provided by the firmware updater script.","title":"Stretch Firmware"},{"location":"stretch-tutorials/getting_started/updating_software/#ubuntu","text":"The operating system upon which Stretch is built is called Ubuntu. This operating system provides the underlying packages that power Stretch's software packages. Furthermore, users of Stretch depend on this operating system and the underlying packages to develop software on Stretch. Therefore, it is important to keep the OS and these underlying packages up to date. In the command line, run the following command: sudo apt update sudo apt upgrade Apt is the package manager that handles updates for all Ubuntu packages.","title":"Ubuntu"},{"location":"stretch-tutorials/getting_started/updating_software/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"stretch-tutorials/getting_started/updating_software/#firmware-mismatch-error","text":"When working with Stretch Body, if you see the following error: ---------------- Firmware protocol mismatch on /dev/XXXX. Protocol on board is pX. Valid protocol is: pX. Disabling device. Please upgrade the firmware and/or version of Stretch Body. ---------------- This error appears because the low-level Python SDK and the firmware cannot communicate with each other. There is a protocol mismatch preventing communication between the two. Simply run the following script and follow its recommendations to upgrade/downgrade the firmware as necessary to match the protocol level of Stretch Body. REx_firmware_updater.py --status All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Firmware Mismatch Error"},{"location":"stretch-tutorials/ros1/","text":"Tutorial Track: Stretch ROS Robot Operating System (ROS) Despite the name, ROS is not an operating system. ROS is a middleware framework that is a collection of transport protocols, development and debugging tools, and open-source packages. As a transport protocol, ROS enables distributed communication via messages between nodes. As a development and debugging toolkit, ROS provides build systems that allows for writing applications in a wide variety of languages (Python and C++ are used in this tutorial track), a launch system to manage the execution of mutiple nodes simultaneously, and command line tools to interact with the running system. Finally, as a popular ecosystem, there are many open-source ROS packages that allow users to quickly prototype with new sensors, actuators, planners, perception stacks, and more. This tutorial track is for users looking to get familiar with programming Stretch robots via ROS. We recommend going through the tutorials in the following order: Basics Tutorial Description 1 Getting Started Setup instructions for ROS on Stretch. 2 Gazebo Basics Use Stretch in a simulated environment with Gazebo. 3 Teleoperating Stretch Control Stretch with a keyboard or xbox controller. 4 Internal State of Stretch Monitor the joint states of Stretch. 5 RViz Basics Visualize topics in Stretch. 6 Navigation Stack Motion planning and control for the mobile base using Nav stack. 7 MoveIt! Basics Motion planning and control for the arm using MoveIt. 8 Follow Joint Trajectory Commands Control joints using joint trajectory server. 9 Perception Use the Realsense D435i camera to visualize the environment. 10 ArUco Marker Detection Localize objects using ArUco markers. 11 ReSpeaker Microphone Array Learn to use the ReSpeaker Microphone Array. 12 FUNMAP Fast Unified Navigation, Manipulation and Planning. Other Examples To help get you get started on your software development, here are examples of nodes to have the stretch perform simple tasks. Tutorial Description 1 Teleoperate Stretch with a Node Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Print Joint States Print the joint states of Stretch. 6 Store Effort Values Print, store, and plot the effort values of the Stretch robot. 7 Capture Image Capture images from the RealSense camera data. 8 Voice to Text Interpret speech and save transcript to a text file. 9 Voice Teleoperation of Base Use speech to teleoperate the mobile base. 10 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 11 PointCloud Transformation Convert PointCloud2 data to a PointCloud and transform to a different frame. 12 ArUco Tag Locator Actuate the head to locate a requested ArUco marker tag and return a transform. 13 2D Navigation Goals Send 2D navigation goals to the move_base ROS node.","title":"Overview"},{"location":"stretch-tutorials/ros1/#tutorial-track-stretch-ros","text":"","title":"Tutorial Track: Stretch ROS"},{"location":"stretch-tutorials/ros1/#robot-operating-system-ros","text":"Despite the name, ROS is not an operating system. ROS is a middleware framework that is a collection of transport protocols, development and debugging tools, and open-source packages. As a transport protocol, ROS enables distributed communication via messages between nodes. As a development and debugging toolkit, ROS provides build systems that allows for writing applications in a wide variety of languages (Python and C++ are used in this tutorial track), a launch system to manage the execution of mutiple nodes simultaneously, and command line tools to interact with the running system. Finally, as a popular ecosystem, there are many open-source ROS packages that allow users to quickly prototype with new sensors, actuators, planners, perception stacks, and more. This tutorial track is for users looking to get familiar with programming Stretch robots via ROS. We recommend going through the tutorials in the following order:","title":"Robot Operating System (ROS)"},{"location":"stretch-tutorials/ros1/#basics","text":"Tutorial Description 1 Getting Started Setup instructions for ROS on Stretch. 2 Gazebo Basics Use Stretch in a simulated environment with Gazebo. 3 Teleoperating Stretch Control Stretch with a keyboard or xbox controller. 4 Internal State of Stretch Monitor the joint states of Stretch. 5 RViz Basics Visualize topics in Stretch. 6 Navigation Stack Motion planning and control for the mobile base using Nav stack. 7 MoveIt! Basics Motion planning and control for the arm using MoveIt. 8 Follow Joint Trajectory Commands Control joints using joint trajectory server. 9 Perception Use the Realsense D435i camera to visualize the environment. 10 ArUco Marker Detection Localize objects using ArUco markers. 11 ReSpeaker Microphone Array Learn to use the ReSpeaker Microphone Array. 12 FUNMAP Fast Unified Navigation, Manipulation and Planning.","title":"Basics"},{"location":"stretch-tutorials/ros1/#other-examples","text":"To help get you get started on your software development, here are examples of nodes to have the stretch perform simple tasks. Tutorial Description 1 Teleoperate Stretch with a Node Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Print Joint States Print the joint states of Stretch. 6 Store Effort Values Print, store, and plot the effort values of the Stretch robot. 7 Capture Image Capture images from the RealSense camera data. 8 Voice to Text Interpret speech and save transcript to a text file. 9 Voice Teleoperation of Base Use speech to teleoperate the mobile base. 10 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 11 PointCloud Transformation Convert PointCloud2 data to a PointCloud and transform to a different frame. 12 ArUco Tag Locator Actuate the head to locate a requested ArUco marker tag and return a transform. 13 2D Navigation Goals Send 2D navigation goals to the move_base ROS node.","title":"Other Examples"},{"location":"stretch-tutorials/ros1/aruco_marker_detection/","text":"ArUco Marker Detector For this tutorial, we will go over how to detect Stretch's ArUco markers and review the files that hold the information for the tags. Visualize ArUco Markers in RViz Begin by running the stretch driver launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_low_resolution.launch Next, in a new terminal, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. roslaunch stretch_core stretch_aruco.launch Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz You are going to need to teleoperate Stretch's head to detect the ArUco marker tags. Run the following command in a new terminal and control the head to point the camera toward the markers. rosrun stretch_core keyboard_teleop The ArUco Marker Dictionary When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml , that holds the information about the markers. If detect_aruco_markers node doesn\u2019t find an entry in stretch_marker_dict.yaml for a particular ArUco marker ID number, it uses the default entry. For example, most robots have shipped with the following default entry: 'default' : 'length_mm' : 24 'use_rgb_only' : False 'name' : 'unknown' 'link' : None and the following entry for the ArUco marker on the top of the wrist '133' : 'length_mm' : 23.5 'use_rgb_only' : False 'name' : 'wrist_top' 'link' : 'link_aruco_top_wrist' Dictionary Breakdown '133' : The dictionary key for each entry is the ArUco marker\u2019s ID number or default . For example, the entry shown above for the ArUco marker on the top of the wrist assumes that the marker\u2019s ID number is 133 . 'length_mm' : 23.5 The length_mm value used by detect_aruco_markers is important for estimating the pose of an ArUco marker. Note If the actual width and height of the marker do not match this value, then pose estimation will be poor. Thus, carefully measure custom Aruco markers. 'use_rgb_only' : False If use_rgb_only is True , detect_aruco_markers will ignore depth images from the Intel RealSense D435i depth camera when estimating the pose of the marker and will instead only use RGB images from the D435i. 'name' : 'wrist_top' name is used for the text string of the ArUco marker\u2019s ROS Marker in the ROS MarkerArray Message published by the detect_aruco_markers ROS node. 'link' : 'link_aruco_top_wrist' link is currently used by stretch_calibration . It is the name of the link associated with a body-mounted ArUco marker in the robot\u2019s URDF . It\u2019s good practice to add an entry to stretch_marker_dict.yaml for each ArUco marker you use. Create a New ArUco Marker At Hello Robot, we\u2019ve used the following guide when generating new ArUco markers. We generate ArUco markers using a 6x6-bit grid (36 bits) with 250 unique codes. This corresponds with DICT_6X6_250 defined in OpenCV . We generate markers using this online ArUco marker generator by setting the Dictionary entry to 6x6 and then setting the Marker ID and Marker size, mm as appropriate for the specific application. We strongly recommend measuring the actual marker by hand before adding an entry for it to stretch_marker_dict.yaml . We select marker ID numbers using the following ranges. 0 - 99: reserved for users 100 - 249: reserved for official use by Hello Robot Inc. 100 - 199: reserved for robots with distinct sets of body-mounted markers Allows different robots near each other to use distinct sets of body-mounted markers to avoid confusion. This could be valuable for various uses of body-mounted markers, including calibration, visual servoing, visual motion capture, and multi-robot tasks. 5 markers per robot = 2 on the mobile base + 2 on the wrist + 1 on the shoulder 20 distinct sets = 100 available ID numbers / 5 ID numbers per robot 200 - 249: reserved for official accessories 245 for the prototype docking station 246-249 for large floor markers When coming up with this guide, we expected the following: Body-mounted accessories with the same ID numbers mounted to different robots could be disambiguated using the expected range of 3D locations of the ArUco markers on the calibrated body. Accessories in the environment with the same ID numbers could be disambiguated using a map or nearby observable features of the environment.","title":"ArUco Marker Detection"},{"location":"stretch-tutorials/ros1/aruco_marker_detection/#aruco-marker-detector","text":"For this tutorial, we will go over how to detect Stretch's ArUco markers and review the files that hold the information for the tags.","title":"ArUco Marker Detector"},{"location":"stretch-tutorials/ros1/aruco_marker_detection/#visualize-aruco-markers-in-rviz","text":"Begin by running the stretch driver launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_low_resolution.launch Next, in a new terminal, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. roslaunch stretch_core stretch_aruco.launch Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz You are going to need to teleoperate Stretch's head to detect the ArUco marker tags. Run the following command in a new terminal and control the head to point the camera toward the markers. rosrun stretch_core keyboard_teleop","title":"Visualize ArUco Markers in RViz"},{"location":"stretch-tutorials/ros1/aruco_marker_detection/#the-aruco-marker-dictionary","text":"When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml , that holds the information about the markers. If detect_aruco_markers node doesn\u2019t find an entry in stretch_marker_dict.yaml for a particular ArUco marker ID number, it uses the default entry. For example, most robots have shipped with the following default entry: 'default' : 'length_mm' : 24 'use_rgb_only' : False 'name' : 'unknown' 'link' : None and the following entry for the ArUco marker on the top of the wrist '133' : 'length_mm' : 23.5 'use_rgb_only' : False 'name' : 'wrist_top' 'link' : 'link_aruco_top_wrist' Dictionary Breakdown '133' : The dictionary key for each entry is the ArUco marker\u2019s ID number or default . For example, the entry shown above for the ArUco marker on the top of the wrist assumes that the marker\u2019s ID number is 133 . 'length_mm' : 23.5 The length_mm value used by detect_aruco_markers is important for estimating the pose of an ArUco marker. Note If the actual width and height of the marker do not match this value, then pose estimation will be poor. Thus, carefully measure custom Aruco markers. 'use_rgb_only' : False If use_rgb_only is True , detect_aruco_markers will ignore depth images from the Intel RealSense D435i depth camera when estimating the pose of the marker and will instead only use RGB images from the D435i. 'name' : 'wrist_top' name is used for the text string of the ArUco marker\u2019s ROS Marker in the ROS MarkerArray Message published by the detect_aruco_markers ROS node. 'link' : 'link_aruco_top_wrist' link is currently used by stretch_calibration . It is the name of the link associated with a body-mounted ArUco marker in the robot\u2019s URDF . It\u2019s good practice to add an entry to stretch_marker_dict.yaml for each ArUco marker you use.","title":"The ArUco Marker Dictionary"},{"location":"stretch-tutorials/ros1/aruco_marker_detection/#create-a-new-aruco-marker","text":"At Hello Robot, we\u2019ve used the following guide when generating new ArUco markers. We generate ArUco markers using a 6x6-bit grid (36 bits) with 250 unique codes. This corresponds with DICT_6X6_250 defined in OpenCV . We generate markers using this online ArUco marker generator by setting the Dictionary entry to 6x6 and then setting the Marker ID and Marker size, mm as appropriate for the specific application. We strongly recommend measuring the actual marker by hand before adding an entry for it to stretch_marker_dict.yaml . We select marker ID numbers using the following ranges. 0 - 99: reserved for users 100 - 249: reserved for official use by Hello Robot Inc. 100 - 199: reserved for robots with distinct sets of body-mounted markers Allows different robots near each other to use distinct sets of body-mounted markers to avoid confusion. This could be valuable for various uses of body-mounted markers, including calibration, visual servoing, visual motion capture, and multi-robot tasks. 5 markers per robot = 2 on the mobile base + 2 on the wrist + 1 on the shoulder 20 distinct sets = 100 available ID numbers / 5 ID numbers per robot 200 - 249: reserved for official accessories 245 for the prototype docking station 246-249 for large floor markers When coming up with this guide, we expected the following: Body-mounted accessories with the same ID numbers mounted to different robots could be disambiguated using the expected range of 3D locations of the ArUco markers on the calibrated body. Accessories in the environment with the same ID numbers could be disambiguated using a map or nearby observable features of the environment.","title":"Create a New ArUco Marker"},{"location":"stretch-tutorials/ros1/example_1/","text":"Example 1 The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending Twist messages to a Stretch robot. Begin by running the following command in a new terminal. roslaunch stretch_core stretch_driver.launch Switch to navigation mode using a rosservice call. Then, in a new terminal, drive the robot forward with the move.py node. rosservice call /switch_to_navigation_mode cd catkin_ws/src/stretch_tutorials/src/ python3 move.py To stop the node from sending twist messages, type Ctrl + c . The Code Below is the code which will send Twist messages to drive the robot forward. #!/usr/bin/env python3 import rospy from geometry_msgs.msg import Twist class Move : \"\"\" A class that sends Twist messages to move the Stretch robot forward. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber. :param self: The self reference. \"\"\" self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo def move_forward ( self ): \"\"\" Function that publishes Twist messages :param self: The self reference. :publishes command: Twist message. \"\"\" command = Twist () command . linear . x = 0.1 command . linear . y = 0.0 command . linear . z = 0.0 command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.0 self . pub . publish ( command ) if __name__ == '__main__' : rospy . init_node ( 'move' ) base_motion = Move () rate = rospy . Rate ( 10 ) while not rospy . is_shutdown (): base_motion . move_forward () rate . sleep () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from geometry_msgs.msg import Twist You need to import rospy if you are writing a ROS Node . The geometry_msgs.msg import is so that we can send velocity commands to the robot. class Move : def __init__ ( self ): self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist . The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. command = Twist () Make a Twist message. We're going to set all of the elements since we can't depend on them defaulting to safe values. command . linear . x = 0.1 command . linear . y = 0.0 command . linear . z = 0.0 A Twist data structure has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y or the z direction. command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.0 A Twist message also has three rotational velocities (in radians per second). Stretch will only respond to rotations around the z (vertical) axis. self . pub . publish ( command ) Publish the Twist commands in the previously defined topic name /stretch/cmd_vel . rospy . init_node ( 'move' ) base_motion = Move () rate = rospy . Rate ( 10 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node. Until rospy has this information, it cannot start communicating with the ROS Master. Note the name must be a base name, i.e. it cannot contain any slashes \"/\". The rospy.Rate() function creates a Rate object. With the help of its method sleep() , it offers a convenient way for looping at the desired rate. With its argument of 10, we should expect to go through the loop 10 times per second (as long as our processing time does not exceed 1/10th of a second!). while not rospy . is_shutdown (): base_motion . move_forward () rate . sleep () This loop is a fairly standard rospy construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl-C event or otherwise). The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop. Move Stretch in Simulation Using your preferred text editor, modify the topic name of the published Twist messages. Please review the edit in the move.py script below. self . pub = rospy . Publisher ( '/stretch_diff_drive_controller/cmd_vel' , Twist , queue_size = 1 ) After saving the edited node, bring up Stretch in the empty world simulation . To drive the robot with the node, type the following in a new terminal cd catkin_ws/src/stretch_tutorials/src/ python3 move.py To stop the node from sending twist messages, type Ctrl + c .","title":"Teleoperate Stretch with a Node"},{"location":"stretch-tutorials/ros1/example_1/#example-1","text":"The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending Twist messages to a Stretch robot. Begin by running the following command in a new terminal. roslaunch stretch_core stretch_driver.launch Switch to navigation mode using a rosservice call. Then, in a new terminal, drive the robot forward with the move.py node. rosservice call /switch_to_navigation_mode cd catkin_ws/src/stretch_tutorials/src/ python3 move.py To stop the node from sending twist messages, type Ctrl + c .","title":"Example 1"},{"location":"stretch-tutorials/ros1/example_1/#the-code","text":"Below is the code which will send Twist messages to drive the robot forward. #!/usr/bin/env python3 import rospy from geometry_msgs.msg import Twist class Move : \"\"\" A class that sends Twist messages to move the Stretch robot forward. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber. :param self: The self reference. \"\"\" self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo def move_forward ( self ): \"\"\" Function that publishes Twist messages :param self: The self reference. :publishes command: Twist message. \"\"\" command = Twist () command . linear . x = 0.1 command . linear . y = 0.0 command . linear . z = 0.0 command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.0 self . pub . publish ( command ) if __name__ == '__main__' : rospy . init_node ( 'move' ) base_motion = Move () rate = rospy . Rate ( 10 ) while not rospy . is_shutdown (): base_motion . move_forward () rate . sleep ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_1/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from geometry_msgs.msg import Twist You need to import rospy if you are writing a ROS Node . The geometry_msgs.msg import is so that we can send velocity commands to the robot. class Move : def __init__ ( self ): self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist . The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. command = Twist () Make a Twist message. We're going to set all of the elements since we can't depend on them defaulting to safe values. command . linear . x = 0.1 command . linear . y = 0.0 command . linear . z = 0.0 A Twist data structure has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y or the z direction. command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.0 A Twist message also has three rotational velocities (in radians per second). Stretch will only respond to rotations around the z (vertical) axis. self . pub . publish ( command ) Publish the Twist commands in the previously defined topic name /stretch/cmd_vel . rospy . init_node ( 'move' ) base_motion = Move () rate = rospy . Rate ( 10 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node. Until rospy has this information, it cannot start communicating with the ROS Master. Note the name must be a base name, i.e. it cannot contain any slashes \"/\". The rospy.Rate() function creates a Rate object. With the help of its method sleep() , it offers a convenient way for looping at the desired rate. With its argument of 10, we should expect to go through the loop 10 times per second (as long as our processing time does not exceed 1/10th of a second!). while not rospy . is_shutdown (): base_motion . move_forward () rate . sleep () This loop is a fairly standard rospy construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl-C event or otherwise). The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_1/#move-stretch-in-simulation","text":"Using your preferred text editor, modify the topic name of the published Twist messages. Please review the edit in the move.py script below. self . pub = rospy . Publisher ( '/stretch_diff_drive_controller/cmd_vel' , Twist , queue_size = 1 ) After saving the edited node, bring up Stretch in the empty world simulation . To drive the robot with the node, type the following in a new terminal cd catkin_ws/src/stretch_tutorials/src/ python3 move.py To stop the node from sending twist messages, type Ctrl + c .","title":"Move Stretch in Simulation"},{"location":"stretch-tutorials/ros1/example_10/","text":"Example 10 This tutorial we will explain how to create a tf2 static broadcaster and listener. tf2 Static Broadcaster For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the link_mast , link_lift , and link_wrist_yaw frames. Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/tf2_broadcaster_example.rviz Then run the tf2_broadcaster.py node to visualize three static frames. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 tf2_broadcaster.py The GIF below visualizes what happens when running the previous node. Tip If you would like to see how the static frames update while the robot is in motion, run the stow_command_node.py and observe the tf frames in RViz. In a terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 stow_command.py The Code #!/usr/bin/env python3 import rospy import tf.transformations from geometry_msgs.msg import TransformStamped from tf2_ros import StaticTransformBroadcaster class FixedFrameBroadcaster (): \"\"\" This node publishes three child static frames in reference to their parent frames as below: parent -> link_mast child -> fk_link_mast parent -> link_lift child -> fk_link_lift parent -> link_wrist_yaw child -> fk_link_wrist_yaw \"\"\" def __init__ ( self ): \"\"\" A function that creates a broadcast node and publishes three new transform frames. :param self: The self reference. \"\"\" self . br = StaticTransformBroadcaster () self . mast = TransformStamped () self . mast . header . stamp = rospy . Time . now () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 2.0 self . mast . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . mast . transform . rotation . x = q [ 0 ] self . mast . transform . rotation . y = q [ 1 ] self . mast . transform . rotation . z = q [ 2 ] self . mast . transform . rotation . w = q [ 3 ] self . lift = TransformStamped () self . lift . header . stamp = rospy . Time . now () self . lift . header . frame_id = 'link_lift' self . lift . child_frame_id = 'fk_link_lift' self . lift . transform . translation . x = 0.0 self . lift . transform . translation . y = 1.0 self . lift . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . lift . transform . rotation . x = q [ 0 ] self . lift . transform . rotation . y = q [ 1 ] self . lift . transform . rotation . z = q [ 2 ] self . lift . transform . rotation . w = q [ 3 ] self . wrist = TransformStamped () self . wrist . header . stamp = rospy . Time . now () self . wrist . header . frame_id = 'link_wrist_yaw' self . wrist . child_frame_id = 'fk_link_wrist_yaw' self . wrist . transform . translation . x = 0.0 self . wrist . transform . translation . y = 1.0 self . wrist . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] self . br . sendTransform ([ self . mast , self . lift , self . wrist ]) rospy . loginfo ( 'Publishing TF frames. Use RViz to visualize' ) if __name__ == '__main__' : rospy . init_node ( 'tf2_broadcaster' ) FixedFrameBroadcaster () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import tf.transformations from geometry_msgs.msg import TransformStamped from tf2_ros import StaticTransformBroadcaster You need to import rospy if you are writing a ROS Node . Import tf.transformations to get quaternion values from Euler angles. Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a tf2_ros.StaticTransformBroadcaster to help make the task of publishing transforms easier. def __init__ ( self ): \"\"\" A function that creates a broadcast node and publishes three new transform frames. :param self: The self reference. \"\"\" self . br = StaticTransformBroadcaster () Here we create a TransformStamped object which will be the message we will send over once populated. self . mast = TransformStamped () self . mast . header . stamp = rospy . Time . now () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' We need to give the transform being published a timestamp, we'll just stamp it with the current time, rospy.Time.now() . Then, we need to set the name of the parent frame of the link we're creating, in this case link_mast . Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is fk_link_mast . self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 2.0 self . mast . transform . translation . z = 0.0 Set the translation values for the child frame. q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] The quaternion_from_euler() function takes in an Euler angle as an argument and returns a quaternion. Then set the rotation values to the transformed quaternions. This process will be completed for the link_lift and link_wrist_yaw as well. self . br . sendTransform ([ self . mast , self . lift , self . wrist ]) Send the three transforms using the sendTransform() function. rospy . init_node ( 'tf2_broadcaster' ) FixedFrameBroadcaster () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the FixedFrameBroadcaster() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages. tf2 Static Listener In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section, we will create a tf2 listener that will find the transform between fk_link_lift and link_grasp_center . Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Then run the tf2_broadcaster.py node in a new terminal to create the three static frames. cd catkin_ws/src/stretch_tutorials/src/ python3 tf2_broadcaster.py Finally, run the tf2_listener.py node in a separate terminal to print the transform between two links. cd catkin_ws/src/stretch_tutorials/src/ python3 tf2_listener.py Within the terminal, the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames. [ INFO ] [ 1659551318 .098168 ] : The pose of target frame link_grasp_center with reference from fk_link_lift is: translation: x: 1 .08415191335 y: -0.176147838153 z: 0 .576720021135 rotation: x: -0.479004489528 y: -0.508053545368 z: -0.502884087254 w: 0 .509454501243 The Code #!/usr/bin/env python3 import rospy from geometry_msgs.msg import TransformStamped import tf2_ros class FrameListener (): \"\"\" This Class prints the transformation between the fk_link_mast frame and the target frame, link_grasp_center. \"\"\" def __init__ ( self ): \"\"\" A function that initializes the variables and looks up a transformation between a target and source frame. :param self: The self reference. \"\"\" tf_buffer = tf2_ros . Buffer () listener = tf2_ros . TransformListener ( tf_buffer ) from_frame_rel = 'link_grasp_center' to_frame_rel = 'fk_link_lift' rospy . sleep ( 1.0 ) rate = rospy . Rate ( 1 ) while not rospy . is_shutdown (): try : trans = tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , rospy . Time ()) rospy . loginfo ( 'The pose of target frame %s with reference from %s is: \\n %s ' , from_frame_rel , to_frame_rel , trans . transform ) except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): rospy . logwarn ( ' Could not transform %s from %s ' , to_frame_rel , from_frame_rel ) rate . sleep () if __name__ == '__main__' : rospy . init_node ( 'tf2_listener' ) FrameListener () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from geometry_msgs.msg import TransformStamped import tf2_ros You need to import rospy if you are writing a ROS Node . Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a tf2_ros.TransformListener to help make the task of receiving transforms easier. tf_buffer = tf2_ros . Buffer () listener = tf2_ros . TransformListener ( tf_buffer ) Here, we create a TransformListener object. Once the listener is created, it starts receiving tf2 transformations and buffers them for up to 10 seconds. from_frame_rel = 'link_grasp_center' to_frame_rel = 'fk_link_lift' Store frame names in variables that will be used to compute transformations. rospy . sleep ( 1.0 ) rate = rospy . Rate ( 1 ) The first line gives the listener some time to accumulate transforms. The second line is the rate at which the node is going to publish information (1 Hz). try : trans = tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , rospy . Time ()) rospy . loginfo ( 'The pose of target frame %s with reference from %s is: \\n %s ' , from_frame_rel , to_frame_rel , trans . transform ) except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): rospy . logwarn ( ' Could not transform %s from %s ' , to_frame_rel , from_frame_rel ) Try to look up the transformation we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between from_frame_rel and to_frame_rel frames with the lookup_transform() function. rospy . init_node ( 'tf2_listener' ) FrameListener () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the FrameListener() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Tf2 Broadcaster and Listener"},{"location":"stretch-tutorials/ros1/example_10/#example-10","text":"This tutorial we will explain how to create a tf2 static broadcaster and listener.","title":"Example 10"},{"location":"stretch-tutorials/ros1/example_10/#tf2-static-broadcaster","text":"For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the link_mast , link_lift , and link_wrist_yaw frames. Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/tf2_broadcaster_example.rviz Then run the tf2_broadcaster.py node to visualize three static frames. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 tf2_broadcaster.py The GIF below visualizes what happens when running the previous node. Tip If you would like to see how the static frames update while the robot is in motion, run the stow_command_node.py and observe the tf frames in RViz. In a terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 stow_command.py","title":"tf2 Static Broadcaster"},{"location":"stretch-tutorials/ros1/example_10/#the-code","text":"#!/usr/bin/env python3 import rospy import tf.transformations from geometry_msgs.msg import TransformStamped from tf2_ros import StaticTransformBroadcaster class FixedFrameBroadcaster (): \"\"\" This node publishes three child static frames in reference to their parent frames as below: parent -> link_mast child -> fk_link_mast parent -> link_lift child -> fk_link_lift parent -> link_wrist_yaw child -> fk_link_wrist_yaw \"\"\" def __init__ ( self ): \"\"\" A function that creates a broadcast node and publishes three new transform frames. :param self: The self reference. \"\"\" self . br = StaticTransformBroadcaster () self . mast = TransformStamped () self . mast . header . stamp = rospy . Time . now () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 2.0 self . mast . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . mast . transform . rotation . x = q [ 0 ] self . mast . transform . rotation . y = q [ 1 ] self . mast . transform . rotation . z = q [ 2 ] self . mast . transform . rotation . w = q [ 3 ] self . lift = TransformStamped () self . lift . header . stamp = rospy . Time . now () self . lift . header . frame_id = 'link_lift' self . lift . child_frame_id = 'fk_link_lift' self . lift . transform . translation . x = 0.0 self . lift . transform . translation . y = 1.0 self . lift . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . lift . transform . rotation . x = q [ 0 ] self . lift . transform . rotation . y = q [ 1 ] self . lift . transform . rotation . z = q [ 2 ] self . lift . transform . rotation . w = q [ 3 ] self . wrist = TransformStamped () self . wrist . header . stamp = rospy . Time . now () self . wrist . header . frame_id = 'link_wrist_yaw' self . wrist . child_frame_id = 'fk_link_wrist_yaw' self . wrist . transform . translation . x = 0.0 self . wrist . transform . translation . y = 1.0 self . wrist . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] self . br . sendTransform ([ self . mast , self . lift , self . wrist ]) rospy . loginfo ( 'Publishing TF frames. Use RViz to visualize' ) if __name__ == '__main__' : rospy . init_node ( 'tf2_broadcaster' ) FixedFrameBroadcaster () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_10/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import tf.transformations from geometry_msgs.msg import TransformStamped from tf2_ros import StaticTransformBroadcaster You need to import rospy if you are writing a ROS Node . Import tf.transformations to get quaternion values from Euler angles. Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a tf2_ros.StaticTransformBroadcaster to help make the task of publishing transforms easier. def __init__ ( self ): \"\"\" A function that creates a broadcast node and publishes three new transform frames. :param self: The self reference. \"\"\" self . br = StaticTransformBroadcaster () Here we create a TransformStamped object which will be the message we will send over once populated. self . mast = TransformStamped () self . mast . header . stamp = rospy . Time . now () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' We need to give the transform being published a timestamp, we'll just stamp it with the current time, rospy.Time.now() . Then, we need to set the name of the parent frame of the link we're creating, in this case link_mast . Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is fk_link_mast . self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 2.0 self . mast . transform . translation . z = 0.0 Set the translation values for the child frame. q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] The quaternion_from_euler() function takes in an Euler angle as an argument and returns a quaternion. Then set the rotation values to the transformed quaternions. This process will be completed for the link_lift and link_wrist_yaw as well. self . br . sendTransform ([ self . mast , self . lift , self . wrist ]) Send the three transforms using the sendTransform() function. rospy . init_node ( 'tf2_broadcaster' ) FixedFrameBroadcaster () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the FixedFrameBroadcaster() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_10/#tf2-static-listener","text":"In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section, we will create a tf2 listener that will find the transform between fk_link_lift and link_grasp_center . Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Then run the tf2_broadcaster.py node in a new terminal to create the three static frames. cd catkin_ws/src/stretch_tutorials/src/ python3 tf2_broadcaster.py Finally, run the tf2_listener.py node in a separate terminal to print the transform between two links. cd catkin_ws/src/stretch_tutorials/src/ python3 tf2_listener.py Within the terminal, the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames. [ INFO ] [ 1659551318 .098168 ] : The pose of target frame link_grasp_center with reference from fk_link_lift is: translation: x: 1 .08415191335 y: -0.176147838153 z: 0 .576720021135 rotation: x: -0.479004489528 y: -0.508053545368 z: -0.502884087254 w: 0 .509454501243","title":"tf2 Static Listener"},{"location":"stretch-tutorials/ros1/example_10/#the-code_1","text":"#!/usr/bin/env python3 import rospy from geometry_msgs.msg import TransformStamped import tf2_ros class FrameListener (): \"\"\" This Class prints the transformation between the fk_link_mast frame and the target frame, link_grasp_center. \"\"\" def __init__ ( self ): \"\"\" A function that initializes the variables and looks up a transformation between a target and source frame. :param self: The self reference. \"\"\" tf_buffer = tf2_ros . Buffer () listener = tf2_ros . TransformListener ( tf_buffer ) from_frame_rel = 'link_grasp_center' to_frame_rel = 'fk_link_lift' rospy . sleep ( 1.0 ) rate = rospy . Rate ( 1 ) while not rospy . is_shutdown (): try : trans = tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , rospy . Time ()) rospy . loginfo ( 'The pose of target frame %s with reference from %s is: \\n %s ' , from_frame_rel , to_frame_rel , trans . transform ) except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): rospy . logwarn ( ' Could not transform %s from %s ' , to_frame_rel , from_frame_rel ) rate . sleep () if __name__ == '__main__' : rospy . init_node ( 'tf2_listener' ) FrameListener () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_10/#the-code-explained_1","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from geometry_msgs.msg import TransformStamped import tf2_ros You need to import rospy if you are writing a ROS Node . Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a tf2_ros.TransformListener to help make the task of receiving transforms easier. tf_buffer = tf2_ros . Buffer () listener = tf2_ros . TransformListener ( tf_buffer ) Here, we create a TransformListener object. Once the listener is created, it starts receiving tf2 transformations and buffers them for up to 10 seconds. from_frame_rel = 'link_grasp_center' to_frame_rel = 'fk_link_lift' Store frame names in variables that will be used to compute transformations. rospy . sleep ( 1.0 ) rate = rospy . Rate ( 1 ) The first line gives the listener some time to accumulate transforms. The second line is the rate at which the node is going to publish information (1 Hz). try : trans = tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , rospy . Time ()) rospy . loginfo ( 'The pose of target frame %s with reference from %s is: \\n %s ' , from_frame_rel , to_frame_rel , trans . transform ) except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): rospy . logwarn ( ' Could not transform %s from %s ' , to_frame_rel , from_frame_rel ) Try to look up the transformation we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between from_frame_rel and to_frame_rel frames with the lookup_transform() function. rospy . init_node ( 'tf2_listener' ) FrameListener () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the FrameListener() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_11/","text":"Example 11 This tutorial highlights how to create a PointCloud message from the data of a PointCloud2 message type, then transform the PointCloud's reference link to a different frame. The data published by RealSense is referencing its camera_color_optical_frame link, and we will be changing its reference to the base_link . Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_low_resolution.launch Then run the pointCloud_transformer.py node. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 pointcloud_transformer.py Within this tutorial package, there is an RViz config file with the PointCloud in the Display tree. You can visualize this topic and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/PointCloud_transformer_example.rviz The GIF below visualizes what happens when running the previous node. The Code #!/usr/bin/env python3 import rospy import tf import sensor_msgs.point_cloud2 as pc2 from sensor_msgs.msg import PointCloud2 , PointCloud from geometry_msgs.msg import Point32 from std_msgs.msg import Header class PointCloudTransformer : \"\"\" A class that takes in a PointCloud2 message and stores its points into a PointCloud message. Then that PointCloud is transformed to reference the 'base_link' frame. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber, publisher, and other variables. :param self: The self reference. \"\"\" self . pointcloud2_sub = rospy . Subscriber ( \"/camera/depth/color/points\" , PointCloud2 , self . callback_pcl2 , queue_size = 1 ) self . pointcloud_pub = rospy . Publisher ( \"/camera_cloud\" , PointCloud , queue_size = 1 ) self . pcl2_cloud = None self . listener = tf . TransformListener ( True , rospy . Duration ( 10.0 )) rospy . loginfo ( 'Publishing transformed PointCloud. Use RViz to visualize' ) def callback_pcl2 ( self , msg ): \"\"\" Callback function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud2 message type. \"\"\" self . pcl2_cloud = msg def pcl_transformer ( self ): \"\"\" A function that extracts the points from the stored PointCloud2 message and appends those points to a PointCloud message. Then the function transforms the PointCloud from its the header frame id, 'camera_color_optical_frame' to the 'base_link' frame. :param self: The self reference. \"\"\" temp_cloud = PointCloud () temp_cloud . header = self . pcl2_cloud . header for data in pc2 . read_points ( self . pcl2_cloud , skip_nans = True ): temp_cloud . points . append ( Point32 ( data [ 0 ], data [ 1 ], data [ 2 ])) transformed_cloud = self . transform_pointcloud ( temp_cloud ) self . pointcloud_pub . publish ( transformed_cloud ) def transform_pointcloud ( self , msg ): \"\"\" Function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud message. :returns new_cloud: The transformed PointCloud message. \"\"\" while not rospy . is_shutdown (): try : new_cloud = self . listener . transformPointCloud ( \"base_link\" , msg ) return new_cloud if new_cloud : break except ( tf . LookupException , tf . ConnectivityException , tf . ExtrapolationException ): pass if __name__ == \"__main__\" : rospy . init_node ( 'pointcloud_transformer' , anonymous = True ) PCT = PointCloudTransformer () rate = rospy . Rate ( 1 ) rospy . sleep ( 1 ) while not rospy . is_shutdown (): PCT . pcl_transformer () rate . sleep () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import tf import sensor_msgs.point_cloud2 as pc2 from sensor_msgs.msg import PointCloud2 , PointCloud from geometry_msgs.msg import Point32 from std_msgs.msg import Header You need to import rospy if you are writing a ROS Node . Import tf to utilize the transformPointCloud function. Import various message types from sensor_msgs . self . pointcloud2_sub = rospy . Subscriber ( \"/camera/depth/color/points\" , PointCloud2 , self . callback_pcl2 , queue_size = 1 ) Set up a subscriber. We're going to subscribe to the topic /camera/depth/color/points , looking for PointCloud2 message. When a message comes in, ROS is going to pass it to the function callback_pcl2() automatically. self . pointcloud_pub = rospy . Publisher ( \"/camera_cloud\" , PointCloud , queue_size = 1 ) This section of code defines the talker's interface to the rest of ROS. self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1) declares that your node is publishing to the /camera_cloud topic using the message type PointCloud . self . pcl2_cloud = None self . listener = tf . TransformListener ( True , rospy . Duration ( 10.0 )) The first line of code initializes self.pcl2_cloud to store the PointCloud2 message. The second line creates a tf.TransformListener object. Once the listener is created, it starts receiving tf transformations and buffers them for up to 10 seconds. def callback_pcl2 ( self , msg ): \"\"\" Callback function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud2 message type. \"\"\" self . pcl2_cloud = msg The callback function then stores the PointCloud2 message. temp_cloud = PointCloud () temp_cloud . header = self . pcl2_cloud . header Create a PointCloud for temporary use. Set the temporary PointCloud header to the stored PointCloud2 header. for data in pc2 . read_points ( self . pcl2_cloud , skip_nans = True ): temp_cloud . points . append ( Point32 ( data [ 0 ], data [ 1 ], data [ 2 ])) Use a for loop to extract PointCloud2 data into a list of x, y, and z points and append those values to the PointCloud message, temp_cloud . transformed_cloud = self . transform_pointcloud ( temp_cloud ) Utilize the transform_pointcloud function to transform the points in the PointCloud message to reference the base_link while not rospy . is_shutdown (): try : new_cloud = self . listener . transformPointCloud ( \"base_link\" , msg ) return new_cloud if new_cloud : break except ( tf . LookupException , tf . ConnectivityException , tf . ExtrapolationException ): pass Try to look up and transform the PointCloud input. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Transform the point cloud data from camera_color_optical_frame to base_link with the transformPointCloud() function. self . pointcloud_pub . publish ( transformed_cloud ) Publish the new transformed PointCloud . rospy . init_node ( 'pointcloud_transformer' , anonymous = True ) PCT = PointCloudTransformer () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Declare a PointCloudTransformer object. rate = rospy . Rate ( 1 ) rospy . sleep ( 1 ) The first line gives the listener some time to accumulate transforms. The second line is the rate at which the node is going to publish information (1 Hz). while not rospy . is_shutdown (): PCT . pcl_transformer () rate . sleep () Run a while loop until the node is shut down. Within the while loop run the pcl_transformer() method.","title":"PointCloud Transformation"},{"location":"stretch-tutorials/ros1/example_11/#example-11","text":"This tutorial highlights how to create a PointCloud message from the data of a PointCloud2 message type, then transform the PointCloud's reference link to a different frame. The data published by RealSense is referencing its camera_color_optical_frame link, and we will be changing its reference to the base_link . Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_low_resolution.launch Then run the pointCloud_transformer.py node. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 pointcloud_transformer.py Within this tutorial package, there is an RViz config file with the PointCloud in the Display tree. You can visualize this topic and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/PointCloud_transformer_example.rviz The GIF below visualizes what happens when running the previous node.","title":"Example 11"},{"location":"stretch-tutorials/ros1/example_11/#the-code","text":"#!/usr/bin/env python3 import rospy import tf import sensor_msgs.point_cloud2 as pc2 from sensor_msgs.msg import PointCloud2 , PointCloud from geometry_msgs.msg import Point32 from std_msgs.msg import Header class PointCloudTransformer : \"\"\" A class that takes in a PointCloud2 message and stores its points into a PointCloud message. Then that PointCloud is transformed to reference the 'base_link' frame. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber, publisher, and other variables. :param self: The self reference. \"\"\" self . pointcloud2_sub = rospy . Subscriber ( \"/camera/depth/color/points\" , PointCloud2 , self . callback_pcl2 , queue_size = 1 ) self . pointcloud_pub = rospy . Publisher ( \"/camera_cloud\" , PointCloud , queue_size = 1 ) self . pcl2_cloud = None self . listener = tf . TransformListener ( True , rospy . Duration ( 10.0 )) rospy . loginfo ( 'Publishing transformed PointCloud. Use RViz to visualize' ) def callback_pcl2 ( self , msg ): \"\"\" Callback function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud2 message type. \"\"\" self . pcl2_cloud = msg def pcl_transformer ( self ): \"\"\" A function that extracts the points from the stored PointCloud2 message and appends those points to a PointCloud message. Then the function transforms the PointCloud from its the header frame id, 'camera_color_optical_frame' to the 'base_link' frame. :param self: The self reference. \"\"\" temp_cloud = PointCloud () temp_cloud . header = self . pcl2_cloud . header for data in pc2 . read_points ( self . pcl2_cloud , skip_nans = True ): temp_cloud . points . append ( Point32 ( data [ 0 ], data [ 1 ], data [ 2 ])) transformed_cloud = self . transform_pointcloud ( temp_cloud ) self . pointcloud_pub . publish ( transformed_cloud ) def transform_pointcloud ( self , msg ): \"\"\" Function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud message. :returns new_cloud: The transformed PointCloud message. \"\"\" while not rospy . is_shutdown (): try : new_cloud = self . listener . transformPointCloud ( \"base_link\" , msg ) return new_cloud if new_cloud : break except ( tf . LookupException , tf . ConnectivityException , tf . ExtrapolationException ): pass if __name__ == \"__main__\" : rospy . init_node ( 'pointcloud_transformer' , anonymous = True ) PCT = PointCloudTransformer () rate = rospy . Rate ( 1 ) rospy . sleep ( 1 ) while not rospy . is_shutdown (): PCT . pcl_transformer () rate . sleep ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_11/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import tf import sensor_msgs.point_cloud2 as pc2 from sensor_msgs.msg import PointCloud2 , PointCloud from geometry_msgs.msg import Point32 from std_msgs.msg import Header You need to import rospy if you are writing a ROS Node . Import tf to utilize the transformPointCloud function. Import various message types from sensor_msgs . self . pointcloud2_sub = rospy . Subscriber ( \"/camera/depth/color/points\" , PointCloud2 , self . callback_pcl2 , queue_size = 1 ) Set up a subscriber. We're going to subscribe to the topic /camera/depth/color/points , looking for PointCloud2 message. When a message comes in, ROS is going to pass it to the function callback_pcl2() automatically. self . pointcloud_pub = rospy . Publisher ( \"/camera_cloud\" , PointCloud , queue_size = 1 ) This section of code defines the talker's interface to the rest of ROS. self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1) declares that your node is publishing to the /camera_cloud topic using the message type PointCloud . self . pcl2_cloud = None self . listener = tf . TransformListener ( True , rospy . Duration ( 10.0 )) The first line of code initializes self.pcl2_cloud to store the PointCloud2 message. The second line creates a tf.TransformListener object. Once the listener is created, it starts receiving tf transformations and buffers them for up to 10 seconds. def callback_pcl2 ( self , msg ): \"\"\" Callback function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud2 message type. \"\"\" self . pcl2_cloud = msg The callback function then stores the PointCloud2 message. temp_cloud = PointCloud () temp_cloud . header = self . pcl2_cloud . header Create a PointCloud for temporary use. Set the temporary PointCloud header to the stored PointCloud2 header. for data in pc2 . read_points ( self . pcl2_cloud , skip_nans = True ): temp_cloud . points . append ( Point32 ( data [ 0 ], data [ 1 ], data [ 2 ])) Use a for loop to extract PointCloud2 data into a list of x, y, and z points and append those values to the PointCloud message, temp_cloud . transformed_cloud = self . transform_pointcloud ( temp_cloud ) Utilize the transform_pointcloud function to transform the points in the PointCloud message to reference the base_link while not rospy . is_shutdown (): try : new_cloud = self . listener . transformPointCloud ( \"base_link\" , msg ) return new_cloud if new_cloud : break except ( tf . LookupException , tf . ConnectivityException , tf . ExtrapolationException ): pass Try to look up and transform the PointCloud input. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Transform the point cloud data from camera_color_optical_frame to base_link with the transformPointCloud() function. self . pointcloud_pub . publish ( transformed_cloud ) Publish the new transformed PointCloud . rospy . init_node ( 'pointcloud_transformer' , anonymous = True ) PCT = PointCloudTransformer () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Declare a PointCloudTransformer object. rate = rospy . Rate ( 1 ) rospy . sleep ( 1 ) The first line gives the listener some time to accumulate transforms. The second line is the rate at which the node is going to publish information (1 Hz). while not rospy . is_shutdown (): PCT . pcl_transformer () rate . sleep () Run a while loop until the node is shut down. Within the while loop run the pcl_transformer() method.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_12/","text":"Example 12 For this example, we will send follow joint trajectory commands for the head camera to search and locate an ArUco tag. In this instance, a Stretch robot will try to locate the docking station's ArUco tag. Modifying Stretch Marker Dictionary YAML File When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml , that holds the information about the markers. A further breakdown of the YAML file can be found in our Aruco Marker Detection tutorial. Below is what needs to be included in the stretch_marker_dict.yaml file so the detect_aruco_markers node can find the docking station's ArUco tag. '245' : 'length_mm' : 88.0 'use_rgb_only' : False 'name' : 'docking_station' 'link' : None Getting Started Begin by running the stretch driver launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_high_resolution.launch Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. In a new terminal, execute: roslaunch stretch_core stretch_aruco.launch Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz Then run the aruco_tag_locator.py node. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 aruco_tag_locator.py The Code #! /usr/bin/env python3 import rospy import time import tf2_ros import numpy as np from math import pi import hello_helpers.hello_misc as hm from sensor_msgs.msg import JointState from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from geometry_msgs.msg import TransformStamped class LocateArUcoTag ( hm . HelloNode ): \"\"\" A class that actuates the RealSense camera to find the docking station's ArUco tag and returns a Transform between the `base_link` and the requested tag. \"\"\" def __init__ ( self ): \"\"\" A function that initializes the subscriber and other needed variables. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . joint_states_sub = rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) self . transform_pub = rospy . Publisher ( 'ArUco_transform' , TransformStamped , queue_size = 10 ) self . joint_state = None self . min_pan_position = - 4.10 self . max_pan_position = 1.50 self . pan_num_steps = 10 self . pan_step_size = abs ( self . min_pan_position - self . max_pan_position ) / self . pan_num_steps self . min_tilt_position = - 0.75 self . tilt_num_steps = 3 self . tilt_step_size = pi / 16 self . rot_vel = 0.5 # radians per sec def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg def send_command ( self , command ): ''' Handles single joint control commands by constructing a FollowJointTrajectoryGoal message and sending it to the trajectory_client created in hello_misc. :param self: The self reference. :param command: A dictionary message type. ''' if ( self . joint_state is not None ) and ( command is not None ): joint_name = command [ 'joint' ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ joint_name ] point = JointTrajectoryPoint () if 'delta' in command : joint_index = self . joint_state . name . index ( joint_name ) joint_value = self . joint_state . position [ joint_index ] delta = command [ 'delta' ] new_value = joint_value + delta point . positions = [ new_value ] elif 'position' in command : point . positions = [ command [ 'position' ]] point . velocities = [ self . rot_vel ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) self . trajectory_client . wait_for_result () def find_tag ( self , tag_name = 'docking_station' ): \"\"\" A function that actuates the camera to search for a defined ArUco tag marker. Then the function returns the pose :param self: The self reference. :param tag_name: A string value of the ArUco marker name. :returns transform: The docking station's TransformStamped message. \"\"\" pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'position' : self . min_tilt_position } self . send_command ( tilt_command ) for i in range ( self . tilt_num_steps ): for j in range ( self . pan_num_steps ): pan_command = { 'joint' : 'joint_head_pan' , 'delta' : self . pan_step_size } self . send_command ( pan_command ) rospy . sleep ( 0.2 ) try : transform = self . tf_buffer . lookup_transform ( 'base_link' , tag_name , rospy . Time ()) rospy . loginfo ( \"Found Requested Tag: \\n %s \" , transform ) self . transform_pub . publish ( transform ) return transform except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): continue pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'delta' : self . tilt_step_size } self . send_command ( tilt_command ) rospy . sleep ( .25 ) rospy . loginfo ( \"The requested tag ' %s ' was not found\" , tag_name ) def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'aruco_tag_locator' , 'aruco_tag_locator' , wait_for_first_pointcloud = False ) self . static_broadcaster = tf2_ros . StaticTransformBroadcaster () self . tf_buffer = tf2_ros . Buffer () self . listener = tf2_ros . TransformListener ( self . tf_buffer ) rospy . sleep ( 1.0 ) rospy . loginfo ( 'Searching for docking ArUco tag.' ) pose = self . find_tag ( \"docking_station\" ) if __name__ == '__main__' : try : node = LocateArUcoTag () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import time import tf2_ros import numpy as np from math import pi import hello_helpers.hello_misc as hm from sensor_msgs.msg import JointState from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from geometry_msgs.msg import TransformStamped You need to import rospy if you are writing a ROS Node . Import other python modules needed for this node. Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros . In this instance, we are importing the hello_misc script. def __init__ ( self ): \"\"\" A function that initializes the subscriber and other needed variables. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . joint_states_sub = rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) self . transform_pub = rospy . Publisher ( 'ArUco_transform' , TransformStamped , queue_size = 10 ) self . joint_state = None The LocateArUcoTag class inherits the HelloNode class from hm and is instantiated. Set up a subscriber with rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback) . We're going to subscribe to the topic stretch/joint_states , looking for JointState messages. When a message comes in, ROS is going to pass it to the function joint_states_callback() automatically. rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10) declares that your node is publishing to the ArUco_transform topic using the message type TransformStamped . The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. self . min_pan_position = - 4.10 self . max_pan_position = 1.50 self . pan_num_steps = 10 self . pan_step_size = abs ( self . min_pan_position - self . max_pan_position ) / self . pan_num_steps Provide the minimum and maximum joint positions for the head pan. These values are needed for sweeping the head to search for the ArUco tag. We also define the number of steps for the sweep, then create the step size for the head pan joint. self . min_tilt_position = - 0.75 self . tilt_num_steps = 3 self . tilt_step_size = pi / 16 Set the minimum position of the tilt joint, the number of steps, and the size of each step. self . rot_vel = 0.5 # radians per sec Define the head actuation rotational velocity. def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg The joint_states_callback() function stores Stretch's joint states. def send_command ( self , command ): ''' Handles single joint control commands by constructing a FollowJointTrajectoryGoal message and sending it to the trajectory_client created in hello_misc. :param self: The self reference. :param command: A dictionary message type. ''' if ( self . joint_state is not None ) and ( command is not None ): joint_name = command [ 'joint' ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ joint_name ] point = JointTrajectoryPoint () Assign trajectory_goal as a FollowJointTrajectoryGoal message type. Then extract the string value from the joint key. Also, assign point as a JointTrajectoryPoint message type. if 'delta' in command : joint_index = self . joint_state . name . index ( joint_name ) joint_value = self . joint_state . position [ joint_index ] delta = command [ 'delta' ] new_value = joint_value + delta point . positions = [ new_value ] Check to see if delta is a key in the command dictionary. Then get the current position of the joint and add the delta as a new position value. elif 'position' in command : point . positions = [ command [ 'position' ]] Check to see if position is a key in the command dictionary. Then extract the position value. point . velocities = [ self . rot_vel ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) self . trajectory_client . wait_for_result () Then trajectory_goal.trajectory.points is defined by the positions set in point . Specify the coordinate frame that we want ( base_link ) and set the time to be now. Make the action call and send the goal. The last line of code waits for the result before it exits the python script. def find_tag ( self , tag_name = 'docking_station' ): \"\"\" A function that actuates the camera to search for a defined ArUco tag marker. Then the function returns the pose :param self: The self reference. :param tag_name: A string value of the ArUco marker name. :returns transform: The docking station's TransformStamped message. \"\"\" pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'position' : self . min_tilt_position } self . send_command ( tilt_command ) Create a dictionary to get the head in its initial position for its search and send the commands with the send_command() function. for i in range ( self . tilt_num_steps ): for j in range ( self . pan_num_steps ): pan_command = { 'joint' : 'joint_head_pan' , 'delta' : self . pan_step_size } self . send_command ( pan_command ) rospy . sleep ( 0.5 ) Utilize a nested for loop to sweep the pan and tilt in increments. Then update the joint_head_pan position by the pan_step_size . Use rospy.sleep() function to give time to the system to do a Transform lookup before the next step. try : transform = self . tf_buffer . lookup_transform ( 'base_link' , tag_name , rospy . Time ()) rospy . loginfo ( \"Found Requested Tag: \\n %s \" , transform ) self . transform_pub . publish ( transform ) return transform except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): continue Use a try-except block to look up the transform between the base_link and the requested ArUco tag. Then publish and return the TransformStamped message. pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'delta' : self . tilt_step_size } self . send_command ( tilt_command ) rospy . sleep ( .25 ) Begin sweep with new tilt angle. def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'aruco_tag_locator' , 'aruco_tag_locator' , wait_for_first_pointcloud = False ) Create a function, main() , to do the setup for the hm.HelloNode class and initialize the aruco_tag_locator node. self . static_broadcaster = tf2_ros . StaticTransformBroadcaster () self . tf_buffer = tf2_ros . Buffer () self . listener = tf2_ros . TransformListener ( self . tf_buffer ) rospy . sleep ( 1.0 ) Create a StaticTranformBoradcaster Node. Also, start a tf buffer that will store the tf information for a few seconds. Then set up a tf listener, which will subscribe to all of the relevant tf topics, and keep track of the information. Include rospy.sleep(1.0) to give the listener some time to accumulate transforms. rospy . loginfo ( 'Searching for docking ArUco tag.' ) pose = self . find_tag ( \"docking_station\" ) Notice Stretch is searching for the ArUco tag with a rospy.loginfo() function. Then search for the ArUco marker for the docking station. if __name__ == '__main__' : try : node = LocateArUcoTag () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare LocateArUcoTag object. Then run the main() method.","title":"ArUco Tag Locator"},{"location":"stretch-tutorials/ros1/example_12/#example-12","text":"For this example, we will send follow joint trajectory commands for the head camera to search and locate an ArUco tag. In this instance, a Stretch robot will try to locate the docking station's ArUco tag.","title":"Example 12"},{"location":"stretch-tutorials/ros1/example_12/#modifying-stretch-marker-dictionary-yaml-file","text":"When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml , that holds the information about the markers. A further breakdown of the YAML file can be found in our Aruco Marker Detection tutorial. Below is what needs to be included in the stretch_marker_dict.yaml file so the detect_aruco_markers node can find the docking station's ArUco tag. '245' : 'length_mm' : 88.0 'use_rgb_only' : False 'name' : 'docking_station' 'link' : None","title":"Modifying Stretch Marker Dictionary YAML File"},{"location":"stretch-tutorials/ros1/example_12/#getting-started","text":"Begin by running the stretch driver launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_high_resolution.launch Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. In a new terminal, execute: roslaunch stretch_core stretch_aruco.launch Within this tutorial package, there is an RViz config file with the topics for the transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz Then run the aruco_tag_locator.py node. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 aruco_tag_locator.py","title":"Getting Started"},{"location":"stretch-tutorials/ros1/example_12/#the-code","text":"#! /usr/bin/env python3 import rospy import time import tf2_ros import numpy as np from math import pi import hello_helpers.hello_misc as hm from sensor_msgs.msg import JointState from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from geometry_msgs.msg import TransformStamped class LocateArUcoTag ( hm . HelloNode ): \"\"\" A class that actuates the RealSense camera to find the docking station's ArUco tag and returns a Transform between the `base_link` and the requested tag. \"\"\" def __init__ ( self ): \"\"\" A function that initializes the subscriber and other needed variables. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . joint_states_sub = rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) self . transform_pub = rospy . Publisher ( 'ArUco_transform' , TransformStamped , queue_size = 10 ) self . joint_state = None self . min_pan_position = - 4.10 self . max_pan_position = 1.50 self . pan_num_steps = 10 self . pan_step_size = abs ( self . min_pan_position - self . max_pan_position ) / self . pan_num_steps self . min_tilt_position = - 0.75 self . tilt_num_steps = 3 self . tilt_step_size = pi / 16 self . rot_vel = 0.5 # radians per sec def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg def send_command ( self , command ): ''' Handles single joint control commands by constructing a FollowJointTrajectoryGoal message and sending it to the trajectory_client created in hello_misc. :param self: The self reference. :param command: A dictionary message type. ''' if ( self . joint_state is not None ) and ( command is not None ): joint_name = command [ 'joint' ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ joint_name ] point = JointTrajectoryPoint () if 'delta' in command : joint_index = self . joint_state . name . index ( joint_name ) joint_value = self . joint_state . position [ joint_index ] delta = command [ 'delta' ] new_value = joint_value + delta point . positions = [ new_value ] elif 'position' in command : point . positions = [ command [ 'position' ]] point . velocities = [ self . rot_vel ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) self . trajectory_client . wait_for_result () def find_tag ( self , tag_name = 'docking_station' ): \"\"\" A function that actuates the camera to search for a defined ArUco tag marker. Then the function returns the pose :param self: The self reference. :param tag_name: A string value of the ArUco marker name. :returns transform: The docking station's TransformStamped message. \"\"\" pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'position' : self . min_tilt_position } self . send_command ( tilt_command ) for i in range ( self . tilt_num_steps ): for j in range ( self . pan_num_steps ): pan_command = { 'joint' : 'joint_head_pan' , 'delta' : self . pan_step_size } self . send_command ( pan_command ) rospy . sleep ( 0.2 ) try : transform = self . tf_buffer . lookup_transform ( 'base_link' , tag_name , rospy . Time ()) rospy . loginfo ( \"Found Requested Tag: \\n %s \" , transform ) self . transform_pub . publish ( transform ) return transform except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): continue pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'delta' : self . tilt_step_size } self . send_command ( tilt_command ) rospy . sleep ( .25 ) rospy . loginfo ( \"The requested tag ' %s ' was not found\" , tag_name ) def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'aruco_tag_locator' , 'aruco_tag_locator' , wait_for_first_pointcloud = False ) self . static_broadcaster = tf2_ros . StaticTransformBroadcaster () self . tf_buffer = tf2_ros . Buffer () self . listener = tf2_ros . TransformListener ( self . tf_buffer ) rospy . sleep ( 1.0 ) rospy . loginfo ( 'Searching for docking ArUco tag.' ) pose = self . find_tag ( \"docking_station\" ) if __name__ == '__main__' : try : node = LocateArUcoTag () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1/example_12/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import time import tf2_ros import numpy as np from math import pi import hello_helpers.hello_misc as hm from sensor_msgs.msg import JointState from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from geometry_msgs.msg import TransformStamped You need to import rospy if you are writing a ROS Node . Import other python modules needed for this node. Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros . In this instance, we are importing the hello_misc script. def __init__ ( self ): \"\"\" A function that initializes the subscriber and other needed variables. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . joint_states_sub = rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) self . transform_pub = rospy . Publisher ( 'ArUco_transform' , TransformStamped , queue_size = 10 ) self . joint_state = None The LocateArUcoTag class inherits the HelloNode class from hm and is instantiated. Set up a subscriber with rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback) . We're going to subscribe to the topic stretch/joint_states , looking for JointState messages. When a message comes in, ROS is going to pass it to the function joint_states_callback() automatically. rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10) declares that your node is publishing to the ArUco_transform topic using the message type TransformStamped . The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. self . min_pan_position = - 4.10 self . max_pan_position = 1.50 self . pan_num_steps = 10 self . pan_step_size = abs ( self . min_pan_position - self . max_pan_position ) / self . pan_num_steps Provide the minimum and maximum joint positions for the head pan. These values are needed for sweeping the head to search for the ArUco tag. We also define the number of steps for the sweep, then create the step size for the head pan joint. self . min_tilt_position = - 0.75 self . tilt_num_steps = 3 self . tilt_step_size = pi / 16 Set the minimum position of the tilt joint, the number of steps, and the size of each step. self . rot_vel = 0.5 # radians per sec Define the head actuation rotational velocity. def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg The joint_states_callback() function stores Stretch's joint states. def send_command ( self , command ): ''' Handles single joint control commands by constructing a FollowJointTrajectoryGoal message and sending it to the trajectory_client created in hello_misc. :param self: The self reference. :param command: A dictionary message type. ''' if ( self . joint_state is not None ) and ( command is not None ): joint_name = command [ 'joint' ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ joint_name ] point = JointTrajectoryPoint () Assign trajectory_goal as a FollowJointTrajectoryGoal message type. Then extract the string value from the joint key. Also, assign point as a JointTrajectoryPoint message type. if 'delta' in command : joint_index = self . joint_state . name . index ( joint_name ) joint_value = self . joint_state . position [ joint_index ] delta = command [ 'delta' ] new_value = joint_value + delta point . positions = [ new_value ] Check to see if delta is a key in the command dictionary. Then get the current position of the joint and add the delta as a new position value. elif 'position' in command : point . positions = [ command [ 'position' ]] Check to see if position is a key in the command dictionary. Then extract the position value. point . velocities = [ self . rot_vel ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) self . trajectory_client . wait_for_result () Then trajectory_goal.trajectory.points is defined by the positions set in point . Specify the coordinate frame that we want ( base_link ) and set the time to be now. Make the action call and send the goal. The last line of code waits for the result before it exits the python script. def find_tag ( self , tag_name = 'docking_station' ): \"\"\" A function that actuates the camera to search for a defined ArUco tag marker. Then the function returns the pose :param self: The self reference. :param tag_name: A string value of the ArUco marker name. :returns transform: The docking station's TransformStamped message. \"\"\" pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'position' : self . min_tilt_position } self . send_command ( tilt_command ) Create a dictionary to get the head in its initial position for its search and send the commands with the send_command() function. for i in range ( self . tilt_num_steps ): for j in range ( self . pan_num_steps ): pan_command = { 'joint' : 'joint_head_pan' , 'delta' : self . pan_step_size } self . send_command ( pan_command ) rospy . sleep ( 0.5 ) Utilize a nested for loop to sweep the pan and tilt in increments. Then update the joint_head_pan position by the pan_step_size . Use rospy.sleep() function to give time to the system to do a Transform lookup before the next step. try : transform = self . tf_buffer . lookup_transform ( 'base_link' , tag_name , rospy . Time ()) rospy . loginfo ( \"Found Requested Tag: \\n %s \" , transform ) self . transform_pub . publish ( transform ) return transform except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): continue Use a try-except block to look up the transform between the base_link and the requested ArUco tag. Then publish and return the TransformStamped message. pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'delta' : self . tilt_step_size } self . send_command ( tilt_command ) rospy . sleep ( .25 ) Begin sweep with new tilt angle. def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'aruco_tag_locator' , 'aruco_tag_locator' , wait_for_first_pointcloud = False ) Create a function, main() , to do the setup for the hm.HelloNode class and initialize the aruco_tag_locator node. self . static_broadcaster = tf2_ros . StaticTransformBroadcaster () self . tf_buffer = tf2_ros . Buffer () self . listener = tf2_ros . TransformListener ( self . tf_buffer ) rospy . sleep ( 1.0 ) Create a StaticTranformBoradcaster Node. Also, start a tf buffer that will store the tf information for a few seconds. Then set up a tf listener, which will subscribe to all of the relevant tf topics, and keep track of the information. Include rospy.sleep(1.0) to give the listener some time to accumulate transforms. rospy . loginfo ( 'Searching for docking ArUco tag.' ) pose = self . find_tag ( \"docking_station\" ) Notice Stretch is searching for the ArUco tag with a rospy.loginfo() function. Then search for the ArUco marker for the docking station. if __name__ == '__main__' : try : node = LocateArUcoTag () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare LocateArUcoTag object. Then run the main() method.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_13/","text":"Example 13 In this example, we will be utilizing the move_base package , a component of the ROS navigation stack , to send base goals to the Stretch robot. Build a map First, begin by building a map of the space the robot will be navigating in. If you need a refresher on how to do this, then check out the Navigation Stack tutorial . Getting Started Next, with your created map, we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Where ${HELLO_FLEET_PATH} is the path of the <map_name>.yaml file. Note It's likely that the robot's location on the map does not match the robot's location in real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in real space. Below is a gif for reference. Now we are going to use a node to send a move_base goal half a meter in front of the map's origin. run the following command in a new terminal to execute the navigation.py node. cd catkin_ws/src/stretch_tutorials/src/ python3 navigation.py The Code #!/usr/bin/env python3 import rospy import actionlib import sys from move_base_msgs.msg import MoveBaseAction , MoveBaseGoal from geometry_msgs.msg import Quaternion from tf import transformations class StretchNavigation : \"\"\" A simple encapsulation of the navigation stack for a Stretch robot. \"\"\" def __init__ ( self ): \"\"\" Create an instance of the simple navigation interface. :param self: The self reference. \"\"\" self . client = actionlib . SimpleActionClient ( 'move_base' , MoveBaseAction ) self . client . wait_for_server () rospy . loginfo ( ' {0} : Made contact with move_base server' . format ( self . __class__ . __name__ )) self . goal = MoveBaseGoal () self . goal . target_pose . header . frame_id = 'map' self . goal . target_pose . header . stamp = rospy . Time () self . goal . target_pose . pose . position . x = 0.0 self . goal . target_pose . pose . position . y = 0.0 self . goal . target_pose . pose . position . z = 0.0 self . goal . target_pose . pose . orientation . x = 0.0 self . goal . target_pose . pose . orientation . y = 0.0 self . goal . target_pose . pose . orientation . z = 0.0 self . goal . target_pose . pose . orientation . w = 1.0 def get_quaternion ( self , theta ): \"\"\" A function to build Quaternians from Euler angles. Since the Stretch only rotates around z, we can zero out the other angles. :param theta: The angle (radians) the robot makes with the x-axis. \"\"\" return Quaternion ( * transformations . quaternion_from_euler ( 0.0 , 0.0 , theta )) def go_to ( self , x , y , theta ): \"\"\" Drive the robot to a particular pose on the map. The Stretch only needs (x, y) coordinates and a heading. :param x: x coordinate in the map frame. :param y: y coordinate in the map frame. :param theta: heading (angle with the x-axis in the map frame) \"\"\" rospy . loginfo ( ' {0} : Heading for ( {1} , {2} ) at {3} radians' . format ( self . __class__ . __name__ , x , y , theta )) self . goal . target_pose . pose . position . x = x self . goal . target_pose . pose . position . y = y self . goal . target_pose . pose . orientation = self . get_quaternion ( theta ) self . client . send_goal ( self . goal , done_cb = self . done_callback ) self . client . wait_for_result () def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. :param self: The self reference. :param status: status attribute from MoveBaseActionResult message. :param result: result attribute from MoveBaseActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( ' {0} : SUCCEEDED in reaching the goal.' . format ( self . __class__ . __name__ )) else : rospy . loginfo ( ' {0} : FAILED in reaching the goal.' . format ( self . __class__ . __name__ )) if __name__ == '__main__' : rospy . init_node ( 'navigation' , argv = sys . argv ) nav = StretchNavigation () nav . go_to ( 0.5 , 0.0 , 0.0 ) The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import actionlib import sys from move_base_msgs.msg import MoveBaseAction , MoveBaseGoal from geometry_msgs.msg import Quaternion from tf import transformations You need to import rospy if you are writing a ROS Node . self . client = actionlib . SimpleActionClient ( 'move_base' , MoveBaseAction ) self . client . wait_for_server () rospy . loginfo ( ' {0} : Made contact with move_base server' . format ( self . __class__ . __name__ )) Set up a client for the navigation action. On the Stretch, this is called move_base , and has type MoveBaseAction . Once we make the client, we wait for the server to be ready. self . goal = MoveBaseGoal () self . goal . target_pose . header . frame_id = 'map' self . goal . target_pose . header . stamp = rospy . Time () Make a goal for the action. Specify the coordinate frame that we want, in this instance the map frame. Then we set the time to be now. self . goal . target_pose . pose . position . x = 0.0 self . goal . target_pose . pose . position . y = 0.0 self . goal . target_pose . pose . position . z = 0.0 self . goal . target_pose . pose . orientation . x = 0.0 self . goal . target_pose . pose . orientation . y = 0.0 self . goal . target_pose . pose . orientation . z = 0.0 self . goal . target_pose . pose . orientation . w = 1.0 Initialize a position in the coordinate frame. def get_quaternion ( self , theta ): \"\"\" A function to build Quaternians from Euler angles. Since the Stretch only rotates around z, we can zero out the other angles. :param theta: The angle (radians) the robot makes with the x-axis. \"\"\" return Quaternion ( * transformations . quaternion_from_euler ( 0.0 , 0.0 , theta )) A function that transforms Euler angles to quaternions and returns those values. def go_to ( self , x , y , theta , wait = False ): \"\"\" Drive the robot to a particular pose on the map. The Stretch only needs (x, y) coordinates and a heading. :param x: x coordinate in the map frame. :param y: y coordinate in the map frame. :param theta: heading (angle with the x-axis in the map frame) \"\"\" rospy . loginfo ( ' {0} : Heading for ( {1} , {2} ) at {3} radians' . format ( self . __class__ . __name__ , x , y , theta )) The go_to() function takes in the 3 arguments, the x and y coordinates in the map frame, and the heading. self . goal . target_pose . pose . position . x = x self . goal . target_pose . pose . position . y = y self . goal . target_pose . pose . orientation = self . get_quaternion ( theta ) The MoveBaseGoal() data structure has three goal positions (in meters), along each of the axes. For Stretch, it will only pay attention to the x and y coordinates, since it can't move in the z-direction. self . client . send_goal ( self . goal , done_cb = self . done_callback ) self . client . wait_for_result () Send the goal and include the done_callback() function in one of the arguments in send_goal() . def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. :param self: The self reference. :param status: status attribute from MoveBaseActionResult message. :param result: result attribute from MoveBaseActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( ' {0} : SUCCEEDED in reaching the goal.' . format ( self . __class__ . __name__ )) else : rospy . loginfo ( ' {0} : FAILED in reaching the goal.' . format ( self . __class__ . __name__ )) Conditional statement to print whether the goal status in the MoveBaseActionResult succeeded or failed. rospy . init_node ( 'navigation' , argv = sys . argv ) nav = StretchNavigation () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Declare the StretchNavigation object. nav . go_to ( 0.5 , 0.0 , 0.0 ) Send a move base goal half a meter in front of the map's origin.","title":"2D Navigation Goals"},{"location":"stretch-tutorials/ros1/example_13/#example-13","text":"In this example, we will be utilizing the move_base package , a component of the ROS navigation stack , to send base goals to the Stretch robot.","title":"Example 13"},{"location":"stretch-tutorials/ros1/example_13/#build-a-map","text":"First, begin by building a map of the space the robot will be navigating in. If you need a refresher on how to do this, then check out the Navigation Stack tutorial .","title":"Build a map"},{"location":"stretch-tutorials/ros1/example_13/#getting-started","text":"Next, with your created map, we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Where ${HELLO_FLEET_PATH} is the path of the <map_name>.yaml file. Note It's likely that the robot's location on the map does not match the robot's location in real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in real space. Below is a gif for reference. Now we are going to use a node to send a move_base goal half a meter in front of the map's origin. run the following command in a new terminal to execute the navigation.py node. cd catkin_ws/src/stretch_tutorials/src/ python3 navigation.py","title":"Getting Started"},{"location":"stretch-tutorials/ros1/example_13/#the-code","text":"#!/usr/bin/env python3 import rospy import actionlib import sys from move_base_msgs.msg import MoveBaseAction , MoveBaseGoal from geometry_msgs.msg import Quaternion from tf import transformations class StretchNavigation : \"\"\" A simple encapsulation of the navigation stack for a Stretch robot. \"\"\" def __init__ ( self ): \"\"\" Create an instance of the simple navigation interface. :param self: The self reference. \"\"\" self . client = actionlib . SimpleActionClient ( 'move_base' , MoveBaseAction ) self . client . wait_for_server () rospy . loginfo ( ' {0} : Made contact with move_base server' . format ( self . __class__ . __name__ )) self . goal = MoveBaseGoal () self . goal . target_pose . header . frame_id = 'map' self . goal . target_pose . header . stamp = rospy . Time () self . goal . target_pose . pose . position . x = 0.0 self . goal . target_pose . pose . position . y = 0.0 self . goal . target_pose . pose . position . z = 0.0 self . goal . target_pose . pose . orientation . x = 0.0 self . goal . target_pose . pose . orientation . y = 0.0 self . goal . target_pose . pose . orientation . z = 0.0 self . goal . target_pose . pose . orientation . w = 1.0 def get_quaternion ( self , theta ): \"\"\" A function to build Quaternians from Euler angles. Since the Stretch only rotates around z, we can zero out the other angles. :param theta: The angle (radians) the robot makes with the x-axis. \"\"\" return Quaternion ( * transformations . quaternion_from_euler ( 0.0 , 0.0 , theta )) def go_to ( self , x , y , theta ): \"\"\" Drive the robot to a particular pose on the map. The Stretch only needs (x, y) coordinates and a heading. :param x: x coordinate in the map frame. :param y: y coordinate in the map frame. :param theta: heading (angle with the x-axis in the map frame) \"\"\" rospy . loginfo ( ' {0} : Heading for ( {1} , {2} ) at {3} radians' . format ( self . __class__ . __name__ , x , y , theta )) self . goal . target_pose . pose . position . x = x self . goal . target_pose . pose . position . y = y self . goal . target_pose . pose . orientation = self . get_quaternion ( theta ) self . client . send_goal ( self . goal , done_cb = self . done_callback ) self . client . wait_for_result () def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. :param self: The self reference. :param status: status attribute from MoveBaseActionResult message. :param result: result attribute from MoveBaseActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( ' {0} : SUCCEEDED in reaching the goal.' . format ( self . __class__ . __name__ )) else : rospy . loginfo ( ' {0} : FAILED in reaching the goal.' . format ( self . __class__ . __name__ )) if __name__ == '__main__' : rospy . init_node ( 'navigation' , argv = sys . argv ) nav = StretchNavigation () nav . go_to ( 0.5 , 0.0 , 0.0 )","title":"The Code"},{"location":"stretch-tutorials/ros1/example_13/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import actionlib import sys from move_base_msgs.msg import MoveBaseAction , MoveBaseGoal from geometry_msgs.msg import Quaternion from tf import transformations You need to import rospy if you are writing a ROS Node . self . client = actionlib . SimpleActionClient ( 'move_base' , MoveBaseAction ) self . client . wait_for_server () rospy . loginfo ( ' {0} : Made contact with move_base server' . format ( self . __class__ . __name__ )) Set up a client for the navigation action. On the Stretch, this is called move_base , and has type MoveBaseAction . Once we make the client, we wait for the server to be ready. self . goal = MoveBaseGoal () self . goal . target_pose . header . frame_id = 'map' self . goal . target_pose . header . stamp = rospy . Time () Make a goal for the action. Specify the coordinate frame that we want, in this instance the map frame. Then we set the time to be now. self . goal . target_pose . pose . position . x = 0.0 self . goal . target_pose . pose . position . y = 0.0 self . goal . target_pose . pose . position . z = 0.0 self . goal . target_pose . pose . orientation . x = 0.0 self . goal . target_pose . pose . orientation . y = 0.0 self . goal . target_pose . pose . orientation . z = 0.0 self . goal . target_pose . pose . orientation . w = 1.0 Initialize a position in the coordinate frame. def get_quaternion ( self , theta ): \"\"\" A function to build Quaternians from Euler angles. Since the Stretch only rotates around z, we can zero out the other angles. :param theta: The angle (radians) the robot makes with the x-axis. \"\"\" return Quaternion ( * transformations . quaternion_from_euler ( 0.0 , 0.0 , theta )) A function that transforms Euler angles to quaternions and returns those values. def go_to ( self , x , y , theta , wait = False ): \"\"\" Drive the robot to a particular pose on the map. The Stretch only needs (x, y) coordinates and a heading. :param x: x coordinate in the map frame. :param y: y coordinate in the map frame. :param theta: heading (angle with the x-axis in the map frame) \"\"\" rospy . loginfo ( ' {0} : Heading for ( {1} , {2} ) at {3} radians' . format ( self . __class__ . __name__ , x , y , theta )) The go_to() function takes in the 3 arguments, the x and y coordinates in the map frame, and the heading. self . goal . target_pose . pose . position . x = x self . goal . target_pose . pose . position . y = y self . goal . target_pose . pose . orientation = self . get_quaternion ( theta ) The MoveBaseGoal() data structure has three goal positions (in meters), along each of the axes. For Stretch, it will only pay attention to the x and y coordinates, since it can't move in the z-direction. self . client . send_goal ( self . goal , done_cb = self . done_callback ) self . client . wait_for_result () Send the goal and include the done_callback() function in one of the arguments in send_goal() . def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. :param self: The self reference. :param status: status attribute from MoveBaseActionResult message. :param result: result attribute from MoveBaseActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( ' {0} : SUCCEEDED in reaching the goal.' . format ( self . __class__ . __name__ )) else : rospy . loginfo ( ' {0} : FAILED in reaching the goal.' . format ( self . __class__ . __name__ )) Conditional statement to print whether the goal status in the MoveBaseActionResult succeeded or failed. rospy . init_node ( 'navigation' , argv = sys . argv ) nav = StretchNavigation () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Declare the StretchNavigation object. nav . go_to ( 0.5 , 0.0 , 0.0 ) Send a move base goal half a meter in front of the map's origin.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_2/","text":"Example 2 This example aims to provide instructions on how to filter scan messages. For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specifications: # Laser scans angles are measured counter clockwise, with Stretch's LiDAR having # both angle_min and angle_max facing forward (very closely along the x-axis) Header header float32 angle_min # start angle of the scan [rad] float32 angle_max # end angle of the scan [rad] float32 angle_increment # angular distance between measurements [rad] float32 time_increment # time between measurements [seconds] float32 scan_time # time between scans [seconds] float32 range_min # minimum range value [m] float32 range_max # maximum range value [m] float32 [] ranges # range data [m] (Note: values < range_min or > range_max should be discarded) float32 [] intensities # intensity data [device-specific units] The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type. For a Stretch robot the start angle of the scan, angle_min , and end angle, angle_max , are closely located along the x-axis of Stretch's frame. angle_min and angle_max are set at -3.1416 and 3.1416 , respectively. This is illustrated by the images below. Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot. First, open a terminal and run the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Then in a new terminal run the rplidar.launch file from stretch_core . roslaunch stretch_core rplidar.launch To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan_filter.py node by typing the following in a new terminal. cd catkin_ws/src/stretch_tutorials/src/ python3 scan_filter.py Then run the following command in a separate terminal to bring up a simple RViz configuration of the Stretch robot. rosrun rviz rviz -d ` rospack find stretch_core ` /rviz/stretch_simple_test.rviz Change the topic name from the LaserScan display from /scan to /filter_scan . The Code #!/usr/bin/env python3 import rospy from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan class ScanFilter : \"\"\" A class that implements a LaserScan filter that removes all of the points that are not in front of the robot. \"\"\" def __init__ ( self ): self . width = 1.0 self . extent = self . width / 2.0 self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . callback ) self . pub = rospy . Publisher ( 'filtered_scan' , LaserScan , queue_size = 10 ) rospy . loginfo ( \"Publishing the filtered_scan topic. Use RViz to visualize.\" ) def callback ( self , msg ): \"\"\" Callback function to deal with incoming LaserScan messages. :param self: The self reference. :param msg: The subscribed LaserScan message. :publishes msg: updated LaserScan message. \"\"\" angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] msg . ranges = new_ranges self . pub . publish ( msg ) if __name__ == '__main__' : rospy . init_node ( 'scan_filter' ) ScanFilter () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan You need to import rospy if you are writing a ROS Node . There are functions from numpy and math that are required within this code, that's why linspace , inf , and sin are imported. The sensor_msgs.msg import is so that we can subscribe and publish LaserScan messages. self . width = 1 self . extent = self . width / 2.0 We're going to assume that the robot is pointing up the x-axis so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered. self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . callback ) Set up a subscriber. We're going to subscribe to the topic scan , looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function callback automatically. self . pub = rospy . Publisher ( 'filtered_scan' , LaserScan , queue_size = 10 ) pub = rospy.Publisher(\"filtered_scan\", LaserScan, queue_size=10) declares that your node is publishing to the filtered_scan topic using the message type LaserScan . This lets the master tell any nodes listening on filtered_scan that we are going to publish data on that topic. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) This line of code utilizes linspace to compute each angle of the subscribed scan. points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". msg . ranges = new_ranges self . pub . publish ( msg ) Substitute the new ranges in the original message, and republish it. rospy . init_node ( 'scan_filter' ) ScanFilter () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the class with ScanFilter() rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Filter Laser Scans"},{"location":"stretch-tutorials/ros1/example_2/#example-2","text":"This example aims to provide instructions on how to filter scan messages. For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specifications: # Laser scans angles are measured counter clockwise, with Stretch's LiDAR having # both angle_min and angle_max facing forward (very closely along the x-axis) Header header float32 angle_min # start angle of the scan [rad] float32 angle_max # end angle of the scan [rad] float32 angle_increment # angular distance between measurements [rad] float32 time_increment # time between measurements [seconds] float32 scan_time # time between scans [seconds] float32 range_min # minimum range value [m] float32 range_max # maximum range value [m] float32 [] ranges # range data [m] (Note: values < range_min or > range_max should be discarded) float32 [] intensities # intensity data [device-specific units] The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type. For a Stretch robot the start angle of the scan, angle_min , and end angle, angle_max , are closely located along the x-axis of Stretch's frame. angle_min and angle_max are set at -3.1416 and 3.1416 , respectively. This is illustrated by the images below. Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot. First, open a terminal and run the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Then in a new terminal run the rplidar.launch file from stretch_core . roslaunch stretch_core rplidar.launch To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan_filter.py node by typing the following in a new terminal. cd catkin_ws/src/stretch_tutorials/src/ python3 scan_filter.py Then run the following command in a separate terminal to bring up a simple RViz configuration of the Stretch robot. rosrun rviz rviz -d ` rospack find stretch_core ` /rviz/stretch_simple_test.rviz Change the topic name from the LaserScan display from /scan to /filter_scan .","title":"Example 2"},{"location":"stretch-tutorials/ros1/example_2/#the-code","text":"#!/usr/bin/env python3 import rospy from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan class ScanFilter : \"\"\" A class that implements a LaserScan filter that removes all of the points that are not in front of the robot. \"\"\" def __init__ ( self ): self . width = 1.0 self . extent = self . width / 2.0 self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . callback ) self . pub = rospy . Publisher ( 'filtered_scan' , LaserScan , queue_size = 10 ) rospy . loginfo ( \"Publishing the filtered_scan topic. Use RViz to visualize.\" ) def callback ( self , msg ): \"\"\" Callback function to deal with incoming LaserScan messages. :param self: The self reference. :param msg: The subscribed LaserScan message. :publishes msg: updated LaserScan message. \"\"\" angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] msg . ranges = new_ranges self . pub . publish ( msg ) if __name__ == '__main__' : rospy . init_node ( 'scan_filter' ) ScanFilter () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_2/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan You need to import rospy if you are writing a ROS Node . There are functions from numpy and math that are required within this code, that's why linspace , inf , and sin are imported. The sensor_msgs.msg import is so that we can subscribe and publish LaserScan messages. self . width = 1 self . extent = self . width / 2.0 We're going to assume that the robot is pointing up the x-axis so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered. self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . callback ) Set up a subscriber. We're going to subscribe to the topic scan , looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function callback automatically. self . pub = rospy . Publisher ( 'filtered_scan' , LaserScan , queue_size = 10 ) pub = rospy.Publisher(\"filtered_scan\", LaserScan, queue_size=10) declares that your node is publishing to the filtered_scan topic using the message type LaserScan . This lets the master tell any nodes listening on filtered_scan that we are going to publish data on that topic. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) This line of code utilizes linspace to compute each angle of the subscribed scan. points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". msg . ranges = new_ranges self . pub . publish ( msg ) Substitute the new ranges in the original message, and republish it. rospy . init_node ( 'scan_filter' ) ScanFilter () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the class with ScanFilter() rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_3/","text":"Example 3 This example aims to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward. Begin by running the following command in a new terminal. roslaunch stretch_core stretch_driver.launch Then, in a new terminal, type the following to activate the LiDAR sensor. roslaunch stretch_core rplidar.launch To set navigation mode and to activate the avoider.py node, type the following in a new terminal. rosservice call /switch_to_navigation_mode cd catkin_ws/src/stretch_tutorials/src/ python3 avoider.py To stop the node from sending twist messages, type Ctrl + c in the terminal running the avoider node. The Code #!/usr/bin/env python3 import rospy from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan class Avoider : \"\"\" A class that implements both a LaserScan filter and base velocity control for collision avoidance. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber, publisher, and marker features. :param self: The self reference. \"\"\" self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . set_speed ) self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 def set_speed ( self , msg ): \"\"\" Callback function to deal with incoming LaserScan messages. :param self: The self reference. :param msg: The subscribed LaserScan message. :publishes self.twist: Twist message. \"\"\" angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] error = min ( new_ranges ) - self . distance self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 self . pub . publish ( self . twist ) if __name__ == '__main__' : rospy . init_node ( 'avoider' ) Avoider () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan You need to import rospy if you are writing a ROS Node . There are functions from numpy and math that are required within this code, thus linspace , inf , tanh , and sin are imported. The sensor_msgs.msg import is so that we can subscribe to LaserScan messages. The geometry_msgs.msg import is so that we can send velocity commands to the robot. self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo This section of the code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist . self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . set_speed ) Set up a subscriber. We're going to subscribe to the topic scan , looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function set_speed() automatically. self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self.width is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing to the x-axis, any points with y coordinates further than half of the defined width ( self.extent ) from the x-axis are not considered. self.distance defines the stopping distance from an object. self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 Allocate a Twist to use, and set everything to zero. We're going to do this when the class is initiated. Redefining this within the callback function, set_speed() can be more computationally taxing. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] This line of code utilizes linspace to compute each angle of the subscribed scan. Here we compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf , which means \"no return\". error = min ( new_ranges ) - self . distance Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as error . self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 Set the speed according to a tanh function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1 self . pub . publish ( self . twist ) Publish the Twist message. rospy . init_node ( 'avoider' ) Avoider () rospy . spin () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate class with Avioder() . Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Mobile Base Collision Avoidance"},{"location":"stretch-tutorials/ros1/example_3/#example-3","text":"This example aims to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward. Begin by running the following command in a new terminal. roslaunch stretch_core stretch_driver.launch Then, in a new terminal, type the following to activate the LiDAR sensor. roslaunch stretch_core rplidar.launch To set navigation mode and to activate the avoider.py node, type the following in a new terminal. rosservice call /switch_to_navigation_mode cd catkin_ws/src/stretch_tutorials/src/ python3 avoider.py To stop the node from sending twist messages, type Ctrl + c in the terminal running the avoider node.","title":"Example 3"},{"location":"stretch-tutorials/ros1/example_3/#the-code","text":"#!/usr/bin/env python3 import rospy from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan class Avoider : \"\"\" A class that implements both a LaserScan filter and base velocity control for collision avoidance. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber, publisher, and marker features. :param self: The self reference. \"\"\" self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . set_speed ) self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 def set_speed ( self , msg ): \"\"\" Callback function to deal with incoming LaserScan messages. :param self: The self reference. :param msg: The subscribed LaserScan message. :publishes self.twist: Twist message. \"\"\" angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] error = min ( new_ranges ) - self . distance self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 self . pub . publish ( self . twist ) if __name__ == '__main__' : rospy . init_node ( 'avoider' ) Avoider () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_3/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan You need to import rospy if you are writing a ROS Node . There are functions from numpy and math that are required within this code, thus linspace , inf , tanh , and sin are imported. The sensor_msgs.msg import is so that we can subscribe to LaserScan messages. The geometry_msgs.msg import is so that we can send velocity commands to the robot. self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo This section of the code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist . self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . set_speed ) Set up a subscriber. We're going to subscribe to the topic scan , looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function set_speed() automatically. self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self.width is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing to the x-axis, any points with y coordinates further than half of the defined width ( self.extent ) from the x-axis are not considered. self.distance defines the stopping distance from an object. self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 Allocate a Twist to use, and set everything to zero. We're going to do this when the class is initiated. Redefining this within the callback function, set_speed() can be more computationally taxing. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] This line of code utilizes linspace to compute each angle of the subscribed scan. Here we compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf , which means \"no return\". error = min ( new_ranges ) - self . distance Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as error . self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 Set the speed according to a tanh function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1 self . pub . publish ( self . twist ) Publish the Twist message. rospy . init_node ( 'avoider' ) Avoider () rospy . spin () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate class with Avioder() . Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_4/","text":"Example 4 Let's bring up Stretch in the Willow Garage world from the gazebo basics tutorial and RViz by using the following command. roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world rviz: = true The rviz flag will open an RViz window to visualize a variety of ROS topics. In a new terminal, run the following commands to execute the marker.py node. cd catkin_ws/src/stretch_tutorials/src/ python3 marker.py The GIF below demonstrates how to add a new Marker display type, and change the topic name from /visualization_marker to /balloon . A red sphere marker should appear above the Stretch robot. The Code #!/usr/bin/env python3 import rospy from visualization_msgs.msg import Marker class Balloon (): \"\"\" A class that attaches a Sphere marker directly above the Stretch robot. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the marker's features. :param self: The self reference. \"\"\" self . publisher = rospy . Publisher ( 'balloon' , Marker , queue_size = 10 ) self . marker = Marker () self . marker . header . frame_id = 'base_link' self . marker . header . stamp = rospy . Time () self . marker . type = self . marker . SPHERE self . marker . id = 0 self . marker . action = self . marker . ADD self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 self . marker . color . a = 1.0 self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 rospy . loginfo ( \"Publishing the balloon topic. Use RViz to visualize.\" ) def publish_marker ( self ): \"\"\" Function that publishes the sphere marker. :param self: The self reference. :publishes self.marker: Marker message. \"\"\" self . publisher . publish ( self . marker ) if __name__ == '__main__' : rospy . init_node ( 'marker' ) balloon = Balloon () rate = rospy . Rate ( 10 ) while not rospy . is_shutdown (): balloon . publish_marker () rate . sleep () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from visualization_msgs.msg import Marker You need to import rospy if you are writing a ROS Node . Import the Marker type from the visualization_msgs.msg package. This import is required to publish a Marker , which will be visualized in RViz. self . pub = rospy . Publisher ( 'balloon' , Marker , queue_size = 10 ) This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"balloon\", Twist, queue_size=1) declares that your node is publishing to the /ballon topic using the message type Twist . self . marker = Marker () self . marker . header . frame_id = 'base_link' self . marker . header . stamp = rospy . Time () self . marker . type = self . marker . SPHERE Create a Marker() message type. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers self . marker . id = 0 Each marker has a unique ID number. If you have more than one marker that you want to be displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number as an existing marker, it will replace the existing marker with that ID number. self . marker . action = self . marker . ADD This line of code sets the action. You can add, delete, or modify markers. self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 These are the size parameters for the marker. These will vary by marker type. self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 Color of the object, specified as r/g/b/a, with values in the range of [0, 1]. self . marker . color . a = 1.0 The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible. self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in frame_id . In this case, the position will always be directly 2 meters above the frame_id ( base_link ), and will move with it. def publish_marker ( self ): self . publisher . publish ( self . marker ) Publish the Marker data structure to be visualized in RViz. rospy . init_node ( 'marker' , argv = sys . argv ) balloon = Balloon () rate = rospy . Rate ( 10 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate class with Balloon() . Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages. while not rospy . is_shutdown (): balloon . publish_marker () rate . sleep () This loop is a fairly standard rospy construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl+c or otherwise). The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop.","title":"Give Stretch a Balloon"},{"location":"stretch-tutorials/ros1/example_4/#example-4","text":"Let's bring up Stretch in the Willow Garage world from the gazebo basics tutorial and RViz by using the following command. roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world rviz: = true The rviz flag will open an RViz window to visualize a variety of ROS topics. In a new terminal, run the following commands to execute the marker.py node. cd catkin_ws/src/stretch_tutorials/src/ python3 marker.py The GIF below demonstrates how to add a new Marker display type, and change the topic name from /visualization_marker to /balloon . A red sphere marker should appear above the Stretch robot.","title":"Example 4"},{"location":"stretch-tutorials/ros1/example_4/#the-code","text":"#!/usr/bin/env python3 import rospy from visualization_msgs.msg import Marker class Balloon (): \"\"\" A class that attaches a Sphere marker directly above the Stretch robot. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the marker's features. :param self: The self reference. \"\"\" self . publisher = rospy . Publisher ( 'balloon' , Marker , queue_size = 10 ) self . marker = Marker () self . marker . header . frame_id = 'base_link' self . marker . header . stamp = rospy . Time () self . marker . type = self . marker . SPHERE self . marker . id = 0 self . marker . action = self . marker . ADD self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 self . marker . color . a = 1.0 self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 rospy . loginfo ( \"Publishing the balloon topic. Use RViz to visualize.\" ) def publish_marker ( self ): \"\"\" Function that publishes the sphere marker. :param self: The self reference. :publishes self.marker: Marker message. \"\"\" self . publisher . publish ( self . marker ) if __name__ == '__main__' : rospy . init_node ( 'marker' ) balloon = Balloon () rate = rospy . Rate ( 10 ) while not rospy . is_shutdown (): balloon . publish_marker () rate . sleep ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_4/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from visualization_msgs.msg import Marker You need to import rospy if you are writing a ROS Node . Import the Marker type from the visualization_msgs.msg package. This import is required to publish a Marker , which will be visualized in RViz. self . pub = rospy . Publisher ( 'balloon' , Marker , queue_size = 10 ) This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"balloon\", Twist, queue_size=1) declares that your node is publishing to the /ballon topic using the message type Twist . self . marker = Marker () self . marker . header . frame_id = 'base_link' self . marker . header . stamp = rospy . Time () self . marker . type = self . marker . SPHERE Create a Marker() message type. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers self . marker . id = 0 Each marker has a unique ID number. If you have more than one marker that you want to be displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number as an existing marker, it will replace the existing marker with that ID number. self . marker . action = self . marker . ADD This line of code sets the action. You can add, delete, or modify markers. self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 These are the size parameters for the marker. These will vary by marker type. self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 Color of the object, specified as r/g/b/a, with values in the range of [0, 1]. self . marker . color . a = 1.0 The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible. self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in frame_id . In this case, the position will always be directly 2 meters above the frame_id ( base_link ), and will move with it. def publish_marker ( self ): self . publisher . publish ( self . marker ) Publish the Marker data structure to be visualized in RViz. rospy . init_node ( 'marker' , argv = sys . argv ) balloon = Balloon () rate = rospy . Rate ( 10 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate class with Balloon() . Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages. while not rospy . is_shutdown (): balloon . publish_marker () rate . sleep () This loop is a fairly standard rospy construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl+c or otherwise). The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_5/","text":"Example 5 In this example, we will review a Python script that prints out the positions of a selected group of Stretch joints. This script is helpful if you need the joint positions after you teleoperated Stretch with the Xbox controller or physically moved the robot to the desired configuration after hitting the run stop button. If you are looking for a continuous print of the joint states while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial . Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch You can then hit the run-stop button (you should hear a beep and the LED light in the button blink) and move the robot's joints to a desired configuration. Once you are satisfied with the configuration, hold the run-stop button until you hear a beep. Then run the following command to execute the joint_state_printer.py which will print the joint positions of the lift, arm, and wrist. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 joint_state_printer.py Your terminal will output the position information of the previously mentioned joints shown below. name: [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] position: [ 0 .6043133175850597, 0 .19873586673129257, 0 .017257283863713464 ] Note Stretch's arm has four prismatic joints and the sum of their positions gives the wrist_extension distance. The wrist_extension is needed when sending joint trajectory commands to the robot. Further, you can not actuate an individual arm joint. Here is an image of the arm joints for reference: The Code #!/usr/bin/env python3 import rospy import sys from sensor_msgs.msg import JointState class JointStatePublisher (): \"\"\" A class that prints the positions of desired joints in Stretch. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber. :param self: The self reference. \"\"\" self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) def callback ( self , msg ): \"\"\" Callback function to deal with the incoming JointState messages. :param self: The self reference. :param msg: The JointState message. \"\"\" self . joint_states = msg def print_states ( self , joints ): \"\"\" print_states function to deal with the incoming JointState messages. :param self: The self reference. :param joints: A list of string values of joint names. \"\"\" joint_positions = [] for joint in joints : if joint == \"wrist_extension\" : index = self . joint_states . name . index ( 'joint_arm_l0' ) joint_positions . append ( 4 * self . joint_states . position [ index ]) continue index = self . joint_states . name . index ( joint ) joint_positions . append ( self . joint_states . position [ index ]) print ( \"name: \" + str ( joints )) print ( \"position: \" + str ( joint_positions )) rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) if __name__ == '__main__' : rospy . init_node ( 'joint_state_printer' , anonymous = True ) JSP = JointStatePublisher () rospy . sleep ( .1 ) joints = [ \"joint_lift\" , \"wrist_extension\" , \"joint_wrist_yaw\" ] #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"] JSP . print_states ( joints ) rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import sys from sensor_msgs.msg import JointState You need to import rospy if you are writing a ROS Node . Import sensor_msgs.msg so that we can subscribe to JointState messages. self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) Set up a subscriber. We're going to subscribe to the topic joint_states , looking for JointState messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically def callback ( self , msg ): self . joint_states = msg This is the callback function where the JointState messages are stored as self.joint_states . Further information about this message type can be found here: JointState Message def print_states ( self , joints ): joint_positions = [] This is the print_states() function which takes in a list of joints of interest as its argument. the is also an empty list set as joint_positions and this is where the positions of the requested joints will be appended. for joint in joints : if joint == \"wrist_extension\" : index = self . joint_states . name . index ( 'joint_arm_l0' ) joint_positions . append ( 4 * self . joint_states . position [ index ]) continue index = self . joint_states . name . index ( joint ) joint_positions . append ( self . joint_states . position [ index ]) In this section of the code, a for loop is used to parse the names of the requested joints from the self.joint_states list. The index() function returns the index of the name of the requested joint and appends the respective position to the joint_positions list. rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter. rospy . init_node ( 'joint_state_printer' , anonymous = True ) JSP = JointStatePublisher () rospy . sleep ( .1 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Declare object, JSP , from the JointStatePublisher class. The use of the rospy.sleep() function is to allow the JSP class to initialize all of its features before requesting to publish joint positions of desired joints (running the print_states() method). joints = [ \"joint_lift\" , \"wrist_extension\" , \"joint_wrist_yaw\" ] #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"] JSP . print_states ( joints ) Create a list of the desired joints that you want positions to be printed. Then use that list as an argument for the print_states() method. rospy . spin () Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Print Joint States"},{"location":"stretch-tutorials/ros1/example_5/#example-5","text":"In this example, we will review a Python script that prints out the positions of a selected group of Stretch joints. This script is helpful if you need the joint positions after you teleoperated Stretch with the Xbox controller or physically moved the robot to the desired configuration after hitting the run stop button. If you are looking for a continuous print of the joint states while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial . Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch You can then hit the run-stop button (you should hear a beep and the LED light in the button blink) and move the robot's joints to a desired configuration. Once you are satisfied with the configuration, hold the run-stop button until you hear a beep. Then run the following command to execute the joint_state_printer.py which will print the joint positions of the lift, arm, and wrist. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 joint_state_printer.py Your terminal will output the position information of the previously mentioned joints shown below. name: [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] position: [ 0 .6043133175850597, 0 .19873586673129257, 0 .017257283863713464 ] Note Stretch's arm has four prismatic joints and the sum of their positions gives the wrist_extension distance. The wrist_extension is needed when sending joint trajectory commands to the robot. Further, you can not actuate an individual arm joint. Here is an image of the arm joints for reference:","title":"Example 5"},{"location":"stretch-tutorials/ros1/example_5/#the-code","text":"#!/usr/bin/env python3 import rospy import sys from sensor_msgs.msg import JointState class JointStatePublisher (): \"\"\" A class that prints the positions of desired joints in Stretch. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber. :param self: The self reference. \"\"\" self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) def callback ( self , msg ): \"\"\" Callback function to deal with the incoming JointState messages. :param self: The self reference. :param msg: The JointState message. \"\"\" self . joint_states = msg def print_states ( self , joints ): \"\"\" print_states function to deal with the incoming JointState messages. :param self: The self reference. :param joints: A list of string values of joint names. \"\"\" joint_positions = [] for joint in joints : if joint == \"wrist_extension\" : index = self . joint_states . name . index ( 'joint_arm_l0' ) joint_positions . append ( 4 * self . joint_states . position [ index ]) continue index = self . joint_states . name . index ( joint ) joint_positions . append ( self . joint_states . position [ index ]) print ( \"name: \" + str ( joints )) print ( \"position: \" + str ( joint_positions )) rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) if __name__ == '__main__' : rospy . init_node ( 'joint_state_printer' , anonymous = True ) JSP = JointStatePublisher () rospy . sleep ( .1 ) joints = [ \"joint_lift\" , \"wrist_extension\" , \"joint_wrist_yaw\" ] #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"] JSP . print_states ( joints ) rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_5/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import sys from sensor_msgs.msg import JointState You need to import rospy if you are writing a ROS Node . Import sensor_msgs.msg so that we can subscribe to JointState messages. self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) Set up a subscriber. We're going to subscribe to the topic joint_states , looking for JointState messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically def callback ( self , msg ): self . joint_states = msg This is the callback function where the JointState messages are stored as self.joint_states . Further information about this message type can be found here: JointState Message def print_states ( self , joints ): joint_positions = [] This is the print_states() function which takes in a list of joints of interest as its argument. the is also an empty list set as joint_positions and this is where the positions of the requested joints will be appended. for joint in joints : if joint == \"wrist_extension\" : index = self . joint_states . name . index ( 'joint_arm_l0' ) joint_positions . append ( 4 * self . joint_states . position [ index ]) continue index = self . joint_states . name . index ( joint ) joint_positions . append ( self . joint_states . position [ index ]) In this section of the code, a for loop is used to parse the names of the requested joints from the self.joint_states list. The index() function returns the index of the name of the requested joint and appends the respective position to the joint_positions list. rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter. rospy . init_node ( 'joint_state_printer' , anonymous = True ) JSP = JointStatePublisher () rospy . sleep ( .1 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Declare object, JSP , from the JointStatePublisher class. The use of the rospy.sleep() function is to allow the JSP class to initialize all of its features before requesting to publish joint positions of desired joints (running the print_states() method). joints = [ \"joint_lift\" , \"wrist_extension\" , \"joint_wrist_yaw\" ] #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"] JSP . print_states ( joints ) Create a list of the desired joints that you want positions to be printed. Then use that list as an argument for the print_states() method. rospy . spin () Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_6/","text":"Example 6 In this example, we will review a Python script that prints and stores the effort values from a specified joint. If you are looking for a continuous print of the joint state efforts while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial . Begin by running the following command in a terminal. roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the effort_sensing.py node. In a new terminal, execute: rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python3 effort_sensing.py This will send a FollowJointTrajectory command to move Stretch's arm or head while also printing the effort of the lift. The Code #!/usr/bin/env python3 import rospy import time import actionlib import os import csv from datetime import datetime from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState import hello_helpers.hello_misc as hm class JointActuatorEffortSensor ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self , export_data = False ): \"\"\" Function that initializes the subscriber,and other features. :param self: The self reference. :param export_data: A boolean message type. \"\"\" hm . HelloNode . __init__ ( self ) self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) self . joints = [ 'joint_lift' ] self . joint_effort = [] self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . export_data = export_data def callback ( self , msg ): \"\"\" Callback function to update and store JointState messages. :param self: The self reference. :param msg: The JointState message. \"\"\" self . joint_states = msg def issue_command ( self ): \"\"\" Function that makes an action call and sends joint trajectory goals to a single joint. :param self: The self reference. \"\"\" trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = self . joints point0 = JointTrajectoryPoint () point0 . positions = [ 0.9 ] trajectory_goal . trajectory . points = [ point0 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal , feedback_cb = self . feedback_callback , done_cb = self . done_callback ) rospy . loginfo ( 'Sent position goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def feedback_callback ( self , feedback ): \"\"\" The feedback_callback function deals with the incoming feedback messages from the trajectory_client. Although, in this function, we do not use the feedback information. :param self: The self reference. :param feedback: FollowJointTrajectoryActionFeedback message. \"\"\" if 'wrist_extension' in self . joints : self . joints . remove ( 'wrist_extension' ) self . joints . append ( 'joint_arm_l0' ) current_effort = [] for joint in self . joints : index = self . joint_states . name . index ( joint ) current_effort . append ( self . joint_states . effort [ index ]) if not self . export_data : print ( \"name: \" + str ( self . joints )) print ( \"effort: \" + str ( current_effort )) else : self . joint_effort . append ( current_effort ) def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. Within this function we export the data to a .txt file in the /stored_data directory. :param self: The self reference. :param status: status attribute from FollowJointTrajectoryActionResult message. :param result: result attribute from FollowJointTrajectoryActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( 'Succeeded' ) else : rospy . loginfo ( 'Failed' ) if self . export_data : file_name = datetime . now () . strftime ( \"%Y-%m- %d _%I:%M:%S-%p\" ) completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"w\" ) as f : writer = csv . writer ( f ) writer . writerow ( self . joints ) writer . writerows ( self . joint_effort ) def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'issue_command' , 'issue_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing command...' ) self . issue_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = JointActuatorEffortSensor ( export_data = True ) node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import time import actionlib import os import csv from datetime import datetime from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState import hello_helpers.hello_misc as hm You need to import rospy if you are writing a ROS Node . Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros . In this instance, we are importing the hello_misc script. class JointActuatorEffortSensor ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self , export_data = False ): \"\"\" Function that initializes the subscriber,and other features. :param self: The self reference. :param export_data: A boolean message type. \"\"\" hm . HelloNode . __init__ ( self ) The JointActuatorEffortSensor class inherits the HelloNode class from hm and is initialized. self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) self . joints = [ 'joint_lift' ] Set up a subscriber. We're going to subscribe to the topic joint_states , looking for JointState messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. Create a list of the desired joints you want to print. self . joint_effort = [] self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . export_data = export_data Create an empty list to store the joint effort values. The self.save_path is the directory path where the .txt file of the effort values will be stored. You can change this path to a preferred directory. The self.export_data is a boolean and its default value is set to False . If set to True , then the joint values will be stored in a .txt file, otherwise, the values will be printed in the terminal where you ran the effort sensing node. self . trajectory_client . send_goal ( trajectory_goal , feedback_cb = self . feedback_callback , done_cb = self . done_callback ) Include the feedback and done_callback functions in the send goal function. def feedback_callback ( self , feedback ): \"\"\" The feedback_callback function deals with the incoming feedback messages from the trajectory_client. Although, in this function, we do not use the feedback information. :param self: The self reference. :param feedback: FollowJointTrajectoryActionFeedback message. \"\"\" The feedback callback function takes in the FollowJointTrajectoryActionFeedback message as its argument. if 'wrist_extension' in self . joints : self . joints . remove ( 'wrist_extension' ) self . joints . append ( 'joint_arm_l0' ) Use a conditional statement to replace wrist_extenstion with joint_arm_l0 . This is because joint_arm_l0 has the effort values that the wrist_extension is experiencing. current_effort = [] for joint in self . joints : index = self . joint_states . name . index ( joint ) current_effort . append ( self . joint_states . effort [ index ]) Create an empty list to store the current effort values. Then use a for loop to parse the joint names and effort values. if not self . export_data : print ( \"name: \" + str ( self . joints )) print ( \"effort: \" + str ( current_effort )) else : self . joint_effort . append ( current_effort ) Use a conditional statement to print effort values in the terminal or store values into a list that will be used for exporting the data in a .txt file. def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. Within this function we export the data to a .txt file in the /stored_data directory. :param self: The self reference. :param status: status attribute from FollowJointTrajectoryActionResult message. :param result: result attribute from FollowJointTrajectoryActionResult message. \"\"\" The done callback function takes in the FollowJointTrajectoryActionResult messages as its arguments. if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( 'Succeeded' ) else : rospy . loginfo ( 'Failed' ) Conditional statement to print whether the goal status in the FollowJointTrajectoryActionResult succeeded or failed. if self . export_data : file_name = datetime . now () . strftime ( \"%Y-%m- %d _%I:%M:%S-%p\" ) completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"w\" ) as f : writer = csv . writer ( f ) writer . writerow ( self . joints ) writer . writerows ( self . joint_effort ) A conditional statement is used to export the data to a .txt file. The file's name is set to the date and time the node was executed. That way, no previous files are overwritten. Plotting/Animating Effort Data We added a simple python script, stored_data_plotter.py , to this package for plotting the stored data. Note You have to change the name of the file you wish to see in the python script. This is shown below. ####################### Copy the file name here! ####################### file_name = '2022-06-30_11:26:20-AM' Once you have changed the file name, then run the following in a new command. cd catkin_ws/src/stretch_tutorials/src/ python3 stored_data_plotter.py Because this is not a node, you don't need roscore to run this script. Please review the comments in the python script for additional guidance.","title":"Store Effort Values"},{"location":"stretch-tutorials/ros1/example_6/#example-6","text":"In this example, we will review a Python script that prints and stores the effort values from a specified joint. If you are looking for a continuous print of the joint state efforts while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial . Begin by running the following command in a terminal. roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the effort_sensing.py node. In a new terminal, execute: rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python3 effort_sensing.py This will send a FollowJointTrajectory command to move Stretch's arm or head while also printing the effort of the lift.","title":"Example 6"},{"location":"stretch-tutorials/ros1/example_6/#the-code","text":"#!/usr/bin/env python3 import rospy import time import actionlib import os import csv from datetime import datetime from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState import hello_helpers.hello_misc as hm class JointActuatorEffortSensor ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self , export_data = False ): \"\"\" Function that initializes the subscriber,and other features. :param self: The self reference. :param export_data: A boolean message type. \"\"\" hm . HelloNode . __init__ ( self ) self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) self . joints = [ 'joint_lift' ] self . joint_effort = [] self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . export_data = export_data def callback ( self , msg ): \"\"\" Callback function to update and store JointState messages. :param self: The self reference. :param msg: The JointState message. \"\"\" self . joint_states = msg def issue_command ( self ): \"\"\" Function that makes an action call and sends joint trajectory goals to a single joint. :param self: The self reference. \"\"\" trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = self . joints point0 = JointTrajectoryPoint () point0 . positions = [ 0.9 ] trajectory_goal . trajectory . points = [ point0 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal , feedback_cb = self . feedback_callback , done_cb = self . done_callback ) rospy . loginfo ( 'Sent position goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def feedback_callback ( self , feedback ): \"\"\" The feedback_callback function deals with the incoming feedback messages from the trajectory_client. Although, in this function, we do not use the feedback information. :param self: The self reference. :param feedback: FollowJointTrajectoryActionFeedback message. \"\"\" if 'wrist_extension' in self . joints : self . joints . remove ( 'wrist_extension' ) self . joints . append ( 'joint_arm_l0' ) current_effort = [] for joint in self . joints : index = self . joint_states . name . index ( joint ) current_effort . append ( self . joint_states . effort [ index ]) if not self . export_data : print ( \"name: \" + str ( self . joints )) print ( \"effort: \" + str ( current_effort )) else : self . joint_effort . append ( current_effort ) def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. Within this function we export the data to a .txt file in the /stored_data directory. :param self: The self reference. :param status: status attribute from FollowJointTrajectoryActionResult message. :param result: result attribute from FollowJointTrajectoryActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( 'Succeeded' ) else : rospy . loginfo ( 'Failed' ) if self . export_data : file_name = datetime . now () . strftime ( \"%Y-%m- %d _%I:%M:%S-%p\" ) completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"w\" ) as f : writer = csv . writer ( f ) writer . writerow ( self . joints ) writer . writerows ( self . joint_effort ) def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'issue_command' , 'issue_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing command...' ) self . issue_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = JointActuatorEffortSensor ( export_data = True ) node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1/example_6/#the-code-explained","text":"This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import time import actionlib import os import csv from datetime import datetime from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState import hello_helpers.hello_misc as hm You need to import rospy if you are writing a ROS Node . Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros . In this instance, we are importing the hello_misc script. class JointActuatorEffortSensor ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self , export_data = False ): \"\"\" Function that initializes the subscriber,and other features. :param self: The self reference. :param export_data: A boolean message type. \"\"\" hm . HelloNode . __init__ ( self ) The JointActuatorEffortSensor class inherits the HelloNode class from hm and is initialized. self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) self . joints = [ 'joint_lift' ] Set up a subscriber. We're going to subscribe to the topic joint_states , looking for JointState messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. Create a list of the desired joints you want to print. self . joint_effort = [] self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . export_data = export_data Create an empty list to store the joint effort values. The self.save_path is the directory path where the .txt file of the effort values will be stored. You can change this path to a preferred directory. The self.export_data is a boolean and its default value is set to False . If set to True , then the joint values will be stored in a .txt file, otherwise, the values will be printed in the terminal where you ran the effort sensing node. self . trajectory_client . send_goal ( trajectory_goal , feedback_cb = self . feedback_callback , done_cb = self . done_callback ) Include the feedback and done_callback functions in the send goal function. def feedback_callback ( self , feedback ): \"\"\" The feedback_callback function deals with the incoming feedback messages from the trajectory_client. Although, in this function, we do not use the feedback information. :param self: The self reference. :param feedback: FollowJointTrajectoryActionFeedback message. \"\"\" The feedback callback function takes in the FollowJointTrajectoryActionFeedback message as its argument. if 'wrist_extension' in self . joints : self . joints . remove ( 'wrist_extension' ) self . joints . append ( 'joint_arm_l0' ) Use a conditional statement to replace wrist_extenstion with joint_arm_l0 . This is because joint_arm_l0 has the effort values that the wrist_extension is experiencing. current_effort = [] for joint in self . joints : index = self . joint_states . name . index ( joint ) current_effort . append ( self . joint_states . effort [ index ]) Create an empty list to store the current effort values. Then use a for loop to parse the joint names and effort values. if not self . export_data : print ( \"name: \" + str ( self . joints )) print ( \"effort: \" + str ( current_effort )) else : self . joint_effort . append ( current_effort ) Use a conditional statement to print effort values in the terminal or store values into a list that will be used for exporting the data in a .txt file. def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. Within this function we export the data to a .txt file in the /stored_data directory. :param self: The self reference. :param status: status attribute from FollowJointTrajectoryActionResult message. :param result: result attribute from FollowJointTrajectoryActionResult message. \"\"\" The done callback function takes in the FollowJointTrajectoryActionResult messages as its arguments. if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( 'Succeeded' ) else : rospy . loginfo ( 'Failed' ) Conditional statement to print whether the goal status in the FollowJointTrajectoryActionResult succeeded or failed. if self . export_data : file_name = datetime . now () . strftime ( \"%Y-%m- %d _%I:%M:%S-%p\" ) completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"w\" ) as f : writer = csv . writer ( f ) writer . writerow ( self . joints ) writer . writerows ( self . joint_effort ) A conditional statement is used to export the data to a .txt file. The file's name is set to the date and time the node was executed. That way, no previous files are overwritten.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_6/#plottinganimating-effort-data","text":"We added a simple python script, stored_data_plotter.py , to this package for plotting the stored data. Note You have to change the name of the file you wish to see in the python script. This is shown below. ####################### Copy the file name here! ####################### file_name = '2022-06-30_11:26:20-AM' Once you have changed the file name, then run the following in a new command. cd catkin_ws/src/stretch_tutorials/src/ python3 stored_data_plotter.py Because this is not a node, you don't need roscore to run this script. Please review the comments in the python script for additional guidance.","title":"Plotting/Animating Effort Data"},{"location":"stretch-tutorials/ros1/example_7/","text":"Example 7 In this example, we will review the image_view ROS package and a Python script that captures an image from the RealSense camera . Begin by running the stretch driver.launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_low_resolution.launch Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz Capture Image with image_view There are a couple of methods to save an image using the image_view package. OPTION 1: Use the image_view node to open a simple image viewer for ROS sensor_msgs/image topics. In a new terminal, execute: rosrun image_view image_view image: = /camera/color/image_raw_upright_view Then you can save the current image by right-clicking on the display window. By default, images will be saved as frame000.jpg, frame000.jpg, etc. Note, that the image will be saved to the terminal's current work directory. OPTION 2: Use the image_saver node to save an image to the terminal's current work directory. In a new terminal, execute: rosrun image_view image_saver image: = /camera/color/image_raw_upright_view Capture Image with Python Script In this section, we will use a Python node to capture an image from the RealSense camera . Execute the capture_image.py node to save a .jpeg image of the image topic /camera/color/image_raw_upright_view . In a terminal, execute: cd ~/catkin_ws/src/stretch_tutorials/src python3 capture_image.py An image named camera_image.jpeg is saved in the stored_data folder in this package. The Code #!/usr/bin/env python3 import rospy import sys import os import cv2 from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError class CaptureImage : \"\"\" A class that converts a subscribed ROS image to a OpenCV image and saves the captured image to a predefined directory. \"\"\" def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and save path. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a CV2 image and stores the image. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) file_name = 'camera_image.jpeg' completeName = os . path . join ( self . save_path , file_name ) cv2 . imwrite ( completeName , image ) rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) if __name__ == '__main__' : rospy . init_node ( 'capture_image' , argv = sys . argv ) CaptureImage () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import sys import os import cv2 You need to import rospy if you are writing a ROS Node . There are functions from sys , os , and cv2 that are required within this code. cv2 is a library of Python functions that implements computer vision algorithms. Further information about cv2 can be found here: OpenCV Python . from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError The sensor_msgs.msg is imported so that we can subscribe to ROS Image messages. Import CvBridge to convert between ROS Image messages and OpenCV images. def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and save path. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' Initialize the CvBridge class, the subscriber, and the directory where the captured image will be stored. def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a cv2 image and stores the image. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) Try to convert the ROS Image message to a cv2 Image message using the imgmsg_to_cv2() function. file_name = 'camera_image.jpeg' completeName = os . path . join ( self . save_path , file_name ) cv2 . imwrite ( completeName , image ) Join the directory and file name using the path.join() function. Then use the imwrite() function to save the image. rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter. rospy . init_node ( 'capture_image' , argv = sys . argv ) CaptureImage () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the CaptureImage() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages. Edge Detection In this section, we highlight a node that utilizes the Canny Edge filter algorithm to detect the edges from an image and convert it back as a ROS image to be visualized in RViz. In a terminal, execute: cd ~/catkin_ws/src/stretch_tutorials/src python3 edge_detection.py The node will publish a new Image topic named /image_edge_detection . This can be visualized in RViz and a gif is provided below for reference. The Code #!/usr/bin/env python3 import rospy import sys import os import cv2 from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError class EdgeDetection : \"\"\" A class that converts a subscribed ROS image to a OpenCV image and saves the captured image to a predefined directory. \"\"\" def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and other parameter values. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . pub = rospy . Publisher ( '/image_edge_detection' , Image , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . lower_thres = 100 self . upper_thres = 200 rospy . loginfo ( \"Publishing the CV2 Image. Use RViz to visualize.\" ) def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a CV2 image and goes through the Canny Edge filter in OpenCV for edge detection. Then publishes that filtered image to be visualized in RViz. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) image = cv2 . Canny ( image , self . lower_thres , self . upper_thres ) image_msg = self . bridge . cv2_to_imgmsg ( image , 'passthrough' ) image_msg . header = msg . header self . pub . publish ( image_msg ) if __name__ == '__main__' : rospy . init_node ( 'edge_detection' , argv = sys . argv ) EdgeDetection () rospy . spin () The Code Explained Since there are similarities in the capture image node, we will only break down the different components of the edge detection node. Define the lower and upper bounds of the Hysteresis Thresholds. image = cv2 . Canny ( image , self . lower_thres , self . upper_thres ) Run the Canny Edge function to detect edges from the cv2 image. image_msg = self . bridge . cv2_to_imgmsg ( image , 'passthrough' ) Convert the cv2 image back to a ROS image so it can be published. image_msg . header = msg . header self . pub . publish ( image_msg ) Publish the ROS image with the same header as the subscribed ROS message.","title":"Capture Image"},{"location":"stretch-tutorials/ros1/example_7/#example-7","text":"In this example, we will review the image_view ROS package and a Python script that captures an image from the RealSense camera . Begin by running the stretch driver.launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_low_resolution.launch Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz","title":"Example 7"},{"location":"stretch-tutorials/ros1/example_7/#capture-image-with-image_view","text":"There are a couple of methods to save an image using the image_view package. OPTION 1: Use the image_view node to open a simple image viewer for ROS sensor_msgs/image topics. In a new terminal, execute: rosrun image_view image_view image: = /camera/color/image_raw_upright_view Then you can save the current image by right-clicking on the display window. By default, images will be saved as frame000.jpg, frame000.jpg, etc. Note, that the image will be saved to the terminal's current work directory. OPTION 2: Use the image_saver node to save an image to the terminal's current work directory. In a new terminal, execute: rosrun image_view image_saver image: = /camera/color/image_raw_upright_view","title":"Capture Image with image_view"},{"location":"stretch-tutorials/ros1/example_7/#capture-image-with-python-script","text":"In this section, we will use a Python node to capture an image from the RealSense camera . Execute the capture_image.py node to save a .jpeg image of the image topic /camera/color/image_raw_upright_view . In a terminal, execute: cd ~/catkin_ws/src/stretch_tutorials/src python3 capture_image.py An image named camera_image.jpeg is saved in the stored_data folder in this package.","title":"Capture Image with Python Script"},{"location":"stretch-tutorials/ros1/example_7/#the-code","text":"#!/usr/bin/env python3 import rospy import sys import os import cv2 from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError class CaptureImage : \"\"\" A class that converts a subscribed ROS image to a OpenCV image and saves the captured image to a predefined directory. \"\"\" def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and save path. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a CV2 image and stores the image. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) file_name = 'camera_image.jpeg' completeName = os . path . join ( self . save_path , file_name ) cv2 . imwrite ( completeName , image ) rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) if __name__ == '__main__' : rospy . init_node ( 'capture_image' , argv = sys . argv ) CaptureImage () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_7/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import sys import os import cv2 You need to import rospy if you are writing a ROS Node . There are functions from sys , os , and cv2 that are required within this code. cv2 is a library of Python functions that implements computer vision algorithms. Further information about cv2 can be found here: OpenCV Python . from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError The sensor_msgs.msg is imported so that we can subscribe to ROS Image messages. Import CvBridge to convert between ROS Image messages and OpenCV images. def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and save path. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' Initialize the CvBridge class, the subscriber, and the directory where the captured image will be stored. def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a cv2 image and stores the image. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) Try to convert the ROS Image message to a cv2 Image message using the imgmsg_to_cv2() function. file_name = 'camera_image.jpeg' completeName = os . path . join ( self . save_path , file_name ) cv2 . imwrite ( completeName , image ) Join the directory and file name using the path.join() function. Then use the imwrite() function to save the image. rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter. rospy . init_node ( 'capture_image' , argv = sys . argv ) CaptureImage () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the CaptureImage() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_7/#edge-detection","text":"In this section, we highlight a node that utilizes the Canny Edge filter algorithm to detect the edges from an image and convert it back as a ROS image to be visualized in RViz. In a terminal, execute: cd ~/catkin_ws/src/stretch_tutorials/src python3 edge_detection.py The node will publish a new Image topic named /image_edge_detection . This can be visualized in RViz and a gif is provided below for reference.","title":"Edge Detection"},{"location":"stretch-tutorials/ros1/example_7/#the-code_1","text":"#!/usr/bin/env python3 import rospy import sys import os import cv2 from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError class EdgeDetection : \"\"\" A class that converts a subscribed ROS image to a OpenCV image and saves the captured image to a predefined directory. \"\"\" def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and other parameter values. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . pub = rospy . Publisher ( '/image_edge_detection' , Image , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . lower_thres = 100 self . upper_thres = 200 rospy . loginfo ( \"Publishing the CV2 Image. Use RViz to visualize.\" ) def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a CV2 image and goes through the Canny Edge filter in OpenCV for edge detection. Then publishes that filtered image to be visualized in RViz. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) image = cv2 . Canny ( image , self . lower_thres , self . upper_thres ) image_msg = self . bridge . cv2_to_imgmsg ( image , 'passthrough' ) image_msg . header = msg . header self . pub . publish ( image_msg ) if __name__ == '__main__' : rospy . init_node ( 'edge_detection' , argv = sys . argv ) EdgeDetection () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_7/#the-code-explained_1","text":"Since there are similarities in the capture image node, we will only break down the different components of the edge detection node. Define the lower and upper bounds of the Hysteresis Thresholds. image = cv2 . Canny ( image , self . lower_thres , self . upper_thres ) Run the Canny Edge function to detect edges from the cv2 image. image_msg = self . bridge . cv2_to_imgmsg ( image , 'passthrough' ) Convert the cv2 image back to a ROS image so it can be published. image_msg . header = msg . header self . pub . publish ( image_msg ) Publish the ROS image with the same header as the subscribed ROS message.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_8/","text":"Example 8 This example will showcase how to save the interpreted speech from Stretch's ReSpeaker Mic Array v2.0 to a text file. Begin by running the respeaker.launch file in a terminal. roslaunch respeaker_ros sample_respeaker.launch Then run the speech_text.py node. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 speech_text.py The ReSpeaker will be listening and will start to interpret speech and save the transcript to a text file. To shut down the node, type Ctrl + c in the terminal. The Code #!/usr/bin/env python3 import rospy import os from speech_recognition_msgs.msg import SpeechRecognitionCandidates class SpeechText : \"\"\" A class that saves the interpreted speech from the ReSpeaker Microphone Array to a text file. \"\"\" def __init__ ( self ): \"\"\" Initialize subscriber and directory to save speech to text file. \"\"\" self . sub = rospy . Subscriber ( \"speech_to_text\" , SpeechRecognitionCandidates , self . callback ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' rospy . loginfo ( \"Listening to speech.\" ) def callback ( self , msg ): \"\"\" A callback function that receives the speech transcript and appends the transcript to a text file. :param self: The self reference. :param msg: The SpeechRecognitionCandidates message type. \"\"\" transcript = ' ' . join ( map ( str , msg . transcript )) file_name = 'speech.txt' completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"a+\" ) as file_object : file_object . write ( \" \\n \" ) file_object . write ( transcript ) if __name__ == '__main__' : rospy . init_node ( 'speech_text' ) SpeechText () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import os You need to import rospy if you are writing a ROS Node . from speech_recognition_msgs.msg import SpeechRecognitionCandidates Import SpeechRecognitionCandidates from the speech_recgonition_msgs.msg so that we can receive the interpreted speech. def __init__ ( self ): \"\"\" Initialize subscriber and directory to save speech to text file. \"\"\" self . sub = rospy . Subscriber ( \"speech_to_text\" , SpeechRecognitionCandidates , self . callback ) Set up a subscriber. We're going to subscribe to the topic speech_to_text , looking for SpeechRecognitionCandidates messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data Define the directory to save the text file. transcript = ' ' . join ( map ( str , msg . transcript )) Take all items in the iterable list and join them into a single string named transcript. file_name = 'speech.txt' completeName = os . path . join ( self . save_path , file_name ) Define the file name and create a complete path directory. with open ( completeName , \"a+\" ) as file_object : file_object . write ( \" \\n \" ) file_object . write ( transcript ) Append the transcript to the text file. rospy . init_node ( 'speech_text' ) SpeechText () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the SpeechText() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Voice to Text"},{"location":"stretch-tutorials/ros1/example_8/#example-8","text":"This example will showcase how to save the interpreted speech from Stretch's ReSpeaker Mic Array v2.0 to a text file. Begin by running the respeaker.launch file in a terminal. roslaunch respeaker_ros sample_respeaker.launch Then run the speech_text.py node. In a new terminal, execute: cd catkin_ws/src/stretch_tutorials/src/ python3 speech_text.py The ReSpeaker will be listening and will start to interpret speech and save the transcript to a text file. To shut down the node, type Ctrl + c in the terminal.","title":"Example 8"},{"location":"stretch-tutorials/ros1/example_8/#the-code","text":"#!/usr/bin/env python3 import rospy import os from speech_recognition_msgs.msg import SpeechRecognitionCandidates class SpeechText : \"\"\" A class that saves the interpreted speech from the ReSpeaker Microphone Array to a text file. \"\"\" def __init__ ( self ): \"\"\" Initialize subscriber and directory to save speech to text file. \"\"\" self . sub = rospy . Subscriber ( \"speech_to_text\" , SpeechRecognitionCandidates , self . callback ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' rospy . loginfo ( \"Listening to speech.\" ) def callback ( self , msg ): \"\"\" A callback function that receives the speech transcript and appends the transcript to a text file. :param self: The self reference. :param msg: The SpeechRecognitionCandidates message type. \"\"\" transcript = ' ' . join ( map ( str , msg . transcript )) file_name = 'speech.txt' completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"a+\" ) as file_object : file_object . write ( \" \\n \" ) file_object . write ( transcript ) if __name__ == '__main__' : rospy . init_node ( 'speech_text' ) SpeechText () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1/example_8/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy import os You need to import rospy if you are writing a ROS Node . from speech_recognition_msgs.msg import SpeechRecognitionCandidates Import SpeechRecognitionCandidates from the speech_recgonition_msgs.msg so that we can receive the interpreted speech. def __init__ ( self ): \"\"\" Initialize subscriber and directory to save speech to text file. \"\"\" self . sub = rospy . Subscriber ( \"speech_to_text\" , SpeechRecognitionCandidates , self . callback ) Set up a subscriber. We're going to subscribe to the topic speech_to_text , looking for SpeechRecognitionCandidates messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data Define the directory to save the text file. transcript = ' ' . join ( map ( str , msg . transcript )) Take all items in the iterable list and join them into a single string named transcript. file_name = 'speech.txt' completeName = os . path . join ( self . save_path , file_name ) Define the file name and create a complete path directory. with open ( completeName , \"a+\" ) as file_object : file_object . write ( \" \\n \" ) file_object . write ( transcript ) Append the transcript to the text file. rospy . init_node ( 'speech_text' ) SpeechText () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. Note The name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the SpeechText() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/example_9/","text":"Example 9 This example aims to combine the ReSpeaker Microphone Array and Follow Joint Trajectory tutorials to voice teleoperate the mobile base of the Stretch robot. Begin by running the following command in a new terminal. roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the respeaker.launch file. In a new terminal, execute: rosservice call /switch_to_position_mode roslaunch stretch_core respeaker.launch Then run the voice_teleoperation_base.py node in a new terminal. cd catkin_ws/src/stretch_tutorials/src/ python3 voice_teleoperation_base.py In terminal 3, a menu of voice commands is printed. You can reference this menu layout below. ------------ VOICE TELEOP MENU ------------ VOICE COMMANDS \"forward\" : BASE FORWARD \"back\" : BASE BACK \"left\" : BASE ROTATE LEFT \"right\" : BASE ROTATE RIGHT \"stretch\" : BASE ROTATES TOWARDS SOUND STEP SIZE \"big\" : BIG \"medium\" : MEDIUM \"small\" : SMALL \"quit\" : QUIT AND CLOSE NODE ------------------------------------------- To stop the node from sending twist messages, type Ctrl + c or say \" quit \". The Code #!/usr/bin/env python3 import math import rospy import sys from sensor_msgs.msg import JointState from std_msgs.msg import Int32 from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm from speech_recognition_msgs.msg import SpeechRecognitionCandidates class GetVoiceCommands : \"\"\" A class that subscribes to the speech to text recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion. \"\"\" def __init__ ( self ): \"\"\" A function that initializes subscribers and defines the three different step sizes. :param self: The self reference. \"\"\" self . step_size = 'medium' self . rad_per_deg = math . pi / 180.0 self . small_deg = 5.0 self . small_rad = self . rad_per_deg * self . small_deg self . small_translate = 0.025 self . medium_deg = 10.0 self . medium_rad = self . rad_per_deg * self . medium_deg self . medium_translate = 0.05 self . big_deg = 20.0 self . big_rad = self . rad_per_deg * self . big_deg self . big_translate = 0.1 self . voice_command = None self . sound_direction = 0 self . speech_to_text_sub = rospy . Subscriber ( \"/speech_to_text\" , SpeechRecognitionCandidates , self . callback_speech ) self . sound_direction_sub = rospy . Subscriber ( \"/sound_direction\" , Int32 , self . callback_direction ) def callback_direction ( self , msg ): \"\"\" A callback function that converts the incoming message, sound direction, from degrees to radians. :param self: The self reference. :param msg: The Int32 message type that represents the sound direction. \"\"\" self . sound_direction = msg . data * - self . rad_per_deg def callback_speech ( self , msg ): \"\"\" A callback function takes the incoming message, a list of the speech to text, and joins all items in that iterable list into a single string. :param self: The self reference. :param msg: The SpeechRecognitionCandidates message type. \"\"\" self . voice_command = ' ' . join ( map ( str , msg . transcript )) def get_inc ( self ): \"\"\" A function that sets the increment size for translational and rotational base motion. :param self:The self reference. :returns inc: A dictionary type the contains the increment size. \"\"\" if self . step_size == 'small' : inc = { 'rad' : self . small_rad , 'translate' : self . small_translate } if self . step_size == 'medium' : inc = { 'rad' : self . medium_rad , 'translate' : self . medium_translate } if self . step_size == 'big' : inc = { 'rad' : self . big_rad , 'translate' : self . big_translate } return inc def print_commands ( self ): \"\"\" A function that prints the voice teleoperation menu. :param self: The self reference. \"\"\" print ( ' ' ) print ( '------------ VOICE TELEOP MENU ------------' ) print ( ' ' ) print ( ' VOICE COMMANDS ' ) print ( ' \"forward\": BASE FORWARD ' ) print ( ' \"back\" : BASE BACK ' ) print ( ' \"left\" : BASE ROTATE LEFT ' ) print ( ' \"right\" : BASE ROTATE RIGHT ' ) print ( ' \"stretch\": BASE ROTATES TOWARDS SOUND ' ) print ( ' ' ) print ( ' STEP SIZE ' ) print ( ' \"big\" : BIG ' ) print ( ' \"medium\" : MEDIUM ' ) print ( ' \"small\" : SMALL ' ) print ( ' ' ) print ( ' ' ) print ( ' \"quit\" : QUIT AND CLOSE NODE ' ) print ( ' ' ) print ( '-------------------------------------------' ) def get_command ( self ): \"\"\" A function that defines the teleoperation command based on the voice command. :param self: The self reference. :returns command: A dictionary type that contains the type of base motion. \"\"\" command = None if self . voice_command == 'forward' : command = { 'joint' : 'translate_mobile_base' , 'inc' : self . get_inc ()[ 'translate' ]} if self . voice_command == 'back' : command = { 'joint' : 'translate_mobile_base' , 'inc' : - self . get_inc ()[ 'translate' ]} if self . voice_command == 'left' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . get_inc ()[ 'rad' ]} if self . voice_command == 'right' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : - self . get_inc ()[ 'rad' ]} if self . voice_command == 'stretch' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . sound_direction } if ( self . voice_command == \"small\" ) or ( self . voice_command == \"medium\" ) or ( self . voice_command == \"big\" ): self . step_size = self . voice_command rospy . loginfo ( 'Step size = {0} ' . format ( self . step_size )) if self . voice_command == 'quit' : rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) self . voice_command = None return command class VoiceTeleopNode ( hm . HelloNode ): \"\"\" A class that inherits the HelloNode class from hm and sends joint trajectory commands. \"\"\" def __init__ ( self ): \"\"\" A function that declares object from the GetVoiceCommands class, instantiates the HelloNode class, and set the publishing rate. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . rate = 10.0 self . joint_state = None self . speech = GetVoiceCommands () def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg def send_command ( self , command ): \"\"\" Function that makes an action call and sends base joint trajectory goals :param self: The self reference. :param command: A dictionary type. \"\"\" joint_state = self . joint_state if ( joint_state is not None ) and ( command is not None ): point = JointTrajectoryPoint () point . time_from_start = rospy . Duration ( 0.0 ) trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . goal_time_tolerance = rospy . Time ( 1.0 ) joint_name = command [ 'joint' ] trajectory_goal . trajectory . joint_names = [ joint_name ] inc = command [ 'inc' ] rospy . loginfo ( 'inc = {0} ' . format ( inc )) new_value = inc point . positions = [ new_value ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time . now () rospy . loginfo ( 'joint_name = {0} , trajectory_goal = {1} ' . format ( joint_name , trajectory_goal )) self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Done sending command.' ) self . speech . print_commands () def main ( self ): \"\"\" The main function that instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'voice_teleop' , 'voice_teleop' , wait_for_first_pointcloud = False ) rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) rate = rospy . Rate ( self . rate ) self . speech . print_commands () while not rospy . is_shutdown (): command = self . speech . get_command () self . send_command ( command ) rate . sleep () if __name__ == '__main__' : try : node = VoiceTeleopNode () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import math import rospy import sys from sensor_msgs.msg import JointState from std_msgs.msg import Int32 from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm from speech_recognition_msgs.msg import SpeechRecognitionCandidates You need to import rospy if you are writing a ROS Node. Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the hello_misc script. Import sensor_msgs.msg so that we can subscribe to JointState messages. class GetVoiceCommands : Create a class that subscribes to the speech-to-text recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion. self . step_size = 'medium' self . rad_per_deg = math . pi / 180.0 Set the default step size as medium and create a float value, self.rad_per_deg , to convert degrees to radians. self . small_deg = 5.0 self . small_rad = self . rad_per_deg * self . small_deg self . small_translate = 0.025 self . medium_deg = 10.0 self . medium_rad = self . rad_per_deg * self . medium_deg self . medium_translate = 0.05 self . big_deg = 20.0 self . big_rad = self . rad_per_deg * self . big_deg self . big_translate = 0.1 Define the three rotation and translation step sizes. self . voice_command = None self . sound_direction = 0 self . speech_to_text_sub = rospy . Subscriber ( \"/speech_to_text\" , SpeechRecognitionCandidates , self . callback_speech ) self . sound_direction_sub = rospy . Subscriber ( \"/sound_direction\" , Int32 , self . callback_direction ) Initialize the voice command and sound direction to values that will not result in moving the base. Set up two subscribers. The first one subscribes to the topic /speech_to_text , looking for SpeechRecognitionCandidates messages. When a message comes in, ROS is going to pass it to the function callback_speech automatically. The second subscribes to /sound_direction message and passes it to the callback_direction function. self . sound_direction = msg . data * - self . rad_per_deg The callback_direction function converts the sound_direction topic from degrees to radians. if self . step_size == 'small' : inc = { 'rad' : self . small_rad , 'translate' : self . small_translate } if self . step_size == 'medium' : inc = { 'rad' : self . medium_rad , 'translate' : self . medium_translate } if self . step_size == 'big' : inc = { 'rad' : self . big_rad , 'translate' : self . big_translate } return inc The callback_speech stores the increment size for translational and rotational base motion in inc . The increment size is contingent on the self.step_size string value. command = None if self . voice_command == 'forward' : command = { 'joint' : 'translate_mobile_base' , 'inc' : self . get_inc ()[ 'translate' ]} if self . voice_command == 'back' : command = { 'joint' : 'translate_mobile_base' , 'inc' : - self . get_inc ()[ 'translate' ]} if self . voice_command == 'left' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . get_inc ()[ 'rad' ]} if self . voice_command == 'right' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : - self . get_inc ()[ 'rad' ]} if self . voice_command == 'stretch' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . sound_direction } In the get_command() function, the command is initialized as None , or set as a dictionary where the joint and inc values are stored. The command message type is dependent on the self.voice_command string value. if ( self . voice_command == \"small\" ) or ( self . voice_command == \"medium\" ) or ( self . voice_command == \"big\" ): self . step_size = self . voice_command rospy . loginfo ( 'Step size = {0} ' . format ( self . step_size )) Based on the self.voice_command value set the step size for the increments. if self . voice_command == 'quit' : rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) If the self.voice_command is equal to quit , then initiate a clean shutdown of ROS and exit the Python interpreter. class VoiceTeleopNode ( hm . HelloNode ): \"\"\" A class that inherits the HelloNode class from hm and sends joint trajectory commands. \"\"\" def __init__ ( self ): \"\"\" A function that declares object from the GetVoiceCommands class, instantiates the HelloNode class, and set the publishing rate. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . rate = 10.0 self . joint_state = None self . speech = GetVoiceCommands () A class that inherits the HelloNode class from hm declares object from the GetVoiceCommands class and sends joint trajectory commands. def send_command ( self , command ): \"\"\" Function that makes an action call and sends base joint trajectory goals :param self: The self reference. :param command: A dictionary type. \"\"\" joint_state = self . joint_state if ( joint_state is not None ) and ( command is not None ): point = JointTrajectoryPoint () point . time_from_start = rospy . Duration ( 0.0 ) The send_command function stores the joint state message and uses a conditional statement to send joint trajectory goals. Also, assign point as a JointTrajectoryPoint message type. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . goal_time_tolerance = rospy . Time ( 1.0 ) Assign trajectory_goal as a FollowJointTrajectoryGoal message type. joint_name = command [ 'joint' ] trajectory_goal . trajectory . joint_names = [ joint_name ] Extract the joint name from the command dictionary. inc = command [ 'inc' ] rospy . loginfo ( 'inc = {0} ' . format ( inc )) new_value = inc Extract the increment type from the command dictionary. point . positions = [ new_value ] trajectory_goal . trajectory . points = [ point ] Assign the new value position to the trajectory goal message type. self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Done sending command.' ) Make the action call and send the goal of the new joint position. self . speech . print_commands () Reprint the voice command menu after the trajectory goal is sent. def main ( self ): \"\"\" The main function that instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'voice_teleop' , 'voice_teleop' , wait_for_first_pointcloud = False ) rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) rate = rospy . Rate ( self . rate ) self . speech . print_commands () The main function instantiates the HelloNode class, initializes the subscriber, and calls other methods in both the VoiceTeleopNode and GetVoiceCommands classes. while not rospy . is_shutdown (): command = self . speech . get_command () self . send_command ( command ) rate . sleep () Run a while loop to continuously check speech commands and send those commands to execute an action. try : node = VoiceTeleopNode () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare a VoiceTeleopNode object. Then execute the main() method.","title":"Voice Teleoperation of Base"},{"location":"stretch-tutorials/ros1/example_9/#example-9","text":"This example aims to combine the ReSpeaker Microphone Array and Follow Joint Trajectory tutorials to voice teleoperate the mobile base of the Stretch robot. Begin by running the following command in a new terminal. roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the respeaker.launch file. In a new terminal, execute: rosservice call /switch_to_position_mode roslaunch stretch_core respeaker.launch Then run the voice_teleoperation_base.py node in a new terminal. cd catkin_ws/src/stretch_tutorials/src/ python3 voice_teleoperation_base.py In terminal 3, a menu of voice commands is printed. You can reference this menu layout below. ------------ VOICE TELEOP MENU ------------ VOICE COMMANDS \"forward\" : BASE FORWARD \"back\" : BASE BACK \"left\" : BASE ROTATE LEFT \"right\" : BASE ROTATE RIGHT \"stretch\" : BASE ROTATES TOWARDS SOUND STEP SIZE \"big\" : BIG \"medium\" : MEDIUM \"small\" : SMALL \"quit\" : QUIT AND CLOSE NODE ------------------------------------------- To stop the node from sending twist messages, type Ctrl + c or say \" quit \".","title":"Example 9"},{"location":"stretch-tutorials/ros1/example_9/#the-code","text":"#!/usr/bin/env python3 import math import rospy import sys from sensor_msgs.msg import JointState from std_msgs.msg import Int32 from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm from speech_recognition_msgs.msg import SpeechRecognitionCandidates class GetVoiceCommands : \"\"\" A class that subscribes to the speech to text recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion. \"\"\" def __init__ ( self ): \"\"\" A function that initializes subscribers and defines the three different step sizes. :param self: The self reference. \"\"\" self . step_size = 'medium' self . rad_per_deg = math . pi / 180.0 self . small_deg = 5.0 self . small_rad = self . rad_per_deg * self . small_deg self . small_translate = 0.025 self . medium_deg = 10.0 self . medium_rad = self . rad_per_deg * self . medium_deg self . medium_translate = 0.05 self . big_deg = 20.0 self . big_rad = self . rad_per_deg * self . big_deg self . big_translate = 0.1 self . voice_command = None self . sound_direction = 0 self . speech_to_text_sub = rospy . Subscriber ( \"/speech_to_text\" , SpeechRecognitionCandidates , self . callback_speech ) self . sound_direction_sub = rospy . Subscriber ( \"/sound_direction\" , Int32 , self . callback_direction ) def callback_direction ( self , msg ): \"\"\" A callback function that converts the incoming message, sound direction, from degrees to radians. :param self: The self reference. :param msg: The Int32 message type that represents the sound direction. \"\"\" self . sound_direction = msg . data * - self . rad_per_deg def callback_speech ( self , msg ): \"\"\" A callback function takes the incoming message, a list of the speech to text, and joins all items in that iterable list into a single string. :param self: The self reference. :param msg: The SpeechRecognitionCandidates message type. \"\"\" self . voice_command = ' ' . join ( map ( str , msg . transcript )) def get_inc ( self ): \"\"\" A function that sets the increment size for translational and rotational base motion. :param self:The self reference. :returns inc: A dictionary type the contains the increment size. \"\"\" if self . step_size == 'small' : inc = { 'rad' : self . small_rad , 'translate' : self . small_translate } if self . step_size == 'medium' : inc = { 'rad' : self . medium_rad , 'translate' : self . medium_translate } if self . step_size == 'big' : inc = { 'rad' : self . big_rad , 'translate' : self . big_translate } return inc def print_commands ( self ): \"\"\" A function that prints the voice teleoperation menu. :param self: The self reference. \"\"\" print ( ' ' ) print ( '------------ VOICE TELEOP MENU ------------' ) print ( ' ' ) print ( ' VOICE COMMANDS ' ) print ( ' \"forward\": BASE FORWARD ' ) print ( ' \"back\" : BASE BACK ' ) print ( ' \"left\" : BASE ROTATE LEFT ' ) print ( ' \"right\" : BASE ROTATE RIGHT ' ) print ( ' \"stretch\": BASE ROTATES TOWARDS SOUND ' ) print ( ' ' ) print ( ' STEP SIZE ' ) print ( ' \"big\" : BIG ' ) print ( ' \"medium\" : MEDIUM ' ) print ( ' \"small\" : SMALL ' ) print ( ' ' ) print ( ' ' ) print ( ' \"quit\" : QUIT AND CLOSE NODE ' ) print ( ' ' ) print ( '-------------------------------------------' ) def get_command ( self ): \"\"\" A function that defines the teleoperation command based on the voice command. :param self: The self reference. :returns command: A dictionary type that contains the type of base motion. \"\"\" command = None if self . voice_command == 'forward' : command = { 'joint' : 'translate_mobile_base' , 'inc' : self . get_inc ()[ 'translate' ]} if self . voice_command == 'back' : command = { 'joint' : 'translate_mobile_base' , 'inc' : - self . get_inc ()[ 'translate' ]} if self . voice_command == 'left' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . get_inc ()[ 'rad' ]} if self . voice_command == 'right' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : - self . get_inc ()[ 'rad' ]} if self . voice_command == 'stretch' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . sound_direction } if ( self . voice_command == \"small\" ) or ( self . voice_command == \"medium\" ) or ( self . voice_command == \"big\" ): self . step_size = self . voice_command rospy . loginfo ( 'Step size = {0} ' . format ( self . step_size )) if self . voice_command == 'quit' : rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) self . voice_command = None return command class VoiceTeleopNode ( hm . HelloNode ): \"\"\" A class that inherits the HelloNode class from hm and sends joint trajectory commands. \"\"\" def __init__ ( self ): \"\"\" A function that declares object from the GetVoiceCommands class, instantiates the HelloNode class, and set the publishing rate. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . rate = 10.0 self . joint_state = None self . speech = GetVoiceCommands () def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg def send_command ( self , command ): \"\"\" Function that makes an action call and sends base joint trajectory goals :param self: The self reference. :param command: A dictionary type. \"\"\" joint_state = self . joint_state if ( joint_state is not None ) and ( command is not None ): point = JointTrajectoryPoint () point . time_from_start = rospy . Duration ( 0.0 ) trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . goal_time_tolerance = rospy . Time ( 1.0 ) joint_name = command [ 'joint' ] trajectory_goal . trajectory . joint_names = [ joint_name ] inc = command [ 'inc' ] rospy . loginfo ( 'inc = {0} ' . format ( inc )) new_value = inc point . positions = [ new_value ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time . now () rospy . loginfo ( 'joint_name = {0} , trajectory_goal = {1} ' . format ( joint_name , trajectory_goal )) self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Done sending command.' ) self . speech . print_commands () def main ( self ): \"\"\" The main function that instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'voice_teleop' , 'voice_teleop' , wait_for_first_pointcloud = False ) rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) rate = rospy . Rate ( self . rate ) self . speech . print_commands () while not rospy . is_shutdown (): command = self . speech . get_command () self . send_command ( command ) rate . sleep () if __name__ == '__main__' : try : node = VoiceTeleopNode () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1/example_9/#the-code-explained","text":"This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import math import rospy import sys from sensor_msgs.msg import JointState from std_msgs.msg import Int32 from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm from speech_recognition_msgs.msg import SpeechRecognitionCandidates You need to import rospy if you are writing a ROS Node. Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the hello_misc script. Import sensor_msgs.msg so that we can subscribe to JointState messages. class GetVoiceCommands : Create a class that subscribes to the speech-to-text recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion. self . step_size = 'medium' self . rad_per_deg = math . pi / 180.0 Set the default step size as medium and create a float value, self.rad_per_deg , to convert degrees to radians. self . small_deg = 5.0 self . small_rad = self . rad_per_deg * self . small_deg self . small_translate = 0.025 self . medium_deg = 10.0 self . medium_rad = self . rad_per_deg * self . medium_deg self . medium_translate = 0.05 self . big_deg = 20.0 self . big_rad = self . rad_per_deg * self . big_deg self . big_translate = 0.1 Define the three rotation and translation step sizes. self . voice_command = None self . sound_direction = 0 self . speech_to_text_sub = rospy . Subscriber ( \"/speech_to_text\" , SpeechRecognitionCandidates , self . callback_speech ) self . sound_direction_sub = rospy . Subscriber ( \"/sound_direction\" , Int32 , self . callback_direction ) Initialize the voice command and sound direction to values that will not result in moving the base. Set up two subscribers. The first one subscribes to the topic /speech_to_text , looking for SpeechRecognitionCandidates messages. When a message comes in, ROS is going to pass it to the function callback_speech automatically. The second subscribes to /sound_direction message and passes it to the callback_direction function. self . sound_direction = msg . data * - self . rad_per_deg The callback_direction function converts the sound_direction topic from degrees to radians. if self . step_size == 'small' : inc = { 'rad' : self . small_rad , 'translate' : self . small_translate } if self . step_size == 'medium' : inc = { 'rad' : self . medium_rad , 'translate' : self . medium_translate } if self . step_size == 'big' : inc = { 'rad' : self . big_rad , 'translate' : self . big_translate } return inc The callback_speech stores the increment size for translational and rotational base motion in inc . The increment size is contingent on the self.step_size string value. command = None if self . voice_command == 'forward' : command = { 'joint' : 'translate_mobile_base' , 'inc' : self . get_inc ()[ 'translate' ]} if self . voice_command == 'back' : command = { 'joint' : 'translate_mobile_base' , 'inc' : - self . get_inc ()[ 'translate' ]} if self . voice_command == 'left' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . get_inc ()[ 'rad' ]} if self . voice_command == 'right' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : - self . get_inc ()[ 'rad' ]} if self . voice_command == 'stretch' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . sound_direction } In the get_command() function, the command is initialized as None , or set as a dictionary where the joint and inc values are stored. The command message type is dependent on the self.voice_command string value. if ( self . voice_command == \"small\" ) or ( self . voice_command == \"medium\" ) or ( self . voice_command == \"big\" ): self . step_size = self . voice_command rospy . loginfo ( 'Step size = {0} ' . format ( self . step_size )) Based on the self.voice_command value set the step size for the increments. if self . voice_command == 'quit' : rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) If the self.voice_command is equal to quit , then initiate a clean shutdown of ROS and exit the Python interpreter. class VoiceTeleopNode ( hm . HelloNode ): \"\"\" A class that inherits the HelloNode class from hm and sends joint trajectory commands. \"\"\" def __init__ ( self ): \"\"\" A function that declares object from the GetVoiceCommands class, instantiates the HelloNode class, and set the publishing rate. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . rate = 10.0 self . joint_state = None self . speech = GetVoiceCommands () A class that inherits the HelloNode class from hm declares object from the GetVoiceCommands class and sends joint trajectory commands. def send_command ( self , command ): \"\"\" Function that makes an action call and sends base joint trajectory goals :param self: The self reference. :param command: A dictionary type. \"\"\" joint_state = self . joint_state if ( joint_state is not None ) and ( command is not None ): point = JointTrajectoryPoint () point . time_from_start = rospy . Duration ( 0.0 ) The send_command function stores the joint state message and uses a conditional statement to send joint trajectory goals. Also, assign point as a JointTrajectoryPoint message type. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . goal_time_tolerance = rospy . Time ( 1.0 ) Assign trajectory_goal as a FollowJointTrajectoryGoal message type. joint_name = command [ 'joint' ] trajectory_goal . trajectory . joint_names = [ joint_name ] Extract the joint name from the command dictionary. inc = command [ 'inc' ] rospy . loginfo ( 'inc = {0} ' . format ( inc )) new_value = inc Extract the increment type from the command dictionary. point . positions = [ new_value ] trajectory_goal . trajectory . points = [ point ] Assign the new value position to the trajectory goal message type. self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Done sending command.' ) Make the action call and send the goal of the new joint position. self . speech . print_commands () Reprint the voice command menu after the trajectory goal is sent. def main ( self ): \"\"\" The main function that instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'voice_teleop' , 'voice_teleop' , wait_for_first_pointcloud = False ) rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) rate = rospy . Rate ( self . rate ) self . speech . print_commands () The main function instantiates the HelloNode class, initializes the subscriber, and calls other methods in both the VoiceTeleopNode and GetVoiceCommands classes. while not rospy . is_shutdown (): command = self . speech . get_command () self . send_command ( command ) rate . sleep () Run a while loop to continuously check speech commands and send those commands to execute an action. try : node = VoiceTeleopNode () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare a VoiceTeleopNode object. Then execute the main() method.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/","text":"FollowJointTrajectory Commands Stretch ROS driver offers a FollowJointTrajectory action service for its arm. Within this tutorial, we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute. Stow Command Example Begin by running the following command in a terminal. roslaunch stretch_core stretch_driver.launch In a new terminal, switch the mode to position mode using a rosservice call. Then run the stow command node. rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python3 stow_command.py This will send a FollowJointTrajectory command to stow Stretch's arm. The Code #!/usr/bin/env python3 import rospy from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm import time class StowCommand ( hm . HelloNode ): ''' A class that sends a joint trajectory goal to stow the Stretch's arm. ''' def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_stow_command ( self ): ''' Function that makes an action call and sends stow position goal. :param self: The self reference. ''' stow_point = JointTrajectoryPoint () stow_point . time_from_start = rospy . Duration ( 0.000 ) stow_point . positions = [ 0.2 , 0.0 , 3.4 ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): ''' Function that initiates stow_command function. :param self: The self reference. ''' hm . HelloNode . main ( self , 'stow_command' , 'stow_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'stowing...' ) self . issue_stow_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = StowCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm import time You need to import rospy if you are writing a ROS Node . Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros . In this instance, we are importing the hello_misc script. class StowCommand ( hm . HelloNode ): def __init__ ( self ): hm . HelloNode . __init__ ( self ) The StowCommand class inherits the HelloNode class from hm and is initialized. def issue_stow_command ( self ): stow_point = JointTrajectoryPoint () stow_point . time_from_start = rospy . Duration ( 0.000 ) stow_point . positions = [ 0.2 , 0.0 , 3.4 ] The issue_stow_command() is the name of the function that will stow Stretch's arm. Within the function, we set stow_point as a JointTrajectoryPoint and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined next. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by the positions set in stow_point . Specify the coordinate frame that we want ( base_link ) and set the time to be now. self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () Make the action call and send the goal. The last line of code waits for the result before it exits the python script. def main ( self ): hm . HelloNode . main ( self , 'stow_command' , 'stow_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'stowing...' ) self . issue_stow_command () time . sleep ( 2 ) Create a function, main() , to set up the hm.HelloNode class and issue the stow command. if __name__ == '__main__' : try : node = StowCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare object, node , from the StowCommand() class. Then run the main() function. Multipoint Command Example Begin by running the following command in a terminal: roslaunch stretch_core stretch_driver.launch In a new terminal, switch the mode to position mode using a rosservice call. Then run the multipoint command node. rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python3 multipoint_command.py This will send a list of JointTrajectoryPoint message types to move Stretch's arm. The Code #!/usr/bin/env python3 import rospy import time from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm class MultiPointCommand ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to the stretch robot. \"\"\" def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_multipoint_command ( self ): \"\"\" Function that makes an action call and sends multiple joint trajectory goals to the joint_lift, wrist_extension, and joint_wrist_yaw. :param self: The self reference. \"\"\" point0 = JointTrajectoryPoint () point0 . positions = [ 0.2 , 0.0 , 3.4 ] point0 . velocities = [ 0.2 , 0.2 , 2.5 ] point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] point1 = JointTrajectoryPoint () point1 . positions = [ 0.3 , 0.1 , 2.0 ] point2 = JointTrajectoryPoint () point2 . positions = [ 0.5 , 0.2 , - 1.0 ] point3 = JointTrajectoryPoint () point3 . positions = [ 0.6 , 0.3 , 0.0 ] point4 = JointTrajectoryPoint () point4 . positions = [ 0.8 , 0.2 , 1.0 ] point5 = JointTrajectoryPoint () point5 . positions = [ 0.5 , 0.1 , 0.0 ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent list of goals = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): \"\"\" Function that initiates the multipoint_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'multipoint_command' , 'multipoint_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing multipoint command...' ) self . issue_multipoint_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = MultiPointCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained Seeing that there are similarities between the multipoint and stow command nodes, we will only break down the different components of the multipoint_command node. point0 = JointTrajectoryPoint () point0 . positions = [ 0.2 , 0.0 , 3.4 ] Set point0 as a JointTrajectoryPoint and provide desired positions. These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. The lift and wrist extension positions are expressed in meters, whereas the wrist yaw is in radians. point0 . velocities = [ 0.2 , 0.2 , 2.5 ] Provide the desired velocity of the lift (m/s), wrist extension (m/s), and wrist yaw (rad/s) for point0 . point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] Provide desired accelerations of the lift (m/s^2), wrist extension (m/s^2), and wrist yaw (rad/s^2). Note The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by a list of the 6 points. Specify the coordinate frame that we want ( base_link ) and set the time to be now. Single Joint Actuator You can also actuate a single joint for the Stretch. Below is the list of joints and their position limit. ############################# JOINT LIMITS ############################# joint_lift: lower_limit = 0 .15, upper_limit = 1 .10 # in meters wrist_extension: lower_limit = 0 .00, upper_limit = 0 .50 # in meters joint_wrist_yaw: lower_limit = -1.75, upper_limit = 4 .00 # in radians joint_head_pan: lower_limit = -2.80, upper_limit = 2 .90 # in radians joint_head_tilt: lower_limit = -1.60, upper_limit = 0 .40 # in radians joint_gripper_finger_left: lower_limit = -0.35, upper_limit = 0 .165 # in radians # INCLUDED JOINTS IN POSITION MODE translate_mobile_base: No lower or upper limit. Defined by a step size in meters rotate_mobile_base: No lower or upper limit. Defined by a step size in radians ######################################################################## Begin by running the following command in a terminal. roslaunch stretch_core stretch_driver.launch In a new terminal, switch the mode to position mode using a rosservice call. Then run the single joint actuator node. rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python3 single_joint_actuator.py This will send a list of JointTrajectoryPoint message types to move Stretch's arm. The joint, joint_gripper_finger_left , is only needed when actuating the gripper. The Code #!/usr/bin/env python3 import rospy import time from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm class SingleJointActuator ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_command ( self ): \"\"\" Function that makes an action call and sends joint trajectory goals to a single joint :param self: The self reference. \"\"\" trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_head_pan' ] point0 = JointTrajectoryPoint () point0 . positions = [ 0.65 ] # point1 = JointTrajectoryPoint() # point1.positions = [0.5] trajectory_goal . trajectory . points = [ point0 ] #, point1] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'issue_command' , 'issue_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing command...' ) self . issue_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = SingleJointActuator () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained Since the code is quite similar to the multipoint_command code, we will only review the parts that differ. Now let's break the code down. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_head_pan' ] Here we only input the joint name that we want to actuate. In this instance, we will actuate the joint_head_pan . point0 = JointTrajectoryPoint () point0 . positions = [ 0.65 ] # point1 = JointTrajectoryPoint() # point1.positions = [0.5] Set point0 as a JointTrajectoryPoint and provide the desired position. You also have the option to send multiple point positions rather than one. trajectory_goal . trajectory . points = [ point0 ] #, point1] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points set by your list of points. Specify the coordinate frame that we want ( base_link ) and set the time to be now.","title":"Follow Joint Trajectory Commands"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#followjointtrajectory-commands","text":"Stretch ROS driver offers a FollowJointTrajectory action service for its arm. Within this tutorial, we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute.","title":"FollowJointTrajectory Commands"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#stow-command-example","text":"Begin by running the following command in a terminal. roslaunch stretch_core stretch_driver.launch In a new terminal, switch the mode to position mode using a rosservice call. Then run the stow command node. rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python3 stow_command.py This will send a FollowJointTrajectory command to stow Stretch's arm.","title":"Stow Command Example"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code","text":"#!/usr/bin/env python3 import rospy from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm import time class StowCommand ( hm . HelloNode ): ''' A class that sends a joint trajectory goal to stow the Stretch's arm. ''' def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_stow_command ( self ): ''' Function that makes an action call and sends stow position goal. :param self: The self reference. ''' stow_point = JointTrajectoryPoint () stow_point . time_from_start = rospy . Duration ( 0.000 ) stow_point . positions = [ 0.2 , 0.0 , 3.4 ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): ''' Function that initiates stow_command function. :param self: The self reference. ''' hm . HelloNode . main ( self , 'stow_command' , 'stow_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'stowing...' ) self . issue_stow_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = StowCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python3 script. import rospy from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm import time You need to import rospy if you are writing a ROS Node . Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros . In this instance, we are importing the hello_misc script. class StowCommand ( hm . HelloNode ): def __init__ ( self ): hm . HelloNode . __init__ ( self ) The StowCommand class inherits the HelloNode class from hm and is initialized. def issue_stow_command ( self ): stow_point = JointTrajectoryPoint () stow_point . time_from_start = rospy . Duration ( 0.000 ) stow_point . positions = [ 0.2 , 0.0 , 3.4 ] The issue_stow_command() is the name of the function that will stow Stretch's arm. Within the function, we set stow_point as a JointTrajectoryPoint and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined next. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by the positions set in stow_point . Specify the coordinate frame that we want ( base_link ) and set the time to be now. self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () Make the action call and send the goal. The last line of code waits for the result before it exits the python script. def main ( self ): hm . HelloNode . main ( self , 'stow_command' , 'stow_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'stowing...' ) self . issue_stow_command () time . sleep ( 2 ) Create a function, main() , to set up the hm.HelloNode class and issue the stow command. if __name__ == '__main__' : try : node = StowCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare object, node , from the StowCommand() class. Then run the main() function.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#multipoint-command-example","text":"Begin by running the following command in a terminal: roslaunch stretch_core stretch_driver.launch In a new terminal, switch the mode to position mode using a rosservice call. Then run the multipoint command node. rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python3 multipoint_command.py This will send a list of JointTrajectoryPoint message types to move Stretch's arm.","title":"Multipoint Command Example"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code_1","text":"#!/usr/bin/env python3 import rospy import time from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm class MultiPointCommand ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to the stretch robot. \"\"\" def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_multipoint_command ( self ): \"\"\" Function that makes an action call and sends multiple joint trajectory goals to the joint_lift, wrist_extension, and joint_wrist_yaw. :param self: The self reference. \"\"\" point0 = JointTrajectoryPoint () point0 . positions = [ 0.2 , 0.0 , 3.4 ] point0 . velocities = [ 0.2 , 0.2 , 2.5 ] point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] point1 = JointTrajectoryPoint () point1 . positions = [ 0.3 , 0.1 , 2.0 ] point2 = JointTrajectoryPoint () point2 . positions = [ 0.5 , 0.2 , - 1.0 ] point3 = JointTrajectoryPoint () point3 . positions = [ 0.6 , 0.3 , 0.0 ] point4 = JointTrajectoryPoint () point4 . positions = [ 0.8 , 0.2 , 1.0 ] point5 = JointTrajectoryPoint () point5 . positions = [ 0.5 , 0.1 , 0.0 ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent list of goals = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): \"\"\" Function that initiates the multipoint_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'multipoint_command' , 'multipoint_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing multipoint command...' ) self . issue_multipoint_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = MultiPointCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code-explained_1","text":"Seeing that there are similarities between the multipoint and stow command nodes, we will only break down the different components of the multipoint_command node. point0 = JointTrajectoryPoint () point0 . positions = [ 0.2 , 0.0 , 3.4 ] Set point0 as a JointTrajectoryPoint and provide desired positions. These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. The lift and wrist extension positions are expressed in meters, whereas the wrist yaw is in radians. point0 . velocities = [ 0.2 , 0.2 , 2.5 ] Provide the desired velocity of the lift (m/s), wrist extension (m/s), and wrist yaw (rad/s) for point0 . point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] Provide desired accelerations of the lift (m/s^2), wrist extension (m/s^2), and wrist yaw (rad/s^2). Note The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by a list of the 6 points. Specify the coordinate frame that we want ( base_link ) and set the time to be now.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#single-joint-actuator","text":"You can also actuate a single joint for the Stretch. Below is the list of joints and their position limit. ############################# JOINT LIMITS ############################# joint_lift: lower_limit = 0 .15, upper_limit = 1 .10 # in meters wrist_extension: lower_limit = 0 .00, upper_limit = 0 .50 # in meters joint_wrist_yaw: lower_limit = -1.75, upper_limit = 4 .00 # in radians joint_head_pan: lower_limit = -2.80, upper_limit = 2 .90 # in radians joint_head_tilt: lower_limit = -1.60, upper_limit = 0 .40 # in radians joint_gripper_finger_left: lower_limit = -0.35, upper_limit = 0 .165 # in radians # INCLUDED JOINTS IN POSITION MODE translate_mobile_base: No lower or upper limit. Defined by a step size in meters rotate_mobile_base: No lower or upper limit. Defined by a step size in radians ######################################################################## Begin by running the following command in a terminal. roslaunch stretch_core stretch_driver.launch In a new terminal, switch the mode to position mode using a rosservice call. Then run the single joint actuator node. rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python3 single_joint_actuator.py This will send a list of JointTrajectoryPoint message types to move Stretch's arm. The joint, joint_gripper_finger_left , is only needed when actuating the gripper.","title":"Single Joint Actuator"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code_2","text":"#!/usr/bin/env python3 import rospy import time from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm class SingleJointActuator ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_command ( self ): \"\"\" Function that makes an action call and sends joint trajectory goals to a single joint :param self: The self reference. \"\"\" trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_head_pan' ] point0 = JointTrajectoryPoint () point0 . positions = [ 0.65 ] # point1 = JointTrajectoryPoint() # point1.positions = [0.5] trajectory_goal . trajectory . points = [ point0 ] #, point1] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'issue_command' , 'issue_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing command...' ) self . issue_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = SingleJointActuator () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1/follow_joint_trajectory/#the-code-explained_2","text":"Since the code is quite similar to the multipoint_command code, we will only review the parts that differ. Now let's break the code down. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_head_pan' ] Here we only input the joint name that we want to actuate. In this instance, we will actuate the joint_head_pan . point0 = JointTrajectoryPoint () point0 . positions = [ 0.65 ] # point1 = JointTrajectoryPoint() # point1.positions = [0.5] Set point0 as a JointTrajectoryPoint and provide the desired position. You also have the option to send multiple point positions rather than one. trajectory_goal . trajectory . points = [ point0 ] #, point1] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points set by your list of points. Specify the coordinate frame that we want ( base_link ) and set the time to be now.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1/gazebo_basics/","text":"Spawning Stretch in Simulation (Gazebo) Empty World Simulation To spawn Stretch in Gazebo's default empty world run the following command in your terminal. roslaunch stretch_gazebo gazebo.launch This will bring up the robot in the gazebo simulation similar to the image shown below. Custom World Simulation In Gazebo, you can spawn Stretch in various worlds. First, source the Gazebo world files by running the following command in a terminal: echo \"source /usr/share/gazebo/setup.sh\" Then using the world argument, you can spawn Stretch in the Willow Garage world by running the following: roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world","title":"Gazebo Basics"},{"location":"stretch-tutorials/ros1/gazebo_basics/#spawning-stretch-in-simulation-gazebo","text":"","title":"Spawning Stretch in Simulation (Gazebo)"},{"location":"stretch-tutorials/ros1/gazebo_basics/#empty-world-simulation","text":"To spawn Stretch in Gazebo's default empty world run the following command in your terminal. roslaunch stretch_gazebo gazebo.launch This will bring up the robot in the gazebo simulation similar to the image shown below.","title":"Empty World Simulation"},{"location":"stretch-tutorials/ros1/gazebo_basics/#custom-world-simulation","text":"In Gazebo, you can spawn Stretch in various worlds. First, source the Gazebo world files by running the following command in a terminal: echo \"source /usr/share/gazebo/setup.sh\" Then using the world argument, you can spawn Stretch in the Willow Garage world by running the following: roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world","title":"Custom World Simulation"},{"location":"stretch-tutorials/ros1/getting_started/","text":"Getting Started Prerequisites A Stretch robot (see below for simulation instructions if you don\u2019t have a robot) Follow the Getting Started guide (hello_robot_xbox_teleop must not be running in the background) Interacting with Linux through the command line Basic understanding of ROS Setup untethered operation (optional) Connecting a Monitor If you cannot access the robot through ssh due to your network settings, you will need to connect an HDMI monitor, USB keyboard, and mouse to the USB ports in the robot's trunk. Setting Up Stretch in Simulation Users who don\u2019t have a Stretch, but want to try the tutorials can set up their computer with Stretch Gazebo. Requirements Although lower specifications might be sufficient, for the best experience we recommend the following for running the simulation: Processor : Intel i7 or comparable Memory : 16 GB Storage : 50 GB OS : Ubuntu 20.04 Graphics Card : NVIDIA GTX2060 (optional) Setup Hello Robot is currently running Stretch on Ubuntu 20.04 and ROS Noetic. To begin the setup, follow the Run the new robot installation script on your system. Finally, follow the Creating a new ROS workspace guide to create a fresh catkin workspace complete with all the dependencies. To begin working with a simulated Stretch, follow the Gazebo basics tutorial.","title":"Getting Started"},{"location":"stretch-tutorials/ros1/getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"stretch-tutorials/ros1/getting_started/#prerequisites","text":"A Stretch robot (see below for simulation instructions if you don\u2019t have a robot) Follow the Getting Started guide (hello_robot_xbox_teleop must not be running in the background) Interacting with Linux through the command line Basic understanding of ROS Setup untethered operation (optional)","title":"Prerequisites"},{"location":"stretch-tutorials/ros1/getting_started/#connecting-a-monitor","text":"If you cannot access the robot through ssh due to your network settings, you will need to connect an HDMI monitor, USB keyboard, and mouse to the USB ports in the robot's trunk.","title":"Connecting a Monitor"},{"location":"stretch-tutorials/ros1/getting_started/#setting-up-stretch-in-simulation","text":"Users who don\u2019t have a Stretch, but want to try the tutorials can set up their computer with Stretch Gazebo.","title":"Setting Up Stretch in Simulation"},{"location":"stretch-tutorials/ros1/getting_started/#requirements","text":"Although lower specifications might be sufficient, for the best experience we recommend the following for running the simulation: Processor : Intel i7 or comparable Memory : 16 GB Storage : 50 GB OS : Ubuntu 20.04 Graphics Card : NVIDIA GTX2060 (optional)","title":"Requirements"},{"location":"stretch-tutorials/ros1/getting_started/#setup","text":"Hello Robot is currently running Stretch on Ubuntu 20.04 and ROS Noetic. To begin the setup, follow the Run the new robot installation script on your system. Finally, follow the Creating a new ROS workspace guide to create a fresh catkin workspace complete with all the dependencies. To begin working with a simulated Stretch, follow the Gazebo basics tutorial.","title":"Setup"},{"location":"stretch-tutorials/ros1/internal_state_of_stretch/","text":"Getting the State of the Robot Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Then utilize the ROS command-line tool rostopic to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a new terminal. rostopic echo /joint_states -n1 Note that the flag, -n1 , at the end of the command defines the count of how many times you wish to publish the current topic information. Remove the flag if you prefer to continuously print the topic for debugging purposes. Your terminal will output the information associated with the /joint_states topic. Your header , position , velocity , and effort information may vary from what is printed below. header: seq: 70999 stamp: secs: 1420 nsecs: 2000000 frame_id: '' name: [ joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left, joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift, joint_right_wheel, joint_wrist_yaw ] position: [ -1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2 .9291786329821434e-07, 1 .3802900147297237e-06, 0 .08154086954434359, 1 .4361499260374905e-07, 0 .4139061738340768, 9 .32603306580404e-07 ] velocity: [ 0 .00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1 .322424459109634e-05, -0.00035084643762840415, 0 .0012164337445918797, 0 .0002138814988808099, 0 .00010419792027496809, 4 .0575263146426684e-05, 0 .00022487596895736357, -0.0007751929074042957, 0 .0002451588607332439 ] effort: [ 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0 ] --- Let's say you are interested in only seeing the header component of the /joint_states topic, you can output this within the rostopic command-line tool by typing the following command. rostopic echo /joint_states/header -n1 Your terminal will then output something similar to this: seq: 97277 stamp: secs: 1945 nsecs: 562000000 frame_id: '' --- Additionally, if you were to type rostopic echo / in the terminal, then press the Tab key on your keyboard, you will see the list of published active topics. A powerful tool to visualize ROS communication is the ROS rqt_graph package . By typing the following in a new terminal, you can see a graph of topics being communicated between nodes. rqt_graph The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.","title":"Internal State of Stretch"},{"location":"stretch-tutorials/ros1/internal_state_of_stretch/#getting-the-state-of-the-robot","text":"Begin by starting up the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Then utilize the ROS command-line tool rostopic to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a new terminal. rostopic echo /joint_states -n1 Note that the flag, -n1 , at the end of the command defines the count of how many times you wish to publish the current topic information. Remove the flag if you prefer to continuously print the topic for debugging purposes. Your terminal will output the information associated with the /joint_states topic. Your header , position , velocity , and effort information may vary from what is printed below. header: seq: 70999 stamp: secs: 1420 nsecs: 2000000 frame_id: '' name: [ joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left, joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift, joint_right_wheel, joint_wrist_yaw ] position: [ -1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2 .9291786329821434e-07, 1 .3802900147297237e-06, 0 .08154086954434359, 1 .4361499260374905e-07, 0 .4139061738340768, 9 .32603306580404e-07 ] velocity: [ 0 .00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1 .322424459109634e-05, -0.00035084643762840415, 0 .0012164337445918797, 0 .0002138814988808099, 0 .00010419792027496809, 4 .0575263146426684e-05, 0 .00022487596895736357, -0.0007751929074042957, 0 .0002451588607332439 ] effort: [ 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0, 0 .0 ] --- Let's say you are interested in only seeing the header component of the /joint_states topic, you can output this within the rostopic command-line tool by typing the following command. rostopic echo /joint_states/header -n1 Your terminal will then output something similar to this: seq: 97277 stamp: secs: 1945 nsecs: 562000000 frame_id: '' --- Additionally, if you were to type rostopic echo / in the terminal, then press the Tab key on your keyboard, you will see the list of published active topics. A powerful tool to visualize ROS communication is the ROS rqt_graph package . By typing the following in a new terminal, you can see a graph of topics being communicated between nodes. rqt_graph The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.","title":"Getting the State of the Robot"},{"location":"stretch-tutorials/ros1/moveit_basics/","text":"MoveIt! Basics <!-- MoveIt! on Stretch To run MoveIt with the actual hardware, (assuming stretch_driver is already running) simply run roslaunch stretch_moveit_config move_group.launch This will run all of the planning capabilities, but without the setup, simulation and interface that the above demo provides. To create plans for the robot with the same interface as the offline demo, you can run roslaunch stretch_moveit_config moveit_rviz.launch ``` --> ## MoveIt! Without Hardware To begin running MoveIt! on stretch, run the demo launch file. This doesn ' t require any simulator or robot to run. ``` { .bash .shell-prompt } roslaunch stretch_moveit_config demo.launch This will bring up an RViz instance where you can move the robot around using interactive markers and create plans between poses. You can reference the bottom gif as a guide to plan and execute motion. Additionally, the demo allows a user to select from the three groups, stretch_arm , stretch_gripper , stretch_head to move. A few notes to be kept in mind: Pre-defined start and goal states can be specified in Start State and Goal State drop-downs in the Planning tab of the Motion Planning RViz plugin. stretch_gripper group does not show markers and is intended to be controlled via the joints tab that is located on the very right of the Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in the Planning tab of the Motion Planning RViz plugin. Additionally, the demo allows a user to select from the three groups, stretch_arm , stretch_gripper , stretch_head to move. A few notes to be kept in mind: Pre-defined start and goal states can be specified in Start State and Goal State drop-downs in the Planning tab of the Motion Planning RViz plugin. stretch_gripper group does not show markers and is intended to be controlled via the joints tab that is located on the very right of the Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in the Planning tab of the Motion Planning RViz plugin. ## Running Gazebo with MoveIt! and Stretch To run in Gazebo, execute: roslaunch stretch_gazebo gazebo.launch Then, in a new terminal, execute: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard In a separate terminal, launch: roslaunch stretch_moveit_config demo_gazebo.launch This will launch a Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via the Motion Planning Rviz plugin. Start and goal positions for joints can be selected similarly to [this moveit tutorial](https://ros-planning.github.io/moveit_tutorials/doc/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#choosing-specific-start-goal-states). Running Gazebo with MoveIt! and Stretch To run in Gazebo, execute: roslaunch stretch_gazebo gazebo.launch Then, in a new terminal, execute: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard In a separate terminal, launch: roslaunch stretch_moveit_config demo_gazebo.launch This will launch a Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via the Motion Planning Rviz plugin. Start and goal positions for joints can be selected similarly to this moveit tutorial .","title":"MoveIt! Basics"},{"location":"stretch-tutorials/ros1/moveit_basics/#moveit-basics","text":"<!--","title":"MoveIt! Basics"},{"location":"stretch-tutorials/ros1/moveit_basics/#moveit-on-stretch","text":"To run MoveIt with the actual hardware, (assuming stretch_driver is already running) simply run roslaunch stretch_moveit_config move_group.launch This will run all of the planning capabilities, but without the setup, simulation and interface that the above demo provides. To create plans for the robot with the same interface as the offline demo, you can run roslaunch stretch_moveit_config moveit_rviz.launch ``` --> ## MoveIt! Without Hardware To begin running MoveIt! on stretch, run the demo launch file. This doesn ' t require any simulator or robot to run. ``` { .bash .shell-prompt } roslaunch stretch_moveit_config demo.launch This will bring up an RViz instance where you can move the robot around using interactive markers and create plans between poses. You can reference the bottom gif as a guide to plan and execute motion. Additionally, the demo allows a user to select from the three groups, stretch_arm , stretch_gripper , stretch_head to move. A few notes to be kept in mind: Pre-defined start and goal states can be specified in Start State and Goal State drop-downs in the Planning tab of the Motion Planning RViz plugin. stretch_gripper group does not show markers and is intended to be controlled via the joints tab that is located on the very right of the Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in the Planning tab of the Motion Planning RViz plugin. Additionally, the demo allows a user to select from the three groups, stretch_arm , stretch_gripper , stretch_head to move. A few notes to be kept in mind: Pre-defined start and goal states can be specified in Start State and Goal State drop-downs in the Planning tab of the Motion Planning RViz plugin. stretch_gripper group does not show markers and is intended to be controlled via the joints tab that is located on the very right of the Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in the Planning tab of the Motion Planning RViz plugin. ## Running Gazebo with MoveIt! and Stretch To run in Gazebo, execute: roslaunch stretch_gazebo gazebo.launch Then, in a new terminal, execute: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard In a separate terminal, launch: roslaunch stretch_moveit_config demo_gazebo.launch This will launch a Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via the Motion Planning Rviz plugin. Start and goal positions for joints can be selected similarly to [this moveit tutorial](https://ros-planning.github.io/moveit_tutorials/doc/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#choosing-specific-start-goal-states).","title":"MoveIt! on Stretch"},{"location":"stretch-tutorials/ros1/moveit_basics/#running-gazebo-with-moveit-and-stretch","text":"To run in Gazebo, execute: roslaunch stretch_gazebo gazebo.launch Then, in a new terminal, execute: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard In a separate terminal, launch: roslaunch stretch_moveit_config demo_gazebo.launch This will launch a Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via the Motion Planning Rviz plugin. Start and goal positions for joints can be selected similarly to this moveit tutorial .","title":"Running Gazebo with MoveIt! and Stretch"},{"location":"stretch-tutorials/ros1/navigation_stack/","text":"Navigation Stack with Actual robot stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive Stretch around a mapped space. Running this code will require the robot to be untethered . Then run the following commands to map the space that the robot will navigate in. roslaunch stretch_navigation mapping.launch Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to stretch_user/ . mkdir -p ~/stretch_user/maps rosrun map_server map_saver -f ${ HELLO_FLEET_PATH } /maps/<map_name> Note The <map_name> does not include an extension. Map_saver will save two files as <map_name>.pgm and <map_name>.yaml . Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Rviz will show the robot in the previously mapped space; however, the robot's location on the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place. It is also possible to send 2D Pose Estimates and Nav Goals programmatically. In your launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages to navigate the robot programmatically. Running in Simulation To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the mapping_gazebo.launch and navigation_gazebo.launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch. roslaunch stretch_navigation mapping_gazebo.launch gazebo_world: = worlds/willowgarage.world Teleop using a Joystick Controller The mapping launch files, mapping.launch and mapping_gazebo.launch , expose the ROS argument teleop_type . By default, this ROS argument is set to keyboard , which launches keyboard teleop in the terminal. If the Xbox controller that ships with Stretch is plugged into your computer, the following command will launch mapping with joystick teleop: roslaunch stretch_navigation mapping.launch teleop_type: = joystick Using ROS Remote Master If you have set up ROS Remote Master for untethered operation , you can use Rviz and teleop locally with the following commands: On the robot, execute: roslaunch stretch_navigation mapping.launch rviz: = false teleop_type: = none On your machine, execute: rviz -d ` rospack find stretch_navigation ` /rviz/mapping.launch In a separate terminal on your machine, execute: roslaunch stretch_core teleop_twist.launch teleop_type: = keyboard","title":"Navigation Stack"},{"location":"stretch-tutorials/ros1/navigation_stack/#navigation-stack-with-actual-robot","text":"stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive Stretch around a mapped space. Running this code will require the robot to be untethered . Then run the following commands to map the space that the robot will navigate in. roslaunch stretch_navigation mapping.launch Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to stretch_user/ . mkdir -p ~/stretch_user/maps rosrun map_server map_saver -f ${ HELLO_FLEET_PATH } /maps/<map_name> Note The <map_name> does not include an extension. Map_saver will save two files as <map_name>.pgm and <map_name>.yaml . Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Rviz will show the robot in the previously mapped space; however, the robot's location on the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place. It is also possible to send 2D Pose Estimates and Nav Goals programmatically. In your launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages to navigate the robot programmatically.","title":"Navigation Stack with Actual robot"},{"location":"stretch-tutorials/ros1/navigation_stack/#running-in-simulation","text":"To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the mapping_gazebo.launch and navigation_gazebo.launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch. roslaunch stretch_navigation mapping_gazebo.launch gazebo_world: = worlds/willowgarage.world","title":"Running in Simulation"},{"location":"stretch-tutorials/ros1/navigation_stack/#teleop-using-a-joystick-controller","text":"The mapping launch files, mapping.launch and mapping_gazebo.launch , expose the ROS argument teleop_type . By default, this ROS argument is set to keyboard , which launches keyboard teleop in the terminal. If the Xbox controller that ships with Stretch is plugged into your computer, the following command will launch mapping with joystick teleop: roslaunch stretch_navigation mapping.launch teleop_type: = joystick","title":"Teleop using a Joystick Controller"},{"location":"stretch-tutorials/ros1/navigation_stack/#using-ros-remote-master","text":"If you have set up ROS Remote Master for untethered operation , you can use Rviz and teleop locally with the following commands: On the robot, execute: roslaunch stretch_navigation mapping.launch rviz: = false teleop_type: = none On your machine, execute: rviz -d ` rospack find stretch_navigation ` /rviz/mapping.launch In a separate terminal on your machine, execute: roslaunch stretch_core teleop_twist.launch teleop_type: = keyboard","title":"Using ROS Remote Master"},{"location":"stretch-tutorials/ros1/perception/","text":"Perception Introduction The Stretch robot is equipped with the Intel RealSense D435i camera , an essential component that allows the robot to measure and analyze the world around it. In this tutorial, we are going to showcase how to visualize the various topics published by the camera. Begin by running the stretch driver.launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_low_resolution.launch Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz PointCloud2 Display A list of displays on the left side of the interface can visualize the camera data. Each display has its properties and status that notify a user if topic messages are received. For the PointCloud2 display, a sensor_msgs/pointCloud2 message named /camera/depth/color/points is received and the GIF below demonstrates the various display properties when visualizing the data. Image Display The Image display when toggled creates a new rendering window that visualizes a sensor_msgs/Image messaged, /camera/color/image_raw . This feature shows the image data from the camera; however, the image comes out sideways. Thus, you can select the /camera/color/image_raw_upright_view from the Image Topic options to get an upright view of the image. Camera Display The Camera display is similar to that of the Image display. In this setting, the rendering window also visualizes other displays, such as the PointCloud2, the RobotModel, and Grid Displays. The visibility property can toggle what displays you are interested in visualizing. DepthCloud Display The DepthCloud display is visualized in the main RViz window. This display takes in the depth image and RGB image provided by RealSense to visualize and register a point cloud. Deep Perception Hello Robot also has a ROS package that uses deep learning models for various detection demos. A link to the package is provided: stretch_deep_perception .","title":"Perception"},{"location":"stretch-tutorials/ros1/perception/#perception-introduction","text":"The Stretch robot is equipped with the Intel RealSense D435i camera , an essential component that allows the robot to measure and analyze the world around it. In this tutorial, we are going to showcase how to visualize the various topics published by the camera. Begin by running the stretch driver.launch file. roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. roslaunch stretch_core d435i_low_resolution.launch Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz","title":"Perception Introduction"},{"location":"stretch-tutorials/ros1/perception/#pointcloud2-display","text":"A list of displays on the left side of the interface can visualize the camera data. Each display has its properties and status that notify a user if topic messages are received. For the PointCloud2 display, a sensor_msgs/pointCloud2 message named /camera/depth/color/points is received and the GIF below demonstrates the various display properties when visualizing the data.","title":"PointCloud2 Display"},{"location":"stretch-tutorials/ros1/perception/#image-display","text":"The Image display when toggled creates a new rendering window that visualizes a sensor_msgs/Image messaged, /camera/color/image_raw . This feature shows the image data from the camera; however, the image comes out sideways. Thus, you can select the /camera/color/image_raw_upright_view from the Image Topic options to get an upright view of the image.","title":"Image Display"},{"location":"stretch-tutorials/ros1/perception/#camera-display","text":"The Camera display is similar to that of the Image display. In this setting, the rendering window also visualizes other displays, such as the PointCloud2, the RobotModel, and Grid Displays. The visibility property can toggle what displays you are interested in visualizing.","title":"Camera Display"},{"location":"stretch-tutorials/ros1/perception/#depthcloud-display","text":"The DepthCloud display is visualized in the main RViz window. This display takes in the depth image and RGB image provided by RealSense to visualize and register a point cloud.","title":"DepthCloud Display"},{"location":"stretch-tutorials/ros1/perception/#deep-perception","text":"Hello Robot also has a ROS package that uses deep learning models for various detection demos. A link to the package is provided: stretch_deep_perception .","title":"Deep Perception"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/","text":"ReSpeaker Microphone Array For this tutorial, we will get a high-level view of how to use Stretch's ReSpeaker Mic Array v2.0 . Stretch Body Package In this section we will use command line tools in the Stretch_Body package, a low-level Python API for Stretch's hardware, to directly interact with the ReSpeaker. Begin by typing the following command in a new terminal. stretch_respeaker_test.py The following will be displayed in your terminal: For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. * waiting for audio... * recording 3 seconds * done * playing audio * done The ReSpeaker Mico Array will wait until it hears audio loud enough to trigger its recording feature. Stretch will record audio for 3 seconds and then replay it through its speakers. This command line is a good method to see if the hardware is working correctly. To stop the python script, type Ctrl + c in the terminal. ReSpeaker_ROS Package A ROS package for the ReSpeaker is utilized for this section. Begin by running the sample_respeaker.launch file in a terminal. roslaunch respeaker_ros sample_respeaker.launch This will bring up the necessary nodes that will allow the ReSpeaker to implement a voice and sound interface with the robot. Below are executables you can run to see the ReSpeaker results. rostopic echo /sound_direction # Result of Direction (in Radians) of Audio rostopic echo /sound_localization # Result of Direction as Pose (Quaternion values) rostopic echo /is_speeching # Result of Voice Activity Detector rostopic echo /audio # Raw audio data rostopic echo /speech_audio # Raw audio data when there is speech rostopic echo /speech_to_text # Voice recognition An example is when you run the speech_to_text executable and speak near the microphone array. In a new terminal, execute: rostopic echo /speech_to_text In this instance, \"hello robot\" was said. The following will be displayed in your terminal: transcript: - hello robot confidence: [] --- You can also set various parameters via dynamic_reconfigure by running the following command in a new terminal. rosrun rqt_reconfigure rqt_reconfigure","title":"ReSpeaker Microphone Array"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#respeaker-microphone-array","text":"For this tutorial, we will get a high-level view of how to use Stretch's ReSpeaker Mic Array v2.0 .","title":"ReSpeaker Microphone Array"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#stretch-body-package","text":"In this section we will use command line tools in the Stretch_Body package, a low-level Python API for Stretch's hardware, to directly interact with the ReSpeaker. Begin by typing the following command in a new terminal. stretch_respeaker_test.py The following will be displayed in your terminal: For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. * waiting for audio... * recording 3 seconds * done * playing audio * done The ReSpeaker Mico Array will wait until it hears audio loud enough to trigger its recording feature. Stretch will record audio for 3 seconds and then replay it through its speakers. This command line is a good method to see if the hardware is working correctly. To stop the python script, type Ctrl + c in the terminal.","title":"Stretch Body Package"},{"location":"stretch-tutorials/ros1/respeaker_microphone_array/#respeaker_ros-package","text":"A ROS package for the ReSpeaker is utilized for this section. Begin by running the sample_respeaker.launch file in a terminal. roslaunch respeaker_ros sample_respeaker.launch This will bring up the necessary nodes that will allow the ReSpeaker to implement a voice and sound interface with the robot. Below are executables you can run to see the ReSpeaker results. rostopic echo /sound_direction # Result of Direction (in Radians) of Audio rostopic echo /sound_localization # Result of Direction as Pose (Quaternion values) rostopic echo /is_speeching # Result of Voice Activity Detector rostopic echo /audio # Raw audio data rostopic echo /speech_audio # Raw audio data when there is speech rostopic echo /speech_to_text # Voice recognition An example is when you run the speech_to_text executable and speak near the microphone array. In a new terminal, execute: rostopic echo /speech_to_text In this instance, \"hello robot\" was said. The following will be displayed in your terminal: transcript: - hello robot confidence: [] --- You can also set various parameters via dynamic_reconfigure by running the following command in a new terminal. rosrun rqt_reconfigure rqt_reconfigure","title":"ReSpeaker_ROS Package"},{"location":"stretch-tutorials/ros1/rviz_basics/","text":"Visualizing with RViz You can utilize RViz to visualize Stretch's sensor information. To begin, in a terminal, run the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Then, run the following command in a separate terminal to bring up a simple RViz configuration of the Stretch robot. rosrun rviz rviz -d ` rospack find stretch_core ` /rviz/stretch_simple_test.rviz An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window. If you want to visualize Stretch's tf transform tree , you need to add the display type to the RViz window. First, click the Add button and include the TF type in the display. You will then see all of the transform frames of the Stretch robot, and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference. There are further tutorials for RViz which can be found here . Running RViz and Gazebo (Simulation) Let's bring up Stretch in the willow garage world from the gazebo basics tutorial and RViz by using the following command. roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world rviz: = true The rviz flag will open an RViz window to visualize a variety of ROS topics. Bring up the keyboard teleop node to drive Stretch and observe its sensor input.","title":"RViz Basics"},{"location":"stretch-tutorials/ros1/rviz_basics/#visualizing-with-rviz","text":"You can utilize RViz to visualize Stretch's sensor information. To begin, in a terminal, run the stretch driver launch file. roslaunch stretch_core stretch_driver.launch Then, run the following command in a separate terminal to bring up a simple RViz configuration of the Stretch robot. rosrun rviz rviz -d ` rospack find stretch_core ` /rviz/stretch_simple_test.rviz An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window. If you want to visualize Stretch's tf transform tree , you need to add the display type to the RViz window. First, click the Add button and include the TF type in the display. You will then see all of the transform frames of the Stretch robot, and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference. There are further tutorials for RViz which can be found here .","title":"Visualizing with RViz"},{"location":"stretch-tutorials/ros1/rviz_basics/#running-rviz-and-gazebo-simulation","text":"Let's bring up Stretch in the willow garage world from the gazebo basics tutorial and RViz by using the following command. roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world rviz: = true The rviz flag will open an RViz window to visualize a variety of ROS topics. Bring up the keyboard teleop node to drive Stretch and observe its sensor input.","title":"Running RViz and Gazebo (Simulation)"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/","text":"Teleoperating Stretch Xbox Controller Teleoperating If you have not already had a look at the Xbox Controller Teleoperation section in the Quick Start guide, now might be a good time to try it. Keyboard Teleoperating: Full Body For full-body teleoperation with the keyboard, you first need to run the stretch_driver.launch in a terminal. roslaunch stretch_core stretch_driver.launch Then in a new terminal, type the following command rosrun stretch_core keyboard_teleop Below are the keyboard commands that allow a user to control all of Stretch's joints. ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- To stop the node from sending twist messages, press Ctrl + c in the terminal. Keyboard Teleoperating: Mobile Base Begin by running the following command in your terminal: roslaunch stretch_core stretch_driver.launch To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to navigation for the robot to receive Twist messages. This is done using a rosservice call in a new terminal. In the same terminal run the teleop_twist_keyboard node with the argument remapping the cmd_vel topic name to stretch/cmd_vel . rosservice call /switch_to_navigation_mode rosrun teleop_twist_keyboard teleop_twist_keyboard.py cmd_vel: = stretch/cmd_vel Below are the keyboard commands that allow a user to move Stretch's base. Reading from the keyboard and Publishing to Twist! --------------------------- Moving around: u i o j k l m , . For Holonomic mode ( strafing ) , hold down the shift key: --------------------------- U I O J K L M < > t : up ( +z ) b : down ( -z ) anything else : stop q/z : increase/decrease max speeds by 10 % w/x : increase/decrease only linear speed by 10 % e/c : increase/decrease only angular speed by 10 % CTRL-C to quit currently: speed 0 .5 turn 1 .0 To stop the node from sending twist messages, type Ctrl + c . Create a node for Mobile Base Teleoperating To move Stretch's mobile base using a python script, please look at Teleoperate Stretch with a node for reference. Teleoperating in Gazebo Keyboard Teleoperating: Mobile Base For keyboard teleoperation of the Stretch's mobile base, first, startup Stretch in simulation . Then run the following command in a new terminal. roslaunch stretch_gazebo gazebo.launch In a new terminal, type the following roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard The same keyboard commands will be presented to a user to move the robot. Xbox Controller Teleoperating An alternative for robot base teleoperation is to use an Xbox controller. Stop the keyboard teleoperation node by typing Ctrl + c in the terminal where the command was executed. Then connect the Xbox controller device to your local machine and run the following command. roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = joystick Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless pressed. For a Logitech F310 joystick, this button is A.","title":"Teleoperating Stretch"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#teleoperating-stretch","text":"","title":"Teleoperating Stretch"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#xbox-controller-teleoperating","text":"If you have not already had a look at the Xbox Controller Teleoperation section in the Quick Start guide, now might be a good time to try it.","title":"Xbox Controller Teleoperating"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#keyboard-teleoperating-full-body","text":"For full-body teleoperation with the keyboard, you first need to run the stretch_driver.launch in a terminal. roslaunch stretch_core stretch_driver.launch Then in a new terminal, type the following command rosrun stretch_core keyboard_teleop Below are the keyboard commands that allow a user to control all of Stretch's joints. ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- To stop the node from sending twist messages, press Ctrl + c in the terminal.","title":"Keyboard Teleoperating: Full Body"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#keyboard-teleoperating-mobile-base","text":"Begin by running the following command in your terminal: roslaunch stretch_core stretch_driver.launch To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to navigation for the robot to receive Twist messages. This is done using a rosservice call in a new terminal. In the same terminal run the teleop_twist_keyboard node with the argument remapping the cmd_vel topic name to stretch/cmd_vel . rosservice call /switch_to_navigation_mode rosrun teleop_twist_keyboard teleop_twist_keyboard.py cmd_vel: = stretch/cmd_vel Below are the keyboard commands that allow a user to move Stretch's base. Reading from the keyboard and Publishing to Twist! --------------------------- Moving around: u i o j k l m , . For Holonomic mode ( strafing ) , hold down the shift key: --------------------------- U I O J K L M < > t : up ( +z ) b : down ( -z ) anything else : stop q/z : increase/decrease max speeds by 10 % w/x : increase/decrease only linear speed by 10 % e/c : increase/decrease only angular speed by 10 % CTRL-C to quit currently: speed 0 .5 turn 1 .0 To stop the node from sending twist messages, type Ctrl + c .","title":"Keyboard Teleoperating: Mobile Base"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#create-a-node-for-mobile-base-teleoperating","text":"To move Stretch's mobile base using a python script, please look at Teleoperate Stretch with a node for reference.","title":"Create a node for Mobile Base Teleoperating"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#teleoperating-in-gazebo","text":"","title":"Teleoperating in Gazebo"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#keyboard-teleoperating-mobile-base_1","text":"For keyboard teleoperation of the Stretch's mobile base, first, startup Stretch in simulation . Then run the following command in a new terminal. roslaunch stretch_gazebo gazebo.launch In a new terminal, type the following roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard The same keyboard commands will be presented to a user to move the robot.","title":"Keyboard Teleoperating: Mobile Base"},{"location":"stretch-tutorials/ros1/teleoperating_stretch/#xbox-controller-teleoperating_1","text":"An alternative for robot base teleoperation is to use an Xbox controller. Stop the keyboard teleoperation node by typing Ctrl + c in the terminal where the command was executed. Then connect the Xbox controller device to your local machine and run the following command. roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = joystick Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless pressed. For a Logitech F310 joystick, this button is A.","title":"Xbox Controller Teleoperating"},{"location":"stretch-tutorials/ros1_melodic/","text":"Tutorial Track: Stretch ROS1 This tutorial track is for users looking to become familiar with programming the Stretch RE1 and RE2 via ROS1 Melodic. We recommend going through the tutorials in the following order: Basics Tutorial Description 1 Getting Started 2 Gazebo Basics 3 Teleoperating Stretch 4 Internal State of Stretch 5 RViz Basics 6 Navigation Stack 7 MoveIt! Basics 8 Follow Joint Trajectory Commands 9 Perception 10 ArUco Marker Detection 11 ReSpeaker Microphone Array 12 FUNMAP Other Examples To help get you get started on your software development, here are examples of nodes to have the stretch perform simple tasks. Tutorial Description 1 Teleoperate Stretch with a Node Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Print Joint States Print the joint states of Stretch. 6 Store Effort Values Print, store, and plot the effort values of the Stretch robot. 7 Capture Image Capture images from the RealSense camera data. 8 Voice to Text Interpret speech and save transcript to a text file. 9 Voice Teleoperation of Base Use speech to teleoperate the mobile base. 10 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 11 PointCloud Transformation Convert PointCloud2 data to a PointCloud and transform to a different frame. 12 ArUco Tag Locator Actuate the head to locate a requested ArUco marker tag and return a transform. 13 2D Navigation Goals Send 2D navigation goals to the move_base ROS node.","title":"Index"},{"location":"stretch-tutorials/ros1_melodic/#tutorial-track-stretch-ros1","text":"This tutorial track is for users looking to become familiar with programming the Stretch RE1 and RE2 via ROS1 Melodic. We recommend going through the tutorials in the following order:","title":"Tutorial Track: Stretch ROS1"},{"location":"stretch-tutorials/ros1_melodic/#basics","text":"Tutorial Description 1 Getting Started 2 Gazebo Basics 3 Teleoperating Stretch 4 Internal State of Stretch 5 RViz Basics 6 Navigation Stack 7 MoveIt! Basics 8 Follow Joint Trajectory Commands 9 Perception 10 ArUco Marker Detection 11 ReSpeaker Microphone Array 12 FUNMAP","title":"Basics"},{"location":"stretch-tutorials/ros1_melodic/#other-examples","text":"To help get you get started on your software development, here are examples of nodes to have the stretch perform simple tasks. Tutorial Description 1 Teleoperate Stretch with a Node Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Print Joint States Print the joint states of Stretch. 6 Store Effort Values Print, store, and plot the effort values of the Stretch robot. 7 Capture Image Capture images from the RealSense camera data. 8 Voice to Text Interpret speech and save transcript to a text file. 9 Voice Teleoperation of Base Use speech to teleoperate the mobile base. 10 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 11 PointCloud Transformation Convert PointCloud2 data to a PointCloud and transform to a different frame. 12 ArUco Tag Locator Actuate the head to locate a requested ArUco marker tag and return a transform. 13 2D Navigation Goals Send 2D navigation goals to the move_base ROS node.","title":"Other Examples"},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/","text":"ArUco Marker Detector For this tutorial, we will go over how to detect Stretch's ArUco markers and how to files the hold the information for each tag. Visualize ArUco Markers in RViz Begin by running the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_low_resolution.launch Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. # Terminal 3 roslaunch stretch_core stretch_aruco.launch Within this tutorial package, there is an RViz config file with the topics for transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 4 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz You are going to need to teleoperate Stretch's head to detect the ArUco marker tags. Run the following command in a new terminal and control the head in order to point the camera towards the markers. # Terminal 5 rosrun stretch_core keyboard_teleop The ArUco Marker Dictionary When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml , that holds the information about the markers. If detect_aruco_markers node doesn\u2019t find an entry in stretch_marker_dict.yaml for a particular ArUco marker ID number, it uses the default entry. For example, most robots have shipped with the following default entry: 'default' : 'length_mm' : 24 'use_rgb_only' : False 'name' : 'unknown' 'link' : None and the following entry for the ArUco marker on the top of the wrist '133' : 'length_mm' : 23.5 'use_rgb_only' : False 'name' : 'wrist_top' 'link' : 'link_aruco_top_wrist' Dictionary Breakdown '133' : The dictionary key for each entry is the ArUco marker\u2019s ID number or default . For example, the entry shown above for the ArUco marker on the top of the wrist assumes that the marker\u2019s ID number is 133 . 'length_mm' : 23.5 The length_mm value used by detect_aruco_markers is important for estimating the pose of an ArUco marker. IMPORTANT NOTE: If the actual width and height of the marker do not match this value, then pose estimation will be poor. Thus, carefully measure custom Aruco markers. 'use_rgb_only' : False If use_rgb_only is True , detect_aruco_markers will ignore depth images from the Intel RealSense D435i depth camera when estimating the pose of the marker and will instead only use RGB images from the D435i. 'name' : 'wrist_top' name is used for the text string of the ArUco marker\u2019s ROS Marker in the ROS MarkerArray Message published by the detect_aruco_markers ROS node. 'link' : 'link_aruco_top_wrist' link is currently used by stretch_calibration . It is the name of the link associated with a body-mounted ArUco marker in the robot\u2019s URDF . It\u2019s good practice to add an entry to stretch_marker_dict.yaml for each ArUco marker you use. Create a New ArUco Marker At Hello Robot, we\u2019ve used the following guide when generating new ArUco markers. We generate ArUco markers using a 6x6 bit grid (36 bits) with 250 unique codes. This corresponds with DICT_6X6_250 defined in OpenCV . We generate markers using this online ArUco marker generator by setting the Dictionary entry to 6x6 and then setting the Marker ID and Marker size, mm as appropriate for the specific application. We strongly recommend to measure the actual marker by hand prior to adding an entry for it to stretch_marker_dict.yaml . We select marker ID numbers using the following ranges. 0 - 99 : reserved for users 100 - 249 : reserved for official use by Hello Robot Inc. 100 - 199 : reserved for robots with distinct sets of body-mounted markers Allows different robots near each other to use distinct sets of body-mounted markers to avoid confusion. This could be valuable for various uses of body-mounted markers, including calibration, visual servoing, visual motion capture, and multi-robot tasks. 5 markers per robot = 2 on the mobile base + 2 on the wrist + 1 on the shoulder 20 distinct sets = 100 available ID numbers / 5 ID numbers per robot 200 - 249 : reserved for official accessories 245 for the prototype docking station 246-249 for large floor markers When coming up with this guide, we expected the following: Body-mounted accessories with the same ID numbers mounted to different robots could be disambiguated using the expected range of 3D locations of the ArUco markers on the calibrated body. Accessories in the environment with the same ID numbers could be disambiguated using a map or nearby observable features of the environment.","title":"Aruco marker detection"},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/#aruco-marker-detector","text":"For this tutorial, we will go over how to detect Stretch's ArUco markers and how to files the hold the information for each tag.","title":"ArUco Marker Detector"},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/#visualize-aruco-markers-in-rviz","text":"Begin by running the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_low_resolution.launch Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. # Terminal 3 roslaunch stretch_core stretch_aruco.launch Within this tutorial package, there is an RViz config file with the topics for transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 4 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz You are going to need to teleoperate Stretch's head to detect the ArUco marker tags. Run the following command in a new terminal and control the head in order to point the camera towards the markers. # Terminal 5 rosrun stretch_core keyboard_teleop","title":"Visualize ArUco Markers in RViz"},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/#the-aruco-marker-dictionary","text":"When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml , that holds the information about the markers. If detect_aruco_markers node doesn\u2019t find an entry in stretch_marker_dict.yaml for a particular ArUco marker ID number, it uses the default entry. For example, most robots have shipped with the following default entry: 'default' : 'length_mm' : 24 'use_rgb_only' : False 'name' : 'unknown' 'link' : None and the following entry for the ArUco marker on the top of the wrist '133' : 'length_mm' : 23.5 'use_rgb_only' : False 'name' : 'wrist_top' 'link' : 'link_aruco_top_wrist' Dictionary Breakdown '133' : The dictionary key for each entry is the ArUco marker\u2019s ID number or default . For example, the entry shown above for the ArUco marker on the top of the wrist assumes that the marker\u2019s ID number is 133 . 'length_mm' : 23.5 The length_mm value used by detect_aruco_markers is important for estimating the pose of an ArUco marker. IMPORTANT NOTE: If the actual width and height of the marker do not match this value, then pose estimation will be poor. Thus, carefully measure custom Aruco markers. 'use_rgb_only' : False If use_rgb_only is True , detect_aruco_markers will ignore depth images from the Intel RealSense D435i depth camera when estimating the pose of the marker and will instead only use RGB images from the D435i. 'name' : 'wrist_top' name is used for the text string of the ArUco marker\u2019s ROS Marker in the ROS MarkerArray Message published by the detect_aruco_markers ROS node. 'link' : 'link_aruco_top_wrist' link is currently used by stretch_calibration . It is the name of the link associated with a body-mounted ArUco marker in the robot\u2019s URDF . It\u2019s good practice to add an entry to stretch_marker_dict.yaml for each ArUco marker you use.","title":"The ArUco Marker Dictionary"},{"location":"stretch-tutorials/ros1_melodic/aruco_marker_detection/#create-a-new-aruco-marker","text":"At Hello Robot, we\u2019ve used the following guide when generating new ArUco markers. We generate ArUco markers using a 6x6 bit grid (36 bits) with 250 unique codes. This corresponds with DICT_6X6_250 defined in OpenCV . We generate markers using this online ArUco marker generator by setting the Dictionary entry to 6x6 and then setting the Marker ID and Marker size, mm as appropriate for the specific application. We strongly recommend to measure the actual marker by hand prior to adding an entry for it to stretch_marker_dict.yaml . We select marker ID numbers using the following ranges. 0 - 99 : reserved for users 100 - 249 : reserved for official use by Hello Robot Inc. 100 - 199 : reserved for robots with distinct sets of body-mounted markers Allows different robots near each other to use distinct sets of body-mounted markers to avoid confusion. This could be valuable for various uses of body-mounted markers, including calibration, visual servoing, visual motion capture, and multi-robot tasks. 5 markers per robot = 2 on the mobile base + 2 on the wrist + 1 on the shoulder 20 distinct sets = 100 available ID numbers / 5 ID numbers per robot 200 - 249 : reserved for official accessories 245 for the prototype docking station 246-249 for large floor markers When coming up with this guide, we expected the following: Body-mounted accessories with the same ID numbers mounted to different robots could be disambiguated using the expected range of 3D locations of the ArUco markers on the calibrated body. Accessories in the environment with the same ID numbers could be disambiguated using a map or nearby observable features of the environment.","title":"Create a New ArUco Marker"},{"location":"stretch-tutorials/ros1_melodic/example_1/","text":"Example 1 The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending Twist messages to a Stretch robot. Begin by running the following command in a new terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to navigation mode using a rosservice call. Then drive the robot forward with the move.py node. # Terminal 2 rosservice call /switch_to_navigation_mode cd catkin_ws/src/stretch_tutorials/src/ python move.py To stop the node from sending twist messages, type Ctrl + c . The Code Below is the code which will send Twist messages to drive the robot forward. #!/usr/bin/env python import rospy from geometry_msgs.msg import Twist class Move : \"\"\" A class that sends Twist messages to move the Stretch robot forward. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber. :param self: The self reference. \"\"\" self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo def move_forward ( self ): \"\"\" Function that publishes Twist messages :param self: The self reference. :publishes command: Twist message. \"\"\" command = Twist () command . linear . x = 0.1 command . linear . y = 0.0 command . linear . z = 0.0 command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.0 self . pub . publish ( command ) if __name__ == '__main__' : rospy . init_node ( 'move' ) base_motion = Move () rate = rospy . Rate ( 10 ) while not rospy . is_shutdown (): base_motion . move_forward () rate . sleep () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from geometry_msgs.msg import Twist You need to import rospy if you are writing a ROS Node . The geometry_msgs.msg import is so that we can send velocity commands to the robot. class Move : def __init__ ( self ): self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist . The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. command = Twist () Make a Twist message. We're going to set all of the elements, since we can't depend on them defaulting to safe values. command . linear . x = 0.1 command . linear . y = 0.0 command . linear . z = 0.0 A Twist data structure has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y direction or the z direction. command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.0 A Twist message also has three rotational velocities (in radians per second). The Stretch will only respond to rotations around the z (vertical) axis. self . pub . publish ( command ) Publish the Twist commands in the previously defined topic name /stretch/cmd_vel . rospy . init_node ( 'move' ) base_motion = Move () rate = rospy . Rate ( 10 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". The rospy.Rate() function creates a Rate object. With the help of its method sleep() , it offers a convenient way for looping at the desired rate. With its argument of 10, we should expect to go through the loop 10 times per second (as long as our processing time does not exceed 1/10th of a second!) while not rospy . is_shutdown (): base_motion . move_forward () rate . sleep () This loop is a fairly standard rospy construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl-C or otherwise). The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop. Move Stretch in Simulation Using your preferred text editor, modify the topic name of the published Twist messages. Please review the edit in the move.py script below. self . pub = rospy . Publisher ( '/stretch_diff_drive_controller/cmd_vel' , Twist , queue_size = 1 ) After saving the edited node, bringup Stretch in the empty world simulation . To drive the robot with the node, type the following in a new terminal cd catkin_ws/src/stretch_tutorials/src/ python move.py To stop the node from sending twist messages, type Ctrl + c .","title":"Example 1"},{"location":"stretch-tutorials/ros1_melodic/example_1/#example-1","text":"The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending Twist messages to a Stretch robot. Begin by running the following command in a new terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to navigation mode using a rosservice call. Then drive the robot forward with the move.py node. # Terminal 2 rosservice call /switch_to_navigation_mode cd catkin_ws/src/stretch_tutorials/src/ python move.py To stop the node from sending twist messages, type Ctrl + c .","title":"Example 1"},{"location":"stretch-tutorials/ros1_melodic/example_1/#the-code","text":"Below is the code which will send Twist messages to drive the robot forward. #!/usr/bin/env python import rospy from geometry_msgs.msg import Twist class Move : \"\"\" A class that sends Twist messages to move the Stretch robot forward. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber. :param self: The self reference. \"\"\" self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo def move_forward ( self ): \"\"\" Function that publishes Twist messages :param self: The self reference. :publishes command: Twist message. \"\"\" command = Twist () command . linear . x = 0.1 command . linear . y = 0.0 command . linear . z = 0.0 command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.0 self . pub . publish ( command ) if __name__ == '__main__' : rospy . init_node ( 'move' ) base_motion = Move () rate = rospy . Rate ( 10 ) while not rospy . is_shutdown (): base_motion . move_forward () rate . sleep ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_1/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from geometry_msgs.msg import Twist You need to import rospy if you are writing a ROS Node . The geometry_msgs.msg import is so that we can send velocity commands to the robot. class Move : def __init__ ( self ): self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist . The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. command = Twist () Make a Twist message. We're going to set all of the elements, since we can't depend on them defaulting to safe values. command . linear . x = 0.1 command . linear . y = 0.0 command . linear . z = 0.0 A Twist data structure has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y direction or the z direction. command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.0 A Twist message also has three rotational velocities (in radians per second). The Stretch will only respond to rotations around the z (vertical) axis. self . pub . publish ( command ) Publish the Twist commands in the previously defined topic name /stretch/cmd_vel . rospy . init_node ( 'move' ) base_motion = Move () rate = rospy . Rate ( 10 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". The rospy.Rate() function creates a Rate object. With the help of its method sleep() , it offers a convenient way for looping at the desired rate. With its argument of 10, we should expect to go through the loop 10 times per second (as long as our processing time does not exceed 1/10th of a second!) while not rospy . is_shutdown (): base_motion . move_forward () rate . sleep () This loop is a fairly standard rospy construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl-C or otherwise). The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_1/#move-stretch-in-simulation","text":"Using your preferred text editor, modify the topic name of the published Twist messages. Please review the edit in the move.py script below. self . pub = rospy . Publisher ( '/stretch_diff_drive_controller/cmd_vel' , Twist , queue_size = 1 ) After saving the edited node, bringup Stretch in the empty world simulation . To drive the robot with the node, type the following in a new terminal cd catkin_ws/src/stretch_tutorials/src/ python move.py To stop the node from sending twist messages, type Ctrl + c .","title":"Move Stretch in Simulation"},{"location":"stretch-tutorials/ros1_melodic/example_10/","text":"Example 10 This tutorial provides you an idea of what tf2 can do in the Python track. We will elaborate how to create a tf2 static broadcaster and listener. tf2 Static Broadcaster For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the link_mast , link_lift , and link_wrist_yaw frames. Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch Within this tutorial package, there is an RViz config file with the topics for transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 2 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/tf2_broadcaster_example.rviz Then run the tf2_broadcaster.py node to visualize three static frames. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python tf2_broadcaster.py The gif below visualizes what happens when running the previous node. OPTIONAL : If you would like to see how the static frames update while the robot is in motion, run the stow_command_node.py and observe the tf frames in RViz. # Terminal 4 cd catkin_ws/src/stretch_tutorials/src/ python stow_command.py The Code #!/usr/bin/env python import rospy import tf.transformations from geometry_msgs.msg import TransformStamped from tf2_ros import StaticTransformBroadcaster class FixedFrameBroadcaster (): \"\"\" This node publishes three child static frames in reference to their parent frames as below: parent -> link_mast child -> fk_link_mast parent -> link_lift child -> fk_link_lift parent -> link_wrist_yaw child -> fk_link_wrist_yaw \"\"\" def __init__ ( self ): \"\"\" A function that creates a broadcast node and publishes three new transform frames. :param self: The self reference. \"\"\" self . br = StaticTransformBroadcaster () self . mast = TransformStamped () self . mast . header . stamp = rospy . Time . now () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 2.0 self . mast . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . mast . transform . rotation . x = q [ 0 ] self . mast . transform . rotation . y = q [ 1 ] self . mast . transform . rotation . z = q [ 2 ] self . mast . transform . rotation . w = q [ 3 ] self . lift = TransformStamped () self . lift . header . stamp = rospy . Time . now () self . lift . header . frame_id = 'link_lift' self . lift . child_frame_id = 'fk_link_lift' self . lift . transform . translation . x = 0.0 self . lift . transform . translation . y = 1.0 self . lift . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . lift . transform . rotation . x = q [ 0 ] self . lift . transform . rotation . y = q [ 1 ] self . lift . transform . rotation . z = q [ 2 ] self . lift . transform . rotation . w = q [ 3 ] self . wrist = TransformStamped () self . wrist . header . stamp = rospy . Time . now () self . wrist . header . frame_id = 'link_wrist_yaw' self . wrist . child_frame_id = 'fk_link_wrist_yaw' self . wrist . transform . translation . x = 0.0 self . wrist . transform . translation . y = 1.0 self . wrist . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] self . br . sendTransform ([ self . mast , self . lift , self . wrist ]) rospy . loginfo ( 'Publishing TF frames. Use RViz to visualize' ) if __name__ == '__main__' : rospy . init_node ( 'tf2_broadcaster' ) FixedFrameBroadcaster () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import tf.transformations from geometry_msgs.msg import TransformStamped from tf2_ros import StaticTransformBroadcaster You need to import rospy if you are writing a ROS Node . Import tf.transformations to get quaternion values from Euler angles. Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a tf2_ros.StaticTransformBroadcaster to help make the task of publishing transforms easier. def __init__ ( self ): \"\"\" A function that creates a broadcast node and publishes three new transform frames. :param self: The self reference. \"\"\" self . br = StaticTransformBroadcaster () Here we create a TransformStamped object which will be the message we will send over once populated. self . mast = TransformStamped () self . mast . header . stamp = rospy . Time . now () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' We need to give the transform being published a timestamp, we'll just stamp it with the current time, rospy.Time.now() . Then, we need to set the name of the parent frame of the link we're creating, in this case link_mast . Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is fk_link_mast . self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 2.0 self . mast . transform . translation . z = 0.0 Set the translation values for the child frame. q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] The quaternion_from_euler() function takes in a Euler angle argument and returns a quaternion values. Then set the rotation values to the transformed quaternions. This process will be completed for the link_lift and link_wrist_yaw as well. self . br . sendTransform ([ self . mast , self . lift , self . wrist ]) Send the three transforms using the sendTransform() function. rospy . init_node ( 'tf2_broadcaster' ) FixedFrameBroadcaster () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the FixedFrameBroadcaster() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages. tf2 Static Listener In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section we will create a tf2 listener that will find the transform between fk_link_lift and link_grasp_center . Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then run the tf2_broadcaster.py node to create the three static frames. # Terminal 2 cd catkin_ws/src/stretch_tutorials/src/ python tf2_broadcaster.py Finally, run the tf2_listener.py node to print the transform between two links. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python tf2_listener.py Within the terminal the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames. [ INFO ] [ 1659551318 .098168 ] : The pose of target frame link_grasp_center with reference from fk_link_lift is: translation: x: 1 .08415191335 y: -0.176147838153 z: 0 .576720021135 rotation: x: -0.479004489528 y: -0.508053545368 z: -0.502884087254 w: 0 .509454501243 The Code #!/usr/bin/env python import rospy from geometry_msgs.msg import TransformStamped import tf2_ros class FrameListener (): \"\"\" This Class prints the transformation between the fk_link_mast frame and the target frame, link_grasp_center. \"\"\" def __init__ ( self ): \"\"\" A function that initializes the variables and looks up a transformation between a target and source frame. :param self: The self reference. \"\"\" tf_buffer = tf2_ros . Buffer () listener = tf2_ros . TransformListener ( tf_buffer ) from_frame_rel = 'link_grasp_center' to_frame_rel = 'fk_link_lift' rospy . sleep ( 1.0 ) rate = rospy . Rate ( 1 ) while not rospy . is_shutdown (): try : trans = tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , rospy . Time ()) rospy . loginfo ( 'The pose of target frame %s with reference from %s is: \\n %s ' , from_frame_rel , to_frame_rel , trans . transform ) except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): rospy . logwarn ( ' Could not transform %s from %s ' , to_frame_rel , from_frame_rel ) rate . sleep () if __name__ == '__main__' : rospy . init_node ( 'tf2_listener' ) FrameListener () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from geometry_msgs.msg import TransformStamped import tf2_ros You need to import rospy if you are writing a ROS Node . Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a tf2_ros.TransformListener to help make the task of receiving transforms easier. tf_buffer = tf2_ros . Buffer () listener = tf2_ros . TransformListener ( tf_buffer ) Here, we create a TransformListener object. Once the listener is created, it starts receiving tf2 transformations over the wire, and buffers them for up to 10 seconds. from_frame_rel = 'link_grasp_center' to_frame_rel = 'fk_link_lift' Store frame names in variables that will be used to compute transformations. rospy . sleep ( 1.0 ) rate = rospy . Rate ( 1 ) The first line gives the listener some time to accumulate transforms. The second line is the rate the node is going to publish information (1 Hz). try : trans = tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , rospy . Time ()) rospy . loginfo ( 'The pose of target frame %s with reference from %s is: \\n %s ' , from_frame_rel , to_frame_rel , trans . transform ) except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): rospy . logwarn ( ' Could not transform %s from %s ' , to_frame_rel , from_frame_rel ) Try to look up the transform we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between from_frame_rel and to_frame_rel frames with the lookup_transform() function. rospy . init_node ( 'tf2_listener' ) FrameListener () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the FrameListener() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Example 10"},{"location":"stretch-tutorials/ros1_melodic/example_10/#example-10","text":"This tutorial provides you an idea of what tf2 can do in the Python track. We will elaborate how to create a tf2 static broadcaster and listener.","title":"Example 10"},{"location":"stretch-tutorials/ros1_melodic/example_10/#tf2-static-broadcaster","text":"For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the link_mast , link_lift , and link_wrist_yaw frames. Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch Within this tutorial package, there is an RViz config file with the topics for transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 2 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/tf2_broadcaster_example.rviz Then run the tf2_broadcaster.py node to visualize three static frames. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python tf2_broadcaster.py The gif below visualizes what happens when running the previous node. OPTIONAL : If you would like to see how the static frames update while the robot is in motion, run the stow_command_node.py and observe the tf frames in RViz. # Terminal 4 cd catkin_ws/src/stretch_tutorials/src/ python stow_command.py","title":"tf2 Static Broadcaster"},{"location":"stretch-tutorials/ros1_melodic/example_10/#the-code","text":"#!/usr/bin/env python import rospy import tf.transformations from geometry_msgs.msg import TransformStamped from tf2_ros import StaticTransformBroadcaster class FixedFrameBroadcaster (): \"\"\" This node publishes three child static frames in reference to their parent frames as below: parent -> link_mast child -> fk_link_mast parent -> link_lift child -> fk_link_lift parent -> link_wrist_yaw child -> fk_link_wrist_yaw \"\"\" def __init__ ( self ): \"\"\" A function that creates a broadcast node and publishes three new transform frames. :param self: The self reference. \"\"\" self . br = StaticTransformBroadcaster () self . mast = TransformStamped () self . mast . header . stamp = rospy . Time . now () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 2.0 self . mast . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . mast . transform . rotation . x = q [ 0 ] self . mast . transform . rotation . y = q [ 1 ] self . mast . transform . rotation . z = q [ 2 ] self . mast . transform . rotation . w = q [ 3 ] self . lift = TransformStamped () self . lift . header . stamp = rospy . Time . now () self . lift . header . frame_id = 'link_lift' self . lift . child_frame_id = 'fk_link_lift' self . lift . transform . translation . x = 0.0 self . lift . transform . translation . y = 1.0 self . lift . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . lift . transform . rotation . x = q [ 0 ] self . lift . transform . rotation . y = q [ 1 ] self . lift . transform . rotation . z = q [ 2 ] self . lift . transform . rotation . w = q [ 3 ] self . wrist = TransformStamped () self . wrist . header . stamp = rospy . Time . now () self . wrist . header . frame_id = 'link_wrist_yaw' self . wrist . child_frame_id = 'fk_link_wrist_yaw' self . wrist . transform . translation . x = 0.0 self . wrist . transform . translation . y = 1.0 self . wrist . transform . translation . z = 0.0 q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] self . br . sendTransform ([ self . mast , self . lift , self . wrist ]) rospy . loginfo ( 'Publishing TF frames. Use RViz to visualize' ) if __name__ == '__main__' : rospy . init_node ( 'tf2_broadcaster' ) FixedFrameBroadcaster () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_10/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import tf.transformations from geometry_msgs.msg import TransformStamped from tf2_ros import StaticTransformBroadcaster You need to import rospy if you are writing a ROS Node . Import tf.transformations to get quaternion values from Euler angles. Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a tf2_ros.StaticTransformBroadcaster to help make the task of publishing transforms easier. def __init__ ( self ): \"\"\" A function that creates a broadcast node and publishes three new transform frames. :param self: The self reference. \"\"\" self . br = StaticTransformBroadcaster () Here we create a TransformStamped object which will be the message we will send over once populated. self . mast = TransformStamped () self . mast . header . stamp = rospy . Time . now () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' We need to give the transform being published a timestamp, we'll just stamp it with the current time, rospy.Time.now() . Then, we need to set the name of the parent frame of the link we're creating, in this case link_mast . Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is fk_link_mast . self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 2.0 self . mast . transform . translation . z = 0.0 Set the translation values for the child frame. q = tf . transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] The quaternion_from_euler() function takes in a Euler angle argument and returns a quaternion values. Then set the rotation values to the transformed quaternions. This process will be completed for the link_lift and link_wrist_yaw as well. self . br . sendTransform ([ self . mast , self . lift , self . wrist ]) Send the three transforms using the sendTransform() function. rospy . init_node ( 'tf2_broadcaster' ) FixedFrameBroadcaster () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the FixedFrameBroadcaster() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_10/#tf2-static-listener","text":"In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section we will create a tf2 listener that will find the transform between fk_link_lift and link_grasp_center . Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then run the tf2_broadcaster.py node to create the three static frames. # Terminal 2 cd catkin_ws/src/stretch_tutorials/src/ python tf2_broadcaster.py Finally, run the tf2_listener.py node to print the transform between two links. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python tf2_listener.py Within the terminal the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames. [ INFO ] [ 1659551318 .098168 ] : The pose of target frame link_grasp_center with reference from fk_link_lift is: translation: x: 1 .08415191335 y: -0.176147838153 z: 0 .576720021135 rotation: x: -0.479004489528 y: -0.508053545368 z: -0.502884087254 w: 0 .509454501243","title":"tf2 Static Listener"},{"location":"stretch-tutorials/ros1_melodic/example_10/#the-code_1","text":"#!/usr/bin/env python import rospy from geometry_msgs.msg import TransformStamped import tf2_ros class FrameListener (): \"\"\" This Class prints the transformation between the fk_link_mast frame and the target frame, link_grasp_center. \"\"\" def __init__ ( self ): \"\"\" A function that initializes the variables and looks up a transformation between a target and source frame. :param self: The self reference. \"\"\" tf_buffer = tf2_ros . Buffer () listener = tf2_ros . TransformListener ( tf_buffer ) from_frame_rel = 'link_grasp_center' to_frame_rel = 'fk_link_lift' rospy . sleep ( 1.0 ) rate = rospy . Rate ( 1 ) while not rospy . is_shutdown (): try : trans = tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , rospy . Time ()) rospy . loginfo ( 'The pose of target frame %s with reference from %s is: \\n %s ' , from_frame_rel , to_frame_rel , trans . transform ) except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): rospy . logwarn ( ' Could not transform %s from %s ' , to_frame_rel , from_frame_rel ) rate . sleep () if __name__ == '__main__' : rospy . init_node ( 'tf2_listener' ) FrameListener () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_10/#the-code-explained_1","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from geometry_msgs.msg import TransformStamped import tf2_ros You need to import rospy if you are writing a ROS Node . Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a tf2_ros.TransformListener to help make the task of receiving transforms easier. tf_buffer = tf2_ros . Buffer () listener = tf2_ros . TransformListener ( tf_buffer ) Here, we create a TransformListener object. Once the listener is created, it starts receiving tf2 transformations over the wire, and buffers them for up to 10 seconds. from_frame_rel = 'link_grasp_center' to_frame_rel = 'fk_link_lift' Store frame names in variables that will be used to compute transformations. rospy . sleep ( 1.0 ) rate = rospy . Rate ( 1 ) The first line gives the listener some time to accumulate transforms. The second line is the rate the node is going to publish information (1 Hz). try : trans = tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , rospy . Time ()) rospy . loginfo ( 'The pose of target frame %s with reference from %s is: \\n %s ' , from_frame_rel , to_frame_rel , trans . transform ) except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): rospy . logwarn ( ' Could not transform %s from %s ' , to_frame_rel , from_frame_rel ) Try to look up the transform we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between from_frame_rel and to_frame_rel frames with the lookup_transform() function. rospy . init_node ( 'tf2_listener' ) FrameListener () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the FrameListener() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_11/","text":"Example 11 This tutorial highlights how to create a PointCloud message from the data of a PointCloud2 message type, then transform the PointCloud's reference link to a different frame. The data published by the RealSense is referencing its camera_color_optical_frame link, and we will be changing its reference to the base_link . Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_low_resolution.launch Then run the pointCloud_transformer.py node. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python pointcloud_transformer.py Within this tutorial package, there is an RViz config file with the PointCloud in the Display tree. You can visualize this topic and the robot model by running the command below in a new terminal. # Terminal 4 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/PointCloud_transformer_example.rviz The gif below visualizes what happens when running the previous node. The Code #!/usr/bin/env python import rospy import tf import sensor_msgs.point_cloud2 as pc2 from sensor_msgs.msg import PointCloud2 , PointCloud from geometry_msgs.msg import Point32 from std_msgs.msg import Header class PointCloudTransformer : \"\"\" A class that takes in a PointCloud2 message and stores its points into a PointCloud message. Then that PointCloud is transformed to reference the 'base_link' frame. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber, publisher, and other variables. :param self: The self reference. \"\"\" self . pointcloud2_sub = rospy . Subscriber ( \"/camera/depth/color/points\" , PointCloud2 , self . callback_pcl2 , queue_size = 1 ) self . pointcloud_pub = rospy . Publisher ( \"/camera_cloud\" , PointCloud , queue_size = 1 ) self . pcl2_cloud = None self . listener = tf . TransformListener ( True , rospy . Duration ( 10.0 )) rospy . loginfo ( 'Publishing transformed PointCloud. Use RViz to visualize' ) def callback_pcl2 ( self , msg ): \"\"\" Callback function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud2 message type. \"\"\" self . pcl2_cloud = msg def pcl_transformer ( self ): \"\"\" A function that extracts the points from the stored PointCloud2 message and appends those points to a PointCloud message. Then the function transforms the PointCloud from its the header frame id, 'camera_color_optical_frame' to the 'base_link' frame. :param self: The self reference. \"\"\" temp_cloud = PointCloud () temp_cloud . header = self . pcl2_cloud . header for data in pc2 . read_points ( self . pcl2_cloud , skip_nans = True ): temp_cloud . points . append ( Point32 ( data [ 0 ], data [ 1 ], data [ 2 ])) transformed_cloud = self . transform_pointcloud ( temp_cloud ) self . pointcloud_pub . publish ( transformed_cloud ) def transform_pointcloud ( self , msg ): \"\"\" Function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud message. :returns new_cloud: The transformed PointCloud message. \"\"\" while not rospy . is_shutdown (): try : new_cloud = self . listener . transformPointCloud ( \"base_link\" , msg ) return new_cloud if new_cloud : break except ( tf . LookupException , tf . ConnectivityException , tf . ExtrapolationException ): pass if __name__ == \"__main__\" : rospy . init_node ( 'pointcloud_transformer' , anonymous = True ) PCT = PointCloudTransformer () rate = rospy . Rate ( 1 ) rospy . sleep ( 1 ) while not rospy . is_shutdown (): PCT . pcl_transformer () rate . sleep () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import tf import sensor_msgs.point_cloud2 as pc2 from sensor_msgs.msg import PointCloud2 , PointCloud from geometry_msgs.msg import Point32 from std_msgs.msg import Header You need to import rospy if you are writing a ROS Node . Import tf to utilize the transformPointCloud function. Import various the message types from sensor_msgs . self . pointcloud2_sub = rospy . Subscriber ( \"/camera/depth/color/points\" , PointCloud2 , self . callback_pcl2 , queue_size = 1 ) Set up a subscriber. We're going to subscribe to the topic /camera/depth/color/points , looking for PointCloud2 message. When a message comes in, ROS is going to pass it to the function callback_pcl2() automatically. self . pointcloud_pub = rospy . Publisher ( \"/camera_cloud\" , PointCloud , queue_size = 1 ) This section of code defines the talker's interface to the rest of ROS. self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1) declares that your node is publishing to the /camera_cloud topic using the message type PointCloud . self . pcl2_cloud = None self . listener = tf . TransformListener ( True , rospy . Duration ( 10.0 )) The first line of code initializes self.pcl2_cloud to store the PointCloud2 message. The second line creates a tf.TransformListener object. Once the listener is created, it starts receiving tf transformations over the wire, and buffers them for up to 10 seconds. def callback_pcl2 ( self , msg ): \"\"\" Callback function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud2 message type. \"\"\" self . pcl2_cloud = msg The callback function that stores the the PointCloud2 message. temp_cloud = PointCloud () temp_cloud . header = self . pcl2_cloud . header Create a PointCloud for temporary use. Set the temporary PointCloud's header to the stored PointCloud2 header. for data in pc2 . read_points ( self . pcl2_cloud , skip_nans = True ): temp_cloud . points . append ( Point32 ( data [ 0 ], data [ 1 ], data [ 2 ])) Use a for loop to extract PointCloud2 data into a list of x, y, z points and append those values to the PointCloud message, temp_cloud . transformed_cloud = self . transform_pointcloud ( temp_cloud ) Utilize the transform_pointcloud function to transform the points in the PointCloud message to reference the base_link while not rospy . is_shutdown (): try : new_cloud = self . listener . transformPointCloud ( \"base_link\" , msg ) return new_cloud if new_cloud : break except ( tf . LookupException , tf . ConnectivityException , tf . ExtrapolationException ): pass Try to look up and transform the PointCloud input. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Transform the point cloud data from camera_color_optical_frame to base_link with the transformPointCloud() function. self . pointcloud_pub . publish ( transformed_cloud ) Publish the new transformed PointCloud . rospy . init_node ( 'pointcloud_transformer' , anonymous = True ) PCT = PointCloudTransformer () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Declare a PointCloudTransformer object. rate = rospy . Rate ( 1 ) rospy . sleep ( 1 ) The first line gives the listener some time to accumulate transforms. The second line is the rate the node is going to publish information (1 Hz). while not rospy . is_shutdown (): PCT . pcl_transformer () rate . sleep () Run a while loop until the node is shutdown. Within the while loop run the pcl_transformer() method.","title":"Example 11"},{"location":"stretch-tutorials/ros1_melodic/example_11/#example-11","text":"This tutorial highlights how to create a PointCloud message from the data of a PointCloud2 message type, then transform the PointCloud's reference link to a different frame. The data published by the RealSense is referencing its camera_color_optical_frame link, and we will be changing its reference to the base_link . Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_low_resolution.launch Then run the pointCloud_transformer.py node. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python pointcloud_transformer.py Within this tutorial package, there is an RViz config file with the PointCloud in the Display tree. You can visualize this topic and the robot model by running the command below in a new terminal. # Terminal 4 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/PointCloud_transformer_example.rviz The gif below visualizes what happens when running the previous node.","title":"Example 11"},{"location":"stretch-tutorials/ros1_melodic/example_11/#the-code","text":"#!/usr/bin/env python import rospy import tf import sensor_msgs.point_cloud2 as pc2 from sensor_msgs.msg import PointCloud2 , PointCloud from geometry_msgs.msg import Point32 from std_msgs.msg import Header class PointCloudTransformer : \"\"\" A class that takes in a PointCloud2 message and stores its points into a PointCloud message. Then that PointCloud is transformed to reference the 'base_link' frame. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber, publisher, and other variables. :param self: The self reference. \"\"\" self . pointcloud2_sub = rospy . Subscriber ( \"/camera/depth/color/points\" , PointCloud2 , self . callback_pcl2 , queue_size = 1 ) self . pointcloud_pub = rospy . Publisher ( \"/camera_cloud\" , PointCloud , queue_size = 1 ) self . pcl2_cloud = None self . listener = tf . TransformListener ( True , rospy . Duration ( 10.0 )) rospy . loginfo ( 'Publishing transformed PointCloud. Use RViz to visualize' ) def callback_pcl2 ( self , msg ): \"\"\" Callback function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud2 message type. \"\"\" self . pcl2_cloud = msg def pcl_transformer ( self ): \"\"\" A function that extracts the points from the stored PointCloud2 message and appends those points to a PointCloud message. Then the function transforms the PointCloud from its the header frame id, 'camera_color_optical_frame' to the 'base_link' frame. :param self: The self reference. \"\"\" temp_cloud = PointCloud () temp_cloud . header = self . pcl2_cloud . header for data in pc2 . read_points ( self . pcl2_cloud , skip_nans = True ): temp_cloud . points . append ( Point32 ( data [ 0 ], data [ 1 ], data [ 2 ])) transformed_cloud = self . transform_pointcloud ( temp_cloud ) self . pointcloud_pub . publish ( transformed_cloud ) def transform_pointcloud ( self , msg ): \"\"\" Function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud message. :returns new_cloud: The transformed PointCloud message. \"\"\" while not rospy . is_shutdown (): try : new_cloud = self . listener . transformPointCloud ( \"base_link\" , msg ) return new_cloud if new_cloud : break except ( tf . LookupException , tf . ConnectivityException , tf . ExtrapolationException ): pass if __name__ == \"__main__\" : rospy . init_node ( 'pointcloud_transformer' , anonymous = True ) PCT = PointCloudTransformer () rate = rospy . Rate ( 1 ) rospy . sleep ( 1 ) while not rospy . is_shutdown (): PCT . pcl_transformer () rate . sleep ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_11/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import tf import sensor_msgs.point_cloud2 as pc2 from sensor_msgs.msg import PointCloud2 , PointCloud from geometry_msgs.msg import Point32 from std_msgs.msg import Header You need to import rospy if you are writing a ROS Node . Import tf to utilize the transformPointCloud function. Import various the message types from sensor_msgs . self . pointcloud2_sub = rospy . Subscriber ( \"/camera/depth/color/points\" , PointCloud2 , self . callback_pcl2 , queue_size = 1 ) Set up a subscriber. We're going to subscribe to the topic /camera/depth/color/points , looking for PointCloud2 message. When a message comes in, ROS is going to pass it to the function callback_pcl2() automatically. self . pointcloud_pub = rospy . Publisher ( \"/camera_cloud\" , PointCloud , queue_size = 1 ) This section of code defines the talker's interface to the rest of ROS. self.pointcloud_pub = rospy.Publisher(\"/camera_cloud\", PointCloud, queue_size=1) declares that your node is publishing to the /camera_cloud topic using the message type PointCloud . self . pcl2_cloud = None self . listener = tf . TransformListener ( True , rospy . Duration ( 10.0 )) The first line of code initializes self.pcl2_cloud to store the PointCloud2 message. The second line creates a tf.TransformListener object. Once the listener is created, it starts receiving tf transformations over the wire, and buffers them for up to 10 seconds. def callback_pcl2 ( self , msg ): \"\"\" Callback function that stores the PointCloud2 message. :param self: The self reference. :param msg: The PointCloud2 message type. \"\"\" self . pcl2_cloud = msg The callback function that stores the the PointCloud2 message. temp_cloud = PointCloud () temp_cloud . header = self . pcl2_cloud . header Create a PointCloud for temporary use. Set the temporary PointCloud's header to the stored PointCloud2 header. for data in pc2 . read_points ( self . pcl2_cloud , skip_nans = True ): temp_cloud . points . append ( Point32 ( data [ 0 ], data [ 1 ], data [ 2 ])) Use a for loop to extract PointCloud2 data into a list of x, y, z points and append those values to the PointCloud message, temp_cloud . transformed_cloud = self . transform_pointcloud ( temp_cloud ) Utilize the transform_pointcloud function to transform the points in the PointCloud message to reference the base_link while not rospy . is_shutdown (): try : new_cloud = self . listener . transformPointCloud ( \"base_link\" , msg ) return new_cloud if new_cloud : break except ( tf . LookupException , tf . ConnectivityException , tf . ExtrapolationException ): pass Try to look up and transform the PointCloud input. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Transform the point cloud data from camera_color_optical_frame to base_link with the transformPointCloud() function. self . pointcloud_pub . publish ( transformed_cloud ) Publish the new transformed PointCloud . rospy . init_node ( 'pointcloud_transformer' , anonymous = True ) PCT = PointCloudTransformer () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Declare a PointCloudTransformer object. rate = rospy . Rate ( 1 ) rospy . sleep ( 1 ) The first line gives the listener some time to accumulate transforms. The second line is the rate the node is going to publish information (1 Hz). while not rospy . is_shutdown (): PCT . pcl_transformer () rate . sleep () Run a while loop until the node is shutdown. Within the while loop run the pcl_transformer() method.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_12/","text":"Example 12 For this example, we will send follow joint trajectory commands for the head camera to search and locate an ArUco tag. In this instance, a Stretch robot will try to locate the docking station's ArUco tag. Modifying Stretch Marker Dictionary YAML File. When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml , that holds the information about the markers. A further breakdown of the yaml file can be found in our Aruco Marker Detection tutorial. Below is what the needs to be included in the stretch_marker_dict.yaml file so the detect_aruco_markers node can find the docking station's ArUco tag. '245' : 'length_mm' : 88.0 'use_rgb_only' : False 'name' : 'docking_station' 'link' : None Getting Started Begin by running the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_high_resolution.launch Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. # Terminal 3 roslaunch stretch_core stretch_aruco.launch Within this tutorial package, there is an RViz config file with the topics for transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 4 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz Then run the aruco_tag_locator.py node. # Terminal 5 cd catkin_ws/src/stretch_tutorials/src/ python aruco_tag_locator.py The Code #! /usr/bin/env python import rospy import time import tf2_ros import numpy as np from math import pi import hello_helpers.hello_misc as hm from sensor_msgs.msg import JointState from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from geometry_msgs.msg import TransformStamped class LocateArUcoTag ( hm . HelloNode ): \"\"\" A class that actuates the RealSense camera to find the docking station's ArUco tag and returns a Transform between the `base_link` and the requested tag. \"\"\" def __init__ ( self ): \"\"\" A function that initializes the subscriber and other needed variables. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . joint_states_sub = rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) self . transform_pub = rospy . Publisher ( 'ArUco_transform' , TransformStamped , queue_size = 10 ) self . joint_state = None self . min_pan_position = - 4.10 self . max_pan_position = 1.50 self . pan_num_steps = 10 self . pan_step_size = abs ( self . min_pan_position - self . max_pan_position ) / self . pan_num_steps self . min_tilt_position = - 0.75 self . tilt_num_steps = 3 self . tilt_step_size = pi / 16 self . rot_vel = 0.5 # radians per sec def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg def send_command ( self , command ): ''' Handles single joint control commands by constructing a FollowJointTrajectoryGoal message and sending it to the trajectory_client created in hello_misc. :param self: The self reference. :param command: A dictionary message type. ''' if ( self . joint_state is not None ) and ( command is not None ): joint_name = command [ 'joint' ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ joint_name ] point = JointTrajectoryPoint () if 'delta' in command : joint_index = self . joint_state . name . index ( joint_name ) joint_value = self . joint_state . position [ joint_index ] delta = command [ 'delta' ] new_value = joint_value + delta point . positions = [ new_value ] elif 'position' in command : point . positions = [ command [ 'position' ]] point . velocities = [ self . rot_vel ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) self . trajectory_client . wait_for_result () def find_tag ( self , tag_name = 'docking_station' ): \"\"\" A function that actuates the camera to search for a defined ArUco tag marker. Then the function returns the pose :param self: The self reference. :param tag_name: A string value of the ArUco marker name. :returns transform: The docking station's TransformStamped message. \"\"\" pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'position' : self . min_tilt_position } self . send_command ( tilt_command ) for i in range ( self . tilt_num_steps ): for j in range ( self . pan_num_steps ): pan_command = { 'joint' : 'joint_head_pan' , 'delta' : self . pan_step_size } self . send_command ( pan_command ) rospy . sleep ( 0.2 ) try : transform = self . tf_buffer . lookup_transform ( 'base_link' , tag_name , rospy . Time ()) rospy . loginfo ( \"Found Requested Tag: \\n %s \" , transform ) self . transform_pub . publish ( transform ) return transform except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): continue pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'delta' : self . tilt_step_size } self . send_command ( tilt_command ) rospy . sleep ( .25 ) rospy . loginfo ( \"The requested tag ' %s ' was not found\" , tag_name ) def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'aruco_tag_locator' , 'aruco_tag_locator' , wait_for_first_pointcloud = False ) self . static_broadcaster = tf2_ros . StaticTransformBroadcaster () self . tf_buffer = tf2_ros . Buffer () self . listener = tf2_ros . TransformListener ( self . tf_buffer ) rospy . sleep ( 1.0 ) rospy . loginfo ( 'Searching for docking ArUco tag.' ) pose = self . find_tag ( \"docking_station\" ) if __name__ == '__main__' : try : node = LocateArUcoTag () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import time import tf2_ros import numpy as np from math import pi import hello_helpers.hello_misc as hm from sensor_msgs.msg import JointState from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from geometry_msgs.msg import TransformStamped You need to import rospy if you are writing a ROS Node . Import other python modules needed for this node. Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module the provides various Python scripts used across stretch_ros . In this instance we are importing the hello_misc script. def __init__ ( self ): \"\"\" A function that initializes the subscriber and other needed variables. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . joint_states_sub = rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) self . transform_pub = rospy . Publisher ( 'ArUco_transform' , TransformStamped , queue_size = 10 ) self . joint_state = None The LocateArUcoTag class inherits the HelloNode class from hm and is instantiated. Set up a subscriber with rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback) . We're going to subscribe to the topic \" stretch/joint_states \", looking for JointState messages. When a message comes in, ROS is going to pass it to the function joint_states_callback() automatically. rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10) declares that your node is publishing to the ArUco_transform topic using the message type TransformStamped . The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. self . min_pan_position = - 4.10 self . max_pan_position = 1.50 self . pan_num_steps = 10 self . pan_step_size = abs ( self . min_pan_position - self . max_pan_position ) / self . pan_num_steps Provide the minimum and maximum joint positions for the head pan. These values are needed for sweeping the head to search for the ArUco tag. We also define the number of steps for the sweep, then create the step size for the head pan joint. self . min_tilt_position = - 0.75 self . tilt_num_steps = 3 self . tilt_step_size = pi / 16 Set the minimum position of the tilt joint, the number of steps, and the size of each step. self . rot_vel = 0.5 # radians per sec Define the head actuation rotational velocity. def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg The joint_states_callback() function stores Stretch's joint states. def send_command ( self , command ): ''' Handles single joint control commands by constructing a FollowJointTrajectoryGoal message and sending it to the trajectory_client created in hello_misc. :param self: The self reference. :param command: A dictionary message type. ''' if ( self . joint_state is not None ) and ( command is not None ): joint_name = command [ 'joint' ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ joint_name ] point = JointTrajectoryPoint () Assign trajectory_goal as a FollowJointTrajectoryGoal message type. Then extract the string value from the joint key. Also, assign point as a JointTrajectoryPoint message type. if 'delta' in command : joint_index = self . joint_state . name . index ( joint_name ) joint_value = self . joint_state . position [ joint_index ] delta = command [ 'delta' ] new_value = joint_value + delta point . positions = [ new_value ] Check to see if delta is a key in the command dictionary. Then get the current position of the joint and add the delta as a a new position value. elif 'position' in command : point . positions = [ command [ 'position' ]] Check to see if position is a key in the command dictionary. Then extract the position value. point . velocities = [ self . rot_vel ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) self . trajectory_client . wait_for_result () Then trajectory_goal.trajectory.points is defined by the positions set in point . Specify the coordinate frame that we want ( base_link ) and set the time to be now. Make the action call and send the goal. The last line of code waits for the result before it exits the python script. def find_tag ( self , tag_name = 'docking_station' ): \"\"\" A function that actuates the camera to search for a defined ArUco tag marker. Then the function returns the pose :param self: The self reference. :param tag_name: A string value of the ArUco marker name. :returns transform: The docking station's TransformStamped message. \"\"\" pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'position' : self . min_tilt_position } self . send_command ( tilt_command ) Create a dictionaries to get the head in its initial position for its search and send the commands the the send_command() function. for i in range ( self . tilt_num_steps ): for j in range ( self . pan_num_steps ): pan_command = { 'joint' : 'joint_head_pan' , 'delta' : self . pan_step_size } self . send_command ( pan_command ) rospy . sleep ( 0.5 ) Utilize nested for loop to sweep the pan and tilt in increments. Then update the joint_head_pan position by the pan_step_size . Use rospy.sleep() function to give time for system to do a Transform lookup before next step. try : transform = self . tf_buffer . lookup_transform ( 'base_link' , tag_name , rospy . Time ()) rospy . loginfo ( \"Found Requested Tag: \\n %s \" , transform ) self . transform_pub . publish ( transform ) return transform except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): continue Use a try-except block to look up the transform between the base_link and requested ArUco tag. Then publish and return the TransformStamped message. pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'delta' : self . tilt_step_size } self . send_command ( tilt_command ) rospy . sleep ( .25 ) Begin sweep with new tilt angle. def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'aruco_tag_locator' , 'aruco_tag_locator' , wait_for_first_pointcloud = False ) Create a funcion, main() , to do all of the setup for the hm.HelloNode class and initialize the aruco_tag_locator node. self . static_broadcaster = tf2_ros . StaticTransformBroadcaster () self . tf_buffer = tf2_ros . Buffer () self . listener = tf2_ros . TransformListener ( self . tf_buffer ) rospy . sleep ( 1.0 ) Create a StaticTranformBoradcaster Node. Also, start a tf buffer that will store the tf information for a few seconds.Then set up a tf listener, which will subscribe to all of the relevant tf topics, and keep track of the information. Include rospy.sleep(1.0) to give the listener some time to accumulate transforms. rospy . loginfo ( 'Searching for docking ArUco tag.' ) pose = self . find_tag ( \"docking_station\" ) Notify Stretch is searching for the ArUco tag with a rospy.loginfo() function. Then search for the ArUco marker for the docking station. if __name__ == '__main__' : try : node = LocateArUcoTag () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare LocateArUcoTag object. Then run the main() method.","title":"Example 12"},{"location":"stretch-tutorials/ros1_melodic/example_12/#example-12","text":"For this example, we will send follow joint trajectory commands for the head camera to search and locate an ArUco tag. In this instance, a Stretch robot will try to locate the docking station's ArUco tag.","title":"Example 12"},{"location":"stretch-tutorials/ros1_melodic/example_12/#modifying-stretch-marker-dictionary-yaml-file","text":"When defining the ArUco markers on Stretch, hello robot utilizes a YAML file, stretch_marker_dict.yaml , that holds the information about the markers. A further breakdown of the yaml file can be found in our Aruco Marker Detection tutorial. Below is what the needs to be included in the stretch_marker_dict.yaml file so the detect_aruco_markers node can find the docking station's ArUco tag. '245' : 'length_mm' : 88.0 'use_rgb_only' : False 'name' : 'docking_station' 'link' : None","title":"Modifying Stretch Marker Dictionary YAML File."},{"location":"stretch-tutorials/ros1_melodic/example_12/#getting-started","text":"Begin by running the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_high_resolution.launch Next, run the stretch ArUco launch file which will bring up the detect_aruco_markers node. # Terminal 3 roslaunch stretch_core stretch_aruco.launch Within this tutorial package, there is an RViz config file with the topics for transform frames in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 4 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/aruco_detector_example.rviz Then run the aruco_tag_locator.py node. # Terminal 5 cd catkin_ws/src/stretch_tutorials/src/ python aruco_tag_locator.py","title":"Getting Started"},{"location":"stretch-tutorials/ros1_melodic/example_12/#the-code","text":"#! /usr/bin/env python import rospy import time import tf2_ros import numpy as np from math import pi import hello_helpers.hello_misc as hm from sensor_msgs.msg import JointState from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from geometry_msgs.msg import TransformStamped class LocateArUcoTag ( hm . HelloNode ): \"\"\" A class that actuates the RealSense camera to find the docking station's ArUco tag and returns a Transform between the `base_link` and the requested tag. \"\"\" def __init__ ( self ): \"\"\" A function that initializes the subscriber and other needed variables. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . joint_states_sub = rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) self . transform_pub = rospy . Publisher ( 'ArUco_transform' , TransformStamped , queue_size = 10 ) self . joint_state = None self . min_pan_position = - 4.10 self . max_pan_position = 1.50 self . pan_num_steps = 10 self . pan_step_size = abs ( self . min_pan_position - self . max_pan_position ) / self . pan_num_steps self . min_tilt_position = - 0.75 self . tilt_num_steps = 3 self . tilt_step_size = pi / 16 self . rot_vel = 0.5 # radians per sec def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg def send_command ( self , command ): ''' Handles single joint control commands by constructing a FollowJointTrajectoryGoal message and sending it to the trajectory_client created in hello_misc. :param self: The self reference. :param command: A dictionary message type. ''' if ( self . joint_state is not None ) and ( command is not None ): joint_name = command [ 'joint' ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ joint_name ] point = JointTrajectoryPoint () if 'delta' in command : joint_index = self . joint_state . name . index ( joint_name ) joint_value = self . joint_state . position [ joint_index ] delta = command [ 'delta' ] new_value = joint_value + delta point . positions = [ new_value ] elif 'position' in command : point . positions = [ command [ 'position' ]] point . velocities = [ self . rot_vel ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) self . trajectory_client . wait_for_result () def find_tag ( self , tag_name = 'docking_station' ): \"\"\" A function that actuates the camera to search for a defined ArUco tag marker. Then the function returns the pose :param self: The self reference. :param tag_name: A string value of the ArUco marker name. :returns transform: The docking station's TransformStamped message. \"\"\" pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'position' : self . min_tilt_position } self . send_command ( tilt_command ) for i in range ( self . tilt_num_steps ): for j in range ( self . pan_num_steps ): pan_command = { 'joint' : 'joint_head_pan' , 'delta' : self . pan_step_size } self . send_command ( pan_command ) rospy . sleep ( 0.2 ) try : transform = self . tf_buffer . lookup_transform ( 'base_link' , tag_name , rospy . Time ()) rospy . loginfo ( \"Found Requested Tag: \\n %s \" , transform ) self . transform_pub . publish ( transform ) return transform except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): continue pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'delta' : self . tilt_step_size } self . send_command ( tilt_command ) rospy . sleep ( .25 ) rospy . loginfo ( \"The requested tag ' %s ' was not found\" , tag_name ) def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'aruco_tag_locator' , 'aruco_tag_locator' , wait_for_first_pointcloud = False ) self . static_broadcaster = tf2_ros . StaticTransformBroadcaster () self . tf_buffer = tf2_ros . Buffer () self . listener = tf2_ros . TransformListener ( self . tf_buffer ) rospy . sleep ( 1.0 ) rospy . loginfo ( 'Searching for docking ArUco tag.' ) pose = self . find_tag ( \"docking_station\" ) if __name__ == '__main__' : try : node = LocateArUcoTag () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_12/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import time import tf2_ros import numpy as np from math import pi import hello_helpers.hello_misc as hm from sensor_msgs.msg import JointState from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from geometry_msgs.msg import TransformStamped You need to import rospy if you are writing a ROS Node . Import other python modules needed for this node. Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module the provides various Python scripts used across stretch_ros . In this instance we are importing the hello_misc script. def __init__ ( self ): \"\"\" A function that initializes the subscriber and other needed variables. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . joint_states_sub = rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) self . transform_pub = rospy . Publisher ( 'ArUco_transform' , TransformStamped , queue_size = 10 ) self . joint_state = None The LocateArUcoTag class inherits the HelloNode class from hm and is instantiated. Set up a subscriber with rospy.Subscriber('/stretch/joint_states', JointState, self.joint_states_callback) . We're going to subscribe to the topic \" stretch/joint_states \", looking for JointState messages. When a message comes in, ROS is going to pass it to the function joint_states_callback() automatically. rospy.Publisher('ArUco_transform', TransformStamped, queue_size=10) declares that your node is publishing to the ArUco_transform topic using the message type TransformStamped . The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. self . min_pan_position = - 4.10 self . max_pan_position = 1.50 self . pan_num_steps = 10 self . pan_step_size = abs ( self . min_pan_position - self . max_pan_position ) / self . pan_num_steps Provide the minimum and maximum joint positions for the head pan. These values are needed for sweeping the head to search for the ArUco tag. We also define the number of steps for the sweep, then create the step size for the head pan joint. self . min_tilt_position = - 0.75 self . tilt_num_steps = 3 self . tilt_step_size = pi / 16 Set the minimum position of the tilt joint, the number of steps, and the size of each step. self . rot_vel = 0.5 # radians per sec Define the head actuation rotational velocity. def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg The joint_states_callback() function stores Stretch's joint states. def send_command ( self , command ): ''' Handles single joint control commands by constructing a FollowJointTrajectoryGoal message and sending it to the trajectory_client created in hello_misc. :param self: The self reference. :param command: A dictionary message type. ''' if ( self . joint_state is not None ) and ( command is not None ): joint_name = command [ 'joint' ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ joint_name ] point = JointTrajectoryPoint () Assign trajectory_goal as a FollowJointTrajectoryGoal message type. Then extract the string value from the joint key. Also, assign point as a JointTrajectoryPoint message type. if 'delta' in command : joint_index = self . joint_state . name . index ( joint_name ) joint_value = self . joint_state . position [ joint_index ] delta = command [ 'delta' ] new_value = joint_value + delta point . positions = [ new_value ] Check to see if delta is a key in the command dictionary. Then get the current position of the joint and add the delta as a a new position value. elif 'position' in command : point . positions = [ command [ 'position' ]] Check to see if position is a key in the command dictionary. Then extract the position value. point . velocities = [ self . rot_vel ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) self . trajectory_client . wait_for_result () Then trajectory_goal.trajectory.points is defined by the positions set in point . Specify the coordinate frame that we want ( base_link ) and set the time to be now. Make the action call and send the goal. The last line of code waits for the result before it exits the python script. def find_tag ( self , tag_name = 'docking_station' ): \"\"\" A function that actuates the camera to search for a defined ArUco tag marker. Then the function returns the pose :param self: The self reference. :param tag_name: A string value of the ArUco marker name. :returns transform: The docking station's TransformStamped message. \"\"\" pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'position' : self . min_tilt_position } self . send_command ( tilt_command ) Create a dictionaries to get the head in its initial position for its search and send the commands the the send_command() function. for i in range ( self . tilt_num_steps ): for j in range ( self . pan_num_steps ): pan_command = { 'joint' : 'joint_head_pan' , 'delta' : self . pan_step_size } self . send_command ( pan_command ) rospy . sleep ( 0.5 ) Utilize nested for loop to sweep the pan and tilt in increments. Then update the joint_head_pan position by the pan_step_size . Use rospy.sleep() function to give time for system to do a Transform lookup before next step. try : transform = self . tf_buffer . lookup_transform ( 'base_link' , tag_name , rospy . Time ()) rospy . loginfo ( \"Found Requested Tag: \\n %s \" , transform ) self . transform_pub . publish ( transform ) return transform except ( tf2_ros . LookupException , tf2_ros . ConnectivityException , tf2_ros . ExtrapolationException ): continue Use a try-except block to look up the transform between the base_link and requested ArUco tag. Then publish and return the TransformStamped message. pan_command = { 'joint' : 'joint_head_pan' , 'position' : self . min_pan_position } self . send_command ( pan_command ) tilt_command = { 'joint' : 'joint_head_tilt' , 'delta' : self . tilt_step_size } self . send_command ( tilt_command ) rospy . sleep ( .25 ) Begin sweep with new tilt angle. def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'aruco_tag_locator' , 'aruco_tag_locator' , wait_for_first_pointcloud = False ) Create a funcion, main() , to do all of the setup for the hm.HelloNode class and initialize the aruco_tag_locator node. self . static_broadcaster = tf2_ros . StaticTransformBroadcaster () self . tf_buffer = tf2_ros . Buffer () self . listener = tf2_ros . TransformListener ( self . tf_buffer ) rospy . sleep ( 1.0 ) Create a StaticTranformBoradcaster Node. Also, start a tf buffer that will store the tf information for a few seconds.Then set up a tf listener, which will subscribe to all of the relevant tf topics, and keep track of the information. Include rospy.sleep(1.0) to give the listener some time to accumulate transforms. rospy . loginfo ( 'Searching for docking ArUco tag.' ) pose = self . find_tag ( \"docking_station\" ) Notify Stretch is searching for the ArUco tag with a rospy.loginfo() function. Then search for the ArUco marker for the docking station. if __name__ == '__main__' : try : node = LocateArUcoTag () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare LocateArUcoTag object. Then run the main() method.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_13/","text":"Example 13 In this example, we will be utilizing the move_base ROS node , a component of the ROS navigation stack to send base goals to the Stretch robot. Build a map First, begin by building a map of the space the robot will be navigating in. If you need a refresher on how to do this, then check out the Navigation Stack tutorial . Getting Started Next, with your created map, we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Where ${HELLO_FLEET_PATH} is the path of the <map_name>.yaml file. IMPORTANT NOTE: It's likely that the robot's location in the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. Below is a gif for reference. Now we are going to use a node to send a a move base goal half a meter in front of the map's origin. run the following command to execute the navigation.py node. # Terminal 2 cd catkin_ws/src/stretch_tutorials/src/ python navigation.py The Code #!/usr/bin/env python import rospy import actionlib import sys from move_base_msgs.msg import MoveBaseAction , MoveBaseGoal from geometry_msgs.msg import Quaternion from tf import transformations class StretchNavigation : \"\"\" A simple encapsulation of the navigation stack for a Stretch robot. \"\"\" def __init__ ( self ): \"\"\" Create an instance of the simple navigation interface. :param self: The self reference. \"\"\" self . client = actionlib . SimpleActionClient ( 'move_base' , MoveBaseAction ) self . client . wait_for_server () rospy . loginfo ( ' {0} : Made contact with move_base server' . format ( self . __class__ . __name__ )) self . goal = MoveBaseGoal () self . goal . target_pose . header . frame_id = 'map' self . goal . target_pose . header . stamp = rospy . Time () self . goal . target_pose . pose . position . x = 0.0 self . goal . target_pose . pose . position . y = 0.0 self . goal . target_pose . pose . position . z = 0.0 self . goal . target_pose . pose . orientation . x = 0.0 self . goal . target_pose . pose . orientation . y = 0.0 self . goal . target_pose . pose . orientation . z = 0.0 self . goal . target_pose . pose . orientation . w = 1.0 def get_quaternion ( self , theta ): \"\"\" A function to build Quaternians from Euler angles. Since the Stretch only rotates around z, we can zero out the other angles. :param theta: The angle (radians) the robot makes with the x-axis. \"\"\" return Quaternion ( * transformations . quaternion_from_euler ( 0.0 , 0.0 , theta )) def go_to ( self , x , y , theta ): \"\"\" Drive the robot to a particular pose on the map. The Stretch only needs (x, y) coordinates and a heading. :param x: x coordinate in the map frame. :param y: y coordinate in the map frame. :param theta: heading (angle with the x-axis in the map frame) \"\"\" rospy . loginfo ( ' {0} : Heading for ( {1} , {2} ) at {3} radians' . format ( self . __class__ . __name__ , x , y , theta )) self . goal . target_pose . pose . position . x = x self . goal . target_pose . pose . position . y = y self . goal . target_pose . pose . orientation = self . get_quaternion ( theta ) self . client . send_goal ( self . goal , done_cb = self . done_callback ) self . client . wait_for_result () def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. :param self: The self reference. :param status: status attribute from MoveBaseActionResult message. :param result: result attribute from MoveBaseActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( ' {0} : SUCCEEDED in reaching the goal.' . format ( self . __class__ . __name__ )) else : rospy . loginfo ( ' {0} : FAILED in reaching the goal.' . format ( self . __class__ . __name__ )) if __name__ == '__main__' : rospy . init_node ( 'navigation' , argv = sys . argv ) nav = StretchNavigation () nav . go_to ( 0.5 , 0.0 , 0.0 ) The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import actionlib import sys from move_base_msgs.msg import MoveBaseAction , MoveBaseGoal from geometry_msgs.msg import Quaternion from tf import transformations You need to import rospy if you are writing a ROS Node . self . client = actionlib . SimpleActionClient ( 'move_base' , MoveBaseAction ) self . client . wait_for_server () rospy . loginfo ( ' {0} : Made contact with move_base server' . format ( self . __class__ . __name__ )) Set up a client for the navigation action. On the Stretch, this is called move_base , and has type MoveBaseAction . Once we make the client, we wait for the server to be ready. self . goal = MoveBaseGoal () self . goal . target_pose . header . frame_id = 'map' self . goal . target_pose . header . stamp = rospy . Time () Make a goal for the action. Specify the coordinate frame that we want, in this instance the map . Then we set the time to be now. self . goal . target_pose . pose . position . x = 0.0 self . goal . target_pose . pose . position . y = 0.0 self . goal . target_pose . pose . position . z = 0.0 self . goal . target_pose . pose . orientation . x = 0.0 self . goal . target_pose . pose . orientation . y = 0.0 self . goal . target_pose . pose . orientation . z = 0.0 self . goal . target_pose . pose . orientation . w = 1.0 Initialize a position in the coordinate frame. def get_quaternion ( self , theta ): \"\"\" A function to build Quaternians from Euler angles. Since the Stretch only rotates around z, we can zero out the other angles. :param theta: The angle (radians) the robot makes with the x-axis. \"\"\" return Quaternion ( * transformations . quaternion_from_euler ( 0.0 , 0.0 , theta )) A function that transforms Euler angles to quaternions and returns those values. def go_to ( self , x , y , theta , wait = False ): \"\"\" Drive the robot to a particular pose on the map. The Stretch only needs (x, y) coordinates and a heading. :param x: x coordinate in the map frame. :param y: y coordinate in the map frame. :param theta: heading (angle with the x-axis in the map frame) \"\"\" rospy . loginfo ( ' {0} : Heading for ( {1} , {2} ) at {3} radians' . format ( self . __class__ . __name__ , x , y , theta )) The go_to() function takes in the 3 arguments, the x and y coordinates in the map frame, and the heading. self . goal . target_pose . pose . position . x = x self . goal . target_pose . pose . position . y = y self . goal . target_pose . pose . orientation = self . get_quaternion ( theta ) The MoveBaseGoal() data structure has three goal positions (in meters), along each of the axes. For Stretch, it will only pay attention to the x and y coordinates, since it can't move in the z direction. self . client . send_goal ( self . goal , done_cb = self . done_callback ) self . client . wait_for_result () Send the goal and include the done_callback() function in one of the arguments in send_goal() . def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. :param self: The self reference. :param status: status attribute from MoveBaseActionResult message. :param result: result attribute from MoveBaseActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( ' {0} : SUCCEEDED in reaching the goal.' . format ( self . __class__ . __name__ )) else : rospy . loginfo ( ' {0} : FAILED in reaching the goal.' . format ( self . __class__ . __name__ )) Conditional statement to print whether the goal status in the MoveBaseActionResult succeeded or failed. rospy . init_node ( 'navigation' , argv = sys . argv ) nav = StretchNavigation () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Declare the StretchNavigation object. nav . go_to ( 0.5 , 0.0 , 0.0 ) Send a move base goal half a meter in front of the map's origin.","title":"Example 13"},{"location":"stretch-tutorials/ros1_melodic/example_13/#example-13","text":"In this example, we will be utilizing the move_base ROS node , a component of the ROS navigation stack to send base goals to the Stretch robot.","title":"Example 13"},{"location":"stretch-tutorials/ros1_melodic/example_13/#build-a-map","text":"First, begin by building a map of the space the robot will be navigating in. If you need a refresher on how to do this, then check out the Navigation Stack tutorial .","title":"Build a map"},{"location":"stretch-tutorials/ros1_melodic/example_13/#getting-started","text":"Next, with your created map, we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Where ${HELLO_FLEET_PATH} is the path of the <map_name>.yaml file. IMPORTANT NOTE: It's likely that the robot's location in the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. Below is a gif for reference. Now we are going to use a node to send a a move base goal half a meter in front of the map's origin. run the following command to execute the navigation.py node. # Terminal 2 cd catkin_ws/src/stretch_tutorials/src/ python navigation.py","title":"Getting Started"},{"location":"stretch-tutorials/ros1_melodic/example_13/#the-code","text":"#!/usr/bin/env python import rospy import actionlib import sys from move_base_msgs.msg import MoveBaseAction , MoveBaseGoal from geometry_msgs.msg import Quaternion from tf import transformations class StretchNavigation : \"\"\" A simple encapsulation of the navigation stack for a Stretch robot. \"\"\" def __init__ ( self ): \"\"\" Create an instance of the simple navigation interface. :param self: The self reference. \"\"\" self . client = actionlib . SimpleActionClient ( 'move_base' , MoveBaseAction ) self . client . wait_for_server () rospy . loginfo ( ' {0} : Made contact with move_base server' . format ( self . __class__ . __name__ )) self . goal = MoveBaseGoal () self . goal . target_pose . header . frame_id = 'map' self . goal . target_pose . header . stamp = rospy . Time () self . goal . target_pose . pose . position . x = 0.0 self . goal . target_pose . pose . position . y = 0.0 self . goal . target_pose . pose . position . z = 0.0 self . goal . target_pose . pose . orientation . x = 0.0 self . goal . target_pose . pose . orientation . y = 0.0 self . goal . target_pose . pose . orientation . z = 0.0 self . goal . target_pose . pose . orientation . w = 1.0 def get_quaternion ( self , theta ): \"\"\" A function to build Quaternians from Euler angles. Since the Stretch only rotates around z, we can zero out the other angles. :param theta: The angle (radians) the robot makes with the x-axis. \"\"\" return Quaternion ( * transformations . quaternion_from_euler ( 0.0 , 0.0 , theta )) def go_to ( self , x , y , theta ): \"\"\" Drive the robot to a particular pose on the map. The Stretch only needs (x, y) coordinates and a heading. :param x: x coordinate in the map frame. :param y: y coordinate in the map frame. :param theta: heading (angle with the x-axis in the map frame) \"\"\" rospy . loginfo ( ' {0} : Heading for ( {1} , {2} ) at {3} radians' . format ( self . __class__ . __name__ , x , y , theta )) self . goal . target_pose . pose . position . x = x self . goal . target_pose . pose . position . y = y self . goal . target_pose . pose . orientation = self . get_quaternion ( theta ) self . client . send_goal ( self . goal , done_cb = self . done_callback ) self . client . wait_for_result () def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. :param self: The self reference. :param status: status attribute from MoveBaseActionResult message. :param result: result attribute from MoveBaseActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( ' {0} : SUCCEEDED in reaching the goal.' . format ( self . __class__ . __name__ )) else : rospy . loginfo ( ' {0} : FAILED in reaching the goal.' . format ( self . __class__ . __name__ )) if __name__ == '__main__' : rospy . init_node ( 'navigation' , argv = sys . argv ) nav = StretchNavigation () nav . go_to ( 0.5 , 0.0 , 0.0 )","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_13/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import actionlib import sys from move_base_msgs.msg import MoveBaseAction , MoveBaseGoal from geometry_msgs.msg import Quaternion from tf import transformations You need to import rospy if you are writing a ROS Node . self . client = actionlib . SimpleActionClient ( 'move_base' , MoveBaseAction ) self . client . wait_for_server () rospy . loginfo ( ' {0} : Made contact with move_base server' . format ( self . __class__ . __name__ )) Set up a client for the navigation action. On the Stretch, this is called move_base , and has type MoveBaseAction . Once we make the client, we wait for the server to be ready. self . goal = MoveBaseGoal () self . goal . target_pose . header . frame_id = 'map' self . goal . target_pose . header . stamp = rospy . Time () Make a goal for the action. Specify the coordinate frame that we want, in this instance the map . Then we set the time to be now. self . goal . target_pose . pose . position . x = 0.0 self . goal . target_pose . pose . position . y = 0.0 self . goal . target_pose . pose . position . z = 0.0 self . goal . target_pose . pose . orientation . x = 0.0 self . goal . target_pose . pose . orientation . y = 0.0 self . goal . target_pose . pose . orientation . z = 0.0 self . goal . target_pose . pose . orientation . w = 1.0 Initialize a position in the coordinate frame. def get_quaternion ( self , theta ): \"\"\" A function to build Quaternians from Euler angles. Since the Stretch only rotates around z, we can zero out the other angles. :param theta: The angle (radians) the robot makes with the x-axis. \"\"\" return Quaternion ( * transformations . quaternion_from_euler ( 0.0 , 0.0 , theta )) A function that transforms Euler angles to quaternions and returns those values. def go_to ( self , x , y , theta , wait = False ): \"\"\" Drive the robot to a particular pose on the map. The Stretch only needs (x, y) coordinates and a heading. :param x: x coordinate in the map frame. :param y: y coordinate in the map frame. :param theta: heading (angle with the x-axis in the map frame) \"\"\" rospy . loginfo ( ' {0} : Heading for ( {1} , {2} ) at {3} radians' . format ( self . __class__ . __name__ , x , y , theta )) The go_to() function takes in the 3 arguments, the x and y coordinates in the map frame, and the heading. self . goal . target_pose . pose . position . x = x self . goal . target_pose . pose . position . y = y self . goal . target_pose . pose . orientation = self . get_quaternion ( theta ) The MoveBaseGoal() data structure has three goal positions (in meters), along each of the axes. For Stretch, it will only pay attention to the x and y coordinates, since it can't move in the z direction. self . client . send_goal ( self . goal , done_cb = self . done_callback ) self . client . wait_for_result () Send the goal and include the done_callback() function in one of the arguments in send_goal() . def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. :param self: The self reference. :param status: status attribute from MoveBaseActionResult message. :param result: result attribute from MoveBaseActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( ' {0} : SUCCEEDED in reaching the goal.' . format ( self . __class__ . __name__ )) else : rospy . loginfo ( ' {0} : FAILED in reaching the goal.' . format ( self . __class__ . __name__ )) Conditional statement to print whether the goal status in the MoveBaseActionResult succeeded or failed. rospy . init_node ( 'navigation' , argv = sys . argv ) nav = StretchNavigation () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Declare the StretchNavigation object. nav . go_to ( 0.5 , 0.0 , 0.0 ) Send a move base goal half a meter in front of the map's origin.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_2/","text":"Example 2 The aim of this example is to provide instruction on how to filter scan messages. For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specification itself: # Laser scans angles are measured counter clockwise, with Stretch's LiDAR having # both angle_min and angle_max facing forward (very closely along the x-axis) Header header float32 angle_min # start angle of the scan [rad] float32 angle_max # end angle of the scan [rad] float32 angle_increment # angular distance between measurements [rad] float32 time_increment # time between measurements [seconds] float32 scan_time # time between scans [seconds] float32 range_min # minimum range value [m] float32 range_max # maximum range value [m] float32[] ranges # range data [m] (Note: values < range_min or > range_max should be discarded) float32[] intensities # intensity data [device-specific units] The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type. For a Stretch robot the start angle of the scan, angle_min , and end angle, angle_max , are closely located along the x-axis of Stretch's frame. angle_min and angle_max are set at -3.1416 and 3.1416 , respectively. This is illustrated by the images below. Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot. First, open a terminal and run the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then in a new terminal run the rplidar.launch file from stretch_core . # Terminal 2 roslaunch stretch_core rplidar.launch To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan_filter.py node by typing the following in a new terminal. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python scan_filter.py Then run the following command to bring up a simple RViz configuration of the Stretch robot. # Terminal 4 rosrun rviz rviz -d ` rospack find stretch_core ` /rviz/stretch_simple_test.rviz Change the topic name from the LaserScan display from /scan to /filter_scan . The Code #!/usr/bin/env python import rospy from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan class ScanFilter : \"\"\" A class that implements a LaserScan filter that removes all of the points that are not in front of the robot. \"\"\" def __init__ ( self ): self . width = 1.0 self . extent = self . width / 2.0 self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . callback ) self . pub = rospy . Publisher ( 'filtered_scan' , LaserScan , queue_size = 10 ) rospy . loginfo ( \"Publishing the filtered_scan topic. Use RViz to visualize.\" ) def callback ( self , msg ): \"\"\" Callback function to deal with incoming LaserScan messages. :param self: The self reference. :param msg: The subscribed LaserScan message. :publishes msg: updated LaserScan message. \"\"\" angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] msg . ranges = new_ranges self . pub . publish ( msg ) if __name__ == '__main__' : rospy . init_node ( 'scan_filter' ) ScanFilter () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan You need to import rospy if you are writing a ROS Node . There are functions from numpy and math that are required within this code, thus why linspace, inf, and sin are imported. The sensor_msgs.msg import is so that we can subscribe and publish LaserScan messages. self . width = 1 self . extent = self . width / 2.0 We're going to assume that the robot is pointing up the x-axis, so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered. self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . callback ) Set up a subscriber. We're going to subscribe to the topic scan , looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. self . pub = rospy . Publisher ( 'filtered_scan' , LaserScan , queue_size = 10 ) pub = rospy.Publisher(\"filtered_scan\", LaserScan, queue_size=10) declares that your node is publishing to the filtered_scan topic using the message type LaserScan . This lets the master tell any nodes listening on filtered_scan that we are going to publish data on that topic. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) This line of code utilizes linspace to compute each angle of the subscribed scan. points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". msg . ranges = new_ranges self . pub . publish ( msg ) Substitute in the new ranges in the original message, and republish it. rospy . init_node ( 'scan_filter' ) ScanFilter () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the class with ScanFilter() rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Example 2"},{"location":"stretch-tutorials/ros1_melodic/example_2/#example-2","text":"The aim of this example is to provide instruction on how to filter scan messages. For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specification itself: # Laser scans angles are measured counter clockwise, with Stretch's LiDAR having # both angle_min and angle_max facing forward (very closely along the x-axis) Header header float32 angle_min # start angle of the scan [rad] float32 angle_max # end angle of the scan [rad] float32 angle_increment # angular distance between measurements [rad] float32 time_increment # time between measurements [seconds] float32 scan_time # time between scans [seconds] float32 range_min # minimum range value [m] float32 range_max # maximum range value [m] float32[] ranges # range data [m] (Note: values < range_min or > range_max should be discarded) float32[] intensities # intensity data [device-specific units] The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type. For a Stretch robot the start angle of the scan, angle_min , and end angle, angle_max , are closely located along the x-axis of Stretch's frame. angle_min and angle_max are set at -3.1416 and 3.1416 , respectively. This is illustrated by the images below. Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot. First, open a terminal and run the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then in a new terminal run the rplidar.launch file from stretch_core . # Terminal 2 roslaunch stretch_core rplidar.launch To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan_filter.py node by typing the following in a new terminal. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python scan_filter.py Then run the following command to bring up a simple RViz configuration of the Stretch robot. # Terminal 4 rosrun rviz rviz -d ` rospack find stretch_core ` /rviz/stretch_simple_test.rviz Change the topic name from the LaserScan display from /scan to /filter_scan .","title":"Example 2"},{"location":"stretch-tutorials/ros1_melodic/example_2/#the-code","text":"#!/usr/bin/env python import rospy from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan class ScanFilter : \"\"\" A class that implements a LaserScan filter that removes all of the points that are not in front of the robot. \"\"\" def __init__ ( self ): self . width = 1.0 self . extent = self . width / 2.0 self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . callback ) self . pub = rospy . Publisher ( 'filtered_scan' , LaserScan , queue_size = 10 ) rospy . loginfo ( \"Publishing the filtered_scan topic. Use RViz to visualize.\" ) def callback ( self , msg ): \"\"\" Callback function to deal with incoming LaserScan messages. :param self: The self reference. :param msg: The subscribed LaserScan message. :publishes msg: updated LaserScan message. \"\"\" angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] msg . ranges = new_ranges self . pub . publish ( msg ) if __name__ == '__main__' : rospy . init_node ( 'scan_filter' ) ScanFilter () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_2/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan You need to import rospy if you are writing a ROS Node . There are functions from numpy and math that are required within this code, thus why linspace, inf, and sin are imported. The sensor_msgs.msg import is so that we can subscribe and publish LaserScan messages. self . width = 1 self . extent = self . width / 2.0 We're going to assume that the robot is pointing up the x-axis, so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered. self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . callback ) Set up a subscriber. We're going to subscribe to the topic scan , looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. self . pub = rospy . Publisher ( 'filtered_scan' , LaserScan , queue_size = 10 ) pub = rospy.Publisher(\"filtered_scan\", LaserScan, queue_size=10) declares that your node is publishing to the filtered_scan topic using the message type LaserScan . This lets the master tell any nodes listening on filtered_scan that we are going to publish data on that topic. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) This line of code utilizes linspace to compute each angle of the subscribed scan. points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". msg . ranges = new_ranges self . pub . publish ( msg ) Substitute in the new ranges in the original message, and republish it. rospy . init_node ( 'scan_filter' ) ScanFilter () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the class with ScanFilter() rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_3/","text":"Example 3 The aim of example 3 is to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward. Begin by running the following commands in a new terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then in a new terminal type the following to activate the LiDAR sensor. # Terminal 2 roslaunch stretch_core rplidar.launch To set navigation mode and to activate the avoider.py node, type the following in a new terminal. # Terminal 3 rosservice call /switch_to_navigation_mode cd catkin_ws/src/stretch_tutorials/src/ python avoider.py To stop the node from sending twist messages, type Ctrl + c in the terminal running the avoider node. The Code #!/usr/bin/env python import rospy from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan class Avoider : \"\"\" A class that implements both a LaserScan filter and base velocity control for collision avoidance. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber, publisher, and marker features. :param self: The self reference. \"\"\" self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . set_speed ) self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 def set_speed ( self , msg ): \"\"\" Callback function to deal with incoming LaserScan messages. :param self: The self reference. :param msg: The subscribed LaserScan message. :publishes self.twist: Twist message. \"\"\" angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] error = min ( new_ranges ) - self . distance self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 self . pub . publish ( self . twist ) if __name__ == '__main__' : rospy . init_node ( 'avoider' ) Avoider () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan You need to import rospy if you are writing a ROS Node . There are functions from numpy and math that are required within this code, thus linspace, inf, tanh, and sin are imported. The sensor_msgs.msg import is so that we can subscribe to LaserScan messages. The geometry_msgs.msg import is so that we can send velocity commands to the robot. self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist . self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . set_speed ) Set up a subscriber. We're going to subscribe to the topic \" scan \", looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function \"set_speed\" automatically. self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self.width is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing in the x-axis, any points with y coordinates further than half of the defined width ( self.extent ) from the x-axis are not considered. self.distance defines the stopping distance from an object. self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 Allocate a Twist to use, and set everything to zero. We're going to do this when the class is initiating. Redefining this within the callback function, set_speed() can be more computationally taxing. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] This line of code utilizes linspace to compute each angle of the subscribed scan. Here we compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". error = min ( new_ranges ) - self . distance Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as error . self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 Set the speed according to a tanh function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1 self . pub . publish ( self . twist ) Publish the Twist message. rospy . init_node ( 'avoider' ) Avoider () rospy . spin () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate class with Avioder() Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Example 3"},{"location":"stretch-tutorials/ros1_melodic/example_3/#example-3","text":"The aim of example 3 is to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward. Begin by running the following commands in a new terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then in a new terminal type the following to activate the LiDAR sensor. # Terminal 2 roslaunch stretch_core rplidar.launch To set navigation mode and to activate the avoider.py node, type the following in a new terminal. # Terminal 3 rosservice call /switch_to_navigation_mode cd catkin_ws/src/stretch_tutorials/src/ python avoider.py To stop the node from sending twist messages, type Ctrl + c in the terminal running the avoider node.","title":"Example 3"},{"location":"stretch-tutorials/ros1_melodic/example_3/#the-code","text":"#!/usr/bin/env python import rospy from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan class Avoider : \"\"\" A class that implements both a LaserScan filter and base velocity control for collision avoidance. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber, publisher, and marker features. :param self: The self reference. \"\"\" self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . set_speed ) self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 def set_speed ( self , msg ): \"\"\" Callback function to deal with incoming LaserScan messages. :param self: The self reference. :param msg: The subscribed LaserScan message. :publishes self.twist: Twist message. \"\"\" angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] error = min ( new_ranges ) - self . distance self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 self . pub . publish ( self . twist ) if __name__ == '__main__' : rospy . init_node ( 'avoider' ) Avoider () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_3/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan You need to import rospy if you are writing a ROS Node . There are functions from numpy and math that are required within this code, thus linspace, inf, tanh, and sin are imported. The sensor_msgs.msg import is so that we can subscribe to LaserScan messages. The geometry_msgs.msg import is so that we can send velocity commands to the robot. self . pub = rospy . Publisher ( '/stretch/cmd_vel' , Twist , queue_size = 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"/stretch/cmd_vel\", Twist, queue_size=1) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist . self . sub = rospy . Subscriber ( '/scan' , LaserScan , self . set_speed ) Set up a subscriber. We're going to subscribe to the topic \" scan \", looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function \"set_speed\" automatically. self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self.width is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing in the x-axis, any points with y coordinates further than half of the defined width ( self.extent ) from the x-axis are not considered. self.distance defines the stopping distance from an object. self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 Allocate a Twist to use, and set everything to zero. We're going to do this when the class is initiating. Redefining this within the callback function, set_speed() can be more computationally taxing. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] This line of code utilizes linspace to compute each angle of the subscribed scan. Here we compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". error = min ( new_ranges ) - self . distance Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as error . self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 Set the speed according to a tanh function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1 self . pub . publish ( self . twist ) Publish the Twist message. rospy . init_node ( 'avoider' ) Avoider () rospy . spin () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate class with Avioder() Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_4/","text":"Example 4 Let's bringup stretch in the willowgarage world from the gazebo basics tutorial and RViz by using the following command. # Terminal 1 roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world rviz: = true the rviz flag will open an RViz window to visualize a variety of ROS topics. In a new terminal run the following commands to execute the marker.py node. # Terminal 2 cd catkin_ws/src/stretch_tutorials/src/ python marker.py The gif below demonstrates how to add a new Marker display type, and change the topic name from /visualization_marker to /balloon . A red sphere Marker should appear above the Stretch robot. The Code #!/usr/bin/env python import rospy from visualization_msgs.msg import Marker class Balloon (): \"\"\" A class that attaches a Sphere marker directly above the Stretch robot. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the marker's features. :param self: The self reference. \"\"\" self . publisher = rospy . Publisher ( 'balloon' , Marker , queue_size = 10 ) self . marker = Marker () self . marker . header . frame_id = 'base_link' self . marker . header . stamp = rospy . Time () self . marker . type = self . marker . SPHERE self . marker . id = 0 self . marker . action = self . marker . ADD self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 self . marker . color . a = 1.0 self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 rospy . loginfo ( \"Publishing the balloon topic. Use RViz to visualize.\" ) def publish_marker ( self ): \"\"\" Function that publishes the sphere marker. :param self: The self reference. :publishes self.marker: Marker message. \"\"\" self . publisher . publish ( self . marker ) if __name__ == '__main__' : rospy . init_node ( 'marker' ) balloon = Balloon () rate = rospy . Rate ( 10 ) while not rospy . is_shutdown (): balloon . publish_marker () rate . sleep () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from visualization_msgs.msg import Marker You need to import rospy if you are writing a ROS Node . Import the Marker type from the visualization_msgs.msg package. This import is required to publish a Marker , which will be visualized in RViz. self . pub = rospy . Publisher ( 'balloon' , Marker , queue_size = 10 ) This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"balloon\", Twist, queue_size=1) declares that your node is publishing to the /ballon topic using the message type Twist . self . marker = Marker () self . marker . header . frame_id = 'base_link' self . marker . header . stamp = rospy . Time () self . marker . type = self . marker . SPHERE Create a Marker() message type. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers self . marker . id = 0 Each marker has a unique ID number. If you have more than one marker that you want displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number of an existing marker, it will replace the existing marker with that ID number. self . marker . action = self . marker . ADD This line of code sets the action. You can add, delete, or modify markers. self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 These are the size parameters for the marker. These will vary by marker type. self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 Color of the object, specified as r/g/b/a, with values in the range of [0, 1]. self . marker . color . a = 1.0 The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible. self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in frame_id . In this case, the position will always be directly 2 meters above the frame_id ( base_link ), and will move with it. def publish_marker ( self ): self . publisher . publish ( self . marker ) Publish the Marker data structure to be visualized in RViz. rospy . init_node ( 'marker' , argv = sys . argv ) balloon = Balloon () rate = rospy . Rate ( 10 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate class with Balloon() Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages. while not rospy . is_shutdown (): balloon . publish_marker () rate . sleep () This loop is a fairly standard rospy construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl-C or otherwise). The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop.","title":"Example 4"},{"location":"stretch-tutorials/ros1_melodic/example_4/#example-4","text":"Let's bringup stretch in the willowgarage world from the gazebo basics tutorial and RViz by using the following command. # Terminal 1 roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world rviz: = true the rviz flag will open an RViz window to visualize a variety of ROS topics. In a new terminal run the following commands to execute the marker.py node. # Terminal 2 cd catkin_ws/src/stretch_tutorials/src/ python marker.py The gif below demonstrates how to add a new Marker display type, and change the topic name from /visualization_marker to /balloon . A red sphere Marker should appear above the Stretch robot.","title":"Example 4"},{"location":"stretch-tutorials/ros1_melodic/example_4/#the-code","text":"#!/usr/bin/env python import rospy from visualization_msgs.msg import Marker class Balloon (): \"\"\" A class that attaches a Sphere marker directly above the Stretch robot. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the marker's features. :param self: The self reference. \"\"\" self . publisher = rospy . Publisher ( 'balloon' , Marker , queue_size = 10 ) self . marker = Marker () self . marker . header . frame_id = 'base_link' self . marker . header . stamp = rospy . Time () self . marker . type = self . marker . SPHERE self . marker . id = 0 self . marker . action = self . marker . ADD self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 self . marker . color . a = 1.0 self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 rospy . loginfo ( \"Publishing the balloon topic. Use RViz to visualize.\" ) def publish_marker ( self ): \"\"\" Function that publishes the sphere marker. :param self: The self reference. :publishes self.marker: Marker message. \"\"\" self . publisher . publish ( self . marker ) if __name__ == '__main__' : rospy . init_node ( 'marker' ) balloon = Balloon () rate = rospy . Rate ( 10 ) while not rospy . is_shutdown (): balloon . publish_marker () rate . sleep ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_4/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy from visualization_msgs.msg import Marker You need to import rospy if you are writing a ROS Node . Import the Marker type from the visualization_msgs.msg package. This import is required to publish a Marker , which will be visualized in RViz. self . pub = rospy . Publisher ( 'balloon' , Marker , queue_size = 10 ) This section of code defines the talker's interface to the rest of ROS. pub = rospy.Publisher(\"balloon\", Twist, queue_size=1) declares that your node is publishing to the /ballon topic using the message type Twist . self . marker = Marker () self . marker . header . frame_id = 'base_link' self . marker . header . stamp = rospy . Time () self . marker . type = self . marker . SPHERE Create a Marker() message type. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers self . marker . id = 0 Each marker has a unique ID number. If you have more than one marker that you want displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number of an existing marker, it will replace the existing marker with that ID number. self . marker . action = self . marker . ADD This line of code sets the action. You can add, delete, or modify markers. self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 These are the size parameters for the marker. These will vary by marker type. self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 Color of the object, specified as r/g/b/a, with values in the range of [0, 1]. self . marker . color . a = 1.0 The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible. self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in frame_id . In this case, the position will always be directly 2 meters above the frame_id ( base_link ), and will move with it. def publish_marker ( self ): self . publisher . publish ( self . marker ) Publish the Marker data structure to be visualized in RViz. rospy . init_node ( 'marker' , argv = sys . argv ) balloon = Balloon () rate = rospy . Rate ( 10 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate class with Balloon() Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages. while not rospy . is_shutdown (): balloon . publish_marker () rate . sleep () This loop is a fairly standard rospy construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl-C or otherwise). The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_5/","text":"Example 5 In this example, we will review a Python script that prints out the positions of a selected group of Stretch's joints. This script is helpful if you need the joint positions after you teleoperated Stretch with the Xbox controller or physically moved the robot to the desired configuration after hitting the run stop button. If you are looking for a continuous print of the joint states while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial . Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch You can then hit the run-stop button (you should hear a beep and the LED light in the button blink) and move the robot's joints to a desired configuration. Once you are satisfied with the configuration, hold the run-stop button until you hear a beep. Then run the following command to excecute the joint_state_printer.py which will print the joint positions of the lift, arm, and wrist. cd catkin_ws/src/stretch_tutorials/src/ python3 joint_state_printer.py Your terminal will output the position information of the previously mentioned joints shown below. name: ['joint_lift', 'wrist_extension', 'joint_wrist_yaw'] position: [0.6043133175850597, 0.19873586673129257, 0.017257283863713464] IMPORTANT NOTE: Stretch's arm has 4 prismatic joints and the sum of these positions gives the wrist_extension distance. The wrist_extension is needed when sending joint trajectory commands to the robot. Further, you can not actuate an individual arm joint. Here is an image of the arm joints for reference: The Code #!/usr/bin/env python import rospy import sys from sensor_msgs.msg import JointState class JointStatePublisher (): \"\"\" A class that prints the positions of desired joints in Stretch. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber. :param self: The self reference. \"\"\" self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) def callback ( self , msg ): \"\"\" Callback function to deal with the incoming JointState messages. :param self: The self reference. :param msg: The JointState message. \"\"\" self . joint_states = msg def print_states ( self , joints ): \"\"\" print_states function to deal with the incoming JointState messages. :param self: The self reference. :param joints: A list of string values of joint names. \"\"\" joint_positions = [] for joint in joints : if joint == \"wrist_extension\" : index = self . joint_states . name . index ( 'joint_arm_l0' ) joint_positions . append ( 4 * self . joint_states . position [ index ]) continue index = self . joint_states . name . index ( joint ) joint_positions . append ( self . joint_states . position [ index ]) print ( \"name: \" + str ( joints )) print ( \"position: \" + str ( joint_positions )) rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) if __name__ == '__main__' : rospy . init_node ( 'joint_state_printer' , anonymous = True ) JSP = JointStatePublisher () rospy . sleep ( .1 ) joints = [ \"joint_lift\" , \"wrist_extension\" , \"joint_wrist_yaw\" ] #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"] JSP . print_states ( joints ) rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import sys from sensor_msgs.msg import JointState You need to import rospy if you are writing a ROS Node . Import sensor_msgs.msg so that we can subscribe to JointState messages. self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) Set up a subscriber. We're going to subscribe to the topic \" joint_states \", looking for JointState messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically def callback ( self , msg ): self . joint_states = msg This is the callback function where he JointState messages are stored as self.joint_states . Further information about the this message type can be found here: JointState Message def print_states ( self , joints ): joint_positions = [] This is the print_states() function which takes in a list of joints of interest as its argument. the is also an empty list set as joint_positions and this is where the positions of the requested joints will be appended. for joint in joints : if joint == \"wrist_extension\" : index = self . joint_states . name . index ( 'joint_arm_l0' ) joint_positions . append ( 4 * self . joint_states . position [ index ]) continue index = self . joint_states . name . index ( joint ) joint_positions . append ( self . joint_states . position [ index ]) In this section of the code, a forloop is used to parse the names of the requested joints from the self.joint_states list. The index() function returns the index a of the name of the requested joint and appends the respective position to our joint_positions list. rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter. rospy . init_node ( 'joint_state_printer' , anonymous = True ) JSP = JointStatePublisher () rospy . sleep ( .1 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Declare object, JSP , from the JointStatePublisher class. The use of the rospy.sleep() function is to allow the JSP class to initialize all of its features before requesting to publish joint positions of desired joints (running the print_states() method). joints = [ \"joint_lift\" , \"wrist_extension\" , \"joint_wrist_yaw\" ] #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"] JSP . print_states ( joints ) Create a list of the desired joints that you want positions to be printed. Then use that list as an argument for the print_states() method. rospy . spin () Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Example 5"},{"location":"stretch-tutorials/ros1_melodic/example_5/#example-5","text":"In this example, we will review a Python script that prints out the positions of a selected group of Stretch's joints. This script is helpful if you need the joint positions after you teleoperated Stretch with the Xbox controller or physically moved the robot to the desired configuration after hitting the run stop button. If you are looking for a continuous print of the joint states while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial . Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch You can then hit the run-stop button (you should hear a beep and the LED light in the button blink) and move the robot's joints to a desired configuration. Once you are satisfied with the configuration, hold the run-stop button until you hear a beep. Then run the following command to excecute the joint_state_printer.py which will print the joint positions of the lift, arm, and wrist. cd catkin_ws/src/stretch_tutorials/src/ python3 joint_state_printer.py Your terminal will output the position information of the previously mentioned joints shown below. name: ['joint_lift', 'wrist_extension', 'joint_wrist_yaw'] position: [0.6043133175850597, 0.19873586673129257, 0.017257283863713464] IMPORTANT NOTE: Stretch's arm has 4 prismatic joints and the sum of these positions gives the wrist_extension distance. The wrist_extension is needed when sending joint trajectory commands to the robot. Further, you can not actuate an individual arm joint. Here is an image of the arm joints for reference:","title":"Example 5"},{"location":"stretch-tutorials/ros1_melodic/example_5/#the-code","text":"#!/usr/bin/env python import rospy import sys from sensor_msgs.msg import JointState class JointStatePublisher (): \"\"\" A class that prints the positions of desired joints in Stretch. \"\"\" def __init__ ( self ): \"\"\" Function that initializes the subscriber. :param self: The self reference. \"\"\" self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) def callback ( self , msg ): \"\"\" Callback function to deal with the incoming JointState messages. :param self: The self reference. :param msg: The JointState message. \"\"\" self . joint_states = msg def print_states ( self , joints ): \"\"\" print_states function to deal with the incoming JointState messages. :param self: The self reference. :param joints: A list of string values of joint names. \"\"\" joint_positions = [] for joint in joints : if joint == \"wrist_extension\" : index = self . joint_states . name . index ( 'joint_arm_l0' ) joint_positions . append ( 4 * self . joint_states . position [ index ]) continue index = self . joint_states . name . index ( joint ) joint_positions . append ( self . joint_states . position [ index ]) print ( \"name: \" + str ( joints )) print ( \"position: \" + str ( joint_positions )) rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) if __name__ == '__main__' : rospy . init_node ( 'joint_state_printer' , anonymous = True ) JSP = JointStatePublisher () rospy . sleep ( .1 ) joints = [ \"joint_lift\" , \"wrist_extension\" , \"joint_wrist_yaw\" ] #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"] JSP . print_states ( joints ) rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_5/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import sys from sensor_msgs.msg import JointState You need to import rospy if you are writing a ROS Node . Import sensor_msgs.msg so that we can subscribe to JointState messages. self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) Set up a subscriber. We're going to subscribe to the topic \" joint_states \", looking for JointState messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically def callback ( self , msg ): self . joint_states = msg This is the callback function where he JointState messages are stored as self.joint_states . Further information about the this message type can be found here: JointState Message def print_states ( self , joints ): joint_positions = [] This is the print_states() function which takes in a list of joints of interest as its argument. the is also an empty list set as joint_positions and this is where the positions of the requested joints will be appended. for joint in joints : if joint == \"wrist_extension\" : index = self . joint_states . name . index ( 'joint_arm_l0' ) joint_positions . append ( 4 * self . joint_states . position [ index ]) continue index = self . joint_states . name . index ( joint ) joint_positions . append ( self . joint_states . position [ index ]) In this section of the code, a forloop is used to parse the names of the requested joints from the self.joint_states list. The index() function returns the index a of the name of the requested joint and appends the respective position to our joint_positions list. rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter. rospy . init_node ( 'joint_state_printer' , anonymous = True ) JSP = JointStatePublisher () rospy . sleep ( .1 ) The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Declare object, JSP , from the JointStatePublisher class. The use of the rospy.sleep() function is to allow the JSP class to initialize all of its features before requesting to publish joint positions of desired joints (running the print_states() method). joints = [ \"joint_lift\" , \"wrist_extension\" , \"joint_wrist_yaw\" ] #joints = [\"joint_head_pan\",\"joint_head_tilt\", joint_gripper_finger_left\", \"joint_gripper_finger_right\"] JSP . print_states ( joints ) Create a list of the desired joints that you want positions to be printed. Then use that list as an argument for the print_states() method. rospy . spin () Give control to ROS with rospy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_6/","text":"Example 6 In this example, we will review a Python script that prints out and stores the effort values from a specified joint. If you are looking for a continuous print of the joint state efforts while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial . Begin by running the following command in the terminal in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the effort_sensing.py node. # Terminal 2 rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python effort_sensing.py This will send a FollowJointTrajectory command to move Stretch's arm or head while also printing the effort of the lift. The Code #!/usr/bin/env python import rospy import time import actionlib import os import csv from datetime import datetime from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState import hello_helpers.hello_misc as hm class JointActuatorEffortSensor ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self , export_data = False ): \"\"\" Function that initializes the subscriber,and other features. :param self: The self reference. :param export_data: A boolean message type. \"\"\" hm . HelloNode . __init__ ( self ) self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) self . joints = [ 'joint_lift' ] self . joint_effort = [] self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . export_data = export_data def callback ( self , msg ): \"\"\" Callback function to update and store JointState messages. :param self: The self reference. :param msg: The JointState message. \"\"\" self . joint_states = msg def issue_command ( self ): \"\"\" Function that makes an action call and sends joint trajectory goals to a single joint. :param self: The self reference. \"\"\" trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = self . joints point0 = JointTrajectoryPoint () point0 . positions = [ 0.9 ] trajectory_goal . trajectory . points = [ point0 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal , feedback_cb = self . feedback_callback , done_cb = self . done_callback ) rospy . loginfo ( 'Sent position goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def feedback_callback ( self , feedback ): \"\"\" The feedback_callback function deals with the incoming feedback messages from the trajectory_client. Although, in this function, we do not use the feedback information. :param self: The self reference. :param feedback: FollowJointTrajectoryActionFeedback message. \"\"\" if 'wrist_extension' in self . joints : self . joints . remove ( 'wrist_extension' ) self . joints . append ( 'joint_arm_l0' ) current_effort = [] for joint in self . joints : index = self . joint_states . name . index ( joint ) current_effort . append ( self . joint_states . effort [ index ]) if not self . export_data : print ( \"name: \" + str ( self . joints )) print ( \"effort: \" + str ( current_effort )) else : self . joint_effort . append ( current_effort ) def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. Within this function we export the data to a .txt file in the /stored_data directory. :param self: The self reference. :param status: status attribute from FollowJointTrajectoryActionResult message. :param result: result attribute from FollowJointTrajectoryActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( 'Succeeded' ) else : rospy . loginfo ( 'Failed' ) if self . export_data : file_name = datetime . now () . strftime ( \"%Y-%m- %d _%I:%M:%S-%p\" ) completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"w\" ) as f : writer = csv . writer ( f ) writer . writerow ( self . joints ) writer . writerows ( self . joint_effort ) def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'issue_command' , 'issue_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing command...' ) self . issue_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = JointActuatorEffortSensor ( export_data = True ) node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import time import actionlib import os import csv from datetime import datetime from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState import hello_helpers.hello_misc as hm You need to import rospy if you are writing a ROS Node . Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros . In this instance, we are importing the hello_misc script. class JointActuatorEffortSensor ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self , export_data = False ): \"\"\" Function that initializes the subscriber,and other features. :param self: The self reference. :param export_data: A boolean message type. \"\"\" hm . HelloNode . __init__ ( self ) The JointActuatorEffortSensor class inherits the HelloNode class from hm and is initialized. self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) self . joints = [ 'joint_lift' ] Set up a subscriber. We're going to subscribe to the topic \" joint_states \", looking for JointState messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. Create a list of the desired joints you want to print. self . joint_effort = [] self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . export_data = export_data Create an empty list to store the joint effort values. The self.save_path is the directory path where the .txt file of the effort values will be stored. You can change this path to a preferred directory. The self.export_data is a boolean and its default value is set to False. If set to True, then the joint values will be stored in a .txt file, otherwise, the values will be printed in the terminal where you ran the effort sensing node. self . trajectory_client . send_goal ( trajectory_goal , feedback_cb = self . feedback_callback , done_cb = self . done_callback ) Include the feedback and done call back functions in the send goal function. def feedback_callback ( self , feedback ): \"\"\" The feedback_callback function deals with the incoming feedback messages from the trajectory_client. Although, in this function, we do not use the feedback information. :param self: The self reference. :param feedback: FollowJointTrajectoryActionFeedback message. \"\"\" The feedback callback function takes in the FollowJointTrajectoryActionFeedback message as its argument. if 'wrist_extension' in self . joints : self . joints . remove ( 'wrist_extension' ) self . joints . append ( 'joint_arm_l0' ) Use a conditional statement to replace wrist_extenstion to joint_arm_l0 . This is because joint_arm_l0 has the effort values that the wrist_extension is experiencing. current_effort = [] for joint in self . joints : index = self . joint_states . name . index ( joint ) current_effort . append ( self . joint_states . effort [ index ]) Create an empty list to store the current effort values. Then use a for loop to parse the joint names and effort values. if not self . export_data : print ( \"name: \" + str ( self . joints )) print ( \"effort: \" + str ( current_effort )) else : self . joint_effort . append ( current_effort ) Use a conditional statement to print effort values in the terminal or store values into a list that will be used for exporting the data in a .txt file. def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. Within this function we export the data to a .txt file in the /stored_data directory. :param self: The self reference. :param status: status attribute from FollowJointTrajectoryActionResult message. :param result: result attribute from FollowJointTrajectoryActionResult message. \"\"\" The done callback function takes in the FollowJointTrajectoryActionResult messages as its arguments. if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( 'Succeeded' ) else : rospy . loginfo ( 'Failed' ) Conditional statement to print whether the goal status in the FollowJointTrajectoryActionResult succeeded or failed. if self . export_data : file_name = datetime . now () . strftime ( \"%Y-%m- %d _%I:%M:%S-%p\" ) completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"w\" ) as f : writer = csv . writer ( f ) writer . writerow ( self . joints ) writer . writerows ( self . joint_effort ) A conditional statement is used to export the data to a .txt file. The file's name is set to the date and time the node was executed. That way, no previous files are overwritten. Plotting/Animating Effort Data We added a simple python script, stored_data_plotter.py , to this package for plotting the stored data. Note you have to change the name of the file you wish to see in the python script. This is shown below: ####################### Copy the file name here! ####################### file_name = '2022-06-30_11:26:20-AM' Once you have changed the file name, then run the following in a new command. cd catkin_ws/src/stretch_tutorials/src/ python3 stored_data_plotter.py Because this is not a node, you don't need roscore to run this script. Please review the comments in the python script for additional guidance.","title":"Example 6"},{"location":"stretch-tutorials/ros1_melodic/example_6/#example-6","text":"In this example, we will review a Python script that prints out and stores the effort values from a specified joint. If you are looking for a continuous print of the joint state efforts while Stretch is in action, then you can use the rostopic command-line tool shown in the Internal State of Stretch Tutorial . Begin by running the following command in the terminal in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the effort_sensing.py node. # Terminal 2 rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python effort_sensing.py This will send a FollowJointTrajectory command to move Stretch's arm or head while also printing the effort of the lift.","title":"Example 6"},{"location":"stretch-tutorials/ros1_melodic/example_6/#the-code","text":"#!/usr/bin/env python import rospy import time import actionlib import os import csv from datetime import datetime from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState import hello_helpers.hello_misc as hm class JointActuatorEffortSensor ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self , export_data = False ): \"\"\" Function that initializes the subscriber,and other features. :param self: The self reference. :param export_data: A boolean message type. \"\"\" hm . HelloNode . __init__ ( self ) self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) self . joints = [ 'joint_lift' ] self . joint_effort = [] self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . export_data = export_data def callback ( self , msg ): \"\"\" Callback function to update and store JointState messages. :param self: The self reference. :param msg: The JointState message. \"\"\" self . joint_states = msg def issue_command ( self ): \"\"\" Function that makes an action call and sends joint trajectory goals to a single joint. :param self: The self reference. \"\"\" trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = self . joints point0 = JointTrajectoryPoint () point0 . positions = [ 0.9 ] trajectory_goal . trajectory . points = [ point0 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal , feedback_cb = self . feedback_callback , done_cb = self . done_callback ) rospy . loginfo ( 'Sent position goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def feedback_callback ( self , feedback ): \"\"\" The feedback_callback function deals with the incoming feedback messages from the trajectory_client. Although, in this function, we do not use the feedback information. :param self: The self reference. :param feedback: FollowJointTrajectoryActionFeedback message. \"\"\" if 'wrist_extension' in self . joints : self . joints . remove ( 'wrist_extension' ) self . joints . append ( 'joint_arm_l0' ) current_effort = [] for joint in self . joints : index = self . joint_states . name . index ( joint ) current_effort . append ( self . joint_states . effort [ index ]) if not self . export_data : print ( \"name: \" + str ( self . joints )) print ( \"effort: \" + str ( current_effort )) else : self . joint_effort . append ( current_effort ) def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. Within this function we export the data to a .txt file in the /stored_data directory. :param self: The self reference. :param status: status attribute from FollowJointTrajectoryActionResult message. :param result: result attribute from FollowJointTrajectoryActionResult message. \"\"\" if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( 'Succeeded' ) else : rospy . loginfo ( 'Failed' ) if self . export_data : file_name = datetime . now () . strftime ( \"%Y-%m- %d _%I:%M:%S-%p\" ) completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"w\" ) as f : writer = csv . writer ( f ) writer . writerow ( self . joints ) writer . writerows ( self . joint_effort ) def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'issue_command' , 'issue_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing command...' ) self . issue_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = JointActuatorEffortSensor ( export_data = True ) node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_6/#the-code-explained","text":"This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import time import actionlib import os import csv from datetime import datetime from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState import hello_helpers.hello_misc as hm You need to import rospy if you are writing a ROS Node . Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros . In this instance, we are importing the hello_misc script. class JointActuatorEffortSensor ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self , export_data = False ): \"\"\" Function that initializes the subscriber,and other features. :param self: The self reference. :param export_data: A boolean message type. \"\"\" hm . HelloNode . __init__ ( self ) The JointActuatorEffortSensor class inherits the HelloNode class from hm and is initialized. self . sub = rospy . Subscriber ( 'joint_states' , JointState , self . callback ) self . joints = [ 'joint_lift' ] Set up a subscriber. We're going to subscribe to the topic \" joint_states \", looking for JointState messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. Create a list of the desired joints you want to print. self . joint_effort = [] self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . export_data = export_data Create an empty list to store the joint effort values. The self.save_path is the directory path where the .txt file of the effort values will be stored. You can change this path to a preferred directory. The self.export_data is a boolean and its default value is set to False. If set to True, then the joint values will be stored in a .txt file, otherwise, the values will be printed in the terminal where you ran the effort sensing node. self . trajectory_client . send_goal ( trajectory_goal , feedback_cb = self . feedback_callback , done_cb = self . done_callback ) Include the feedback and done call back functions in the send goal function. def feedback_callback ( self , feedback ): \"\"\" The feedback_callback function deals with the incoming feedback messages from the trajectory_client. Although, in this function, we do not use the feedback information. :param self: The self reference. :param feedback: FollowJointTrajectoryActionFeedback message. \"\"\" The feedback callback function takes in the FollowJointTrajectoryActionFeedback message as its argument. if 'wrist_extension' in self . joints : self . joints . remove ( 'wrist_extension' ) self . joints . append ( 'joint_arm_l0' ) Use a conditional statement to replace wrist_extenstion to joint_arm_l0 . This is because joint_arm_l0 has the effort values that the wrist_extension is experiencing. current_effort = [] for joint in self . joints : index = self . joint_states . name . index ( joint ) current_effort . append ( self . joint_states . effort [ index ]) Create an empty list to store the current effort values. Then use a for loop to parse the joint names and effort values. if not self . export_data : print ( \"name: \" + str ( self . joints )) print ( \"effort: \" + str ( current_effort )) else : self . joint_effort . append ( current_effort ) Use a conditional statement to print effort values in the terminal or store values into a list that will be used for exporting the data in a .txt file. def done_callback ( self , status , result ): \"\"\" The done_callback function will be called when the joint action is complete. Within this function we export the data to a .txt file in the /stored_data directory. :param self: The self reference. :param status: status attribute from FollowJointTrajectoryActionResult message. :param result: result attribute from FollowJointTrajectoryActionResult message. \"\"\" The done callback function takes in the FollowJointTrajectoryActionResult messages as its arguments. if status == actionlib . GoalStatus . SUCCEEDED : rospy . loginfo ( 'Succeeded' ) else : rospy . loginfo ( 'Failed' ) Conditional statement to print whether the goal status in the FollowJointTrajectoryActionResult succeeded or failed. if self . export_data : file_name = datetime . now () . strftime ( \"%Y-%m- %d _%I:%M:%S-%p\" ) completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"w\" ) as f : writer = csv . writer ( f ) writer . writerow ( self . joints ) writer . writerows ( self . joint_effort ) A conditional statement is used to export the data to a .txt file. The file's name is set to the date and time the node was executed. That way, no previous files are overwritten.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_6/#plottinganimating-effort-data","text":"We added a simple python script, stored_data_plotter.py , to this package for plotting the stored data. Note you have to change the name of the file you wish to see in the python script. This is shown below: ####################### Copy the file name here! ####################### file_name = '2022-06-30_11:26:20-AM' Once you have changed the file name, then run the following in a new command. cd catkin_ws/src/stretch_tutorials/src/ python3 stored_data_plotter.py Because this is not a node, you don't need roscore to run this script. Please review the comments in the python script for additional guidance.","title":"Plotting/Animating Effort Data"},{"location":"stretch-tutorials/ros1_melodic/example_7/","text":"Example 7 In this example, we will review the image_view ROS package and a Python script that captures an image from the RealSense camera . BBegin by checking out the feature/upright_camera_view branch in the stretch_ros repository. The configuration of the camera results in the images being displayed sideways. Thus, this branch publishes a new topic that rotates the raw image upright. cd ~/catkin_ws/src/stretch_ros/stretch_core git checkout feature/upright_camera_view Then run the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_low_resolution.launch Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 3 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz Capture Image with image_view There are a couple of methods to save an image using the image_view package. OPTION 1: Use the image_view node to open a simple image viewer for ROS sensor_msgs/image topics. # Terminal 4 rosrun image_view image_view image: = /camera/color/image_raw_upright_view Then you can save the current image by right-clicking on the display window. By deafult, images will be saved as frame000.jpg, frame000.jpg, etc. Note, that the image will be saved to the terminal's current work directory. OPTION 2: Use the image_saver node to save an image to the terminals current work directory. # Terminal 4 rosrun image_view image_saver image: = /camera/color/image_raw_upright_view Capture Image with Python Script In this section, you can use a Python node to capture an image from the RealSense camera . Execute the capture_image.py node to save a .jpeg image of the image topic /camera/color/image_raw_upright_view . # Terminal 4 cd ~/catkin_ws/src/stretch_tutorials/src python capture_image.py An image named camera_image.jpeg is saved in the stored_data folder in this package. The Code #!/usr/bin/env python import rospy import sys import os import cv2 from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError class CaptureImage : \"\"\" A class that converts a subscribed ROS image to a OpenCV image and saves the captured image to a predefined directory. \"\"\" def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and save path. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a CV2 image and stores the image. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError , e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) file_name = 'camera_image.jpeg' completeName = os . path . join ( self . save_path , file_name ) cv2 . imwrite ( completeName , image ) rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) if __name__ == '__main__' : rospy . init_node ( 'capture_image' , argv = sys . argv ) CaptureImage () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import sys import os import cv2 You need to import rospy if you are writing a ROS Node . There are functions from sys, os, and cv2 that are required within this code. cv2 is a library of Python functions that implements computer vision algorithms. Further information about cv2 can be found here: OpenCV Python . from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError The sensor_msgs.msg is imported so that we can subscribe to ROS Image messages. Import CvBridge to convert between ROS Image messages and OpenCV images. def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and save path. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' Initialize the CvBridge class, the subscriber, and the directory of where the captured image will be stored. def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a cv2 image and stores the image. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) Try to convert the ROS Image message to a cv2 Image message using the imgmsg_to_cv2() function. file_name = 'camera_image.jpeg' completeName = os . path . join ( self . save_path , file_name ) cv2 . imwrite ( completeName , image ) Join the directory and file name using the path.join() function. Then use the imwrite() function to save the image. rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter. rospy . init_node ( 'capture_image' , argv = sys . argv ) CaptureImage () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the CaptureImage() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages. Edge Detection In this section, we highlight a node that utilizes the Canny Edge filter algorithm to detect the edges from an image and convert it back as a ROS image to be visualized in RViz. Begin by running the following commands. # Terminal 4 cd ~/catkin_ws/src/stretch_tutorials/src python edge_detection.py The node will publish a new Image topic named /image_edge_detection . This can be visualized in RViz and a gif is provided below for reference. The Code #!/usr/bin/env python import rospy import sys import os import cv2 from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError class EdgeDetection : \"\"\" A class that converts a subscribed ROS image to a OpenCV image and saves the captured image to a predefined directory. \"\"\" def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and other parameter values. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . pub = rospy . Publisher ( '/image_edge_detection' , Image , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . lower_thres = 100 self . upper_thres = 200 rospy . loginfo ( \"Publishing the CV2 Image. Use RViz to visualize.\" ) def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a CV2 image and goes through the Canny Edge filter in OpenCV for edge detection. Then publishes that filtered image to be visualized in RViz. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) image = cv2 . Canny ( image , self . lower_thres , self . upper_thres ) image_msg = self . bridge . cv2_to_imgmsg ( image , 'passthrough' ) image_msg . header = msg . header self . pub . publish ( image_msg ) if __name__ == '__main__' : rospy . init_node ( 'edge_detection' , argv = sys . argv ) EdgeDetection () rospy . spin () The Code Explained Since that there are similarities in the capture image node, we will only breakdown the different components of the edge detection node. self . lower_thres = 100 self . upper_thres = 200 Define lower and upper bounds of the Hysteresis Thresholds. image = cv2 . Canny ( image , self . lower_thres , self . upper_thres ) Run the Canny Edge function to detect edges from the cv2 image. image_msg = self . bridge . cv2_to_imgmsg ( image , 'passthrough' ) Convert the cv2 image back to a ROS image so it can be published. image_msg . header = msg . header self . pub . publish ( image_msg ) Publish the ROS image with the same header as the subscribed ROS message.","title":"Example 7"},{"location":"stretch-tutorials/ros1_melodic/example_7/#example-7","text":"In this example, we will review the image_view ROS package and a Python script that captures an image from the RealSense camera . BBegin by checking out the feature/upright_camera_view branch in the stretch_ros repository. The configuration of the camera results in the images being displayed sideways. Thus, this branch publishes a new topic that rotates the raw image upright. cd ~/catkin_ws/src/stretch_ros/stretch_core git checkout feature/upright_camera_view Then run the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_low_resolution.launch Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 3 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz","title":"Example 7"},{"location":"stretch-tutorials/ros1_melodic/example_7/#capture-image-with-image_view","text":"There are a couple of methods to save an image using the image_view package. OPTION 1: Use the image_view node to open a simple image viewer for ROS sensor_msgs/image topics. # Terminal 4 rosrun image_view image_view image: = /camera/color/image_raw_upright_view Then you can save the current image by right-clicking on the display window. By deafult, images will be saved as frame000.jpg, frame000.jpg, etc. Note, that the image will be saved to the terminal's current work directory. OPTION 2: Use the image_saver node to save an image to the terminals current work directory. # Terminal 4 rosrun image_view image_saver image: = /camera/color/image_raw_upright_view","title":"Capture Image with image_view"},{"location":"stretch-tutorials/ros1_melodic/example_7/#capture-image-with-python-script","text":"In this section, you can use a Python node to capture an image from the RealSense camera . Execute the capture_image.py node to save a .jpeg image of the image topic /camera/color/image_raw_upright_view . # Terminal 4 cd ~/catkin_ws/src/stretch_tutorials/src python capture_image.py An image named camera_image.jpeg is saved in the stored_data folder in this package.","title":"Capture Image with Python Script"},{"location":"stretch-tutorials/ros1_melodic/example_7/#the-code","text":"#!/usr/bin/env python import rospy import sys import os import cv2 from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError class CaptureImage : \"\"\" A class that converts a subscribed ROS image to a OpenCV image and saves the captured image to a predefined directory. \"\"\" def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and save path. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a CV2 image and stores the image. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError , e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) file_name = 'camera_image.jpeg' completeName = os . path . join ( self . save_path , file_name ) cv2 . imwrite ( completeName , image ) rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) if __name__ == '__main__' : rospy . init_node ( 'capture_image' , argv = sys . argv ) CaptureImage () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_7/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import sys import os import cv2 You need to import rospy if you are writing a ROS Node . There are functions from sys, os, and cv2 that are required within this code. cv2 is a library of Python functions that implements computer vision algorithms. Further information about cv2 can be found here: OpenCV Python . from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError The sensor_msgs.msg is imported so that we can subscribe to ROS Image messages. Import CvBridge to convert between ROS Image messages and OpenCV images. def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and save path. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' Initialize the CvBridge class, the subscriber, and the directory of where the captured image will be stored. def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a cv2 image and stores the image. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) Try to convert the ROS Image message to a cv2 Image message using the imgmsg_to_cv2() function. file_name = 'camera_image.jpeg' completeName = os . path . join ( self . save_path , file_name ) cv2 . imwrite ( completeName , image ) Join the directory and file name using the path.join() function. Then use the imwrite() function to save the image. rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) The first line of code initiates a clean shutdown of ROS. The second line of code exits the Python interpreter. rospy . init_node ( 'capture_image' , argv = sys . argv ) CaptureImage () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the CaptureImage() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_7/#edge-detection","text":"In this section, we highlight a node that utilizes the Canny Edge filter algorithm to detect the edges from an image and convert it back as a ROS image to be visualized in RViz. Begin by running the following commands. # Terminal 4 cd ~/catkin_ws/src/stretch_tutorials/src python edge_detection.py The node will publish a new Image topic named /image_edge_detection . This can be visualized in RViz and a gif is provided below for reference.","title":"Edge Detection"},{"location":"stretch-tutorials/ros1_melodic/example_7/#the-code_1","text":"#!/usr/bin/env python import rospy import sys import os import cv2 from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError class EdgeDetection : \"\"\" A class that converts a subscribed ROS image to a OpenCV image and saves the captured image to a predefined directory. \"\"\" def __init__ ( self ): \"\"\" A function that initializes a CvBridge class, subscriber, and other parameter values. :param self: The self reference. \"\"\" self . bridge = CvBridge () self . sub = rospy . Subscriber ( '/camera/color/image_raw' , Image , self . callback , queue_size = 1 ) self . pub = rospy . Publisher ( '/image_edge_detection' , Image , queue_size = 1 ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' self . lower_thres = 100 self . upper_thres = 200 rospy . loginfo ( \"Publishing the CV2 Image. Use RViz to visualize.\" ) def callback ( self , msg ): \"\"\" A callback function that converts the ROS image to a CV2 image and goes through the Canny Edge filter in OpenCV for edge detection. Then publishes that filtered image to be visualized in RViz. :param self: The self reference. :param msg: The ROS image message type. \"\"\" try : image = self . bridge . imgmsg_to_cv2 ( msg , 'bgr8' ) except CvBridgeError as e : rospy . logwarn ( 'CV Bridge error: {0} ' . format ( e )) image = cv2 . Canny ( image , self . lower_thres , self . upper_thres ) image_msg = self . bridge . cv2_to_imgmsg ( image , 'passthrough' ) image_msg . header = msg . header self . pub . publish ( image_msg ) if __name__ == '__main__' : rospy . init_node ( 'edge_detection' , argv = sys . argv ) EdgeDetection () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_7/#the-code-explained_1","text":"Since that there are similarities in the capture image node, we will only breakdown the different components of the edge detection node. self . lower_thres = 100 self . upper_thres = 200 Define lower and upper bounds of the Hysteresis Thresholds. image = cv2 . Canny ( image , self . lower_thres , self . upper_thres ) Run the Canny Edge function to detect edges from the cv2 image. image_msg = self . bridge . cv2_to_imgmsg ( image , 'passthrough' ) Convert the cv2 image back to a ROS image so it can be published. image_msg . header = msg . header self . pub . publish ( image_msg ) Publish the ROS image with the same header as the subscribed ROS message.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_8/","text":"Example 8 This example will showcase how to save the interpreted speech from Stretch's ReSpeaker Mic Array v2.0 to a text file. Begin by running the respeaker.launch file in a terminal. # Terminal 1 roslaunch respeaker_ros sample_respeaker.launch Then run the speech_text.py node. # Terminal 2 cd catkin_ws/src/stretch_tutorials/src/ python speech_text.py The ReSpeaker will be listening and will start to interpret speech and save the transcript to a text file. To stop shutdown the node, type Ctrl + c in the terminal. The Code #!/usr/bin/env python import rospy import os from speech_recognition_msgs.msg import SpeechRecognitionCandidates class SpeechText : \"\"\" A class that saves the interpreted speech from the ReSpeaker Microphone Array to a text file. \"\"\" def __init__ ( self ): \"\"\" Initialize subscriber and directory to save speech to text file. \"\"\" self . sub = rospy . Subscriber ( \"speech_to_text\" , SpeechRecognitionCandidates , self . callback ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' rospy . loginfo ( \"Listening to speech.\" ) def callback ( self , msg ): \"\"\" A callback function that receives the speech transcript and appends the transcript to a text file. :param self: The self reference. :param msg: The SpeechRecognitionCandidates message type. \"\"\" transcript = ' ' . join ( map ( str , msg . transcript )) file_name = 'speech.txt' completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"a+\" ) as file_object : file_object . write ( \" \\n \" ) file_object . write ( transcript ) if __name__ == '__main__' : rospy . init_node ( 'speech_text' ) SpeechText () rospy . spin () The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import os You need to import rospy if you are writing a ROS Node . from speech_recognition_msgs.msg import SpeechRecognitionCandidates Import SpeechRecognitionCandidates from the speech_recgonition_msgs.msg so that we can receive the interpreted speech. def __init__ ( self ): \"\"\" Initialize subscriber and directory to save speech to text file. \"\"\" self . sub = rospy . Subscriber ( \"speech_to_text\" , SpeechRecognitionCandidates , self . callback ) Set up a subscriber. We're going to subscribe to the topic \" speech_to_text \", looking for SpeechRecognitionCandidates messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data Define the directory to save the text file. transcript = ' ' . join ( map ( str , msg . transcript )) Take all items in the iterable list and join them into a single string named transcript. file_name = 'speech.txt' completeName = os . path . join ( self . save_path , file_name ) Define the file name and create a complete path directory. with open ( completeName , \"a+\" ) as file_object : file_object . write ( \" \\n \" ) file_object . write ( transcript ) Append the transcript to the text file. rospy . init_node ( 'speech_text' ) SpeechText () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the SpeechText() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Example 8"},{"location":"stretch-tutorials/ros1_melodic/example_8/#example-8","text":"This example will showcase how to save the interpreted speech from Stretch's ReSpeaker Mic Array v2.0 to a text file. Begin by running the respeaker.launch file in a terminal. # Terminal 1 roslaunch respeaker_ros sample_respeaker.launch Then run the speech_text.py node. # Terminal 2 cd catkin_ws/src/stretch_tutorials/src/ python speech_text.py The ReSpeaker will be listening and will start to interpret speech and save the transcript to a text file. To stop shutdown the node, type Ctrl + c in the terminal.","title":"Example 8"},{"location":"stretch-tutorials/ros1_melodic/example_8/#the-code","text":"#!/usr/bin/env python import rospy import os from speech_recognition_msgs.msg import SpeechRecognitionCandidates class SpeechText : \"\"\" A class that saves the interpreted speech from the ReSpeaker Microphone Array to a text file. \"\"\" def __init__ ( self ): \"\"\" Initialize subscriber and directory to save speech to text file. \"\"\" self . sub = rospy . Subscriber ( \"speech_to_text\" , SpeechRecognitionCandidates , self . callback ) self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data' rospy . loginfo ( \"Listening to speech.\" ) def callback ( self , msg ): \"\"\" A callback function that receives the speech transcript and appends the transcript to a text file. :param self: The self reference. :param msg: The SpeechRecognitionCandidates message type. \"\"\" transcript = ' ' . join ( map ( str , msg . transcript )) file_name = 'speech.txt' completeName = os . path . join ( self . save_path , file_name ) with open ( completeName , \"a+\" ) as file_object : file_object . write ( \" \\n \" ) file_object . write ( transcript ) if __name__ == '__main__' : rospy . init_node ( 'speech_text' ) SpeechText () rospy . spin ()","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_8/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rospy import os You need to import rospy if you are writing a ROS Node . from speech_recognition_msgs.msg import SpeechRecognitionCandidates Import SpeechRecognitionCandidates from the speech_recgonition_msgs.msg so that we can receive the interpreted speech. def __init__ ( self ): \"\"\" Initialize subscriber and directory to save speech to text file. \"\"\" self . sub = rospy . Subscriber ( \"speech_to_text\" , SpeechRecognitionCandidates , self . callback ) Set up a subscriber. We're going to subscribe to the topic \" speech_to_text \", looking for SpeechRecognitionCandidates messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. self . save_path = '/home/hello-robot/catkin_ws/src/stretch_tutorials/stored_data Define the directory to save the text file. transcript = ' ' . join ( map ( str , msg . transcript )) Take all items in the iterable list and join them into a single string named transcript. file_name = 'speech.txt' completeName = os . path . join ( self . save_path , file_name ) Define the file name and create a complete path directory. with open ( completeName , \"a+\" ) as file_object : file_object . write ( \" \\n \" ) file_object . write ( transcript ) Append the transcript to the text file. rospy . init_node ( 'speech_text' ) SpeechText () The next line, rospy.init_node(NAME, ...) , is very important as it tells rospy the name of your node -- until rospy has this information, it cannot start communicating with the ROS Master. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Instantiate the SpeechText() class. rospy . spin () Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/example_9/","text":"Example 9 The aim of example 9 is to combine the ReSpeaker Microphone Array and Follow Joint Trajectory tutorials to voice teleoperate the mobile base of the Stretch robot. Begin by running the following command in a new terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the respeaker.launch file. # Terminal 2 rosservice call /switch_to_position_mode roslaunch stretch_core respeaker.launch Then run the voice_teleoperation_base.py node in a new terminal. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python voice_teleoperation_base.py In terminal 3, a menu of voice commands is printed. You can reference this menu layout below. ------------ VOICE TELEOP MENU ------------ VOICE COMMANDS \"forward\": BASE FORWARD \"back\" : BASE BACK \"left\" : BASE ROTATE LEFT \"right\" : BASE ROTATE RIGHT \"stretch\": BASE ROTATES TOWARDS SOUND STEP SIZE \"big\" : BIG \"medium\" : MEDIUM \"small\" : SMALL \"quit\" : QUIT AND CLOSE NODE ------------------------------------------- To stop the node from sending twist messages, type Ctrl + c or say \" quit \". The Code #!/usr/bin/env python import math import rospy import sys from sensor_msgs.msg import JointState from std_msgs.msg import Int32 from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm from speech_recognition_msgs.msg import SpeechRecognitionCandidates class GetVoiceCommands : \"\"\" A class that subscribes to the speech to text recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion. \"\"\" def __init__ ( self ): \"\"\" A function that initializes subscribers and defines the three different step sizes. :param self: The self reference. \"\"\" self . step_size = 'medium' self . rad_per_deg = math . pi / 180.0 self . small_deg = 5.0 self . small_rad = self . rad_per_deg * self . small_deg self . small_translate = 0.025 self . medium_deg = 10.0 self . medium_rad = self . rad_per_deg * self . medium_deg self . medium_translate = 0.05 self . big_deg = 20.0 self . big_rad = self . rad_per_deg * self . big_deg self . big_translate = 0.1 self . voice_command = None self . sound_direction = 0 self . speech_to_text_sub = rospy . Subscriber ( \"/speech_to_text\" , SpeechRecognitionCandidates , self . callback_speech ) self . sound_direction_sub = rospy . Subscriber ( \"/sound_direction\" , Int32 , self . callback_direction ) def callback_direction ( self , msg ): \"\"\" A callback function that converts the incoming message, sound direction, from degrees to radians. :param self: The self reference. :param msg: The Int32 message type that represents the sound direction. \"\"\" self . sound_direction = msg . data * - self . rad_per_deg def callback_speech ( self , msg ): \"\"\" A callback function takes the incoming message, a list of the speech to text, and joins all items in that iterable list into a single string. :param self: The self reference. :param msg: The SpeechRecognitionCandidates message type. \"\"\" self . voice_command = ' ' . join ( map ( str , msg . transcript )) def get_inc ( self ): \"\"\" A function that sets the increment size for translational and rotational base motion. :param self:The self reference. :returns inc: A dictionary type the contains the increment size. \"\"\" if self . step_size == 'small' : inc = { 'rad' : self . small_rad , 'translate' : self . small_translate } if self . step_size == 'medium' : inc = { 'rad' : self . medium_rad , 'translate' : self . medium_translate } if self . step_size == 'big' : inc = { 'rad' : self . big_rad , 'translate' : self . big_translate } return inc def print_commands ( self ): \"\"\" A function that prints the voice teleoperation menu. :param self: The self reference. \"\"\" print ( ' ' ) print ( '------------ VOICE TELEOP MENU ------------' ) print ( ' ' ) print ( ' VOICE COMMANDS ' ) print ( ' \"forward\": BASE FORWARD ' ) print ( ' \"back\" : BASE BACK ' ) print ( ' \"left\" : BASE ROTATE LEFT ' ) print ( ' \"right\" : BASE ROTATE RIGHT ' ) print ( ' \"stretch\": BASE ROTATES TOWARDS SOUND ' ) print ( ' ' ) print ( ' STEP SIZE ' ) print ( ' \"big\" : BIG ' ) print ( ' \"medium\" : MEDIUM ' ) print ( ' \"small\" : SMALL ' ) print ( ' ' ) print ( ' ' ) print ( ' \"quit\" : QUIT AND CLOSE NODE ' ) print ( ' ' ) print ( '-------------------------------------------' ) def get_command ( self ): \"\"\" A function that defines the teleoperation command based on the voice command. :param self: The self reference. :returns command: A dictionary type that contains the type of base motion. \"\"\" command = None if self . voice_command == 'forward' : command = { 'joint' : 'translate_mobile_base' , 'inc' : self . get_inc ()[ 'translate' ]} if self . voice_command == 'back' : command = { 'joint' : 'translate_mobile_base' , 'inc' : - self . get_inc ()[ 'translate' ]} if self . voice_command == 'left' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . get_inc ()[ 'rad' ]} if self . voice_command == 'right' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : - self . get_inc ()[ 'rad' ]} if self . voice_command == 'stretch' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . sound_direction } if ( self . voice_command == \"small\" ) or ( self . voice_command == \"medium\" ) or ( self . voice_command == \"big\" ): self . step_size = self . voice_command rospy . loginfo ( 'Step size = {0} ' . format ( self . step_size )) if self . voice_command == 'quit' : rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) self . voice_command = None return command class VoiceTeleopNode ( hm . HelloNode ): \"\"\" A class that inherits the HelloNode class from hm and sends joint trajectory commands. \"\"\" def __init__ ( self ): \"\"\" A function that declares object from the GetVoiceCommands class, instantiates the HelloNode class, and set the publishing rate. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . rate = 10.0 self . joint_state = None self . speech = GetVoiceCommands () def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg def send_command ( self , command ): \"\"\" Function that makes an action call and sends base joint trajectory goals :param self: The self reference. :param command: A dictionary type. \"\"\" joint_state = self . joint_state if ( joint_state is not None ) and ( command is not None ): point = JointTrajectoryPoint () point . time_from_start = rospy . Duration ( 0.0 ) trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . goal_time_tolerance = rospy . Time ( 1.0 ) joint_name = command [ 'joint' ] trajectory_goal . trajectory . joint_names = [ joint_name ] inc = command [ 'inc' ] rospy . loginfo ( 'inc = {0} ' . format ( inc )) new_value = inc point . positions = [ new_value ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time . now () rospy . loginfo ( 'joint_name = {0} , trajectory_goal = {1} ' . format ( joint_name , trajectory_goal )) self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Done sending command.' ) self . speech . print_commands () def main ( self ): \"\"\" The main function that instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'voice_teleop' , 'voice_teleop' , wait_for_first_pointcloud = False ) rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) rate = rospy . Rate ( self . rate ) self . speech . print_commands () while not rospy . is_shutdown (): command = self . speech . get_command () self . send_command ( command ) rate . sleep () if __name__ == '__main__' : try : node = VoiceTeleopNode () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import math import rospy import sys from sensor_msgs.msg import JointState from std_msgs.msg import Int32 from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm from speech_recognition_msgs.msg import SpeechRecognitionCandidates You need to import rospy if you are writing a ROS Node. Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the hello_misc script. Import sensor_msgs.msg so that we can subscribe to JointState messages. class GetVoiceCommands : Create a class that subscribes to the speech to text recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion. self . step_size = 'medium' self . rad_per_deg = math . pi / 180.0 Set the default step size as medium and create a float value, self.rad_per_deg , to convert degrees to radians. self . small_deg = 5.0 self . small_rad = self . rad_per_deg * self . small_deg self . small_translate = 0.025 self . medium_deg = 10.0 self . medium_rad = self . rad_per_deg * self . medium_deg self . medium_translate = 0.05 self . big_deg = 20.0 self . big_rad = self . rad_per_deg * self . big_deg self . big_translate = 0.1 Define the three rotation and translation step sizes. self . voice_command = None self . sound_direction = 0 self . speech_to_text_sub = rospy . Subscriber ( \"/speech_to_text\" , SpeechRecognitionCandidates , self . callback_speech ) self . sound_direction_sub = rospy . Subscriber ( \"/sound_direction\" , Int32 , self . callback_direction ) Initialize the voice command and sound direction to values that will not result in moving the base. Set up two subscribers. The first one subscribes to the topic /speech_to_text , looking for SpeechRecognitionCandidates messages. When a message comes in, ROS is going to pass it to the function callback_speech automatically. The second subscribes to /sound_direction message and passes it to the callback_direction function. self . sound_direction = msg . data * - self . rad_per_deg The callback_direction function converts the sound_direction topic from degrees to radians. if self . step_size == 'small' : inc = { 'rad' : self . small_rad , 'translate' : self . small_translate } if self . step_size == 'medium' : inc = { 'rad' : self . medium_rad , 'translate' : self . medium_translate } if self . step_size == 'big' : inc = { 'rad' : self . big_rad , 'translate' : self . big_translate } return inc The callback_speech stores the increment size for translational and rotational base motion to inc . The increment size is contingent on the self.step_size string value. command = None if self . voice_command == 'forward' : command = { 'joint' : 'translate_mobile_base' , 'inc' : self . get_inc ()[ 'translate' ]} if self . voice_command == 'back' : command = { 'joint' : 'translate_mobile_base' , 'inc' : - self . get_inc ()[ 'translate' ]} if self . voice_command == 'left' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . get_inc ()[ 'rad' ]} if self . voice_command == 'right' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : - self . get_inc ()[ 'rad' ]} if self . voice_command == 'stretch' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . sound_direction } In the get_command() function, the command is initialized as None, or set as a dictionary where the joint and inc values are stored. The command message type is dependent on the self.voice_command string value. if ( self . voice_command == \"small\" ) or ( self . voice_command == \"medium\" ) or ( self . voice_command == \"big\" ): self . step_size = self . voice_command rospy . loginfo ( 'Step size = {0} ' . format ( self . step_size )) Based on the self.voice_command value, set the step size for the increments. if self . voice_command == 'quit' : rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) If the self.voice_command is equal to \"quit\", then initiate a clean shutdown of ROS and exit the Python interpreter. class VoiceTeleopNode ( hm . HelloNode ): \"\"\" A class that inherits the HelloNode class from hm and sends joint trajectory commands. \"\"\" def __init__ ( self ): \"\"\" A function that declares object from the GetVoiceCommands class, instantiates the HelloNode class, and set the publishing rate. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . rate = 10.0 self . joint_state = None self . speech = GetVoiceCommands () A class that inherits the HelloNode class from hm , declares object from the GetVoiceCommands class, and sends joint trajectory commands. def send_command ( self , command ): \"\"\" Function that makes an action call and sends base joint trajectory goals :param self: The self reference. :param command: A dictionary type. \"\"\" joint_state = self . joint_state if ( joint_state is not None ) and ( command is not None ): point = JointTrajectoryPoint () point . time_from_start = rospy . Duration ( 0.0 ) The send_command function stores the joint state message and uses a conditional statement to send joint trajectory goals. Also, assign point as a JointTrajectoryPoint message type. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . goal_time_tolerance = rospy . Time ( 1.0 ) Assign trajectory_goal as a FollowJointTrajectoryGoal message type. joint_name = command [ 'joint' ] trajectory_goal . trajectory . joint_names = [ joint_name ] Extract the joint name from the command dictionary. inc = command [ 'inc' ] rospy . loginfo ( 'inc = {0} ' . format ( inc )) new_value = inc Extract the increment type from the command dictionary. point . positions = [ new_value ] trajectory_goal . trajectory . points = [ point ] Assign the new value position to the trajectory goal message type. self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Done sending command.' ) Make the action call and send goal of the new joint position. self . speech . print_commands () Reprint the voice command menu after the trajectory goal is sent. def main ( self ): \"\"\" The main function that instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'voice_teleop' , 'voice_teleop' , wait_for_first_pointcloud = False ) rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) rate = rospy . Rate ( self . rate ) self . speech . print_commands () The main function instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. while not rospy . is_shutdown (): command = self . speech . get_command () self . send_command ( command ) rate . sleep () Run a while loop to continuously check speech commands and send those commands to execute an action. try : node = VoiceTeleopNode () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare a VoiceTeleopNode object. Then execute the main() method.","title":"Example 9"},{"location":"stretch-tutorials/ros1_melodic/example_9/#example-9","text":"The aim of example 9 is to combine the ReSpeaker Microphone Array and Follow Joint Trajectory tutorials to voice teleoperate the mobile base of the Stretch robot. Begin by running the following command in a new terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the respeaker.launch file. # Terminal 2 rosservice call /switch_to_position_mode roslaunch stretch_core respeaker.launch Then run the voice_teleoperation_base.py node in a new terminal. # Terminal 3 cd catkin_ws/src/stretch_tutorials/src/ python voice_teleoperation_base.py In terminal 3, a menu of voice commands is printed. You can reference this menu layout below. ------------ VOICE TELEOP MENU ------------ VOICE COMMANDS \"forward\": BASE FORWARD \"back\" : BASE BACK \"left\" : BASE ROTATE LEFT \"right\" : BASE ROTATE RIGHT \"stretch\": BASE ROTATES TOWARDS SOUND STEP SIZE \"big\" : BIG \"medium\" : MEDIUM \"small\" : SMALL \"quit\" : QUIT AND CLOSE NODE ------------------------------------------- To stop the node from sending twist messages, type Ctrl + c or say \" quit \".","title":"Example 9"},{"location":"stretch-tutorials/ros1_melodic/example_9/#the-code","text":"#!/usr/bin/env python import math import rospy import sys from sensor_msgs.msg import JointState from std_msgs.msg import Int32 from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm from speech_recognition_msgs.msg import SpeechRecognitionCandidates class GetVoiceCommands : \"\"\" A class that subscribes to the speech to text recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion. \"\"\" def __init__ ( self ): \"\"\" A function that initializes subscribers and defines the three different step sizes. :param self: The self reference. \"\"\" self . step_size = 'medium' self . rad_per_deg = math . pi / 180.0 self . small_deg = 5.0 self . small_rad = self . rad_per_deg * self . small_deg self . small_translate = 0.025 self . medium_deg = 10.0 self . medium_rad = self . rad_per_deg * self . medium_deg self . medium_translate = 0.05 self . big_deg = 20.0 self . big_rad = self . rad_per_deg * self . big_deg self . big_translate = 0.1 self . voice_command = None self . sound_direction = 0 self . speech_to_text_sub = rospy . Subscriber ( \"/speech_to_text\" , SpeechRecognitionCandidates , self . callback_speech ) self . sound_direction_sub = rospy . Subscriber ( \"/sound_direction\" , Int32 , self . callback_direction ) def callback_direction ( self , msg ): \"\"\" A callback function that converts the incoming message, sound direction, from degrees to radians. :param self: The self reference. :param msg: The Int32 message type that represents the sound direction. \"\"\" self . sound_direction = msg . data * - self . rad_per_deg def callback_speech ( self , msg ): \"\"\" A callback function takes the incoming message, a list of the speech to text, and joins all items in that iterable list into a single string. :param self: The self reference. :param msg: The SpeechRecognitionCandidates message type. \"\"\" self . voice_command = ' ' . join ( map ( str , msg . transcript )) def get_inc ( self ): \"\"\" A function that sets the increment size for translational and rotational base motion. :param self:The self reference. :returns inc: A dictionary type the contains the increment size. \"\"\" if self . step_size == 'small' : inc = { 'rad' : self . small_rad , 'translate' : self . small_translate } if self . step_size == 'medium' : inc = { 'rad' : self . medium_rad , 'translate' : self . medium_translate } if self . step_size == 'big' : inc = { 'rad' : self . big_rad , 'translate' : self . big_translate } return inc def print_commands ( self ): \"\"\" A function that prints the voice teleoperation menu. :param self: The self reference. \"\"\" print ( ' ' ) print ( '------------ VOICE TELEOP MENU ------------' ) print ( ' ' ) print ( ' VOICE COMMANDS ' ) print ( ' \"forward\": BASE FORWARD ' ) print ( ' \"back\" : BASE BACK ' ) print ( ' \"left\" : BASE ROTATE LEFT ' ) print ( ' \"right\" : BASE ROTATE RIGHT ' ) print ( ' \"stretch\": BASE ROTATES TOWARDS SOUND ' ) print ( ' ' ) print ( ' STEP SIZE ' ) print ( ' \"big\" : BIG ' ) print ( ' \"medium\" : MEDIUM ' ) print ( ' \"small\" : SMALL ' ) print ( ' ' ) print ( ' ' ) print ( ' \"quit\" : QUIT AND CLOSE NODE ' ) print ( ' ' ) print ( '-------------------------------------------' ) def get_command ( self ): \"\"\" A function that defines the teleoperation command based on the voice command. :param self: The self reference. :returns command: A dictionary type that contains the type of base motion. \"\"\" command = None if self . voice_command == 'forward' : command = { 'joint' : 'translate_mobile_base' , 'inc' : self . get_inc ()[ 'translate' ]} if self . voice_command == 'back' : command = { 'joint' : 'translate_mobile_base' , 'inc' : - self . get_inc ()[ 'translate' ]} if self . voice_command == 'left' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . get_inc ()[ 'rad' ]} if self . voice_command == 'right' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : - self . get_inc ()[ 'rad' ]} if self . voice_command == 'stretch' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . sound_direction } if ( self . voice_command == \"small\" ) or ( self . voice_command == \"medium\" ) or ( self . voice_command == \"big\" ): self . step_size = self . voice_command rospy . loginfo ( 'Step size = {0} ' . format ( self . step_size )) if self . voice_command == 'quit' : rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) self . voice_command = None return command class VoiceTeleopNode ( hm . HelloNode ): \"\"\" A class that inherits the HelloNode class from hm and sends joint trajectory commands. \"\"\" def __init__ ( self ): \"\"\" A function that declares object from the GetVoiceCommands class, instantiates the HelloNode class, and set the publishing rate. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . rate = 10.0 self . joint_state = None self . speech = GetVoiceCommands () def joint_states_callback ( self , msg ): \"\"\" A callback function that stores Stretch's joint states. :param self: The self reference. :param msg: The JointState message type. \"\"\" self . joint_state = msg def send_command ( self , command ): \"\"\" Function that makes an action call and sends base joint trajectory goals :param self: The self reference. :param command: A dictionary type. \"\"\" joint_state = self . joint_state if ( joint_state is not None ) and ( command is not None ): point = JointTrajectoryPoint () point . time_from_start = rospy . Duration ( 0.0 ) trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . goal_time_tolerance = rospy . Time ( 1.0 ) joint_name = command [ 'joint' ] trajectory_goal . trajectory . joint_names = [ joint_name ] inc = command [ 'inc' ] rospy . loginfo ( 'inc = {0} ' . format ( inc )) new_value = inc point . positions = [ new_value ] trajectory_goal . trajectory . points = [ point ] trajectory_goal . trajectory . header . stamp = rospy . Time . now () rospy . loginfo ( 'joint_name = {0} , trajectory_goal = {1} ' . format ( joint_name , trajectory_goal )) self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Done sending command.' ) self . speech . print_commands () def main ( self ): \"\"\" The main function that instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'voice_teleop' , 'voice_teleop' , wait_for_first_pointcloud = False ) rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) rate = rospy . Rate ( self . rate ) self . speech . print_commands () while not rospy . is_shutdown (): command = self . speech . get_command () self . send_command ( command ) rate . sleep () if __name__ == '__main__' : try : node = VoiceTeleopNode () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/example_9/#the-code-explained","text":"This code is similar to that of the multipoint_command and joint_state_printer node. Therefore, this example will highlight sections that are different from those tutorials. Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import math import rospy import sys from sensor_msgs.msg import JointState from std_msgs.msg import Int32 from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm from speech_recognition_msgs.msg import SpeechRecognitionCandidates You need to import rospy if you are writing a ROS Node. Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module that provides various Python scripts used across stretch_ros. In this instance, we are importing the hello_misc script. Import sensor_msgs.msg so that we can subscribe to JointState messages. class GetVoiceCommands : Create a class that subscribes to the speech to text recognition messages, prints a voice command menu, and defines step size for translational and rotational mobile base motion. self . step_size = 'medium' self . rad_per_deg = math . pi / 180.0 Set the default step size as medium and create a float value, self.rad_per_deg , to convert degrees to radians. self . small_deg = 5.0 self . small_rad = self . rad_per_deg * self . small_deg self . small_translate = 0.025 self . medium_deg = 10.0 self . medium_rad = self . rad_per_deg * self . medium_deg self . medium_translate = 0.05 self . big_deg = 20.0 self . big_rad = self . rad_per_deg * self . big_deg self . big_translate = 0.1 Define the three rotation and translation step sizes. self . voice_command = None self . sound_direction = 0 self . speech_to_text_sub = rospy . Subscriber ( \"/speech_to_text\" , SpeechRecognitionCandidates , self . callback_speech ) self . sound_direction_sub = rospy . Subscriber ( \"/sound_direction\" , Int32 , self . callback_direction ) Initialize the voice command and sound direction to values that will not result in moving the base. Set up two subscribers. The first one subscribes to the topic /speech_to_text , looking for SpeechRecognitionCandidates messages. When a message comes in, ROS is going to pass it to the function callback_speech automatically. The second subscribes to /sound_direction message and passes it to the callback_direction function. self . sound_direction = msg . data * - self . rad_per_deg The callback_direction function converts the sound_direction topic from degrees to radians. if self . step_size == 'small' : inc = { 'rad' : self . small_rad , 'translate' : self . small_translate } if self . step_size == 'medium' : inc = { 'rad' : self . medium_rad , 'translate' : self . medium_translate } if self . step_size == 'big' : inc = { 'rad' : self . big_rad , 'translate' : self . big_translate } return inc The callback_speech stores the increment size for translational and rotational base motion to inc . The increment size is contingent on the self.step_size string value. command = None if self . voice_command == 'forward' : command = { 'joint' : 'translate_mobile_base' , 'inc' : self . get_inc ()[ 'translate' ]} if self . voice_command == 'back' : command = { 'joint' : 'translate_mobile_base' , 'inc' : - self . get_inc ()[ 'translate' ]} if self . voice_command == 'left' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . get_inc ()[ 'rad' ]} if self . voice_command == 'right' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : - self . get_inc ()[ 'rad' ]} if self . voice_command == 'stretch' : command = { 'joint' : 'rotate_mobile_base' , 'inc' : self . sound_direction } In the get_command() function, the command is initialized as None, or set as a dictionary where the joint and inc values are stored. The command message type is dependent on the self.voice_command string value. if ( self . voice_command == \"small\" ) or ( self . voice_command == \"medium\" ) or ( self . voice_command == \"big\" ): self . step_size = self . voice_command rospy . loginfo ( 'Step size = {0} ' . format ( self . step_size )) Based on the self.voice_command value, set the step size for the increments. if self . voice_command == 'quit' : rospy . signal_shutdown ( \"done\" ) sys . exit ( 0 ) If the self.voice_command is equal to \"quit\", then initiate a clean shutdown of ROS and exit the Python interpreter. class VoiceTeleopNode ( hm . HelloNode ): \"\"\" A class that inherits the HelloNode class from hm and sends joint trajectory commands. \"\"\" def __init__ ( self ): \"\"\" A function that declares object from the GetVoiceCommands class, instantiates the HelloNode class, and set the publishing rate. :param self: The self reference. \"\"\" hm . HelloNode . __init__ ( self ) self . rate = 10.0 self . joint_state = None self . speech = GetVoiceCommands () A class that inherits the HelloNode class from hm , declares object from the GetVoiceCommands class, and sends joint trajectory commands. def send_command ( self , command ): \"\"\" Function that makes an action call and sends base joint trajectory goals :param self: The self reference. :param command: A dictionary type. \"\"\" joint_state = self . joint_state if ( joint_state is not None ) and ( command is not None ): point = JointTrajectoryPoint () point . time_from_start = rospy . Duration ( 0.0 ) The send_command function stores the joint state message and uses a conditional statement to send joint trajectory goals. Also, assign point as a JointTrajectoryPoint message type. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . goal_time_tolerance = rospy . Time ( 1.0 ) Assign trajectory_goal as a FollowJointTrajectoryGoal message type. joint_name = command [ 'joint' ] trajectory_goal . trajectory . joint_names = [ joint_name ] Extract the joint name from the command dictionary. inc = command [ 'inc' ] rospy . loginfo ( 'inc = {0} ' . format ( inc )) new_value = inc Extract the increment type from the command dictionary. point . positions = [ new_value ] trajectory_goal . trajectory . points = [ point ] Assign the new value position to the trajectory goal message type. self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Done sending command.' ) Make the action call and send goal of the new joint position. self . speech . print_commands () Reprint the voice command menu after the trajectory goal is sent. def main ( self ): \"\"\" The main function that instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'voice_teleop' , 'voice_teleop' , wait_for_first_pointcloud = False ) rospy . Subscriber ( '/stretch/joint_states' , JointState , self . joint_states_callback ) rate = rospy . Rate ( self . rate ) self . speech . print_commands () The main function instantiates the HelloNode class, initializes the subscriber, and call other methods in both the VoiceTeleopNode and GetVoiceCommands classes. while not rospy . is_shutdown (): command = self . speech . get_command () self . send_command ( command ) rate . sleep () Run a while loop to continuously check speech commands and send those commands to execute an action. try : node = VoiceTeleopNode () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare a VoiceTeleopNode object. Then execute the main() method.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/","text":"FollowJointTrajectory Commands Stretch ROS driver offers a FollowJointTrajectory action service for its arm. Within this tutorial we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute. Stow Command Example Begin by running the following command in the terminal in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the stow command node. # Terminal 2 rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python stow_command.py This will send a FollowJointTrajectory command to stow Stretch's arm. The Code #!/usr/bin/env python import rospy from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm import time class StowCommand ( hm . HelloNode ): ''' A class that sends a joint trajectory goal to stow the Stretch's arm. ''' def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_stow_command ( self ): ''' Function that makes an action call and sends stow position goal. :param self: The self reference. ''' stow_point = JointTrajectoryPoint () stow_point . time_from_start = rospy . Duration ( 0.000 ) stow_point . positions = [ 0.2 , 0.0 , 3.4 ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): ''' Function that initiates stow_command function. :param self: The self reference. ''' hm . HelloNode . main ( self , 'stow_command' , 'stow_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'stowing...' ) self . issue_stow_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = StowCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a python script. import rospy from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm import time You need to import rospy if you are writing a ROS Node . Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module the provides various Python scripts used across stretch_ros . In this instance we are importing the hello_misc script. class StowCommand ( hm . HelloNode ): def __init__ ( self ): hm . HelloNode . __init__ ( self ) The StowCommand class inherits the HelloNode class from hm and is initialized. def issue_stow_command ( self ): stow_point = JointTrajectoryPoint () stow_point . time_from_start = rospy . Duration ( 0.000 ) stow_point . positions = [ 0.2 , 0.0 , 3.4 ] The issue_stow_command() is the name of the function that will stow Stretch's arm. Within the function, we set stow_point as a JointTrajectoryPoint and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined in the next set of the code. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by the positions set in stow_point . Specify the coordinate frame that we want ( base_link ) and set the time to be now. self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () Make the action call and send the goal. The last line of code waits for the result before it exits the python script. def main ( self ): hm . HelloNode . main ( self , 'stow_command' , 'stow_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'stowing...' ) self . issue_stow_command () time . sleep ( 2 ) Create a funcion, main() , to do all of the setup the hm.HelloNode class and issue the stow command. if __name__ == '__main__' : try : node = StowCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare object, node , from the StowCommand() class. Then run the main() function. Multipoint Command Example Begin by running the following command in the terminal in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the multipoint command node. # Terminal 2 rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python multipoint_command.py This will send a list of JointTrajectoryPoint message types to move Stretch's arm. The Code #!/usr/bin/env python import rospy import time from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm class MultiPointCommand ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to the stretch robot. \"\"\" def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_multipoint_command ( self ): \"\"\" Function that makes an action call and sends multiple joint trajectory goals to the joint_lift, wrist_extension, and joint_wrist_yaw. :param self: The self reference. \"\"\" point0 = JointTrajectoryPoint () point0 . positions = [ 0.2 , 0.0 , 3.4 ] point0 . velocities = [ 0.2 , 0.2 , 2.5 ] point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] point1 = JointTrajectoryPoint () point1 . positions = [ 0.3 , 0.1 , 2.0 ] point2 = JointTrajectoryPoint () point2 . positions = [ 0.5 , 0.2 , - 1.0 ] point3 = JointTrajectoryPoint () point3 . positions = [ 0.6 , 0.3 , 0.0 ] point4 = JointTrajectoryPoint () point4 . positions = [ 0.8 , 0.2 , 1.0 ] point5 = JointTrajectoryPoint () point5 . positions = [ 0.5 , 0.1 , 0.0 ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent list of goals = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): \"\"\" Function that initiates the multipoint_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'multipoint_command' , 'multipoint_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing multipoint command...' ) self . issue_multipoint_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = MultiPointCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained Seeing that there are similarities between the multipoint and stow command nodes, we will only breakdown the different components of the multipoint_command node. point0 = JointTrajectoryPoint () point0 . positions = [ 0.2 , 0.0 , 3.4 ] Set point0 as a JointTrajectoryPoint and provide desired positions. These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. The lift and wrist extension positions are expressed in meters, where as the wrist yaw is in radians. point0 . velocities = [ 0.2 , 0.2 , 2.5 ] Provide desired velocity of the lift (m/s), wrist extension (m/s), and wrist yaw (rad/s) for point0 . point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] Provide desired accelerations of the lift (m/s^2), wrist extension (m/s^2), and wrist yaw (rad/s^2). IMPORTANT NOTE : The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by a list of the 6 points. Specify the coordinate frame that we want ( base_link ) and set the time to be now. Single Joint Actuator You can also actuate a single joint for the Stretch. Below are the list of joints and their position limit. ############################# JOINT LIMITS ############################# joint_lift: lower_limit = 0.15, upper_limit = 1.10 # in meters wrist_extension: lower_limit = 0.00, upper_limit = 0.50 # in meters joint_wrist_yaw: lower_limit = -1.75, upper_limit = 4.00 # in radians joint_head_pan: lower_limit = -2.80, upper_limit = 2.90 # in radians joint_head_tilt: lower_limit = -1.60, upper_limit = 0.40 # in radians joint_gripper_finger_left: lower_limit = -0.35, upper_limit = 0.165 # in radians # INCLUDED JOINTS IN POSITION MODE translate_mobile_base: No lower or upper limit. Defined by a step size in meters rotate_mobile_base: No lower or upper limit. Defined by a step size in radians ######################################################################## Begin by running the following command in the terminal in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the single joint actuator node. # Terminal 2 rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python single_joint_actuator.py This will send a list of JointTrajectoryPoint message types to move Stretch's arm. The joint, joint_gripper_finger_left , is only needed when actuating the gripper. The Code #!/usr/bin/env python import rospy import time from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm class SingleJointActuator ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_command ( self ): \"\"\" Function that makes an action call and sends joint trajectory goals to a single joint :param self: The self reference. \"\"\" trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_head_pan' ] point0 = JointTrajectoryPoint () point0 . positions = [ 0.65 ] # point1 = JointTrajectoryPoint() # point1.positions = [0.5] trajectory_goal . trajectory . points = [ point0 ] #, point1] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'issue_command' , 'issue_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing command...' ) self . issue_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = SingleJointActuator () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) The Code Explained Since the code is quite similar to the multipoint_command code, we will only review the parts that differ. Now let's break the code down. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_head_pan' ] Here we only input joint name that we want to actuate. In this instance, we will actuate the joint_head_pan . point0 = JointTrajectoryPoint () point0 . positions = [ 0.65 ] # point1 = JointTrajectoryPoint() # point1.positions = [0.5] Set point0 as a JointTrajectoryPoint and provide desired position. You also have the option to send multiple point positions rather than one. trajectory_goal . trajectory . points = [ point0 ] #, point1] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points set by your list of points. Specify the coordinate frame that we want ( base_link ) and set the time to be now.","title":"Follow joint trajectory"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#followjointtrajectory-commands","text":"Stretch ROS driver offers a FollowJointTrajectory action service for its arm. Within this tutorial we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute.","title":"FollowJointTrajectory Commands"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#stow-command-example","text":"Begin by running the following command in the terminal in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the stow command node. # Terminal 2 rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python stow_command.py This will send a FollowJointTrajectory command to stow Stretch's arm.","title":"Stow Command Example"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code","text":"#!/usr/bin/env python import rospy from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm import time class StowCommand ( hm . HelloNode ): ''' A class that sends a joint trajectory goal to stow the Stretch's arm. ''' def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_stow_command ( self ): ''' Function that makes an action call and sends stow position goal. :param self: The self reference. ''' stow_point = JointTrajectoryPoint () stow_point . time_from_start = rospy . Duration ( 0.000 ) stow_point . positions = [ 0.2 , 0.0 , 3.4 ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): ''' Function that initiates stow_command function. :param self: The self reference. ''' hm . HelloNode . main ( self , 'stow_command' , 'stow_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'stowing...' ) self . issue_stow_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = StowCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a python script. import rospy from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm import time You need to import rospy if you are writing a ROS Node . Import the FollowJointTrajectoryGoal from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. The hello_helpers package consists of a module the provides various Python scripts used across stretch_ros . In this instance we are importing the hello_misc script. class StowCommand ( hm . HelloNode ): def __init__ ( self ): hm . HelloNode . __init__ ( self ) The StowCommand class inherits the HelloNode class from hm and is initialized. def issue_stow_command ( self ): stow_point = JointTrajectoryPoint () stow_point . time_from_start = rospy . Duration ( 0.000 ) stow_point . positions = [ 0.2 , 0.0 , 3.4 ] The issue_stow_command() is the name of the function that will stow Stretch's arm. Within the function, we set stow_point as a JointTrajectoryPoint and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined in the next set of the code. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by the positions set in stow_point . Specify the coordinate frame that we want ( base_link ) and set the time to be now. self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () Make the action call and send the goal. The last line of code waits for the result before it exits the python script. def main ( self ): hm . HelloNode . main ( self , 'stow_command' , 'stow_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'stowing...' ) self . issue_stow_command () time . sleep ( 2 ) Create a funcion, main() , to do all of the setup the hm.HelloNode class and issue the stow command. if __name__ == '__main__' : try : node = StowCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' ) Declare object, node , from the StowCommand() class. Then run the main() function.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#multipoint-command-example","text":"Begin by running the following command in the terminal in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the multipoint command node. # Terminal 2 rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python multipoint_command.py This will send a list of JointTrajectoryPoint message types to move Stretch's arm.","title":"Multipoint Command Example"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code_1","text":"#!/usr/bin/env python import rospy import time from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm class MultiPointCommand ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to the stretch robot. \"\"\" def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_multipoint_command ( self ): \"\"\" Function that makes an action call and sends multiple joint trajectory goals to the joint_lift, wrist_extension, and joint_wrist_yaw. :param self: The self reference. \"\"\" point0 = JointTrajectoryPoint () point0 . positions = [ 0.2 , 0.0 , 3.4 ] point0 . velocities = [ 0.2 , 0.2 , 2.5 ] point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] point1 = JointTrajectoryPoint () point1 . positions = [ 0.3 , 0.1 , 2.0 ] point2 = JointTrajectoryPoint () point2 . positions = [ 0.5 , 0.2 , - 1.0 ] point3 = JointTrajectoryPoint () point3 . positions = [ 0.6 , 0.3 , 0.0 ] point4 = JointTrajectoryPoint () point4 . positions = [ 0.8 , 0.2 , 1.0 ] point5 = JointTrajectoryPoint () point5 . positions = [ 0.5 , 0.1 , 0.0 ] trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent list of goals = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): \"\"\" Function that initiates the multipoint_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'multipoint_command' , 'multipoint_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing multipoint command...' ) self . issue_multipoint_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = MultiPointCommand () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code-explained_1","text":"Seeing that there are similarities between the multipoint and stow command nodes, we will only breakdown the different components of the multipoint_command node. point0 = JointTrajectoryPoint () point0 . positions = [ 0.2 , 0.0 , 3.4 ] Set point0 as a JointTrajectoryPoint and provide desired positions. These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. The lift and wrist extension positions are expressed in meters, where as the wrist yaw is in radians. point0 . velocities = [ 0.2 , 0.2 , 2.5 ] Provide desired velocity of the lift (m/s), wrist extension (m/s), and wrist yaw (rad/s) for point0 . point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] Provide desired accelerations of the lift (m/s^2), wrist extension (m/s^2), and wrist yaw (rad/s^2). IMPORTANT NOTE : The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by a list of the 6 points. Specify the coordinate frame that we want ( base_link ) and set the time to be now.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#single-joint-actuator","text":"You can also actuate a single joint for the Stretch. Below are the list of joints and their position limit. ############################# JOINT LIMITS ############################# joint_lift: lower_limit = 0.15, upper_limit = 1.10 # in meters wrist_extension: lower_limit = 0.00, upper_limit = 0.50 # in meters joint_wrist_yaw: lower_limit = -1.75, upper_limit = 4.00 # in radians joint_head_pan: lower_limit = -2.80, upper_limit = 2.90 # in radians joint_head_tilt: lower_limit = -1.60, upper_limit = 0.40 # in radians joint_gripper_finger_left: lower_limit = -0.35, upper_limit = 0.165 # in radians # INCLUDED JOINTS IN POSITION MODE translate_mobile_base: No lower or upper limit. Defined by a step size in meters rotate_mobile_base: No lower or upper limit. Defined by a step size in radians ######################################################################## Begin by running the following command in the terminal in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Switch the mode to position mode using a rosservice call. Then run the single joint actuator node. # Terminal 2 rosservice call /switch_to_position_mode cd catkin_ws/src/stretch_tutorials/src/ python single_joint_actuator.py This will send a list of JointTrajectoryPoint message types to move Stretch's arm. The joint, joint_gripper_finger_left , is only needed when actuating the gripper.","title":"Single Joint Actuator"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code_2","text":"#!/usr/bin/env python import rospy import time from control_msgs.msg import FollowJointTrajectoryGoal from trajectory_msgs.msg import JointTrajectoryPoint import hello_helpers.hello_misc as hm class SingleJointActuator ( hm . HelloNode ): \"\"\" A class that sends multiple joint trajectory goals to a single joint. \"\"\" def __init__ ( self ): hm . HelloNode . __init__ ( self ) def issue_command ( self ): \"\"\" Function that makes an action call and sends joint trajectory goals to a single joint :param self: The self reference. \"\"\" trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_head_pan' ] point0 = JointTrajectoryPoint () point0 . positions = [ 0.65 ] # point1 = JointTrajectoryPoint() # point1.positions = [0.5] trajectory_goal . trajectory . points = [ point0 ] #, point1] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal ( trajectory_goal ) rospy . loginfo ( 'Sent goal = {0} ' . format ( trajectory_goal )) self . trajectory_client . wait_for_result () def main ( self ): \"\"\" Function that initiates the issue_command function. :param self: The self reference. \"\"\" hm . HelloNode . main ( self , 'issue_command' , 'issue_command' , wait_for_first_pointcloud = False ) rospy . loginfo ( 'issuing command...' ) self . issue_command () time . sleep ( 2 ) if __name__ == '__main__' : try : node = SingleJointActuator () node . main () except KeyboardInterrupt : rospy . loginfo ( 'interrupt received, so shutting down' )","title":"The Code"},{"location":"stretch-tutorials/ros1_melodic/follow_joint_trajectory/#the-code-explained_2","text":"Since the code is quite similar to the multipoint_command code, we will only review the parts that differ. Now let's break the code down. trajectory_goal = FollowJointTrajectoryGoal () trajectory_goal . trajectory . joint_names = [ 'joint_head_pan' ] Here we only input joint name that we want to actuate. In this instance, we will actuate the joint_head_pan . point0 = JointTrajectoryPoint () point0 . positions = [ 0.65 ] # point1 = JointTrajectoryPoint() # point1.positions = [0.5] Set point0 as a JointTrajectoryPoint and provide desired position. You also have the option to send multiple point positions rather than one. trajectory_goal . trajectory . points = [ point0 ] #, point1] trajectory_goal . trajectory . header . stamp = rospy . Time ( 0.0 ) trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectoryGoal and define the joint names as a list. Then trajectory_goal.trajectory.points set by your list of points. Specify the coordinate frame that we want ( base_link ) and set the time to be now.","title":"The Code Explained"},{"location":"stretch-tutorials/ros1_melodic/gazebo_basics/","text":"Spawning Stretch in Simulation (Gazebo) Empty World Simulation To spawn the Stretch in gazebo's default empty world run the following command in your terminal. roslaunch stretch_gazebo gazebo.launch This will bringup the robot in the gazebo simulation similar to the image shown below. Custom World Simulation In gazebo, you are able to spawn Stretch in various worlds. First, source the gazebo world files by running the following command in a terminal echo \"source /usr/share/gazebo/setup.sh\" Then using the world argument, you can spawn the stretch in the willowgarage world by running the following roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world","title":"Spawning Stretch in Simulation (Gazebo)"},{"location":"stretch-tutorials/ros1_melodic/gazebo_basics/#spawning-stretch-in-simulation-gazebo","text":"","title":"Spawning Stretch in Simulation (Gazebo)"},{"location":"stretch-tutorials/ros1_melodic/gazebo_basics/#empty-world-simulation","text":"To spawn the Stretch in gazebo's default empty world run the following command in your terminal. roslaunch stretch_gazebo gazebo.launch This will bringup the robot in the gazebo simulation similar to the image shown below.","title":"Empty World Simulation"},{"location":"stretch-tutorials/ros1_melodic/gazebo_basics/#custom-world-simulation","text":"In gazebo, you are able to spawn Stretch in various worlds. First, source the gazebo world files by running the following command in a terminal echo \"source /usr/share/gazebo/setup.sh\" Then using the world argument, you can spawn the stretch in the willowgarage world by running the following roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world","title":"Custom World Simulation"},{"location":"stretch-tutorials/ros1_melodic/getting_started/","text":"Getting Started Ubuntu Hello Robot utilizes Ubuntu, an open-source Linux operating system, for the Stretch platform. If you are unfamiliar with the operating system, we encourage you to review a tutorial provided by Ubuntu. Additionally, the Linux command line, BASH, is used to execute commands and is needed to run ROS on the Stretch robot. Here is a tutorial on getting started with BASH. Creating Workspace Create a catkin workspace for your ROS packages. Here is an installation guide for creating a workspace . Once your system is set up, clone the stretch_tutorials to your workspace and build the package in your workspace. This can be done by copying the commands below and pasting them into your terminal. cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_tutorials.git cd ~/catkin_ws catkin_make Connecting a Monitor If you cannot access the robot through ssh due to your network settings, you will need to connect an HDMI monitor, USB keyboard, and mouse to the USB ports in the robot's trunk. ROS Setup on Local Computer Hello Robot is currently running Stretch on Ubuntu 20.04 and ROS Noetic. To begin the setup, start with installing Ubuntu desktop on your local machine. Then follow the installation guide for ROS Noetic on your system. Currently, the Realsense2_description package isn't installed by rosdep and requires a user to install the package manually. Run the following command in your terminal sudo apt-get install ros-noetic-realsense2-camera After your system is setup, clone the stretch_ros , stretch_tutorials , and realsense_gazebo_plugin packages to your src folder in your preferred workspace. cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros git clone https://github.com/pal-robotics/realsense_gazebo_plugin git clone https://github.com/hello-robot/stretch_tutorials.git Change the directory to that of your catkin workspace and install system dependencies of the ROS packages. Then build your workspace. cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make Once caktin_make has finished compiling,source your workspace and .bashrc file echo \"source /home/USER_NAME/catkin_ws/devel/setup.bash\" source ~/.bashrc RoboMaker If you cannot dual boot and install ubuntu on your local machine, an alternative is to use AWS RoboMaker . AWS RoboMaker extends the ROS framework with cloud services. The service provides a robotics simulation service, allowing for testing the Stretch platform. If you are a first-time user of AWS RoboMaker, follow the guide here to get up and running with the service.","title":"Getting Started"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#ubuntu","text":"Hello Robot utilizes Ubuntu, an open-source Linux operating system, for the Stretch platform. If you are unfamiliar with the operating system, we encourage you to review a tutorial provided by Ubuntu. Additionally, the Linux command line, BASH, is used to execute commands and is needed to run ROS on the Stretch robot. Here is a tutorial on getting started with BASH.","title":"Ubuntu"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#creating-workspace","text":"Create a catkin workspace for your ROS packages. Here is an installation guide for creating a workspace . Once your system is set up, clone the stretch_tutorials to your workspace and build the package in your workspace. This can be done by copying the commands below and pasting them into your terminal. cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_tutorials.git cd ~/catkin_ws catkin_make","title":"Creating Workspace"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#connecting-a-monitor","text":"If you cannot access the robot through ssh due to your network settings, you will need to connect an HDMI monitor, USB keyboard, and mouse to the USB ports in the robot's trunk.","title":"Connecting a Monitor"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#ros-setup-on-local-computer","text":"Hello Robot is currently running Stretch on Ubuntu 20.04 and ROS Noetic. To begin the setup, start with installing Ubuntu desktop on your local machine. Then follow the installation guide for ROS Noetic on your system. Currently, the Realsense2_description package isn't installed by rosdep and requires a user to install the package manually. Run the following command in your terminal sudo apt-get install ros-noetic-realsense2-camera After your system is setup, clone the stretch_ros , stretch_tutorials , and realsense_gazebo_plugin packages to your src folder in your preferred workspace. cd ~/catkin_ws/src git clone https://github.com/hello-robot/stretch_ros git clone https://github.com/pal-robotics/realsense_gazebo_plugin git clone https://github.com/hello-robot/stretch_tutorials.git Change the directory to that of your catkin workspace and install system dependencies of the ROS packages. Then build your workspace. cd ~/catkin_ws rosdep install --from-paths src --ignore-src -r -y catkin_make Once caktin_make has finished compiling,source your workspace and .bashrc file echo \"source /home/USER_NAME/catkin_ws/devel/setup.bash\" source ~/.bashrc","title":"ROS Setup on Local Computer"},{"location":"stretch-tutorials/ros1_melodic/getting_started/#robomaker","text":"If you cannot dual boot and install ubuntu on your local machine, an alternative is to use AWS RoboMaker . AWS RoboMaker extends the ROS framework with cloud services. The service provides a robotics simulation service, allowing for testing the Stretch platform. If you are a first-time user of AWS RoboMaker, follow the guide here to get up and running with the service.","title":"RoboMaker"},{"location":"stretch-tutorials/ros1_melodic/internal_state_of_stretch/","text":"Getting the State of the Robot Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then utilize the ROS command-line tool, rostopic , to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a terminal. # Terminal 2 rostopic echo /joint_states -n1 Note that the flag, -n1 , at the end of the command defines the count of how many times you wish to publish the current topic information. Remove the flag if you prefer to continuously print the topic for debugging purposes. Your terminal will output the information associated with the /joint_states topic. Your header , position , velocity , and effort information may vary from what is printed below. header: seq: 70999 stamp: secs: 1420 nsecs: 2000000 frame_id: '' name: [joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left, joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift, joint_right_wheel, joint_wrist_yaw] position: [-1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2.9291786329821434e-07, 1.3802900147297237e-06, 0.08154086954434359, 1.4361499260374905e-07, 0.4139061738340768, 9.32603306580404e-07] velocity: [0.00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1.322424459109634e-05, -0.00035084643762840415, 0.0012164337445918797, 0.0002138814988808099, 0.00010419792027496809, 4.0575263146426684e-05, 0.00022487596895736357, -0.0007751929074042957, 0.0002451588607332439] effort: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] --- Let's say you are interested in only seeing the header component of the /joint_states topic, you can output this within the rostopic command-line tool by typing the following command. # Terminal 2 rostopic echo /joint_states/header -n1 Your terminal will then output something similar to this: seq: 97277 stamp: secs: 1945 nsecs: 562000000 frame_id: '' --- Additionally, if you were to type rostopic echo / in the terminal, then press your Tab key on your keyboard, you will see the list of published active topics. A powerful tool to visualize the ROS communication is the ROS rqt_graph package . By typing the following, you can see a graph of topics being communicated between nodes. # Terminal 3 rqt_graph The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.","title":"Internal state of stretch"},{"location":"stretch-tutorials/ros1_melodic/internal_state_of_stretch/#getting-the-state-of-the-robot","text":"Begin by starting up the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then utilize the ROS command-line tool, rostopic , to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a terminal. # Terminal 2 rostopic echo /joint_states -n1 Note that the flag, -n1 , at the end of the command defines the count of how many times you wish to publish the current topic information. Remove the flag if you prefer to continuously print the topic for debugging purposes. Your terminal will output the information associated with the /joint_states topic. Your header , position , velocity , and effort information may vary from what is printed below. header: seq: 70999 stamp: secs: 1420 nsecs: 2000000 frame_id: '' name: [joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left, joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift, joint_right_wheel, joint_wrist_yaw] position: [-1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2.9291786329821434e-07, 1.3802900147297237e-06, 0.08154086954434359, 1.4361499260374905e-07, 0.4139061738340768, 9.32603306580404e-07] velocity: [0.00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1.322424459109634e-05, -0.00035084643762840415, 0.0012164337445918797, 0.0002138814988808099, 0.00010419792027496809, 4.0575263146426684e-05, 0.00022487596895736357, -0.0007751929074042957, 0.0002451588607332439] effort: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] --- Let's say you are interested in only seeing the header component of the /joint_states topic, you can output this within the rostopic command-line tool by typing the following command. # Terminal 2 rostopic echo /joint_states/header -n1 Your terminal will then output something similar to this: seq: 97277 stamp: secs: 1945 nsecs: 562000000 frame_id: '' --- Additionally, if you were to type rostopic echo / in the terminal, then press your Tab key on your keyboard, you will see the list of published active topics. A powerful tool to visualize the ROS communication is the ROS rqt_graph package . By typing the following, you can see a graph of topics being communicated between nodes. # Terminal 3 rqt_graph The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.","title":"Getting the State of the Robot"},{"location":"stretch-tutorials/ros1_melodic/moveit_basics/","text":"MoveIt! Basics <!-- MoveIt! on Stretch To run MoveIt with the actual hardware, (assuming stretch_driver is already running) simply run roslaunch stretch_moveit_config move_group.launch This will runs all of the planning capabilities, but without the setup, simulation and interface that the above demo provides. In order to create plans for the robot with the same interface as the offline demo, you can run roslaunch stretch_moveit_config moveit_rviz.launch ``` --> ## MoveIt! Without Hardware To begin running MoveIt! on stretch, run the demo launch file. This doesn ' t require any simulator or robot to run. ``` bash roslaunch stretch_moveit_config demo.launch This will brining up an RViz instance where you can move the robot around using interactive markers and create plans between poses. You can reference the bottom gif as a guide to plan and execute motion. Additionally, the demo allows a user to select from the three groups, stretch_arm , stretch_gripper , stretch_head to move. The option to change groups in the in Planning Request section in the Displays tree. A few notes to be kept in mind: Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning RViz plugin. stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning RViz plugin. Additionally, the demo allows a user to select from the three groups, stretch_arm , stretch_gripper , stretch_head to move. The option to change groups in the in Planning Request section in the Displays tree. A few notes to be kept in mind: Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning RViz plugin. stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning RViz plugin. ## Running Gazebo with MoveIt! and Stretch # Terminal 1: roslaunch stretch_gazebo gazebo.launch # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller # Terminal 3 roslaunch stretch_moveit_config demo_gazebo.launch This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to [this moveit tutorial](https://ros-planning.github.io/moveit_tutorials/doc/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#choosing-specific-start-goal-states). Running Gazebo with MoveIt! and Stretch # Terminal 1: roslaunch stretch_gazebo gazebo.launch # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller # Terminal 3 roslaunch stretch_moveit_config demo_gazebo.launch This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to this moveit tutorial .","title":"MoveIt! Basics"},{"location":"stretch-tutorials/ros1_melodic/moveit_basics/#moveit-basics","text":"<!--","title":"MoveIt! Basics"},{"location":"stretch-tutorials/ros1_melodic/moveit_basics/#moveit-on-stretch","text":"To run MoveIt with the actual hardware, (assuming stretch_driver is already running) simply run roslaunch stretch_moveit_config move_group.launch This will runs all of the planning capabilities, but without the setup, simulation and interface that the above demo provides. In order to create plans for the robot with the same interface as the offline demo, you can run roslaunch stretch_moveit_config moveit_rviz.launch ``` --> ## MoveIt! Without Hardware To begin running MoveIt! on stretch, run the demo launch file. This doesn ' t require any simulator or robot to run. ``` bash roslaunch stretch_moveit_config demo.launch This will brining up an RViz instance where you can move the robot around using interactive markers and create plans between poses. You can reference the bottom gif as a guide to plan and execute motion. Additionally, the demo allows a user to select from the three groups, stretch_arm , stretch_gripper , stretch_head to move. The option to change groups in the in Planning Request section in the Displays tree. A few notes to be kept in mind: Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning RViz plugin. stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning RViz plugin. Additionally, the demo allows a user to select from the three groups, stretch_arm , stretch_gripper , stretch_head to move. The option to change groups in the in Planning Request section in the Displays tree. A few notes to be kept in mind: Pre-defined start and goal states can be specified in Start State and Goal State drop downs in Planning tab of Motion Planning RViz plugin. stretch_gripper group does not show markers, and is intended to be controlled via the joints tab that is located in the very right of Motion Planning Rviz plugin. When planning with stretch_head group make sure you select Approx IK Solutions in Planning tab of Motion Planning RViz plugin. ## Running Gazebo with MoveIt! and Stretch # Terminal 1: roslaunch stretch_gazebo gazebo.launch # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller # Terminal 3 roslaunch stretch_moveit_config demo_gazebo.launch This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to [this moveit tutorial](https://ros-planning.github.io/moveit_tutorials/doc/quickstart_in_rviz/quickstart_in_rviz_tutorial.html#choosing-specific-start-goal-states).","title":"MoveIt! on Stretch"},{"location":"stretch-tutorials/ros1_melodic/moveit_basics/#running-gazebo-with-moveit-and-stretch","text":"# Terminal 1: roslaunch stretch_gazebo gazebo.launch # Terminal 2: roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller # Terminal 3 roslaunch stretch_moveit_config demo_gazebo.launch This will launch an Rviz instance that visualizes the joints with markers and an empty world in Gazebo with Stretch and load all the controllers. There are pre-defined positions for each joint group for demonstration purposes. There are three joint groups, namely stretch_arm, stretch_gripper and stretch_head that can be controlled individually via Motion Planning Rviz plugin. Start and goal positions for joints can be selected similar to this moveit tutorial .","title":"Running Gazebo with MoveIt! and Stretch"},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/","text":"Navigation Stack with Actual robot stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive Stretch around a mapped space. Running this code will require the robot to be untethered. Then run the following commands to map the space that the robot will navigate in. roslaunch stretch_navigation mapping.launch Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to stretch_user/ . mkdir -p ~/stretch_user/maps rosrun map_server map_saver -f ${ HELLO_FLEET_PATH } /maps/<map_name> The <map_name> does not include an extension. Map_saver will save two files as <map_name>.pgm and <map_name>.yaml . Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Rviz will show the robot in the previously mapped space; however, the robot's location on the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place. It is also possible to send 2D Pose Estimates and Nav Goals programmatically. In your launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages in order to navigate the robot programmatically. Running in Simulation To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the mapping_gazebo.launch and navigation_gazebo.launch launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch. # Terminal 1 roslaunch stretch_navigation mapping_gazebo.launch gazebo_world: = worlds/willowgarage.world Teleop using a Joystick Controller The mapping launch files, mapping.launch and mapping_gazebo.launch expose the ROS argument, \"teleop_type\". By default, this ROS arg is set to \"keyboard\", which launches keyboard teleop in the terminal. If the xbox controller that ships with Stretch is plugged into your computer, the following command will launch mapping with joystick teleop: # Terminal 2 roslaunch stretch_navigation mapping.launch teleop_type: = joystick Using ROS Remote Master If you have set up ROS Remote Master for untethered operation , you can use Rviz and teleop locally with the following commands: # On Robot roslaunch stretch_navigation mapping.launch rviz: = false teleop_type: = none # On your machine, Terminal 1: rviz -d ` rospack find stretch_navigation ` /rviz/mapping.launch # On your machine, Terminal 2: roslaunch stretch_core teleop_twist.launch teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller","title":"Navigation stack"},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/#navigation-stack-with-actual-robot","text":"stretch_navigation provides the standard ROS navigation stack as two launch files. This package utilizes gmapping, move_base, and AMCL to drive Stretch around a mapped space. Running this code will require the robot to be untethered. Then run the following commands to map the space that the robot will navigate in. roslaunch stretch_navigation mapping.launch Rviz will show the robot and the map that is being constructed. With the terminal open, use the instructions printed by the teleop package to teleoperate the robot around the room. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, you can run the following commands to save the map to stretch_user/ . mkdir -p ~/stretch_user/maps rosrun map_server map_saver -f ${ HELLO_FLEET_PATH } /maps/<map_name> The <map_name> does not include an extension. Map_saver will save two files as <map_name>.pgm and <map_name>.yaml . Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: roslaunch stretch_navigation navigation.launch map_yaml: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Rviz will show the robot in the previously mapped space; however, the robot's location on the map does not match the robot's location in the real space. In the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. AMCL, the localization package, will better localize our pose once we give the robot a 2D Nav Goal. In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to go. In the terminal, you'll see move_base go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior: spinning around 360 degrees in place. It is also possible to send 2D Pose Estimates and Nav Goals programmatically. In your launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send move_base_msgs::MoveBaseGoal messages in order to navigate the robot programmatically.","title":"Navigation Stack with Actual robot"},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/#running-in-simulation","text":"To perform mapping and navigation in the Gazebo simulation of Stretch, substitute the mapping_gazebo.launch and navigation_gazebo.launch launch files into the commands above. The default Gazebo environment is the Willow Garage HQ. Use the \"world\" ROS argument to specify the Gazebo world within which to spawn Stretch. # Terminal 1 roslaunch stretch_navigation mapping_gazebo.launch gazebo_world: = worlds/willowgarage.world","title":"Running in Simulation"},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/#teleop-using-a-joystick-controller","text":"The mapping launch files, mapping.launch and mapping_gazebo.launch expose the ROS argument, \"teleop_type\". By default, this ROS arg is set to \"keyboard\", which launches keyboard teleop in the terminal. If the xbox controller that ships with Stretch is plugged into your computer, the following command will launch mapping with joystick teleop: # Terminal 2 roslaunch stretch_navigation mapping.launch teleop_type: = joystick","title":"Teleop using a Joystick Controller"},{"location":"stretch-tutorials/ros1_melodic/navigation_stack/#using-ros-remote-master","text":"If you have set up ROS Remote Master for untethered operation , you can use Rviz and teleop locally with the following commands: # On Robot roslaunch stretch_navigation mapping.launch rviz: = false teleop_type: = none # On your machine, Terminal 1: rviz -d ` rospack find stretch_navigation ` /rviz/mapping.launch # On your machine, Terminal 2: roslaunch stretch_core teleop_twist.launch teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller","title":"Using ROS Remote Master"},{"location":"stretch-tutorials/ros1_melodic/perception/","text":"Perception Introduction The Stretch robot is equipped with the Intel RealSense D435i camera , an essential component that allows the robot to measure and analyze the world around it. In this tutorial, we are going to showcase how to visualize the various topics published from the camera. Begin by running the stretch driver.launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_low_resolution.launch Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 3 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz PointCloud2 Display A list of displays on the left side of the interface can visualize the camera data. Each display has its properties and status that notify a user if topic messages are received. For the PointCloud2 display, a sensor_msgs/pointCloud2 message named /camera/depth/color/points , is received and the gif below demonstrates the various display properties when visualizing the data. Image Display The Image display when toggled creates a new rendering window that visualizes a sensor_msgs/Image messaged, /camera/color/image_raw . This feature shows the image data from the camera; however, the image comes out sideways. Thus, you can select the /camera/color/image_raw_upright_view from the Image Topic options to get an upright view of the image. Camera Display The Camera display is similar to that of the Image display. In this setting, the rendering window also visualizes other displays, such as the PointCloud2, the RobotModel, and Grid Displays. The visibility property can toggle what displays your are interested in visualizing. DepthCloud Display The DepthCloud display is visualized in the main RViz window. This display takes in the depth image and RGB image, provided by the RealSense, to visualize and register a point cloud. Deep Perception Hello Robot also has a ROS package that uses deep learning models for various detection demos. A link to the package is provided: stretch_deep_perception .","title":"Perception"},{"location":"stretch-tutorials/ros1_melodic/perception/#perception-introduction","text":"The Stretch robot is equipped with the Intel RealSense D435i camera , an essential component that allows the robot to measure and analyze the world around it. In this tutorial, we are going to showcase how to visualize the various topics published from the camera. Begin by running the stretch driver.launch file. # Terminal 1 roslaunch stretch_core stretch_driver.launch To activate the RealSense camera and publish topics to be visualized, run the following launch file in a new terminal. # Terminal 2 roslaunch stretch_core d435i_low_resolution.launch Within this tutorial package, there is an RViz config file with the topics for perception already in the Display tree. You can visualize these topics and the robot model by running the command below in a new terminal. # Terminal 3 rosrun rviz rviz -d /home/hello-robot/catkin_ws/src/stretch_tutorials/rviz/perception_example.rviz","title":"Perception Introduction"},{"location":"stretch-tutorials/ros1_melodic/perception/#pointcloud2-display","text":"A list of displays on the left side of the interface can visualize the camera data. Each display has its properties and status that notify a user if topic messages are received. For the PointCloud2 display, a sensor_msgs/pointCloud2 message named /camera/depth/color/points , is received and the gif below demonstrates the various display properties when visualizing the data.","title":"PointCloud2 Display"},{"location":"stretch-tutorials/ros1_melodic/perception/#image-display","text":"The Image display when toggled creates a new rendering window that visualizes a sensor_msgs/Image messaged, /camera/color/image_raw . This feature shows the image data from the camera; however, the image comes out sideways. Thus, you can select the /camera/color/image_raw_upright_view from the Image Topic options to get an upright view of the image.","title":"Image Display"},{"location":"stretch-tutorials/ros1_melodic/perception/#camera-display","text":"The Camera display is similar to that of the Image display. In this setting, the rendering window also visualizes other displays, such as the PointCloud2, the RobotModel, and Grid Displays. The visibility property can toggle what displays your are interested in visualizing.","title":"Camera Display"},{"location":"stretch-tutorials/ros1_melodic/perception/#depthcloud-display","text":"The DepthCloud display is visualized in the main RViz window. This display takes in the depth image and RGB image, provided by the RealSense, to visualize and register a point cloud.","title":"DepthCloud Display"},{"location":"stretch-tutorials/ros1_melodic/perception/#deep-perception","text":"Hello Robot also has a ROS package that uses deep learning models for various detection demos. A link to the package is provided: stretch_deep_perception .","title":"Deep Perception"},{"location":"stretch-tutorials/ros1_melodic/respeaker_microphone_array/","text":"ReSpeaker Microphone Array For this tutorial, we will go over on a high level how to use Stretch's ReSpeaker Mic Array v2.0 . Stretch Body Package In this tutorial's section we will use command line tools in the Stretch_Body package, a low level Python API for Stretch's hardware, to directly interact with the ReSpeaker. Begin by typing the following command in a new terminal. stretch_respeaker_test.py The following will be displayed in your terminal hello-robot@stretch-re1-1005:~$ stretch_respeaker_test.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. * waiting for audio... * recording 3 seconds * done * playing audio * done The ReSpeaker Mico Array will wait until it hears audio loud enough to trigger its recording feature. Stretch will record audio for 3 seconds and then replay it through its speakers. This command line is a good method to see if the hardware is working correctly. To stop the python script, type Ctrl + c in the terminal. ReSpeaker_ROS Package A ROS package for the ReSpeaker is utilized for this tutorial's section. Begin by running the sample_respeaker.launch file in a terminal. # Terminal 1 roslaunch respeaker_ros sample_respeaker.launch This will bring up the necessary nodes that will allow the ReSpeaker to implement a voice and sound interface with the robot. Below are executables you can run and see the ReSpeaker results. rostopic echo /sound_direction # Result of Direction (in Radians) of Audio rostopic echo /sound_localization # Result of Direction as Pose (Quaternion values) rostopic echo /is_speeching # Result of Voice Activity Detector rostopic echo /audio # Raw audio data rostopic echo /speech_audio # Raw audio data when there is speech rostopic echo /speech_to_text # Voice recognition An example is when you run the speech_to_text executable and speak near the microphone array. In this instance, \"hello robot\" was said. # Terminal 2 hello-robot@stretch-re1-1005:~$ rostopic echo /speech_to_text transcript: - hello robot confidence: [] --- You can also set various parameters via dynamic_reconfigure running the following command in a new terminal. # Terminal 3 rosrun rqt_reconfigure rqt_reconfigure","title":"Respeaker microphone array"},{"location":"stretch-tutorials/ros1_melodic/respeaker_microphone_array/#respeaker-microphone-array","text":"For this tutorial, we will go over on a high level how to use Stretch's ReSpeaker Mic Array v2.0 .","title":"ReSpeaker Microphone Array"},{"location":"stretch-tutorials/ros1_melodic/respeaker_microphone_array/#stretch-body-package","text":"In this tutorial's section we will use command line tools in the Stretch_Body package, a low level Python API for Stretch's hardware, to directly interact with the ReSpeaker. Begin by typing the following command in a new terminal. stretch_respeaker_test.py The following will be displayed in your terminal hello-robot@stretch-re1-1005:~$ stretch_respeaker_test.py For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. * waiting for audio... * recording 3 seconds * done * playing audio * done The ReSpeaker Mico Array will wait until it hears audio loud enough to trigger its recording feature. Stretch will record audio for 3 seconds and then replay it through its speakers. This command line is a good method to see if the hardware is working correctly. To stop the python script, type Ctrl + c in the terminal.","title":"Stretch Body Package"},{"location":"stretch-tutorials/ros1_melodic/respeaker_microphone_array/#respeaker_ros-package","text":"A ROS package for the ReSpeaker is utilized for this tutorial's section. Begin by running the sample_respeaker.launch file in a terminal. # Terminal 1 roslaunch respeaker_ros sample_respeaker.launch This will bring up the necessary nodes that will allow the ReSpeaker to implement a voice and sound interface with the robot. Below are executables you can run and see the ReSpeaker results. rostopic echo /sound_direction # Result of Direction (in Radians) of Audio rostopic echo /sound_localization # Result of Direction as Pose (Quaternion values) rostopic echo /is_speeching # Result of Voice Activity Detector rostopic echo /audio # Raw audio data rostopic echo /speech_audio # Raw audio data when there is speech rostopic echo /speech_to_text # Voice recognition An example is when you run the speech_to_text executable and speak near the microphone array. In this instance, \"hello robot\" was said. # Terminal 2 hello-robot@stretch-re1-1005:~$ rostopic echo /speech_to_text transcript: - hello robot confidence: [] --- You can also set various parameters via dynamic_reconfigure running the following command in a new terminal. # Terminal 3 rosrun rqt_reconfigure rqt_reconfigure","title":"ReSpeaker_ROS Package"},{"location":"stretch-tutorials/ros1_melodic/rviz_basics/","text":"Visualizing with RViz You can utilize RViz to visualize Stretch's sensor information. To begin, run the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.roslaunch Then run the following command to bring up a simple RViz configuration of the Stretch robot. # Terminal 2 rosrun rviz rviz -d ` rospack find stretch_core ` /rviz/stretch_simple_test.rviz An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window. If you want the visualize Stretch's tf transform tree , you need to add the display type to the RViz window. First, click the Add button and include the TF type in the display. You will then see all of the transform frames of the Stretch robot, and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference. There are further tutorials for RViz and can be found here . Running RViz and Gazebo (Simulation) Let's bringup stretch in the willowgarage world from the gazebo basics tutorial and RViz by using the following command. roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world rviz: = true the rviz flag will open an RViz window to visualize a variety of ROS topics. Bringup the keyboard teleop to drive Stretch and observe its sensor input.","title":"Rviz basics"},{"location":"stretch-tutorials/ros1_melodic/rviz_basics/#visualizing-with-rviz","text":"You can utilize RViz to visualize Stretch's sensor information. To begin, run the stretch driver launch file. # Terminal 1 roslaunch stretch_core stretch_driver.roslaunch Then run the following command to bring up a simple RViz configuration of the Stretch robot. # Terminal 2 rosrun rviz rviz -d ` rospack find stretch_core ` /rviz/stretch_simple_test.rviz An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window. If you want the visualize Stretch's tf transform tree , you need to add the display type to the RViz window. First, click the Add button and include the TF type in the display. You will then see all of the transform frames of the Stretch robot, and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference. There are further tutorials for RViz and can be found here .","title":"Visualizing with RViz"},{"location":"stretch-tutorials/ros1_melodic/rviz_basics/#running-rviz-and-gazebo-simulation","text":"Let's bringup stretch in the willowgarage world from the gazebo basics tutorial and RViz by using the following command. roslaunch stretch_gazebo gazebo.launch world: = worlds/willowgarage.world rviz: = true the rviz flag will open an RViz window to visualize a variety of ROS topics. Bringup the keyboard teleop to drive Stretch and observe its sensor input.","title":"Running RViz and Gazebo (Simulation)"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/","text":"Teleoperating Stretch Xbox Controller Teleoperating Stretch comes ready to run out of the box. The Xbox Teleoperation demo will let you quickly test out the robot capabilities by teleoperating it with an Xbox Controller. Note: Make sure the USB Dongle is plugged into the the USB port of the base trunk. To start the demo: Remove the 'trunk' cover and power on the robot Wait for about 45 seconds. You will hear the Ubuntu startup sound, followed by two beeps (indicating the demo is running). Hit the Connect button on the controller. The upper two LEDs of the ring will illuminate. Hit the Home Robot button. Stretch will go through its homing calibration routine. Note : make sure the space around the robot is clear before running the Home function You're ready to go! A few things to try: Hit the Stow Robot button. The robot will assume the stow pose. Practice driving the robot around. Pull the Fast Base trigger while driving. When stowed, it will make Stretch drive faster Manually stop the arm or lift from moving to make it stop upon contact. Try picking up your cellphone from the floor Try grasping cup from a counter top Try delivering an object to a person If you're done, hold down the Shutdown PC button for 2 seconds. This will cause the PC to turn off. You can then power down the robot. Keyboard Teleoperating: Mobile Base Begin by running the following command in your terminal: # Terminal 1 roslaunch stretch_core stretch_driver.launch To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to nagivation for the robot to receive Twist messages. This is done using a rosservice call in a new terminal. In the same terminal run the teleop_twist_keyboard node with the argument remapping the cmd_vel topic name to stretch/cmd_vel . # Terminal 2 rosservice call /switch_to_navigation_mode rosrun teleop_twist_keyboard teleop_twist_keyboard.py cmd_vel: = stretch/cmd_vel Below are the keyboard commands that allow a user to move Stretch's base. Reading from the keyboard and Publishing to Twist! --------------------------- Moving around: u i o j k l m , . For Holonomic mode (strafing), hold down the shift key: --------------------------- U I O J K L M < > t : up (+z) b : down (-z) anything else : stop q/z : increase/decrease max speeds by 10% w/x : increase/decrease only linear speed by 10% e/c : increase/decrease only angular speed by 10% CTRL-C to quit currently: speed 0.5 turn 1.0 To stop the node from sending twist messages, type Ctrl + c . Create a node for Mobile Base Teleoperating To move Stretch's mobile base using a python script, please look at example 1 for reference. Keyboard Teleoperating: Full Body For full body teleoperation with the keyboard, you first need to run the stretch_driver.launch in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then in a new terminal, type the following command # Terminal 2 rosrun stretch_core keyboard_teleop Below are the keyboard commands that allow a user to control all of Stretch's joints. ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- To stop the node from sending twist messages, type Ctrl + c . Teleoperating in Gazebo Keyboard Teleoperating: Mobile Base For keyboard teleoperation of the Stretch's mobile base, first startup Stretch in simulation . Then run the following command in a new terminal. # Terminal 1 roslaunch stretch_gazebo gazebo.launch In a new terminal, type the following # Terminal 2 roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller The same keyboard commands will be presented to a user to move the robot. Xbox Controller Teleoperating An alternative for robot base teleoperation is to use an Xbox controller. Stop the keyboard teleoperation node by typing Ctrl + c in the terminal where the command was executed. Then connect the Xbox controller device to your local machine and run the following command. # Terminal 2 roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = joystick Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless pressed. For a Logitech F310 joystick, this button is A.","title":"Teleoperating stretch"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#teleoperating-stretch","text":"","title":"Teleoperating Stretch"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#xbox-controller-teleoperating","text":"Stretch comes ready to run out of the box. The Xbox Teleoperation demo will let you quickly test out the robot capabilities by teleoperating it with an Xbox Controller. Note: Make sure the USB Dongle is plugged into the the USB port of the base trunk. To start the demo: Remove the 'trunk' cover and power on the robot Wait for about 45 seconds. You will hear the Ubuntu startup sound, followed by two beeps (indicating the demo is running). Hit the Connect button on the controller. The upper two LEDs of the ring will illuminate. Hit the Home Robot button. Stretch will go through its homing calibration routine. Note : make sure the space around the robot is clear before running the Home function You're ready to go! A few things to try: Hit the Stow Robot button. The robot will assume the stow pose. Practice driving the robot around. Pull the Fast Base trigger while driving. When stowed, it will make Stretch drive faster Manually stop the arm or lift from moving to make it stop upon contact. Try picking up your cellphone from the floor Try grasping cup from a counter top Try delivering an object to a person If you're done, hold down the Shutdown PC button for 2 seconds. This will cause the PC to turn off. You can then power down the robot.","title":"Xbox Controller Teleoperating"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#keyboard-teleoperating-mobile-base","text":"Begin by running the following command in your terminal: # Terminal 1 roslaunch stretch_core stretch_driver.launch To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to nagivation for the robot to receive Twist messages. This is done using a rosservice call in a new terminal. In the same terminal run the teleop_twist_keyboard node with the argument remapping the cmd_vel topic name to stretch/cmd_vel . # Terminal 2 rosservice call /switch_to_navigation_mode rosrun teleop_twist_keyboard teleop_twist_keyboard.py cmd_vel: = stretch/cmd_vel Below are the keyboard commands that allow a user to move Stretch's base. Reading from the keyboard and Publishing to Twist! --------------------------- Moving around: u i o j k l m , . For Holonomic mode (strafing), hold down the shift key: --------------------------- U I O J K L M < > t : up (+z) b : down (-z) anything else : stop q/z : increase/decrease max speeds by 10% w/x : increase/decrease only linear speed by 10% e/c : increase/decrease only angular speed by 10% CTRL-C to quit currently: speed 0.5 turn 1.0 To stop the node from sending twist messages, type Ctrl + c .","title":"Keyboard Teleoperating: Mobile Base"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#create-a-node-for-mobile-base-teleoperating","text":"To move Stretch's mobile base using a python script, please look at example 1 for reference.","title":"Create a node for Mobile Base Teleoperating"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#keyboard-teleoperating-full-body","text":"For full body teleoperation with the keyboard, you first need to run the stretch_driver.launch in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then in a new terminal, type the following command # Terminal 2 rosrun stretch_core keyboard_teleop Below are the keyboard commands that allow a user to control all of Stretch's joints. ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- To stop the node from sending twist messages, type Ctrl + c .","title":"Keyboard Teleoperating: Full Body"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#teleoperating-in-gazebo","text":"","title":"Teleoperating in Gazebo"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#keyboard-teleoperating-mobile-base_1","text":"For keyboard teleoperation of the Stretch's mobile base, first startup Stretch in simulation . Then run the following command in a new terminal. # Terminal 1 roslaunch stretch_gazebo gazebo.launch In a new terminal, type the following # Terminal 2 roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = keyboard # or use teleop_type:=joystick if you have a controller The same keyboard commands will be presented to a user to move the robot.","title":"Keyboard Teleoperating: Mobile Base"},{"location":"stretch-tutorials/ros1_melodic/teleoperating_stretch/#xbox-controller-teleoperating_1","text":"An alternative for robot base teleoperation is to use an Xbox controller. Stop the keyboard teleoperation node by typing Ctrl + c in the terminal where the command was executed. Then connect the Xbox controller device to your local machine and run the following command. # Terminal 2 roslaunch stretch_core teleop_twist.launch twist_topic: = /stretch_diff_drive_controller/cmd_vel linear: = 1 .0 angular: = 2 .0 teleop_type: = joystick Note that the teleop_twist_joy package has a deadman switch by default which disables the drive commands to be published unless pressed. For a Logitech F310 joystick, this button is A.","title":"Xbox Controller Teleoperating"},{"location":"stretch-tutorials/ros2/","text":"Tutorial Track: Stretch ROS 2 (Beta) NOTE: Stretch's ROS2 packages and this ROS2 tutorial track are both under active development. They are considered 'beta', and we welcome any feedback. If you find any issues or bugs, please see the Stretch ROS2 and Stretch Tutorials issue trackers. Robot Operating System 2 (ROS 2) Despite the name, ROS is not an operating system. ROS is a middleware framework that is a collection of transport protocols, development and debugging tools, and open-source packages. As a transport protocol, ROS enables distributed communication via messages between nodes. As a development and debugging toolkit, ROS provides build systems that allows for writing applications in a wide variety of languages (Python and C++ are used in this tutorial track), a launch system to manage the execution of mutiple nodes simultaneously, and command line tools to interact with the running system. Finally, as a popular ecosystem, there are many open-source ROS packages that allow users to quickly prototype with new sensors, actuators, planners, perception stacks, and more. This tutorial track is for users looking to get familiar with programming Stretch robots via ROS 2. We recommend going through the tutorials in the following order: Basics Tutorial Description 1 Getting Started Setup instructions for ROS 2 on Stretch 2 Follow Joint Trajectory Commands Control joints using joint trajectory server. 3 Internal State of Stretch Monitor the joint states of Stretch. 4 RViz Basics Visualize topics in Stretch. 5 MoveIt2 Basics Motion planning and control for the arm using MoveIt. 6 MoveIt2 with Rviz Motion planning and control for the arm using MoveIt. 7 MoveIt2 MoveGroup C++ API Motion planning and control for the arm using MoveIt. Other Examples To help get you get started on your software development, here are examples of nodes to have the stretch perform simple tasks. Tutorial Description 1 Teleoperate Stretch with a Node Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 6 Obstacle Avoider Avoid obstacles using the planar lidar. 7 Align to ArUco Detect ArUco fiducials using OpenCV and align to them. 8 Deep Perception Use YOLOv5 to detect 3D objects in a point cloud.","title":"Overview"},{"location":"stretch-tutorials/ros2/#tutorial-track-stretch-ros-2-beta","text":"NOTE: Stretch's ROS2 packages and this ROS2 tutorial track are both under active development. They are considered 'beta', and we welcome any feedback. If you find any issues or bugs, please see the Stretch ROS2 and Stretch Tutorials issue trackers.","title":"Tutorial Track: Stretch ROS 2 (Beta)"},{"location":"stretch-tutorials/ros2/#robot-operating-system-2-ros-2","text":"Despite the name, ROS is not an operating system. ROS is a middleware framework that is a collection of transport protocols, development and debugging tools, and open-source packages. As a transport protocol, ROS enables distributed communication via messages between nodes. As a development and debugging toolkit, ROS provides build systems that allows for writing applications in a wide variety of languages (Python and C++ are used in this tutorial track), a launch system to manage the execution of mutiple nodes simultaneously, and command line tools to interact with the running system. Finally, as a popular ecosystem, there are many open-source ROS packages that allow users to quickly prototype with new sensors, actuators, planners, perception stacks, and more. This tutorial track is for users looking to get familiar with programming Stretch robots via ROS 2. We recommend going through the tutorials in the following order:","title":"Robot Operating System 2 (ROS 2)"},{"location":"stretch-tutorials/ros2/#basics","text":"Tutorial Description 1 Getting Started Setup instructions for ROS 2 on Stretch 2 Follow Joint Trajectory Commands Control joints using joint trajectory server. 3 Internal State of Stretch Monitor the joint states of Stretch. 4 RViz Basics Visualize topics in Stretch. 5 MoveIt2 Basics Motion planning and control for the arm using MoveIt. 6 MoveIt2 with Rviz Motion planning and control for the arm using MoveIt. 7 MoveIt2 MoveGroup C++ API Motion planning and control for the arm using MoveIt.","title":"Basics"},{"location":"stretch-tutorials/ros2/#other-examples","text":"To help get you get started on your software development, here are examples of nodes to have the stretch perform simple tasks. Tutorial Description 1 Teleoperate Stretch with a Node Use a python script that sends velocity commands. 2 Filter Laser Scans Publish new scan ranges that are directly in front of Stretch. 3 Mobile Base Collision Avoidance Stop Stretch from running into a wall. 4 Give Stretch a Balloon Create a \"balloon\" marker that goes where ever Stretch goes. 5 Tf2 Broadcaster and Listener Create a tf2 broadcaster and listener. 6 Obstacle Avoider Avoid obstacles using the planar lidar. 7 Align to ArUco Detect ArUco fiducials using OpenCV and align to them. 8 Deep Perception Use YOLOv5 to detect 3D objects in a point cloud.","title":"Other Examples"},{"location":"stretch-tutorials/ros2/align_to_aruco/","text":"Align to ArUco ArUco markers are a type of fiducials that are used extensively in robotics for identification and pose estimation. In this tutorial we will learn how to identify ArUco markers with the ArUco detection node and enable Stretch to navigate and align itself with respect to the marker. ArUco Detection Stretch uses the OpenCV ArUco detection library and is configured to identify a specific set of ArUco markers belonging to the 6x6, 250 dictionary. To understand why this is important, please refer to this handy guide provided by OpenCV. Stretch comes preconfigured to identify ArUco markers. The ROS node that enables this is the detect_aruco_markers node in the stretch_core package. Thanks to this node, identifying and estimating the pose of a marker is as easy as pointing the camera at the marker and running the detection node. It is also possible and quite convenient to visualize the detections with RViz. Computing Transformations If you have not already done so, now might be a good time to review the tf listener tutorial. Go on, we can wait\u2026 Now that we know how to program stretch to return the transform between known reference frames, we can use this knowledge to compute the transform between the detected marker and the robot base_link. From its current pose, for Stretch to align itself in front of the marker, we need to command it to reach there. But even before that, we need to program Stretch to know the goal pose. We define the goal pose to be 0.5 metre outward from the marker in the marker negative y-axis (Green axis). This is easier to visualize through the figure below. By monitoring the /aruco/marker_array and /aruco/axes topics, we can visualize the markers in RViz. The detection node also publishes the tf pose of the detected markers. This can be visualized by using the TF plugin and selecting the detected marker to inspect the pose. Next, we will use exactly that to compute the transform between the detected marker and the base_link of the robot. Now, we can compute the transformation from the robot base_link frame to the goal pose and pass this as an SE2 pose to the mobile base. Since we want Stretch to stop at a fixed distance with respect to the marker, we define a 0.5m offset in the marker y-axis where Stretch would come to a stop. At the same time, we also want Stretch to align its orientation to point its arm towards the marker so as to make the subsequent manipulation tasks easier to accomplish. This would result in the end pose of the base_link as shown in the above figure. Sweet! The next task is to generate a simple motion plan for the mobile base to reach this end pose. We do this in three steps: 1. Turn theta degrees towards the goal position. This would be the angle formed between the robot x-axis and the line connecting the start and the goal positions. 2. Travel straight to the goal position. This would be the euclidean distance between the start and the goal positions. 3. Turn phi degrees to attain the goal orientation. This would be the correction angle necessary to align the robot y-axis with the marker x-axis. Luckily, we know how to command Stretch to execute a trajectory using the joint trajectory server. If you are just starting, have a look at the Follow Joint Trajectory Commands tutorial to know how to command Stretch using the Joint trajectory Server. Warnings Since we won't be using the arm for this demo, it's safer to stow Stretch's arm in. Execute the command: stretch_robot_stow.py See It In Action First, we need to point the camera towards the marker. To do this, you could use the keyboard teleop node. To do this, run: ros2 launch stretch_core keyboard_teleop.launch.py When you are ready, execute the following command: ros2 launch stretch_core align_to_aruco.launch.py Code Breakdown Let's jump into the code to see how things work under the hood. Follow along here to have a look at the entire script. We make use of two separate Python classes for this demo. The FrameListener class is derived from the Node class and is the place where we compute the TF transformations. For an explantion of this class, you can refer to the TF listener tutorial. class FrameListener ( Node ): The AlignToAruco class is where we command Stretch to the pose goal. This class is derived from the FrameListener class so that they can both share the node instance. class AlignToAruco ( FrameListener ): The constructor initializes the Joint trajectory action client. It also initializes the attribute called offset that determines the end distance between the marker and the robot. def __init__ ( self , node , offset = 0.75 ): self . trans_base = TransformStamped () self . trans_camera = TransformStamped () self . joint_state = JointState () self . offset = offset self . node = node self . trajectory_client = ActionClient ( self . node , FollowJointTrajectory , '/stretch_controller/follow_joint_trajectory' ) server_reached = self . trajectory_client . wait_for_server ( timeout_sec = 60.0 ) if not server_reached : self . node . get_logger () . error ( 'Unable to connect to arm action server. Timeout exceeded.' ) sys . exit () The joint_states_callback is the callback method that receives the most recent joint state messages published on the /stretch/joint_states topic. def joint_states_callback ( self , joint_state ): self . joint_state = joint_state The copute_difference() method is where we call the get_transform() method from the FrameListener class to compute the difference between the base_link and base_right frame with an offset of 0.5 m in the negative y-axis. def compute_difference ( self ): self . trans_base , self . trans_camera = self . node . get_transforms () To compute the (x, y) coordinates of the SE2 pose goal, we compute the transformation here. R = quaternion_matrix (( x , y , z , w )) P_dash = np . array ([[ 0 ], [ - self . offset ], [ 0 ], [ 1 ]]) P = np . array ([[ self . trans_base . transform . translation . x ], [ self . trans_base . transform . translation . y ], [ 0 ], [ 1 ]]) X = np . matmul ( R , P_dash ) P_base = X + P base_position_x = P_base [ 0 , 0 ] base_position_y = P_base [ 1 , 0 ] From this, it is relatively straightforward to compute the angle phi and the euclidean distance dist. We then compute the angle z_rot_base to perform the last angle correction. phi = atan2 ( base_position_y , base_position_x ) dist = sqrt ( pow ( base_position_x , 2 ) + pow ( base_position_y , 2 )) x_rot_base , y_rot_base , z_rot_base = euler_from_quaternion ([ x , y , z , w ]) z_rot_base = - phi + z_rot_base + 3.14159 The align_to_marker() method is where we command Stretch to the pose goal in three steps using the Joint Trajectory action server. For an explanation on how to form the trajectory goal, you can refer to the Follow Joint Trajectory Commands tutorial. def align_to_marker ( self ): If you want to work with a different ArUco marker than the one we used in this tutorial, you can do so by changing line 44 in the code to the one you wish to detect. Also, don't forget to add the marker in the stretch_marker_dict.yaml ArUco marker dictionary.","title":"Align to ArUco"},{"location":"stretch-tutorials/ros2/align_to_aruco/#align-to-aruco","text":"ArUco markers are a type of fiducials that are used extensively in robotics for identification and pose estimation. In this tutorial we will learn how to identify ArUco markers with the ArUco detection node and enable Stretch to navigate and align itself with respect to the marker.","title":"Align to ArUco"},{"location":"stretch-tutorials/ros2/align_to_aruco/#aruco-detection","text":"Stretch uses the OpenCV ArUco detection library and is configured to identify a specific set of ArUco markers belonging to the 6x6, 250 dictionary. To understand why this is important, please refer to this handy guide provided by OpenCV. Stretch comes preconfigured to identify ArUco markers. The ROS node that enables this is the detect_aruco_markers node in the stretch_core package. Thanks to this node, identifying and estimating the pose of a marker is as easy as pointing the camera at the marker and running the detection node. It is also possible and quite convenient to visualize the detections with RViz.","title":"ArUco Detection"},{"location":"stretch-tutorials/ros2/align_to_aruco/#computing-transformations","text":"If you have not already done so, now might be a good time to review the tf listener tutorial. Go on, we can wait\u2026 Now that we know how to program stretch to return the transform between known reference frames, we can use this knowledge to compute the transform between the detected marker and the robot base_link. From its current pose, for Stretch to align itself in front of the marker, we need to command it to reach there. But even before that, we need to program Stretch to know the goal pose. We define the goal pose to be 0.5 metre outward from the marker in the marker negative y-axis (Green axis). This is easier to visualize through the figure below. By monitoring the /aruco/marker_array and /aruco/axes topics, we can visualize the markers in RViz. The detection node also publishes the tf pose of the detected markers. This can be visualized by using the TF plugin and selecting the detected marker to inspect the pose. Next, we will use exactly that to compute the transform between the detected marker and the base_link of the robot. Now, we can compute the transformation from the robot base_link frame to the goal pose and pass this as an SE2 pose to the mobile base. Since we want Stretch to stop at a fixed distance with respect to the marker, we define a 0.5m offset in the marker y-axis where Stretch would come to a stop. At the same time, we also want Stretch to align its orientation to point its arm towards the marker so as to make the subsequent manipulation tasks easier to accomplish. This would result in the end pose of the base_link as shown in the above figure. Sweet! The next task is to generate a simple motion plan for the mobile base to reach this end pose. We do this in three steps: 1. Turn theta degrees towards the goal position. This would be the angle formed between the robot x-axis and the line connecting the start and the goal positions. 2. Travel straight to the goal position. This would be the euclidean distance between the start and the goal positions. 3. Turn phi degrees to attain the goal orientation. This would be the correction angle necessary to align the robot y-axis with the marker x-axis. Luckily, we know how to command Stretch to execute a trajectory using the joint trajectory server. If you are just starting, have a look at the Follow Joint Trajectory Commands tutorial to know how to command Stretch using the Joint trajectory Server.","title":"Computing Transformations"},{"location":"stretch-tutorials/ros2/align_to_aruco/#warnings","text":"Since we won't be using the arm for this demo, it's safer to stow Stretch's arm in. Execute the command: stretch_robot_stow.py","title":"Warnings"},{"location":"stretch-tutorials/ros2/align_to_aruco/#see-it-in-action","text":"First, we need to point the camera towards the marker. To do this, you could use the keyboard teleop node. To do this, run: ros2 launch stretch_core keyboard_teleop.launch.py When you are ready, execute the following command: ros2 launch stretch_core align_to_aruco.launch.py","title":"See It In Action"},{"location":"stretch-tutorials/ros2/align_to_aruco/#code-breakdown","text":"Let's jump into the code to see how things work under the hood. Follow along here to have a look at the entire script. We make use of two separate Python classes for this demo. The FrameListener class is derived from the Node class and is the place where we compute the TF transformations. For an explantion of this class, you can refer to the TF listener tutorial. class FrameListener ( Node ): The AlignToAruco class is where we command Stretch to the pose goal. This class is derived from the FrameListener class so that they can both share the node instance. class AlignToAruco ( FrameListener ): The constructor initializes the Joint trajectory action client. It also initializes the attribute called offset that determines the end distance between the marker and the robot. def __init__ ( self , node , offset = 0.75 ): self . trans_base = TransformStamped () self . trans_camera = TransformStamped () self . joint_state = JointState () self . offset = offset self . node = node self . trajectory_client = ActionClient ( self . node , FollowJointTrajectory , '/stretch_controller/follow_joint_trajectory' ) server_reached = self . trajectory_client . wait_for_server ( timeout_sec = 60.0 ) if not server_reached : self . node . get_logger () . error ( 'Unable to connect to arm action server. Timeout exceeded.' ) sys . exit () The joint_states_callback is the callback method that receives the most recent joint state messages published on the /stretch/joint_states topic. def joint_states_callback ( self , joint_state ): self . joint_state = joint_state The copute_difference() method is where we call the get_transform() method from the FrameListener class to compute the difference between the base_link and base_right frame with an offset of 0.5 m in the negative y-axis. def compute_difference ( self ): self . trans_base , self . trans_camera = self . node . get_transforms () To compute the (x, y) coordinates of the SE2 pose goal, we compute the transformation here. R = quaternion_matrix (( x , y , z , w )) P_dash = np . array ([[ 0 ], [ - self . offset ], [ 0 ], [ 1 ]]) P = np . array ([[ self . trans_base . transform . translation . x ], [ self . trans_base . transform . translation . y ], [ 0 ], [ 1 ]]) X = np . matmul ( R , P_dash ) P_base = X + P base_position_x = P_base [ 0 , 0 ] base_position_y = P_base [ 1 , 0 ] From this, it is relatively straightforward to compute the angle phi and the euclidean distance dist. We then compute the angle z_rot_base to perform the last angle correction. phi = atan2 ( base_position_y , base_position_x ) dist = sqrt ( pow ( base_position_x , 2 ) + pow ( base_position_y , 2 )) x_rot_base , y_rot_base , z_rot_base = euler_from_quaternion ([ x , y , z , w ]) z_rot_base = - phi + z_rot_base + 3.14159 The align_to_marker() method is where we command Stretch to the pose goal in three steps using the Joint Trajectory action server. For an explanation on how to form the trajectory goal, you can refer to the Follow Joint Trajectory Commands tutorial. def align_to_marker ( self ): If you want to work with a different ArUco marker than the one we used in this tutorial, you can do so by changing line 44 in the code to the one you wish to detect. Also, don't forget to add the marker in the stretch_marker_dict.yaml ArUco marker dictionary.","title":"Code Breakdown"},{"location":"stretch-tutorials/ros2/coming_soon/","text":"ROS 2 tutorials are still under active development. Coming soon.","title":"Coming soon"},{"location":"stretch-tutorials/ros2/deep_perception/","text":"Deep Perception Ever wondered if there is a way to make a robot do awesome things without explicitly having to program it to do so? Deep Perception is a branch of Deep Learning that enables sensing the elements that make up an environment with the help of artificial neural networks without writing complicated code. Well, almost. The most wonderful thing about Stretch is that it comes preloaded with software that makes it a breeze to get started with topics such as Deep Learning. In this tutorial, we will deploy deep neural networks on Stretch using two popular Deep Learning frameworks, namely, PyTorch and OpenVino. YOLOv5 with PyTorch PyTorch is an open source end-to-end machine learning framework that makes many pretrained production quality neural networks available for general use. In this tutorial we will use the YOLOv5s model trained on the COCO dataset. YOLOv5 is a popular object detection model that divides a supplied image into a grid and detects objects in each cell of the grid recursively. The YOLOv5s model that we have deployed on Stretch has been pretrained on the COCO dataset which allows Stretch to detect a wide range of day to day objects. However, that\u2019s not all, in this demo we want to go a step further and use this extremely versatile object detection model to extract useful information about the scene. Extracting Bounding Boxes and Depth Information Often, it\u2019s not enough to simply identify an object. Stretch is a mobile manipulator and its job is to manipulate objects in its environment. But before it can do that, it needs information of where exactly the object is located with respect to itself so that a motion plan to reach the object can be generated. This is possible by knowing which pixels correspond to the object of interest in the image frame and then using that to extract the depth information in the camera frame. Once we have this information, it is possible to compute a transform of these points in the end effector frame for Stretch to generate a motion plan. For the sake of brevity, we will limit the scope of this tutorial to drawing bounding boxes around objects of interest to point to pixels in the image frame, and drawing a detection plane corresponding to depth pixels in the camera frame. Warning Running inference on Stretch results in continuous high current draw by the CPU. Pleas ensure proper ventilation with the onboard fan. It is recommended to run the demo in tethered mode. See It In Action Go ahead and execute the following command to run the inference and visualize the detections in RViz: ros2 launch stretch_deep_perception stretch_detect_objects.launch.py Voila! You just executed your first deep learning model on Stretch! Code Breakdown Luckily, the stretch_deep_pereption package is extremely modular and is designed to work with a wide array of detectors. Although most of the heavy lifting in this tutorial is being done by the neural network, let's attempt to breakdown the code into funtional blocks to understand the detection pipeline. The control flow begins with executing the detect_objects.py script. In the main() function, we create an instance of the ObjectDetector class from the object_detect_pytorch.py script where we configure the YOLOv5s model. Next, we pass this detector to an instance of the DetectionNode class from the detection_node.py script and call the main function. def main (): confidence_threshold = 0.0 detector = od . ObjectDetector ( confidence_threshold = confidence_threshold ) default_marker_name = 'object' node_name = 'DetectObjectsNode' topic_base_name = 'objects' fit_plane = False node = dn . DetectionNode ( detector , default_marker_name , node_name , topic_base_name , fit_plane ) node . main () Let's skim through the object_detect_pytorch.py script to understand the configuration. The constructor is where we load the pretrained YOLOv5s model using the torch.hub.load() PyTorch wrapper. We set the confidence threshold to be 0.2, which says that a detection is only considered valid if the probability is higher than 0.2. This can be tweaked, although lower numbers often result in false positives and higher numbers often disregard blurry or smaller valid objects. class ObjectDetector : def __init__ ( self , confidence_threshold = 0.2 ): # Load the models self . model = torch . hub . load ( 'ultralytics/yolov5' , 'yolov5s' ) # or yolov5m, yolov5l, yolov5x, custom self . confidence_threshold = confidence_threshold The apply_to_image() method passes the stream of RGB images from the realsense camera to the YOLOv5s model and returns detections in the form of a dictionary consisting of class_id, label, confidence and bouding box coordinates. The last part is exactly what we need for further computations. def apply_to_image ( self , rgb_image , draw_output = False ): results = self . model ( rgb_image ) ... if draw_output : output_image = rgb_image . copy () for detection_dict in results : self . draw_detection ( output_image , detection_dict ) return results , output_image This method calls the draw_detection() method to draw bounding boxes with the object labels and confidence thresholds over detected objects in the image using OpenCV. def draw_detection ( self , image , detection_dict ): ... cv2 . rectangle ( image , ( x_min , y_min ), ( x_max , y_max ), color , rectangle_line_thickness ) ... cv2 . rectangle ( image , ( label_x_min , label_y_min ), ( label_x_max , label_y_max ), ( 255 , 255 , 255 ), cv2 . FILLED ) cv2 . putText ( image , output_string , ( text_x , text_y ), font , font_scale , line_color , line_width , cv2 . LINE_AA ) Next, the script detection_node.py contains the class DetectionNode which is the main ROS node that subscribes to the RGB and depth images from the realsense camera and feeds them to the detector to run inference. The image_callback() method runs in a loop to subscribe to synchronized RGB and depth images. The RGB images are then rotated 90 degrees and passed to the apply_to_image() method. The returned output image is published on the visualize_object_detections_pub publisher, while the detections_2d dictionary is passed to the detections_2d_to_3d() method for further processing and drawing the detection plane. For detectors that also return markers and axes, it also publishes this information. def image_callback ( self , ros_rgb_image , ros_depth_image , rgb_camera_info ): ... detection_box_image = cv2 . rotate ( self . rgb_image , cv2 . ROTATE_90_CLOCKWISE ) ... detections_2d , output_image = self . detector . apply_to_image ( detection_box_image , draw_output = debug_output ) ... if output_image is not None : output_image = ros2_numpy . msgify ( Image , output_image , encoding = 'rgb8' ) if output_image is not None : self . visualize_object_detections_pub . publish ( output_image ) detections_3d = d2 . detections_2d_to_3d ( detections_2d , self . rgb_image , self . camera_info , self . depth_image , fit_plane = self . fit_plane , min_box_side_m = self . min_box_side_m , max_box_side_m = self . max_box_side_m ) ... if self . publish_marker_point_clouds : for marker in self . marker_collection : marker_points = marker . get_marker_point_cloud () self . add_point_array_to_point_cloud ( marker_points ) publish_plane_points = False if publish_plane_points : plane_points = marker . get_plane_fit_point_cloud () self . add_point_array_to_point_cloud ( plane_points ) self . publish_point_cloud () self . visualize_markers_pub . publish ( marker_array ) if axes_array is not None : self . visualize_axes_pub . publish ( axes_array ) Face Detection, Facial Landmarks Detection and Head Pose Estimation with OpenVINO and OpenCV Detecting objects is just one thing Stretch can do well, it can do much more using pretrained models. For this part of the tutorial, we will be using Intel\u2019s OpenVINO toolkit with OpenCV. The cool thing about this demo is that it uses three different models in tandem to not just detect human faces, but also important features of the human face such as the eyes, nose and the lips with head pose information. This is important in the context of precise manipulation tasks such as assisted feeding where we want to know the exact location of the facial features the end effector must reach. OpenVINO is a toolkit popularized by Intel to optimize and deploy machine learning inference that can utilize hardware acceleration dongles such as the Intel Neural Compute Stick with Intel based compute architectures. More convenient is the fact that most of the deep learning models in the Open Model Zoo are accessible and configurable using the familiar OpenCV API with the opencv-python-inference-engine library. With that, let\u2019s jump right into it! Warning Running inference on Stretch results in continuous high current draw by the CPU. Pleas ensure proper ventilation with the onboard fan. It is recommended to run the demo in tethered mode. See It In Action First, let\u2019s execute the following command to see what it looks like: ros2 launch stretch_deep_perception stretch_detect_faces.launch.py Code Breakdown Ain't that something! If you followed the breakdown in object detection, you'll find that the only change if you are looking to detect faces, facial landmarks or estimat head pose instead of detecting objects is in using a different deep learning model that does just that. For this, we will explore how to use the OpenVINO toolkit. Let's head to the detect_faces.py node to begin. In the main() method, we see a similar structure as with the object detction node. We first create an instance of the detector using the HeadPoseEstimator class from the head_estimator.py script to configure the deep learning models. Next, we pass this to an instance of the DetectionNode class from the detection_node.py script and call the main function. ... detector = he . HeadPoseEstimator ( models_directory , use_neural_compute_stick = use_neural_compute_stick ) default_marker_name = 'face' node_name = 'DetectFacesNode' topic_base_name = 'faces' fit_plane = False node = dn . DetectionNode ( detector , default_marker_name , node_name , topic_base_name , fit_plane , min_box_side_m = min_head_m , max_box_side_m = max_head_m ) node . main () In addition to detecting faces, this class also enables detecting facial landmarks as well as estimating head pose. The constructor initializes and configures three separate models, namely head_detection_model, head_pose_model and landmarks_model, with the help of the renamed_cv2.dnn.readNet() wrappers. Note that renamed_cv2 is simply the opencv_python_inference_engine library compiled under a different namespace for use with Stretch so as not to conflict with the regular OpenCV library and having functionalities from both available to users concurrently. class HeadPoseEstimator : def __init__ ( self , models_directory , use_neural_compute_stick = False ): ... self . head_detection_model = renamed_cv2 . dnn . readNetFromCaffe ( head_detection_model_prototxt_filename , head_detection_model_caffemodel_filename ) dm . print_model_info ( self . head_detection_model , 'head_detection_model' ) ... self . head_pose_model = renamed_cv2 . dnn . readNet ( head_pose_weights_filename , head_pose_config_filename ) ... self . landmarks_model = renamed_cv2 . dnn . readNet ( landmarks_weights_filename , landmarks_config_filename ) The apply_to_image() method calls individual methods like detect_faces(), estimate_head_pose() and detect_facial_landmarks() that each runs the inference using the models we configured above. The bounding_boxes from the face detection model are used to supply the cropped image of the faces to head pose and facial landmark models to make their job way more efficient. def apply_to_image ( self , rgb_image , draw_output = False ): ... boxes = self . detect_faces ( rgb_image ) facial_landmark_names = self . landmark_names . copy () for bounding_box in boxes : if draw_output : self . draw_bounding_box ( output_image , bounding_box ) yaw , pitch , roll = self . estimate_head_pose ( rgb_image , bounding_box , enlarge_box = True , enlarge_scale = 1.15 ) if yaw is not None : ypr = ( yaw , pitch , roll ) if draw_output : self . draw_head_pose ( output_image , yaw , pitch , roll , bounding_box ) else : ypr = None landmarks , landmark_names = self . detect_facial_landmarks ( rgb_image , bounding_box , enlarge_box = True , enlarge_scale = 1.15 ) if ( landmarks is not None ) and draw_output : self . draw_landmarks ( output_image , landmarks ) heads . append ({ 'box' : bounding_box , 'ypr' : ypr , 'landmarks' : landmarks }) return heads , output_image The detecion_node.py script then takes over as we saw with the object detection tutorial to publish the detections on pertinent topics. Now go ahead and experiment with a few more pretrained models using PyTorch or OpenVINO on Stretch. If you are feeling extra motivated, try creating your own neural networks and training them. Stretch is ready to deploy them!","title":"Deep Perception"},{"location":"stretch-tutorials/ros2/deep_perception/#deep-perception","text":"Ever wondered if there is a way to make a robot do awesome things without explicitly having to program it to do so? Deep Perception is a branch of Deep Learning that enables sensing the elements that make up an environment with the help of artificial neural networks without writing complicated code. Well, almost. The most wonderful thing about Stretch is that it comes preloaded with software that makes it a breeze to get started with topics such as Deep Learning. In this tutorial, we will deploy deep neural networks on Stretch using two popular Deep Learning frameworks, namely, PyTorch and OpenVino.","title":"Deep Perception"},{"location":"stretch-tutorials/ros2/deep_perception/#yolov5-with-pytorch","text":"PyTorch is an open source end-to-end machine learning framework that makes many pretrained production quality neural networks available for general use. In this tutorial we will use the YOLOv5s model trained on the COCO dataset. YOLOv5 is a popular object detection model that divides a supplied image into a grid and detects objects in each cell of the grid recursively. The YOLOv5s model that we have deployed on Stretch has been pretrained on the COCO dataset which allows Stretch to detect a wide range of day to day objects. However, that\u2019s not all, in this demo we want to go a step further and use this extremely versatile object detection model to extract useful information about the scene.","title":"YOLOv5 with PyTorch"},{"location":"stretch-tutorials/ros2/deep_perception/#extracting-bounding-boxes-and-depth-information","text":"Often, it\u2019s not enough to simply identify an object. Stretch is a mobile manipulator and its job is to manipulate objects in its environment. But before it can do that, it needs information of where exactly the object is located with respect to itself so that a motion plan to reach the object can be generated. This is possible by knowing which pixels correspond to the object of interest in the image frame and then using that to extract the depth information in the camera frame. Once we have this information, it is possible to compute a transform of these points in the end effector frame for Stretch to generate a motion plan. For the sake of brevity, we will limit the scope of this tutorial to drawing bounding boxes around objects of interest to point to pixels in the image frame, and drawing a detection plane corresponding to depth pixels in the camera frame.","title":"Extracting Bounding Boxes and Depth Information"},{"location":"stretch-tutorials/ros2/deep_perception/#warning","text":"Running inference on Stretch results in continuous high current draw by the CPU. Pleas ensure proper ventilation with the onboard fan. It is recommended to run the demo in tethered mode.","title":"Warning"},{"location":"stretch-tutorials/ros2/deep_perception/#see-it-in-action","text":"Go ahead and execute the following command to run the inference and visualize the detections in RViz: ros2 launch stretch_deep_perception stretch_detect_objects.launch.py Voila! You just executed your first deep learning model on Stretch!","title":"See It In Action"},{"location":"stretch-tutorials/ros2/deep_perception/#code-breakdown","text":"Luckily, the stretch_deep_pereption package is extremely modular and is designed to work with a wide array of detectors. Although most of the heavy lifting in this tutorial is being done by the neural network, let's attempt to breakdown the code into funtional blocks to understand the detection pipeline. The control flow begins with executing the detect_objects.py script. In the main() function, we create an instance of the ObjectDetector class from the object_detect_pytorch.py script where we configure the YOLOv5s model. Next, we pass this detector to an instance of the DetectionNode class from the detection_node.py script and call the main function. def main (): confidence_threshold = 0.0 detector = od . ObjectDetector ( confidence_threshold = confidence_threshold ) default_marker_name = 'object' node_name = 'DetectObjectsNode' topic_base_name = 'objects' fit_plane = False node = dn . DetectionNode ( detector , default_marker_name , node_name , topic_base_name , fit_plane ) node . main () Let's skim through the object_detect_pytorch.py script to understand the configuration. The constructor is where we load the pretrained YOLOv5s model using the torch.hub.load() PyTorch wrapper. We set the confidence threshold to be 0.2, which says that a detection is only considered valid if the probability is higher than 0.2. This can be tweaked, although lower numbers often result in false positives and higher numbers often disregard blurry or smaller valid objects. class ObjectDetector : def __init__ ( self , confidence_threshold = 0.2 ): # Load the models self . model = torch . hub . load ( 'ultralytics/yolov5' , 'yolov5s' ) # or yolov5m, yolov5l, yolov5x, custom self . confidence_threshold = confidence_threshold The apply_to_image() method passes the stream of RGB images from the realsense camera to the YOLOv5s model and returns detections in the form of a dictionary consisting of class_id, label, confidence and bouding box coordinates. The last part is exactly what we need for further computations. def apply_to_image ( self , rgb_image , draw_output = False ): results = self . model ( rgb_image ) ... if draw_output : output_image = rgb_image . copy () for detection_dict in results : self . draw_detection ( output_image , detection_dict ) return results , output_image This method calls the draw_detection() method to draw bounding boxes with the object labels and confidence thresholds over detected objects in the image using OpenCV. def draw_detection ( self , image , detection_dict ): ... cv2 . rectangle ( image , ( x_min , y_min ), ( x_max , y_max ), color , rectangle_line_thickness ) ... cv2 . rectangle ( image , ( label_x_min , label_y_min ), ( label_x_max , label_y_max ), ( 255 , 255 , 255 ), cv2 . FILLED ) cv2 . putText ( image , output_string , ( text_x , text_y ), font , font_scale , line_color , line_width , cv2 . LINE_AA ) Next, the script detection_node.py contains the class DetectionNode which is the main ROS node that subscribes to the RGB and depth images from the realsense camera and feeds them to the detector to run inference. The image_callback() method runs in a loop to subscribe to synchronized RGB and depth images. The RGB images are then rotated 90 degrees and passed to the apply_to_image() method. The returned output image is published on the visualize_object_detections_pub publisher, while the detections_2d dictionary is passed to the detections_2d_to_3d() method for further processing and drawing the detection plane. For detectors that also return markers and axes, it also publishes this information. def image_callback ( self , ros_rgb_image , ros_depth_image , rgb_camera_info ): ... detection_box_image = cv2 . rotate ( self . rgb_image , cv2 . ROTATE_90_CLOCKWISE ) ... detections_2d , output_image = self . detector . apply_to_image ( detection_box_image , draw_output = debug_output ) ... if output_image is not None : output_image = ros2_numpy . msgify ( Image , output_image , encoding = 'rgb8' ) if output_image is not None : self . visualize_object_detections_pub . publish ( output_image ) detections_3d = d2 . detections_2d_to_3d ( detections_2d , self . rgb_image , self . camera_info , self . depth_image , fit_plane = self . fit_plane , min_box_side_m = self . min_box_side_m , max_box_side_m = self . max_box_side_m ) ... if self . publish_marker_point_clouds : for marker in self . marker_collection : marker_points = marker . get_marker_point_cloud () self . add_point_array_to_point_cloud ( marker_points ) publish_plane_points = False if publish_plane_points : plane_points = marker . get_plane_fit_point_cloud () self . add_point_array_to_point_cloud ( plane_points ) self . publish_point_cloud () self . visualize_markers_pub . publish ( marker_array ) if axes_array is not None : self . visualize_axes_pub . publish ( axes_array )","title":"Code Breakdown"},{"location":"stretch-tutorials/ros2/deep_perception/#face-detection-facial-landmarks-detection-and-head-pose-estimation-with-openvino-and-opencv","text":"Detecting objects is just one thing Stretch can do well, it can do much more using pretrained models. For this part of the tutorial, we will be using Intel\u2019s OpenVINO toolkit with OpenCV. The cool thing about this demo is that it uses three different models in tandem to not just detect human faces, but also important features of the human face such as the eyes, nose and the lips with head pose information. This is important in the context of precise manipulation tasks such as assisted feeding where we want to know the exact location of the facial features the end effector must reach. OpenVINO is a toolkit popularized by Intel to optimize and deploy machine learning inference that can utilize hardware acceleration dongles such as the Intel Neural Compute Stick with Intel based compute architectures. More convenient is the fact that most of the deep learning models in the Open Model Zoo are accessible and configurable using the familiar OpenCV API with the opencv-python-inference-engine library. With that, let\u2019s jump right into it!","title":"Face Detection, Facial Landmarks Detection and Head Pose Estimation with OpenVINO and OpenCV"},{"location":"stretch-tutorials/ros2/deep_perception/#warning_1","text":"Running inference on Stretch results in continuous high current draw by the CPU. Pleas ensure proper ventilation with the onboard fan. It is recommended to run the demo in tethered mode.","title":"Warning"},{"location":"stretch-tutorials/ros2/deep_perception/#see-it-in-action_1","text":"First, let\u2019s execute the following command to see what it looks like: ros2 launch stretch_deep_perception stretch_detect_faces.launch.py","title":"See It In Action"},{"location":"stretch-tutorials/ros2/deep_perception/#code-breakdown_1","text":"Ain't that something! If you followed the breakdown in object detection, you'll find that the only change if you are looking to detect faces, facial landmarks or estimat head pose instead of detecting objects is in using a different deep learning model that does just that. For this, we will explore how to use the OpenVINO toolkit. Let's head to the detect_faces.py node to begin. In the main() method, we see a similar structure as with the object detction node. We first create an instance of the detector using the HeadPoseEstimator class from the head_estimator.py script to configure the deep learning models. Next, we pass this to an instance of the DetectionNode class from the detection_node.py script and call the main function. ... detector = he . HeadPoseEstimator ( models_directory , use_neural_compute_stick = use_neural_compute_stick ) default_marker_name = 'face' node_name = 'DetectFacesNode' topic_base_name = 'faces' fit_plane = False node = dn . DetectionNode ( detector , default_marker_name , node_name , topic_base_name , fit_plane , min_box_side_m = min_head_m , max_box_side_m = max_head_m ) node . main () In addition to detecting faces, this class also enables detecting facial landmarks as well as estimating head pose. The constructor initializes and configures three separate models, namely head_detection_model, head_pose_model and landmarks_model, with the help of the renamed_cv2.dnn.readNet() wrappers. Note that renamed_cv2 is simply the opencv_python_inference_engine library compiled under a different namespace for use with Stretch so as not to conflict with the regular OpenCV library and having functionalities from both available to users concurrently. class HeadPoseEstimator : def __init__ ( self , models_directory , use_neural_compute_stick = False ): ... self . head_detection_model = renamed_cv2 . dnn . readNetFromCaffe ( head_detection_model_prototxt_filename , head_detection_model_caffemodel_filename ) dm . print_model_info ( self . head_detection_model , 'head_detection_model' ) ... self . head_pose_model = renamed_cv2 . dnn . readNet ( head_pose_weights_filename , head_pose_config_filename ) ... self . landmarks_model = renamed_cv2 . dnn . readNet ( landmarks_weights_filename , landmarks_config_filename ) The apply_to_image() method calls individual methods like detect_faces(), estimate_head_pose() and detect_facial_landmarks() that each runs the inference using the models we configured above. The bounding_boxes from the face detection model are used to supply the cropped image of the faces to head pose and facial landmark models to make their job way more efficient. def apply_to_image ( self , rgb_image , draw_output = False ): ... boxes = self . detect_faces ( rgb_image ) facial_landmark_names = self . landmark_names . copy () for bounding_box in boxes : if draw_output : self . draw_bounding_box ( output_image , bounding_box ) yaw , pitch , roll = self . estimate_head_pose ( rgb_image , bounding_box , enlarge_box = True , enlarge_scale = 1.15 ) if yaw is not None : ypr = ( yaw , pitch , roll ) if draw_output : self . draw_head_pose ( output_image , yaw , pitch , roll , bounding_box ) else : ypr = None landmarks , landmark_names = self . detect_facial_landmarks ( rgb_image , bounding_box , enlarge_box = True , enlarge_scale = 1.15 ) if ( landmarks is not None ) and draw_output : self . draw_landmarks ( output_image , landmarks ) heads . append ({ 'box' : bounding_box , 'ypr' : ypr , 'landmarks' : landmarks }) return heads , output_image The detecion_node.py script then takes over as we saw with the object detection tutorial to publish the detections on pertinent topics. Now go ahead and experiment with a few more pretrained models using PyTorch or OpenVINO on Stretch. If you are feeling extra motivated, try creating your own neural networks and training them. Stretch is ready to deploy them!","title":"Code Breakdown"},{"location":"stretch-tutorials/ros2/example_1/","text":"Example 1 NOTE : ROS 2 tutorials are still under active development. The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending Twist messages to a Stretch robot. ros2 launch stretch_core stretch_driver.launch.py To drive the robot in circles with the move node, type the following in a new terminal. ros2 run stetch_ros_tutorials move To stop the node from sending twist messages, type Ctrl + c . The Code Below is the code which will send Twist messages to drive the robot in circles. #!/usr/bin/env python3 import rclpy from rclpy.node import Node from geometry_msgs.msg import Twist class Move ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_base_move' ) self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 10 ) self . get_logger () . info ( \"Starting to move in circle...\" ) timer_period = 0.5 # seconds self . timer = self . create_timer ( timer_period , self . move_around ) def move_around ( self ): command = Twist () command . linear . x = 0.0 command . linear . y = 0.0 command . linear . z = 0.0 command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.5 self . publisher_ . publish ( command ) def main ( args = None ): rclpy . init ( args = args ) base_motion = Move () rclpy . spin ( base_motion ) base_motion . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from geometry_msgs.msg import Twist You need to import rclpy if you are writing a ROS 2 Node. The geometry_msgs.msg import is so that we can send velocity commands to the robot. class Move ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_base_move' ) self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 10 ) This section of code defines the talker's interface to the rest of ROS. self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 10) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist. The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. timer_period = 0.5 # seconds self . timer = self . create_timer ( timer_period , self . move_around ) We create a timer with a period of 0.5 seconds. This timer ensures that the function move_around is called every 0.5 seconds. This ensures a constant rate of 2Hz for the execution loop. command = Twist () Make a Twist message. We're going to set all of the elements, since we can't depend on them defaulting to safe values. command . linear . x = 0.0 command . linear . y = 0.0 command . linear . z = 0.0 A Twist has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y direction or the z direction. We set the linear velocities to 0. command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.5 A Twist also has three rotational velocities (in radians per second). The Stretch will only respond to rotations around the z (vertical) axis. We set this to a non-zero value. self . publisher_ . publish ( command ) Publish the Twist commands in the previously defined topic name /stretch/cmd_vel . def main ( args = None ): rclpy . init ( args = args ) base_motion = Move () rclpy . spin ( base_motion ) base_motion . destroy_node () rclpy . shutdown () The next line, rclpy.init(args=args), is very important as it tells ROS to initialize the node. Until rclpy has this information, it cannot start communicating with the ROS Master. In this case, your node will take on the name 'stretch_base_move'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". We then create an instance called base_motion of the class Move(). This is then spun using the spin function in rclpy to call the callback functions, in our case the timer that ensures the move_around function is called at a steady rate of 2Hz. To stop the node from sending twist messages, type Ctrl + c .","title":"Teleoperate Stretch with a Node"},{"location":"stretch-tutorials/ros2/example_1/#example-1","text":"NOTE : ROS 2 tutorials are still under active development. The goal of this example is to give you an enhanced understanding of how to control the mobile base by sending Twist messages to a Stretch robot. ros2 launch stretch_core stretch_driver.launch.py To drive the robot in circles with the move node, type the following in a new terminal. ros2 run stetch_ros_tutorials move To stop the node from sending twist messages, type Ctrl + c .","title":"Example 1"},{"location":"stretch-tutorials/ros2/example_1/#the-code","text":"Below is the code which will send Twist messages to drive the robot in circles. #!/usr/bin/env python3 import rclpy from rclpy.node import Node from geometry_msgs.msg import Twist class Move ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_base_move' ) self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 10 ) self . get_logger () . info ( \"Starting to move in circle...\" ) timer_period = 0.5 # seconds self . timer = self . create_timer ( timer_period , self . move_around ) def move_around ( self ): command = Twist () command . linear . x = 0.0 command . linear . y = 0.0 command . linear . z = 0.0 command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.5 self . publisher_ . publish ( command ) def main ( args = None ): rclpy . init ( args = args ) base_motion = Move () rclpy . spin ( base_motion ) base_motion . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main ()","title":"The Code"},{"location":"stretch-tutorials/ros2/example_1/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from geometry_msgs.msg import Twist You need to import rclpy if you are writing a ROS 2 Node. The geometry_msgs.msg import is so that we can send velocity commands to the robot. class Move ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_base_move' ) self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 10 ) This section of code defines the talker's interface to the rest of ROS. self.publisher_ = self.create_publisher(Twist, '/stretch/cmd_vel', 10) declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist. The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. timer_period = 0.5 # seconds self . timer = self . create_timer ( timer_period , self . move_around ) We create a timer with a period of 0.5 seconds. This timer ensures that the function move_around is called every 0.5 seconds. This ensures a constant rate of 2Hz for the execution loop. command = Twist () Make a Twist message. We're going to set all of the elements, since we can't depend on them defaulting to safe values. command . linear . x = 0.0 command . linear . y = 0.0 command . linear . z = 0.0 A Twist has three linear velocities (in meters per second), along each of the axes. For Stretch, it will only pay attention to the x velocity, since it can't directly move in the y direction or the z direction. We set the linear velocities to 0. command . angular . x = 0.0 command . angular . y = 0.0 command . angular . z = 0.5 A Twist also has three rotational velocities (in radians per second). The Stretch will only respond to rotations around the z (vertical) axis. We set this to a non-zero value. self . publisher_ . publish ( command ) Publish the Twist commands in the previously defined topic name /stretch/cmd_vel . def main ( args = None ): rclpy . init ( args = args ) base_motion = Move () rclpy . spin ( base_motion ) base_motion . destroy_node () rclpy . shutdown () The next line, rclpy.init(args=args), is very important as it tells ROS to initialize the node. Until rclpy has this information, it cannot start communicating with the ROS Master. In this case, your node will take on the name 'stretch_base_move'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". We then create an instance called base_motion of the class Move(). This is then spun using the spin function in rclpy to call the callback functions, in our case the timer that ensures the move_around function is called at a steady rate of 2Hz. To stop the node from sending twist messages, type Ctrl + c .","title":"The Code Explained"},{"location":"stretch-tutorials/ros2/example_10/","text":"Example 10 NOTE : ROS 2 tutorials are still under active development. This tutorial provides you an idea of what tf2 can do in the Python track. We will elaborate how to create a tf2 static broadcaster and listener. tf2 Static Broadcaster For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the link_mast , link_lift , and link_wrist_yaw frames. Begin by starting up the stretch driver launch file. # Terminal 1 ros2 launch stretch_core stretch_driver.launch.py Open RViz in another terminal and add the RobotModel and TF plugins in the left hand panel # Terminal 2 ros2 run rviz2 rviz2 Then run the tf2 broadcaster node to visualize three static frames. # Terminal 3 ros2 run stretch_ros_tutorials tf2_broadcaster The gif below visualizes what happens when running the previous node. OPTIONAL : If you would like to see how the static frames update while the robot is in motion, run the stow command node and observe the tf frames in RViz. # Terminal 4 ros2 run stretch_ros_tutorials stow_command The Code #!/usr/bin/env python import rclpy from rclpy.node import Node from tf2_ros import TransformBroadcaster import tf_transformations from geometry_msgs.msg import TransformStamped # This node publishes three child static frames in reference to their parent frames as below: # parent -> link_mast child -> fk_link_mast # parent -> link_lift child -> fk_link_lift # parent -> link_wrist_yaw child -> fk_link_wrist_yaw class FixedFrameBroadcaster ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_tf_broadcaster' ) self . br = TransformBroadcaster ( self ) time_period = 0.1 # seconds self . timer = self . create_timer ( time_period , self . broadcast_timer_callback ) self . mast = TransformStamped () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 0.0 self . mast . transform . translation . z = 0.0 q = tf_transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . mast . transform . rotation . x = q [ 0 ] self . mast . transform . rotation . y = q [ 1 ] self . mast . transform . rotation . z = q [ 2 ] self . mast . transform . rotation . w = q [ 3 ] self . lift = TransformStamped () self . lift . header . stamp = self . get_clock () . now () . to_msg () self . lift . header . frame_id = 'link_lift' self . lift . child_frame_id = 'fk_link_lift' self . lift . transform . translation . x = 0.0 self . lift . transform . translation . y = 2.0 self . lift . transform . translation . z = 0.0 q = tf_transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . lift . transform . rotation . x = q [ 0 ] self . lift . transform . rotation . y = q [ 1 ] self . lift . transform . rotation . z = q [ 2 ] self . lift . transform . rotation . w = q [ 3 ] self . br . sendTransform ( self . lift ) self . wrist = TransformStamped () self . wrist . header . stamp = self . get_clock () . now () . to_msg () self . wrist . header . frame_id = 'link_wrist_yaw' self . wrist . child_frame_id = 'fk_link_wrist_yaw' self . wrist . transform . translation . x = 0.0 self . wrist . transform . translation . y = 2.0 self . wrist . transform . translation . z = 0.0 q = tf_transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] self . br . sendTransform ( self . wrist ) self . get_logger () . info ( \"Publishing Tf frames. Use RViz to visualize.\" ) def broadcast_timer_callback ( self ): self . mast . header . stamp = self . get_clock () . now () . to_msg () self . br . sendTransform ( self . mast ) self . lift . header . stamp = self . get_clock () . now () . to_msg () self . br . sendTransform ( self . lift ) self . wrist . header . stamp = self . get_clock () . now () . to_msg () self . br . sendTransform ( self . wrist ) def main ( args = None ): rclpy . init ( args = args ) tf_broadcaster = FixedFrameBroadcaster () rclpy . spin ( tf_broadcaster ) tf_broadcaster . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from tf2_ros import TransformBroadcaster import tf_transformations from geometry_msgs.msg import TransformStamped You need to import rclpy if you are writing a ROS 2 node. Import tf_transformations to get quaternion values from Euler angles. Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a TransformBroadcaster. to help make the task of publishing transforms easier. class FixedFrameBroadcaster ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_tf_broadcaster' ) self . br = TransformBroadcaster ( self ) Here we create a TransformStamped object which will be the message we will send over once populated. self . lift = TransformStamped () self . lift . header . stamp = self . get_clock () . now () . to_msg () self . lift . header . frame_id = 'link_lift' self . lift . child_frame_id = 'fk_link_lift' We need to give the transform being published a timestamp, we'll just stamp it with the current time, self.get_clock().now().to_msg() . Then, we need to set the name of the parent frame of the link we're creating, in this case link_lift . Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is fk_link_lift . self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 0.0 self . mast . transform . translation . z = 0.0 Set the translation values for the child frame. q = tf_transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . lift . transform . rotation . x = q [ 0 ] self . lift . transform . rotation . y = q [ 1 ] self . lift . transform . rotation . z = q [ 2 ] self . lift . transform . rotation . w = q [ 3 ] The quaternion_from_euler() function takes in a Euler angle argument and returns a quaternion values. Then set the rotation values to the transformed quaternions. This process will be completed for the link_mast and link_wrist_yaw as well. self . br . sendTransform ( self . lift ) Send the three transforms using the sendTransform() function. def main ( args = None ): rclpy . init ( args = args ) tf_broadcaster = FixedFrameBroadcaster () Instantiate the FixedFrameBroadcaster() class. rclpy . spin ( tf_broadcaster ) Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages. tf2 Static Listener In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section we will create a tf2 listener that will find the transform between fk_link_lift and link_grasp_center . Begin by starting up the stretch driver launch file. # Terminal 1 ros2 launch stretch_core stretch_driver.launch.py Then run the tf2 broadcaster node to create the three static frames. # Terminal 2 ros2 run stretch_ros_tutorials tf2_broadcaster Finally, run the tf2 listener node to print the transform between two links. # Terminal 3 ros2 run stretch_ros_tutorials tf2_listener Within the terminal the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames. [ INFO ] [ 1659551318 .098168 ] : The pose of target frame link_grasp_center with reference from fk_link_lift is: translation: x: 1 .08415191335 y: -0.176147838153 z: 0 .576720021135 rotation: x: -0.479004489528 y: -0.508053545368 z: -0.502884087254 w: 0 .509454501243 The Code import rclpy from rclpy.node import Node from rclpy.time import Time from tf2_ros import TransformException from tf2_ros.buffer import Buffer from tf2_ros.transform_listener import TransformListener class FrameListener ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_tf_listener' ) self . declare_parameter ( 'target_frame' , 'link_grasp_center' ) self . target_frame = self . get_parameter ( 'target_frame' ) . get_parameter_value () . string_value self . tf_buffer = Buffer () self . tf_listener = TransformListener ( self . tf_buffer , self ) time_period = 1.0 # seconds self . timer = self . create_timer ( time_period , self . on_timer ) def on_timer ( self ): from_frame_rel = self . target_frame to_frame_rel = 'fk_link_mast' try : now = Time () trans = self . tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , now ) except TransformException as ex : self . get_logger () . info ( f 'Could not transform { to_frame_rel } to { from_frame_rel } : { ex } ' ) return self . get_logger () . info ( f 'the pose of target frame { from_frame_rel } with reference to { to_frame_rel } is: { trans } ' ) def main (): rclpy . init () node = FrameListener () try : rclpy . spin ( node ) except KeyboardInterrupt : pass rclpy . shutdown () if __name__ == '__main__' : main () The Code Explained self . tf_buffer = Buffer () self . tf_listener = TransformListener ( self . tf_buffer , self ) Here, we create a TransformListener object. Once the listener is created, it starts receiving tf2 transformations over the wire, and buffers them for up to 10 seconds. from_frame_rel = self . target_frame to_frame_rel = 'fk_link_mast' Store frame names in variables that will be used to compute transformations. try : now = Time () trans = self . tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , now ) except TransformException as ex : self . get_logger () . info ( f 'Could not transform { to_frame_rel } to { from_frame_rel } : { ex } ' ) return Try to look up the transform we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between from_frame_rel and to_frame_rel frames with the lookup_transform() function.","title":"Tf2 Broadcaster and Listener"},{"location":"stretch-tutorials/ros2/example_10/#example-10","text":"NOTE : ROS 2 tutorials are still under active development. This tutorial provides you an idea of what tf2 can do in the Python track. We will elaborate how to create a tf2 static broadcaster and listener.","title":"Example 10"},{"location":"stretch-tutorials/ros2/example_10/#tf2-static-broadcaster","text":"For the tf2 static broadcaster node, we will be publishing three child static frames in reference to the link_mast , link_lift , and link_wrist_yaw frames. Begin by starting up the stretch driver launch file. # Terminal 1 ros2 launch stretch_core stretch_driver.launch.py Open RViz in another terminal and add the RobotModel and TF plugins in the left hand panel # Terminal 2 ros2 run rviz2 rviz2 Then run the tf2 broadcaster node to visualize three static frames. # Terminal 3 ros2 run stretch_ros_tutorials tf2_broadcaster The gif below visualizes what happens when running the previous node. OPTIONAL : If you would like to see how the static frames update while the robot is in motion, run the stow command node and observe the tf frames in RViz. # Terminal 4 ros2 run stretch_ros_tutorials stow_command","title":"tf2 Static Broadcaster"},{"location":"stretch-tutorials/ros2/example_10/#the-code","text":"#!/usr/bin/env python import rclpy from rclpy.node import Node from tf2_ros import TransformBroadcaster import tf_transformations from geometry_msgs.msg import TransformStamped # This node publishes three child static frames in reference to their parent frames as below: # parent -> link_mast child -> fk_link_mast # parent -> link_lift child -> fk_link_lift # parent -> link_wrist_yaw child -> fk_link_wrist_yaw class FixedFrameBroadcaster ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_tf_broadcaster' ) self . br = TransformBroadcaster ( self ) time_period = 0.1 # seconds self . timer = self . create_timer ( time_period , self . broadcast_timer_callback ) self . mast = TransformStamped () self . mast . header . frame_id = 'link_mast' self . mast . child_frame_id = 'fk_link_mast' self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 0.0 self . mast . transform . translation . z = 0.0 q = tf_transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . mast . transform . rotation . x = q [ 0 ] self . mast . transform . rotation . y = q [ 1 ] self . mast . transform . rotation . z = q [ 2 ] self . mast . transform . rotation . w = q [ 3 ] self . lift = TransformStamped () self . lift . header . stamp = self . get_clock () . now () . to_msg () self . lift . header . frame_id = 'link_lift' self . lift . child_frame_id = 'fk_link_lift' self . lift . transform . translation . x = 0.0 self . lift . transform . translation . y = 2.0 self . lift . transform . translation . z = 0.0 q = tf_transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . lift . transform . rotation . x = q [ 0 ] self . lift . transform . rotation . y = q [ 1 ] self . lift . transform . rotation . z = q [ 2 ] self . lift . transform . rotation . w = q [ 3 ] self . br . sendTransform ( self . lift ) self . wrist = TransformStamped () self . wrist . header . stamp = self . get_clock () . now () . to_msg () self . wrist . header . frame_id = 'link_wrist_yaw' self . wrist . child_frame_id = 'fk_link_wrist_yaw' self . wrist . transform . translation . x = 0.0 self . wrist . transform . translation . y = 2.0 self . wrist . transform . translation . z = 0.0 q = tf_transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . wrist . transform . rotation . x = q [ 0 ] self . wrist . transform . rotation . y = q [ 1 ] self . wrist . transform . rotation . z = q [ 2 ] self . wrist . transform . rotation . w = q [ 3 ] self . br . sendTransform ( self . wrist ) self . get_logger () . info ( \"Publishing Tf frames. Use RViz to visualize.\" ) def broadcast_timer_callback ( self ): self . mast . header . stamp = self . get_clock () . now () . to_msg () self . br . sendTransform ( self . mast ) self . lift . header . stamp = self . get_clock () . now () . to_msg () self . br . sendTransform ( self . lift ) self . wrist . header . stamp = self . get_clock () . now () . to_msg () self . br . sendTransform ( self . wrist ) def main ( args = None ): rclpy . init ( args = args ) tf_broadcaster = FixedFrameBroadcaster () rclpy . spin ( tf_broadcaster ) tf_broadcaster . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main ()","title":"The Code"},{"location":"stretch-tutorials/ros2/example_10/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from tf2_ros import TransformBroadcaster import tf_transformations from geometry_msgs.msg import TransformStamped You need to import rclpy if you are writing a ROS 2 node. Import tf_transformations to get quaternion values from Euler angles. Import the TransformStamped from the geometry_msgs.msg package because we will be publishing static frames and it requires this message type. The tf2_ros package provides an implementation of a TransformBroadcaster. to help make the task of publishing transforms easier. class FixedFrameBroadcaster ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_tf_broadcaster' ) self . br = TransformBroadcaster ( self ) Here we create a TransformStamped object which will be the message we will send over once populated. self . lift = TransformStamped () self . lift . header . stamp = self . get_clock () . now () . to_msg () self . lift . header . frame_id = 'link_lift' self . lift . child_frame_id = 'fk_link_lift' We need to give the transform being published a timestamp, we'll just stamp it with the current time, self.get_clock().now().to_msg() . Then, we need to set the name of the parent frame of the link we're creating, in this case link_lift . Finally, we need to set the name of the child frame of the link we're creating. In this instance, the child frame is fk_link_lift . self . mast . transform . translation . x = 0.0 self . mast . transform . translation . y = 0.0 self . mast . transform . translation . z = 0.0 Set the translation values for the child frame. q = tf_transformations . quaternion_from_euler ( 1.5707 , 0 , - 1.5707 ) self . lift . transform . rotation . x = q [ 0 ] self . lift . transform . rotation . y = q [ 1 ] self . lift . transform . rotation . z = q [ 2 ] self . lift . transform . rotation . w = q [ 3 ] The quaternion_from_euler() function takes in a Euler angle argument and returns a quaternion values. Then set the rotation values to the transformed quaternions. This process will be completed for the link_mast and link_wrist_yaw as well. self . br . sendTransform ( self . lift ) Send the three transforms using the sendTransform() function. def main ( args = None ): rclpy . init ( args = args ) tf_broadcaster = FixedFrameBroadcaster () Instantiate the FixedFrameBroadcaster() class. rclpy . spin ( tf_broadcaster ) Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros2/example_10/#tf2-static-listener","text":"In the previous section of the tutorial, we created a tf2 broadcaster to publish three static transform frames. In this section we will create a tf2 listener that will find the transform between fk_link_lift and link_grasp_center . Begin by starting up the stretch driver launch file. # Terminal 1 ros2 launch stretch_core stretch_driver.launch.py Then run the tf2 broadcaster node to create the three static frames. # Terminal 2 ros2 run stretch_ros_tutorials tf2_broadcaster Finally, run the tf2 listener node to print the transform between two links. # Terminal 3 ros2 run stretch_ros_tutorials tf2_listener Within the terminal the transform will be printed every 1 second. Below is an example of what will be printed in the terminal. There is also an image for reference of the two frames. [ INFO ] [ 1659551318 .098168 ] : The pose of target frame link_grasp_center with reference from fk_link_lift is: translation: x: 1 .08415191335 y: -0.176147838153 z: 0 .576720021135 rotation: x: -0.479004489528 y: -0.508053545368 z: -0.502884087254 w: 0 .509454501243","title":"tf2 Static Listener"},{"location":"stretch-tutorials/ros2/example_10/#the-code_1","text":"import rclpy from rclpy.node import Node from rclpy.time import Time from tf2_ros import TransformException from tf2_ros.buffer import Buffer from tf2_ros.transform_listener import TransformListener class FrameListener ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_tf_listener' ) self . declare_parameter ( 'target_frame' , 'link_grasp_center' ) self . target_frame = self . get_parameter ( 'target_frame' ) . get_parameter_value () . string_value self . tf_buffer = Buffer () self . tf_listener = TransformListener ( self . tf_buffer , self ) time_period = 1.0 # seconds self . timer = self . create_timer ( time_period , self . on_timer ) def on_timer ( self ): from_frame_rel = self . target_frame to_frame_rel = 'fk_link_mast' try : now = Time () trans = self . tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , now ) except TransformException as ex : self . get_logger () . info ( f 'Could not transform { to_frame_rel } to { from_frame_rel } : { ex } ' ) return self . get_logger () . info ( f 'the pose of target frame { from_frame_rel } with reference to { to_frame_rel } is: { trans } ' ) def main (): rclpy . init () node = FrameListener () try : rclpy . spin ( node ) except KeyboardInterrupt : pass rclpy . shutdown () if __name__ == '__main__' : main ()","title":"The Code"},{"location":"stretch-tutorials/ros2/example_10/#the-code-explained_1","text":"self . tf_buffer = Buffer () self . tf_listener = TransformListener ( self . tf_buffer , self ) Here, we create a TransformListener object. Once the listener is created, it starts receiving tf2 transformations over the wire, and buffers them for up to 10 seconds. from_frame_rel = self . target_frame to_frame_rel = 'fk_link_mast' Store frame names in variables that will be used to compute transformations. try : now = Time () trans = self . tf_buffer . lookup_transform ( to_frame_rel , from_frame_rel , now ) except TransformException as ex : self . get_logger () . info ( f 'Could not transform { to_frame_rel } to { from_frame_rel } : { ex } ' ) return Try to look up the transform we want. Use a try-except block, since it may fail on any single call, due to internal timing issues in the transform publishers. Look up transform between from_frame_rel and to_frame_rel frames with the lookup_transform() function.","title":"The Code Explained"},{"location":"stretch-tutorials/ros2/example_2/","text":"Example 2 NOTE : ROS 2 tutorials are still under active development. The aim of this example is to provide instruction on how to filter scan messages. For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specification itself: # # Laser scans angles are measured counter clockwise, # with Stretch's LiDAR having both angle_min and angle_max facing forward # (very closely along the x-axis) of the device frame # std_msgs/Header header # timestamp data in a particular coordinate frame float32 angle_min # start angle of the scan [rad] float32 angle_max # end angle of the scan [rad] float32 angle_increment # angular distance between measurements [rad] float32 time_increment # time between measurements [seconds] float32 scan_time # time between scans [seconds] float32 range_min # minimum range value [m] float32 range_max # maximum range value [m] float32[] ranges # range data [m] (Note: values < range_min or > range_max should be discarded) float32[] intensities # intensity data [device-specific units] The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type. For a Stretch robot the start angle of the scan, angle_min , and end angle, angle_max , are closely located along the x-axis of Stretch's frame. angle_min and angle_max are set at -3.1416 and 3.1416 , respectively. This is illustrated by the images below. Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot. First, open a terminal and run the stretch driver launch file. ros2 launch stretch_core stretch_driver.launch.py Then in a new terminal run the rplidar launch file from stretch_core . ros2 launch stretch_core rplidar.launch.py To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan filter node by typing the following in a new terminal. ros2 run stretch_ros_tutorials scan_filter Then run the following command to bring up a simple RViz configuration of the Stretch robot. ros2 run rviz2 rviz2 -d ` ros2 pkg prefix stretch_calibration ` /rviz/stretch_simple_test.rviz Change the topic name from the LaserScan display from /scan to /filter_scan . The Code #!/usr/bin/env python3 import rclpy from rclpy.node import Node from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan class ScanFilter ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_scan_filter' ) self . pub = self . create_publisher ( LaserScan , '/filtered_scan' , 10 ) self . sub = self . create_subscription ( LaserScan , '/scan' , self . scan_filter_callback , 10 ) self . width = 1 self . extent = self . width / 2.0 self . get_logger () . info ( \"Publishing the filtered_scan topic. Use RViz to visualize.\" ) def scan_filter_callback ( self , msg ): angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] msg . ranges = new_ranges self . pub . publish ( msg ) def main ( args = None ): rclpy . init ( args = args ) scan_filter = ScanFilter () rclpy . spin ( scan_filter ) scan_filter . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan You need to import rclpy if you are writing a ROS Node. There are functions from numpy and math that are required within this code, that's why linspace, inf, and sin are imported. The sensor_msgs.msg import is so that we can subscribe and publish LaserScan messages. self . width = 1 self . extent = self . width / 2.0 We're going to assume that the robot is pointing up the x-axis, so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered. self . sub = self . create_subscription ( LaserScan , '/scan' , self . scan_filter_callback , 10 ) Set up a subscriber. We're going to subscribe to the topic \"/scan\", looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. self . pub = self . create_publisher ( LaserScan , '/filtered_scan' , 10 ) This declares that your node is publishing to the filtered_scan topic using the message type LaserScan. This lets any nodes listening on filtered_scan that we are going to publish data on that topic. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) This line of code utilizes linspace to compute each angle of the subscribed scan. points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". msg . ranges = new_ranges self . pub . publish ( msg ) Substitute in the new ranges in the original message, and republish it. def main ( args = None ): rclpy . init ( args = args ) scan_filter = ScanFilter () The next line, rclpy.init_node initializes the node. In this case, your node will take on the name 'stretch_scan_filter'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Setup Scanfilter class with scan_filter = Scanfilter() rclpy . spin ( scan_filter ) Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Filter Laser Scans"},{"location":"stretch-tutorials/ros2/example_2/#example-2","text":"NOTE : ROS 2 tutorials are still under active development. The aim of this example is to provide instruction on how to filter scan messages. For robots with laser scanners, ROS provides a special Message type in the sensor_msgs package called LaserScan to hold information about a given scan. Let's take a look at the message specification itself: # # Laser scans angles are measured counter clockwise, # with Stretch's LiDAR having both angle_min and angle_max facing forward # (very closely along the x-axis) of the device frame # std_msgs/Header header # timestamp data in a particular coordinate frame float32 angle_min # start angle of the scan [rad] float32 angle_max # end angle of the scan [rad] float32 angle_increment # angular distance between measurements [rad] float32 time_increment # time between measurements [seconds] float32 scan_time # time between scans [seconds] float32 range_min # minimum range value [m] float32 range_max # maximum range value [m] float32[] ranges # range data [m] (Note: values < range_min or > range_max should be discarded) float32[] intensities # intensity data [device-specific units] The above message tells you everything you need to know about a scan. Most importantly, you have the angle of each hit and its distance (range) from the scanner. If you want to work with raw range data, then the above message is all you need. There is also an image below that illustrates the components of the message type. For a Stretch robot the start angle of the scan, angle_min , and end angle, angle_max , are closely located along the x-axis of Stretch's frame. angle_min and angle_max are set at -3.1416 and 3.1416 , respectively. This is illustrated by the images below. Knowing the orientation of the LiDAR allows us to filter the scan values for a desired range. In this case, we are only considering the scan ranges in front of the stretch robot. First, open a terminal and run the stretch driver launch file. ros2 launch stretch_core stretch_driver.launch.py Then in a new terminal run the rplidar launch file from stretch_core . ros2 launch stretch_core rplidar.launch.py To filter the lidar scans for ranges that are directly in front of Stretch (width of 1 meter) run the scan filter node by typing the following in a new terminal. ros2 run stretch_ros_tutorials scan_filter Then run the following command to bring up a simple RViz configuration of the Stretch robot. ros2 run rviz2 rviz2 -d ` ros2 pkg prefix stretch_calibration ` /rviz/stretch_simple_test.rviz Change the topic name from the LaserScan display from /scan to /filter_scan .","title":"Example 2"},{"location":"stretch-tutorials/ros2/example_2/#the-code","text":"#!/usr/bin/env python3 import rclpy from rclpy.node import Node from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan class ScanFilter ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_scan_filter' ) self . pub = self . create_publisher ( LaserScan , '/filtered_scan' , 10 ) self . sub = self . create_subscription ( LaserScan , '/scan' , self . scan_filter_callback , 10 ) self . width = 1 self . extent = self . width / 2.0 self . get_logger () . info ( \"Publishing the filtered_scan topic. Use RViz to visualize.\" ) def scan_filter_callback ( self , msg ): angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] msg . ranges = new_ranges self . pub . publish ( msg ) def main ( args = None ): rclpy . init ( args = args ) scan_filter = ScanFilter () rclpy . spin ( scan_filter ) scan_filter . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main ()","title":"The Code"},{"location":"stretch-tutorials/ros2/example_2/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from numpy import linspace , inf from math import sin from sensor_msgs.msg import LaserScan You need to import rclpy if you are writing a ROS Node. There are functions from numpy and math that are required within this code, that's why linspace, inf, and sin are imported. The sensor_msgs.msg import is so that we can subscribe and publish LaserScan messages. self . width = 1 self . extent = self . width / 2.0 We're going to assume that the robot is pointing up the x-axis, so that any points with y coordinates further than half of the defined width (1 meter) from the axis are not considered. self . sub = self . create_subscription ( LaserScan , '/scan' , self . scan_filter_callback , 10 ) Set up a subscriber. We're going to subscribe to the topic \"/scan\", looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function \"callback\" automatically. self . pub = self . create_publisher ( LaserScan , '/filtered_scan' , 10 ) This declares that your node is publishing to the filtered_scan topic using the message type LaserScan. This lets any nodes listening on filtered_scan that we are going to publish data on that topic. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) This line of code utilizes linspace to compute each angle of the subscribed scan. points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] Here we are computing the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". msg . ranges = new_ranges self . pub . publish ( msg ) Substitute in the new ranges in the original message, and republish it. def main ( args = None ): rclpy . init ( args = args ) scan_filter = ScanFilter () The next line, rclpy.init_node initializes the node. In this case, your node will take on the name 'stretch_scan_filter'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Setup Scanfilter class with scan_filter = Scanfilter() rclpy . spin ( scan_filter ) Give control to ROS. This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros2/example_3/","text":"Example 3 NOTE : ROS 2 tutorials are still under active development. The aim of example 3 is to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward. ros2 launch stretch_core stretch_driver.launch.py Then in a new terminal type the following to activate the LiDAR sensor. ros2 launch stretch_core rplidar.launch.py To activate the avoider node, type the following in a new terminal. ros2 run stretch_ros_tutorials avoider To stop the node from sending twist messages, type Ctrl + c in the terminal running the avoider node. The Code #!/usr/bin/env python3 import rclpy from rclpy.node import Node from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan class Avoider ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_avoider' ) self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 1 ) self . subscriber_ = self . create_subscription ( LaserScan , '/scan' , self . set_speed , 10 ) def set_speed ( self , msg ): angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] error = min ( new_ranges ) - self . distance self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 self . publisher_ . publish ( self . twist ) def main ( args = None ): rclpy . init ( args = args ) avoider = Avoider () rclpy . spin ( avoider ) avoider . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan You need to import rclpy if you are writing a ROS Node. There are functions from numpy and math that are required within this code, thus linspace, inf, tanh, and sin are imported. The sensor_msgs.msg import is so that we can subscribe to LaserScan messages. The geometry_msgs.msg import is so that we can send velocity commands to the robot. self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 1 ) This declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist. self . subscriber_ = self . create_subscription ( LaserScan , '/scan' , self . set_speed , 10 ) Set up a subscriber. We're going to subscribe to the topic \"/scan\", looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function \"set_speed\" automatically. self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self.width is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing in the x-axis, any points with y coordinates further than half of the defined width ( self.extent ) from the x-axis are not considered. self.distance defines the stopping distance from an object. self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 Allocate a Twist to use, and set everything to zero. We're going to do this when the class is initiating. Redefining this within the callback function, set_speed() can be more computationally taxing. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] This line of code utilizes linspace to compute each angle of the subscribed scan. Here we compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". error = min ( new_ranges ) - self . distance Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as error . self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 Set the speed according to a tanh function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1 self . publisher_ . publish ( self . twist ) Publish the Twist message. def main ( args = None ): rclpy . init ( args = args ) avoider = Avoider () rclpy . spin ( avoider ) avoider . destroy_node () rclpy . shutdown () The next line, rclpy.init() method initializes the node. In this case, your node will take on the name 'stretch_avoider'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Setup Avoider class with avoider = Avioder() Give control to ROS with rclpy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"Mobile Base Collision Avoidance"},{"location":"stretch-tutorials/ros2/example_3/#example-3","text":"NOTE : ROS 2 tutorials are still under active development. The aim of example 3 is to combine the two previous examples and have Stretch utilize its laser scan data to avoid collision with objects as it drives forward. ros2 launch stretch_core stretch_driver.launch.py Then in a new terminal type the following to activate the LiDAR sensor. ros2 launch stretch_core rplidar.launch.py To activate the avoider node, type the following in a new terminal. ros2 run stretch_ros_tutorials avoider To stop the node from sending twist messages, type Ctrl + c in the terminal running the avoider node.","title":"Example 3"},{"location":"stretch-tutorials/ros2/example_3/#the-code","text":"#!/usr/bin/env python3 import rclpy from rclpy.node import Node from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan class Avoider ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_avoider' ) self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 1 ) self . subscriber_ = self . create_subscription ( LaserScan , '/scan' , self . set_speed , 10 ) def set_speed ( self , msg ): angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] error = min ( new_ranges ) - self . distance self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 self . publisher_ . publish ( self . twist ) def main ( args = None ): rclpy . init ( args = args ) avoider = Avoider () rclpy . spin ( avoider ) avoider . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main ()","title":"The Code"},{"location":"stretch-tutorials/ros2/example_3/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from numpy import linspace , inf , tanh from math import sin from geometry_msgs.msg import Twist from sensor_msgs.msg import LaserScan You need to import rclpy if you are writing a ROS Node. There are functions from numpy and math that are required within this code, thus linspace, inf, tanh, and sin are imported. The sensor_msgs.msg import is so that we can subscribe to LaserScan messages. The geometry_msgs.msg import is so that we can send velocity commands to the robot. self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 1 ) This declares that your node is publishing to the /stretch/cmd_vel topic using the message type Twist. self . subscriber_ = self . create_subscription ( LaserScan , '/scan' , self . set_speed , 10 ) Set up a subscriber. We're going to subscribe to the topic \"/scan\", looking for LaserScan messages. When a message comes in, ROS is going to pass it to the function \"set_speed\" automatically. self . width = 1 self . extent = self . width / 2.0 self . distance = 0.5 self.width is the width of the laser scan we want in front of Stretch. Since Stretch's front is pointing in the x-axis, any points with y coordinates further than half of the defined width ( self.extent ) from the x-axis are not considered. self.distance defines the stopping distance from an object. self . twist = Twist () self . twist . linear . x = 0.0 self . twist . linear . y = 0.0 self . twist . linear . z = 0.0 self . twist . angular . x = 0.0 self . twist . angular . y = 0.0 self . twist . angular . z = 0.0 Allocate a Twist to use, and set everything to zero. We're going to do this when the class is initiating. Redefining this within the callback function, set_speed() can be more computationally taxing. angles = linspace ( msg . angle_min , msg . angle_max , len ( msg . ranges )) points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] new_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , points )] This line of code utilizes linspace to compute each angle of the subscribed scan. Here we compute the y coordinates of the ranges that are below -2.5 and above 2.5 radians of the scan angles. These limits are sufficient for considering scan ranges in front of Stretch, but these values can be altered to your preference. If the absolute value of a point's y-coordinate is under self.extent then keep the range, otherwise use inf, which means \"no return\". error = min ( new_ranges ) - self . distance Calculate the difference of the closest measured scan and where we want the robot to stop. We define this as error . self . twist . linear . x = tanh ( error ) if ( error > 0.05 or error < - 0.05 ) else 0 Set the speed according to a tanh function. This method gives a nice smooth mapping from distance to speed, and asymptotes at +/- 1 self . publisher_ . publish ( self . twist ) Publish the Twist message. def main ( args = None ): rclpy . init ( args = args ) avoider = Avoider () rclpy . spin ( avoider ) avoider . destroy_node () rclpy . shutdown () The next line, rclpy.init() method initializes the node. In this case, your node will take on the name 'stretch_avoider'. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Setup Avoider class with avoider = Avioder() Give control to ROS with rclpy.spin() . This will allow the callback to be called whenever new messages come in. If we don't put this line in, then the node will not work, and ROS will not process any messages.","title":"The Code Explained"},{"location":"stretch-tutorials/ros2/example_4/","text":"Example 4 NOTE : ROS 2 tutorials are still under active development. Let's bringup stretch in RViz by using the following command. ros2 launch stretch_core stretch_driver.launch.py ros2 run rviz2 rviz2 -d ` ros2 pkg prefix stretch_calibrtion ` /rviz/stretch_simple_test.rviz In a new terminal run the following commands to create a marker. ros2 run stretch_ros_tutorials marker The gif below demonstrates how to add a new Marker display type, and change the topic name from visualization_marker to balloon . A red sphere Marker should appear above the Stretch robot. The Code #!/usr/bin/env python3 import rclpy from rclpy.node import Node from visualization_msgs.msg import Marker class Balloon ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_marker' ) self . publisher_ = self . create_publisher ( Marker , 'balloon' , 10 ) self . marker = Marker () self . marker . header . frame_id = '/base_link' self . marker . header . stamp = self . get_clock () . now () . to_msg () self . marker . type = self . marker . SPHERE self . marker . id = 0 self . marker . action = self . marker . ADD self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 self . marker . color . a = 1.0 self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 self . get_logger () . info ( \"Publishing the balloon topic. Use RViz to visualize.\" ) def publish_marker ( self ): self . publisher_ . publish ( self . marker ) def main ( args = None ): rclpy . init ( args = args ) balloon = Balloon () while rclpy . ok (): balloon . publish_marker () balloon . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from visualization_msgs.msg import Marker You need to import rclpy if you are writing a ROS 2 Node. Import the Marker type from the visualization_msgs.msg package. This import is required to publish a Marker, which will be visualized in RViz. self . publisher_ = self . create_publisher ( Marker , 'balloon' , 10 ) This declares that your node is publishing to the /ballon topic using the message type Twist . self . marker = Marker () self . marker . header . frame_id = '/base_link' self . marker . header . stamp = self . get_clock () . now () . to_msg () self . marker . type = self . marker . SPHERE Create a maker. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers self . marker . id = 0 Each marker has a unique ID number. If you have more than one marker that you want displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number of an existing marker, it will replace the existing marker with that ID number. self . marker . action = self . marker . ADD This line of code sets the action. You can add, delete, or modify markers. self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 These are the size parameters for the marker. These will vary by marker type. self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 Color of the object, specified as r/g/b/a, with values in the range of [0, 1]. self . marker . color . a = 1.0 The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible. self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in frame_id. In this case, the position will always be directly 2 meters above the frame_id ( base_link ), and will move with it. def publish_marker ( self ): self . publisher_ . publish ( self . marker ) Publish the Marker data structure to be visualized in RViz. def main ( args = None ): rclpy . init ( args = args ) balloon = Balloon () The next line, rospy.init. In this case, your node will take on the name talker. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Setup Balloon class with balloon = Balloon() while rclpy . ok (): balloon . publish_marker () balloon . destroy_node () rclpy . shutdown () This loop is a fairly standard rclpy construct: checking the rclpy.ok() flag and then doing work. You have to run this check to see if your program should exit (e.g. if there is a Ctrl-C or otherwise).","title":"Give Stretch a Balloon"},{"location":"stretch-tutorials/ros2/example_4/#example-4","text":"NOTE : ROS 2 tutorials are still under active development. Let's bringup stretch in RViz by using the following command. ros2 launch stretch_core stretch_driver.launch.py ros2 run rviz2 rviz2 -d ` ros2 pkg prefix stretch_calibrtion ` /rviz/stretch_simple_test.rviz In a new terminal run the following commands to create a marker. ros2 run stretch_ros_tutorials marker The gif below demonstrates how to add a new Marker display type, and change the topic name from visualization_marker to balloon . A red sphere Marker should appear above the Stretch robot.","title":"Example 4"},{"location":"stretch-tutorials/ros2/example_4/#the-code","text":"#!/usr/bin/env python3 import rclpy from rclpy.node import Node from visualization_msgs.msg import Marker class Balloon ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_marker' ) self . publisher_ = self . create_publisher ( Marker , 'balloon' , 10 ) self . marker = Marker () self . marker . header . frame_id = '/base_link' self . marker . header . stamp = self . get_clock () . now () . to_msg () self . marker . type = self . marker . SPHERE self . marker . id = 0 self . marker . action = self . marker . ADD self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 self . marker . color . a = 1.0 self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 self . get_logger () . info ( \"Publishing the balloon topic. Use RViz to visualize.\" ) def publish_marker ( self ): self . publisher_ . publish ( self . marker ) def main ( args = None ): rclpy . init ( args = args ) balloon = Balloon () while rclpy . ok (): balloon . publish_marker () balloon . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main ()","title":"The Code"},{"location":"stretch-tutorials/ros2/example_4/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from visualization_msgs.msg import Marker You need to import rclpy if you are writing a ROS 2 Node. Import the Marker type from the visualization_msgs.msg package. This import is required to publish a Marker, which will be visualized in RViz. self . publisher_ = self . create_publisher ( Marker , 'balloon' , 10 ) This declares that your node is publishing to the /ballon topic using the message type Twist . self . marker = Marker () self . marker . header . frame_id = '/base_link' self . marker . header . stamp = self . get_clock () . now () . to_msg () self . marker . type = self . marker . SPHERE Create a maker. Markers of all shapes share a common type. Set the frame ID and type. The frame ID is the frame in which the position of the marker is specified. The type is the shape of the marker. Further details on marker shapes can be found here: RViz Markers self . marker . id = 0 Each marker has a unique ID number. If you have more than one marker that you want displayed at a given time, then each needs to have a unique ID number. If you publish a new marker with the same ID number of an existing marker, it will replace the existing marker with that ID number. self . marker . action = self . marker . ADD This line of code sets the action. You can add, delete, or modify markers. self . marker . scale . x = 0.5 self . marker . scale . y = 0.5 self . marker . scale . z = 0.5 These are the size parameters for the marker. These will vary by marker type. self . marker . color . r = 1.0 self . marker . color . g = 0.0 self . marker . color . b = 0.0 Color of the object, specified as r/g/b/a, with values in the range of [0, 1]. self . marker . color . a = 1.0 The alpha value is from 0 (invisible) to 1 (opaque). If you don't set this then it will automatically default to zero, making your marker invisible. self . marker . pose . position . x = 0.0 self . marker . pose . position . y = 0.0 self . marker . pose . position . z = 2.0 Specify the pose of the marker. Since spheres are rotationally invariant, we're only going to specify the positional elements. As usual, these are in the coordinate frame named in frame_id. In this case, the position will always be directly 2 meters above the frame_id ( base_link ), and will move with it. def publish_marker ( self ): self . publisher_ . publish ( self . marker ) Publish the Marker data structure to be visualized in RViz. def main ( args = None ): rclpy . init ( args = args ) balloon = Balloon () The next line, rospy.init. In this case, your node will take on the name talker. NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". Setup Balloon class with balloon = Balloon() while rclpy . ok (): balloon . publish_marker () balloon . destroy_node () rclpy . shutdown () This loop is a fairly standard rclpy construct: checking the rclpy.ok() flag and then doing work. You have to run this check to see if your program should exit (e.g. if there is a Ctrl-C or otherwise).","title":"The Code Explained"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/","text":"FollowJointTrajectory Commands NOTE : ROS 2 tutorials are still under active development. Stretch driver offers a FollowJointTrajectory action service for its arm. Within this tutorial we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute. Stow Command Example Begin by launching stretch_driver in a terminal. ros2 launch stretch_core stretch_driver.launch.py In a new terminal type the following commands. ros2 run stretch_ros_tutorials stow_command This will send a FollowJointTrajectory command to stow Stretch's arm. The Code #!/usr/bin/env python3 import rclpy from rclpy.node import Node from rclpy.duration import Duration from rclpy.action import ActionClient import sys from control_msgs.action import FollowJointTrajectory from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState class StowCommand ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_stow_command' ) self . joint_state = JointState () self . trajectory_client = ActionClient ( self , FollowJointTrajectory , '/stretch_controller/follow_joint_trajectory' ) server_reached = self . trajectory_client . wait_for_server ( timeout_sec = 60.0 ) if not server_reached : self . get_logger () . error ( 'Unable to connect to arm action server. Timeout exceeded.' ) sys . exit () self . subscription = self . create_subscription ( JointState , '/stretch/joint_states' , self . joint_states_callback , 1 ) self . subscription def joint_states_callback ( self , joint_state ): self . joint_state = joint_state def issue_stow_command ( self ): joint_state = self . joint_state if ( joint_state is not None ): self . get_logger () . info ( 'stowing...' ) stow_point1 = JointTrajectoryPoint () stow_point2 = JointTrajectoryPoint () duration1 = Duration ( seconds = 0.0 ) duration2 = Duration ( seconds = 4.0 ) stow_point1 . time_from_start = duration1 . to_msg () stow_point2 . time_from_start = duration2 . to_msg () joint_value1 = joint_state . position [ 1 ] # joint_lift is at index 1 joint_value2 = joint_state . position [ 0 ] # wrist_extension is at index 0 joint_value3 = joint_state . position [ 8 ] # joint_wrist_yaw is at index 8 stow_point1 . positions = [ joint_value1 , joint_value2 , joint_value3 ] stow_point2 . positions = [ 0.2 , 0.0 , 3.14 ] trajectory_goal = FollowJointTrajectory . Goal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point1 , stow_point2 ] trajectory_goal . trajectory . header . stamp = self . get_clock () . now () . to_msg () trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal_async ( trajectory_goal ) self . get_logger () . info ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) def main ( args = None ): rclpy . init ( args = args ) stow_command = StowCommand () rclpy . spin_once ( stow_command ) stow_command . issue_stow_command () rclpy . spin ( stow_command ) stow_command . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main () The Code Explained Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from rclpy.duration import Duration from rclpy.action import ActionClient import sys from control_msgs.action import FollowJointTrajectory from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState You need to import rclpy if you are writing a ROS 2 Node. Import the FollowJointTrajectory from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. class StowCommand ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_stow_command' ) The StowCommand class inherits from the Node class from and is initialized. def issue_stow_command ( self ): The issue_stow_command() method will stow Stretch's arm. Within the function, we set stow_point as a JointTrajectoryPoint and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined in the next set of the code. stow_point1 = JointTrajectoryPoint () stow_point2 = JointTrajectoryPoint () duration1 = Duration ( seconds = 0.0 ) duration2 = Duration ( seconds = 4.0 ) stow_point1 . time_from_start = duration1 . to_msg () stow_point2 . time_from_start = duration2 . to_msg () joint_value1 = joint_state . position [ 1 ] # joint_lift is at index 1 joint_value2 = joint_state . position [ 0 ] # wrist_extension is at index 0 joint_value3 = joint_state . position [ 8 ] # joint_wrist_yaw is at index 8 stow_point1 . positions = [ joint_value1 , joint_value2 , joint_value3 ] stow_point2 . positions = [ 0.2 , 0.0 , 3.14 ] trajectory_goal = FollowJointTrajectory . Goal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point1 , stow_point2 ] trajectory_goal . trajectory . header . stamp = self . get_clock () . now () . to_msg () trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectory.Goal() and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by the positions set in stow_point . Specify the coordinate frame that we want (base_link) and set the time to be now. self . trajectory_client . send_goal_async ( trajectory_goal ) self . get_logger () . info ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) Make the action call and send the goal. def main ( args = None ): rclpy . init ( args = args ) stow_command = StowCommand () rclpy . spin_once ( stow_command ) stow_command . issue_stow_command () rclpy . spin ( stow_command ) stow_command . destroy_node () rclpy . shutdown () Create a funcion, main() , to do all of the setup in the class and issue the stow command. Initialize the StowCommand() class and set it to node and run the main() function. if __name__ == '__main__' : main () To make the script executable call the main() function like above. Multipoint Command Example If you have killed the above instance of stretch_driver relaunch it again through the terminal. ros2 launch stretch_core stretch_driver.launch.py In a new terminal type the following commands. ros2 run stretch_ros_tutorials multipoint_command This will send a list of JointTrajectoryPoint's to move Stretch's arm. The Code #!/usr/bin/env python3 import sys import rclpy from rclpy.node import Node from rclpy.action import ActionClient from rclpy.duration import Duration from control_msgs.action import FollowJointTrajectory from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState class MultiPointCommand ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_multipoint_command' ) self . trajectory_client = ActionClient ( self , FollowJointTrajectory , '/stretch_controller/follow_joint_trajectory' ) server_reached = self . trajectory_client . wait_for_server ( timeout_sec = 60.0 ) if not server_reached : self . get_logger () . error ( 'Unable to connect to arm action server. Timeout exceeded.' ) sys . exit () self . subscription = self . create_subscription ( JointState , '/stretch/joint_states' , self . joint_states_callback , 1 ) self . subscription self . get_logger () . info ( 'issuing multipoint command...' ) def joint_states_callback ( self , joint_state ): self . joint_state = joint_state def issue_multipoint_command ( self ): joint_state = self . joint_state duration0 = Duration ( seconds = 0.0 ) duration1 = Duration ( seconds = 2.0 ) duration2 = Duration ( seconds = 4.0 ) duration3 = Duration ( seconds = 6.0 ) duration4 = Duration ( seconds = 8.0 ) duration5 = Duration ( seconds = 10.0 ) joint_value1 = joint_state . position [ 1 ] # joint_lift is at index 1 joint_value2 = joint_state . position [ 0 ] # wrist_extension is at index 0 joint_value3 = joint_state . position [ 8 ] # joint_wrist_yaw is at index 8 point0 = JointTrajectoryPoint () point0 . positions = [ joint_value1 , joint_value2 , joint_value3 ] point0 . velocities = [ 0.2 , 0.2 , 2.5 ] point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] point0 . time_from_start = duration0 . to_msg () point1 = JointTrajectoryPoint () point1 . positions = [ 0.3 , 0.1 , 2.0 ] point1 . time_from_start = duration1 . to_msg () point2 = JointTrajectoryPoint () point2 . positions = [ 0.5 , 0.2 , - 1.0 ] point2 . time_from_start = duration2 . to_msg () point3 = JointTrajectoryPoint () point3 . positions = [ 0.6 , 0.3 , 0.0 ] point3 . time_from_start = duration3 . to_msg () point4 = JointTrajectoryPoint () point4 . positions = [ 0.8 , 0.2 , 1.0 ] point4 . time_from_start = duration4 . to_msg () point5 = JointTrajectoryPoint () point5 . positions = [ 0.5 , 0.1 , 0.0 ] point5 . time_from_start = duration5 . to_msg () trajectory_goal = FollowJointTrajectory . Goal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = self . get_clock () . now () . to_msg () trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal_async ( trajectory_goal ) self . get_logger () . info ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) def main ( args = None ): rclpy . init ( args = args ) multipoint_command = MultiPointCommand () rclpy . spin_once ( multipoint_command ) multipoint_command . issue_multipoint_command () rclpy . spin ( multipoint_command ) multipoint_command . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main () The Code Explained. Seeing that there are similarities between the multipoint and stow command nodes, we will only breakdown the different components of the multipoint_command node. point1 = JointTrajectoryPoint () point1 . positions = [ 0.3 , 0.1 , 2.0 ] point1 . time_from_start = duration1 . to_msg () Set point1 as a JointTrajectoryPoint and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. IMPORTANT NOTE : The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated. trajectory_goal = FollowJointTrajectory . Goal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = self . get_clock () . now () . to_msg () trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal_async ( trajectory_goal ) self . get_logger () . info ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) Set trajectory_goal as a FollowJointTrajectory.Goal() and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by a list of the 6 points. Specify the coordinate frame that we want (base_link) and set the time to be now.","title":"Follow Joint Trajectory Commands"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#followjointtrajectory-commands","text":"NOTE : ROS 2 tutorials are still under active development. Stretch driver offers a FollowJointTrajectory action service for its arm. Within this tutorial we will have a simple FollowJointTrajectory command sent to a Stretch robot to execute.","title":"FollowJointTrajectory Commands"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#stow-command-example","text":"Begin by launching stretch_driver in a terminal. ros2 launch stretch_core stretch_driver.launch.py In a new terminal type the following commands. ros2 run stretch_ros_tutorials stow_command This will send a FollowJointTrajectory command to stow Stretch's arm.","title":"Stow Command Example"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#the-code","text":"#!/usr/bin/env python3 import rclpy from rclpy.node import Node from rclpy.duration import Duration from rclpy.action import ActionClient import sys from control_msgs.action import FollowJointTrajectory from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState class StowCommand ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_stow_command' ) self . joint_state = JointState () self . trajectory_client = ActionClient ( self , FollowJointTrajectory , '/stretch_controller/follow_joint_trajectory' ) server_reached = self . trajectory_client . wait_for_server ( timeout_sec = 60.0 ) if not server_reached : self . get_logger () . error ( 'Unable to connect to arm action server. Timeout exceeded.' ) sys . exit () self . subscription = self . create_subscription ( JointState , '/stretch/joint_states' , self . joint_states_callback , 1 ) self . subscription def joint_states_callback ( self , joint_state ): self . joint_state = joint_state def issue_stow_command ( self ): joint_state = self . joint_state if ( joint_state is not None ): self . get_logger () . info ( 'stowing...' ) stow_point1 = JointTrajectoryPoint () stow_point2 = JointTrajectoryPoint () duration1 = Duration ( seconds = 0.0 ) duration2 = Duration ( seconds = 4.0 ) stow_point1 . time_from_start = duration1 . to_msg () stow_point2 . time_from_start = duration2 . to_msg () joint_value1 = joint_state . position [ 1 ] # joint_lift is at index 1 joint_value2 = joint_state . position [ 0 ] # wrist_extension is at index 0 joint_value3 = joint_state . position [ 8 ] # joint_wrist_yaw is at index 8 stow_point1 . positions = [ joint_value1 , joint_value2 , joint_value3 ] stow_point2 . positions = [ 0.2 , 0.0 , 3.14 ] trajectory_goal = FollowJointTrajectory . Goal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point1 , stow_point2 ] trajectory_goal . trajectory . header . stamp = self . get_clock () . now () . to_msg () trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal_async ( trajectory_goal ) self . get_logger () . info ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) def main ( args = None ): rclpy . init ( args = args ) stow_command = StowCommand () rclpy . spin_once ( stow_command ) stow_command . issue_stow_command () rclpy . spin ( stow_command ) stow_command . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main ()","title":"The Code"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#the-code-explained","text":"Now let's break the code down. #!/usr/bin/env python3 Every Python ROS Node will have this declaration at the top. The first line makes sure your script is executed as a Python script. import rclpy from rclpy.node import Node from rclpy.duration import Duration from rclpy.action import ActionClient import sys from control_msgs.action import FollowJointTrajectory from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState You need to import rclpy if you are writing a ROS 2 Node. Import the FollowJointTrajectory from the control_msgs.msg package to control the Stretch robot. Import JointTrajectoryPoint from the trajectory_msgs package to define robot trajectories. class StowCommand ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_stow_command' ) The StowCommand class inherits from the Node class from and is initialized. def issue_stow_command ( self ): The issue_stow_command() method will stow Stretch's arm. Within the function, we set stow_point as a JointTrajectoryPoint and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. These are defined in the next set of the code. stow_point1 = JointTrajectoryPoint () stow_point2 = JointTrajectoryPoint () duration1 = Duration ( seconds = 0.0 ) duration2 = Duration ( seconds = 4.0 ) stow_point1 . time_from_start = duration1 . to_msg () stow_point2 . time_from_start = duration2 . to_msg () joint_value1 = joint_state . position [ 1 ] # joint_lift is at index 1 joint_value2 = joint_state . position [ 0 ] # wrist_extension is at index 0 joint_value3 = joint_state . position [ 8 ] # joint_wrist_yaw is at index 8 stow_point1 . positions = [ joint_value1 , joint_value2 , joint_value3 ] stow_point2 . positions = [ 0.2 , 0.0 , 3.14 ] trajectory_goal = FollowJointTrajectory . Goal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ stow_point1 , stow_point2 ] trajectory_goal . trajectory . header . stamp = self . get_clock () . now () . to_msg () trajectory_goal . trajectory . header . frame_id = 'base_link' Set trajectory_goal as a FollowJointTrajectory.Goal() and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by the positions set in stow_point . Specify the coordinate frame that we want (base_link) and set the time to be now. self . trajectory_client . send_goal_async ( trajectory_goal ) self . get_logger () . info ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) Make the action call and send the goal. def main ( args = None ): rclpy . init ( args = args ) stow_command = StowCommand () rclpy . spin_once ( stow_command ) stow_command . issue_stow_command () rclpy . spin ( stow_command ) stow_command . destroy_node () rclpy . shutdown () Create a funcion, main() , to do all of the setup in the class and issue the stow command. Initialize the StowCommand() class and set it to node and run the main() function. if __name__ == '__main__' : main () To make the script executable call the main() function like above.","title":"The Code Explained"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#multipoint-command-example","text":"If you have killed the above instance of stretch_driver relaunch it again through the terminal. ros2 launch stretch_core stretch_driver.launch.py In a new terminal type the following commands. ros2 run stretch_ros_tutorials multipoint_command This will send a list of JointTrajectoryPoint's to move Stretch's arm.","title":"Multipoint Command Example"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#the-code_1","text":"#!/usr/bin/env python3 import sys import rclpy from rclpy.node import Node from rclpy.action import ActionClient from rclpy.duration import Duration from control_msgs.action import FollowJointTrajectory from trajectory_msgs.msg import JointTrajectoryPoint from sensor_msgs.msg import JointState class MultiPointCommand ( Node ): def __init__ ( self ): super () . __init__ ( 'stretch_multipoint_command' ) self . trajectory_client = ActionClient ( self , FollowJointTrajectory , '/stretch_controller/follow_joint_trajectory' ) server_reached = self . trajectory_client . wait_for_server ( timeout_sec = 60.0 ) if not server_reached : self . get_logger () . error ( 'Unable to connect to arm action server. Timeout exceeded.' ) sys . exit () self . subscription = self . create_subscription ( JointState , '/stretch/joint_states' , self . joint_states_callback , 1 ) self . subscription self . get_logger () . info ( 'issuing multipoint command...' ) def joint_states_callback ( self , joint_state ): self . joint_state = joint_state def issue_multipoint_command ( self ): joint_state = self . joint_state duration0 = Duration ( seconds = 0.0 ) duration1 = Duration ( seconds = 2.0 ) duration2 = Duration ( seconds = 4.0 ) duration3 = Duration ( seconds = 6.0 ) duration4 = Duration ( seconds = 8.0 ) duration5 = Duration ( seconds = 10.0 ) joint_value1 = joint_state . position [ 1 ] # joint_lift is at index 1 joint_value2 = joint_state . position [ 0 ] # wrist_extension is at index 0 joint_value3 = joint_state . position [ 8 ] # joint_wrist_yaw is at index 8 point0 = JointTrajectoryPoint () point0 . positions = [ joint_value1 , joint_value2 , joint_value3 ] point0 . velocities = [ 0.2 , 0.2 , 2.5 ] point0 . accelerations = [ 1.0 , 1.0 , 3.5 ] point0 . time_from_start = duration0 . to_msg () point1 = JointTrajectoryPoint () point1 . positions = [ 0.3 , 0.1 , 2.0 ] point1 . time_from_start = duration1 . to_msg () point2 = JointTrajectoryPoint () point2 . positions = [ 0.5 , 0.2 , - 1.0 ] point2 . time_from_start = duration2 . to_msg () point3 = JointTrajectoryPoint () point3 . positions = [ 0.6 , 0.3 , 0.0 ] point3 . time_from_start = duration3 . to_msg () point4 = JointTrajectoryPoint () point4 . positions = [ 0.8 , 0.2 , 1.0 ] point4 . time_from_start = duration4 . to_msg () point5 = JointTrajectoryPoint () point5 . positions = [ 0.5 , 0.1 , 0.0 ] point5 . time_from_start = duration5 . to_msg () trajectory_goal = FollowJointTrajectory . Goal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = self . get_clock () . now () . to_msg () trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal_async ( trajectory_goal ) self . get_logger () . info ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) def main ( args = None ): rclpy . init ( args = args ) multipoint_command = MultiPointCommand () rclpy . spin_once ( multipoint_command ) multipoint_command . issue_multipoint_command () rclpy . spin ( multipoint_command ) multipoint_command . destroy_node () rclpy . shutdown () if __name__ == '__main__' : main ()","title":"The Code"},{"location":"stretch-tutorials/ros2/follow_joint_trajectory/#the-code-explained_1","text":"Seeing that there are similarities between the multipoint and stow command nodes, we will only breakdown the different components of the multipoint_command node. point1 = JointTrajectoryPoint () point1 . positions = [ 0.3 , 0.1 , 2.0 ] point1 . time_from_start = duration1 . to_msg () Set point1 as a JointTrajectoryPoint and provide desired positions (in meters). These are the positions of the lift, wrist extension, and yaw of the wrist, respectively. IMPORTANT NOTE : The lift and wrist extension can only go up to 0.2 m/s. If you do not provide any velocities or accelerations for the lift or wrist extension, then they go to their default values. However, the Velocity and Acceleration of the wrist yaw will stay the same from the previous value unless updated. trajectory_goal = FollowJointTrajectory . Goal () trajectory_goal . trajectory . joint_names = [ 'joint_lift' , 'wrist_extension' , 'joint_wrist_yaw' ] trajectory_goal . trajectory . points = [ point0 , point1 , point2 , point3 , point4 , point5 ] trajectory_goal . trajectory . header . stamp = self . get_clock () . now () . to_msg () trajectory_goal . trajectory . header . frame_id = 'base_link' self . trajectory_client . send_goal_async ( trajectory_goal ) self . get_logger () . info ( 'Sent stow goal = {0} ' . format ( trajectory_goal )) Set trajectory_goal as a FollowJointTrajectory.Goal() and define the joint names as a list. Then trajectory_goal.trajectory.points is defined by a list of the 6 points. Specify the coordinate frame that we want (base_link) and set the time to be now.","title":"The Code Explained."},{"location":"stretch-tutorials/ros2/gazebo_basics/","text":"Spawning Stretch in Simulation (Gazebo) NOTE : ROS 2 tutorials are still under active development. NOTE Simulation support for Stretch in ROS 2 is under active development. Please reach out to us if you want to work with Stretch in a simulated environment like Gazebo/Ignition in ROS 2. Refer to the instructions below if you want to test this functionality in ROS 1. Empty World Simulation To spawn the Stretch in gazebo's default empty world run the following command in your terminal. roslaunch stretch_gazebo gazebo.launch This will bringup the robot in the gazebo simulation similar to the image shown below. Custom World Simulation In gazebo, you are able to spawn Stretch in various worlds. First, source the gazebo world files by running the following command in a terminal echo \"source /usr/share/gazebo/setup.sh\" Then using the world argument, you can spawn the stretch in the willowgarage world by running the following roslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world","title":"Spawning Stretch in Simulation (Gazebo)"},{"location":"stretch-tutorials/ros2/gazebo_basics/#spawning-stretch-in-simulation-gazebo","text":"NOTE : ROS 2 tutorials are still under active development.","title":"Spawning Stretch in Simulation (Gazebo)"},{"location":"stretch-tutorials/ros2/gazebo_basics/#note","text":"Simulation support for Stretch in ROS 2 is under active development. Please reach out to us if you want to work with Stretch in a simulated environment like Gazebo/Ignition in ROS 2. Refer to the instructions below if you want to test this functionality in ROS 1.","title":"NOTE"},{"location":"stretch-tutorials/ros2/gazebo_basics/#empty-world-simulation","text":"To spawn the Stretch in gazebo's default empty world run the following command in your terminal. roslaunch stretch_gazebo gazebo.launch This will bringup the robot in the gazebo simulation similar to the image shown below.","title":"Empty World Simulation"},{"location":"stretch-tutorials/ros2/gazebo_basics/#custom-world-simulation","text":"In gazebo, you are able to spawn Stretch in various worlds. First, source the gazebo world files by running the following command in a terminal echo \"source /usr/share/gazebo/setup.sh\" Then using the world argument, you can spawn the stretch in the willowgarage world by running the following roslaunch stretch_gazebo gazebo.launch world:=worlds/willowgarage.world","title":"Custom World Simulation"},{"location":"stretch-tutorials/ros2/getting_started/","text":"Getting Started NOTE : ROS 2 tutorials are still under active development. Prerequisites A Stretch RE1 or RE2 robot, turned on and connected to a keyboard, mouse, and monitor Alternatively, setup untethered operation . This avoids the HDMI/USB cables getting pulled while the robot is moving. Running the Ubuntu 20.04 software stack All RE2s ship with Ubuntu 20.04, however RE1s had shipped with Ubuntu 18.04 until summer 2022. RE1 users should run lsb_release -sd in a terminal and confirm \"Ubuntu 20.04.5 LTS\" or similar is printed out. If you are running Ubuntu 18.04, follow the upgrade guide . Already went through the Start Coding section of the Getting Started guide (hello_robot_xbox_teleop must not be running in the background) Switching to ROS2 It's recommended that ROS1 and ROS2 systems not run at the same time. Therefore, the default installation starts with ROS1 enabled and ROS2 disabled. This is configured in the \"STRETCH BASHRC SETUP\", which you can see by running gedit ~/.bashrc in a terminal and scrolling to the bottom. We will disable ROS1 by commenting out the ROS1 related lines by adding '#' in front of them, and enable ROS2 by uncommenting the ROS2 related lines by deleting the '#' in front of them. The result will look like: Save this configuration using Ctrl + S . Close out of the current terminal and open a new one. ROS2 is now enabled! Refreshing the ROS2 workspace While Stretch ROS2 is in beta, there will be frequent updates to the ROS2 software. Therefore, it makes sense to refresh the ROS2 software to the latest available release. In the ROS and ROS2 world, software is organized into \"ROS Workspaces\", where packages can be developed, compiled, and be made available to run from the command line. We are going to refresh the ROS2 workspace, which is called \"~/ament_ws\" and available in the home directory. Follow the Create a new ROS Workspace guide to run the stretch_create_ament_workspace.sh script. This will delete the existing \"~/ament_ws\", create a new one with all of the required ROS2 packages for Stretch, and compile it. Testing Keyboard Teleop We can test whether the ROS2 workspace was enabled successfully by testing out the ROS2 drivers package, called \"stretch_core\", with keyboard teleop. In one terminal, we'll launch Stretch's ROS2 drivers using: ros2 launch stretch_core stretch_driver.launch.py mode: = manipulation In the second terminal, launch the keyboard teleop node using: ros2 run stretch_core keyboard_teleop The following menu will be outputted to the terminal and you can press a key to move the corresponding joint on the robot. When you're ready to exit, press Ctrl + C . [INFO] [1672878953.011453154] [keyboard_teleop]: keyboard_teleop started [INFO] [1672878953.041154084] [keyboard_teleop]: Node keyboard_teleop connected to /stop_the_robot service. ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- ROS2 is setup! Move onto the next tutorial: Follow Joint Trajectory Commands .","title":"Getting Started"},{"location":"stretch-tutorials/ros2/getting_started/#getting-started","text":"NOTE : ROS 2 tutorials are still under active development.","title":"Getting Started"},{"location":"stretch-tutorials/ros2/getting_started/#prerequisites","text":"A Stretch RE1 or RE2 robot, turned on and connected to a keyboard, mouse, and monitor Alternatively, setup untethered operation . This avoids the HDMI/USB cables getting pulled while the robot is moving. Running the Ubuntu 20.04 software stack All RE2s ship with Ubuntu 20.04, however RE1s had shipped with Ubuntu 18.04 until summer 2022. RE1 users should run lsb_release -sd in a terminal and confirm \"Ubuntu 20.04.5 LTS\" or similar is printed out. If you are running Ubuntu 18.04, follow the upgrade guide . Already went through the Start Coding section of the Getting Started guide (hello_robot_xbox_teleop must not be running in the background)","title":"Prerequisites"},{"location":"stretch-tutorials/ros2/getting_started/#switching-to-ros2","text":"It's recommended that ROS1 and ROS2 systems not run at the same time. Therefore, the default installation starts with ROS1 enabled and ROS2 disabled. This is configured in the \"STRETCH BASHRC SETUP\", which you can see by running gedit ~/.bashrc in a terminal and scrolling to the bottom. We will disable ROS1 by commenting out the ROS1 related lines by adding '#' in front of them, and enable ROS2 by uncommenting the ROS2 related lines by deleting the '#' in front of them. The result will look like: Save this configuration using Ctrl + S . Close out of the current terminal and open a new one. ROS2 is now enabled!","title":"Switching to ROS2"},{"location":"stretch-tutorials/ros2/getting_started/#refreshing-the-ros2-workspace","text":"While Stretch ROS2 is in beta, there will be frequent updates to the ROS2 software. Therefore, it makes sense to refresh the ROS2 software to the latest available release. In the ROS and ROS2 world, software is organized into \"ROS Workspaces\", where packages can be developed, compiled, and be made available to run from the command line. We are going to refresh the ROS2 workspace, which is called \"~/ament_ws\" and available in the home directory. Follow the Create a new ROS Workspace guide to run the stretch_create_ament_workspace.sh script. This will delete the existing \"~/ament_ws\", create a new one with all of the required ROS2 packages for Stretch, and compile it.","title":"Refreshing the ROS2 workspace"},{"location":"stretch-tutorials/ros2/getting_started/#testing-keyboard-teleop","text":"We can test whether the ROS2 workspace was enabled successfully by testing out the ROS2 drivers package, called \"stretch_core\", with keyboard teleop. In one terminal, we'll launch Stretch's ROS2 drivers using: ros2 launch stretch_core stretch_driver.launch.py mode: = manipulation In the second terminal, launch the keyboard teleop node using: ros2 run stretch_core keyboard_teleop The following menu will be outputted to the terminal and you can press a key to move the corresponding joint on the robot. When you're ready to exit, press Ctrl + C . [INFO] [1672878953.011453154] [keyboard_teleop]: keyboard_teleop started [INFO] [1672878953.041154084] [keyboard_teleop]: Node keyboard_teleop connected to /stop_the_robot service. ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- ROS2 is setup! Move onto the next tutorial: Follow Joint Trajectory Commands .","title":"Testing Keyboard Teleop"},{"location":"stretch-tutorials/ros2/internal_state_of_stretch/","text":"Getting the State of the Robot NOTE : ROS 2 tutorials are still under active development. Begin by starting up the stretch driver launch file by typing the following in a terminal. ros2 launch stretch_core stretch_driver.launch.py Then utilize the ROS command-line tool, ros2 topic, to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a terminal. ros2 topic echo /stretch/joint_states Your terminal will then output the information associated with the /stretch/joint_states topic. Your header , position , velocity , and effort information may vary from what is printed below. header: seq: 70999 stamp: secs: 1420 nsecs: 2000000 frame_id: '' name: [joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left, joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift, joint_right_wheel, joint_wrist_yaw] position: [-1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2.9291786329821434e-07, 1.3802900147297237e-06, 0.08154086954434359, 1.4361499260374905e-07, 0.4139061738340768, 9.32603306580404e-07] velocity: [0.00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1.322424459109634e-05, -0.00035084643762840415, 0.0012164337445918797, 0.0002138814988808099, 0.00010419792027496809, 4.0575263146426684e-05, 0.00022487596895736357, -0.0007751929074042957, 0.0002451588607332439] effort: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] --- Additionally, if you type ros2 topic list in the terminal, you will see the list of active topics being published. A powerful tool to visualize the ROS communication is through the rqt_graph package. You can see a graph of topics being communicated between nodes by typing the following. ros2 run rqt_graph rqt_graph The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.","title":"Internal State of Stretch"},{"location":"stretch-tutorials/ros2/internal_state_of_stretch/#getting-the-state-of-the-robot","text":"NOTE : ROS 2 tutorials are still under active development. Begin by starting up the stretch driver launch file by typing the following in a terminal. ros2 launch stretch_core stretch_driver.launch.py Then utilize the ROS command-line tool, ros2 topic, to display Stretch's internal state information. For instance, to view the current state of the robot's joints, simply type the following in a terminal. ros2 topic echo /stretch/joint_states Your terminal will then output the information associated with the /stretch/joint_states topic. Your header , position , velocity , and effort information may vary from what is printed below. header: seq: 70999 stamp: secs: 1420 nsecs: 2000000 frame_id: '' name: [joint_arm_l0, joint_arm_l1, joint_arm_l2, joint_arm_l3, joint_gripper_finger_left, joint_gripper_finger_right, joint_head_pan, joint_head_tilt, joint_left_wheel, joint_lift, joint_right_wheel, joint_wrist_yaw] position: [-1.6137320244357253e-08, -2.9392484829061376e-07, -2.8036125938539207e-07, -2.056847528567165e-07, -2.0518734302754638e-06, -5.98271107676851e-06, 2.9291786329821434e-07, 1.3802900147297237e-06, 0.08154086954434359, 1.4361499260374905e-07, 0.4139061738340768, 9.32603306580404e-07] velocity: [0.00015598730463972836, -0.00029395074514369584, -0.0002803845454217379, 1.322424459109634e-05, -0.00035084643762840415, 0.0012164337445918797, 0.0002138814988808099, 0.00010419792027496809, 4.0575263146426684e-05, 0.00022487596895736357, -0.0007751929074042957, 0.0002451588607332439] effort: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] --- Additionally, if you type ros2 topic list in the terminal, you will see the list of active topics being published. A powerful tool to visualize the ROS communication is through the rqt_graph package. You can see a graph of topics being communicated between nodes by typing the following. ros2 run rqt_graph rqt_graph The graph allows a user to observe and affirm if topics are broadcasted to the correct nodes. This method can also be utilized to debug communication issues.","title":"Getting the State of the Robot"},{"location":"stretch-tutorials/ros2/moveit_basics/","text":"MoveIt! Basics NOTE : ROS 2 tutorials are still under active development. Overview MoveIt 2 is a whole-body motion planning framework for mobile manipulators that allows planning pose and joint goals in environments with and without obstacles. Stretch being a mobile manipulator is uniquely well-suited to utilize the planning capabilities of MoveIt 2 in different scenarios. Motivation Stretch has a kinematically simple 3 DoF arm (+2 with DexWrist) that is suitable for pick and place tasks of varied objects. Its mobile base provides it with 2 additional degrees of freedom that afford it more manipulability and also the ability to move around freely in its environment. To fully utilize these capabilities, we need a planner that can plan for both the arm and the mobile base at the same time. With MoveIt 2 and ROS 2, it is now possible to achieve this, empowering users to plan more complicated robot trajectories in difficult and uncertain environments. Demo with Stretch Robot Before we proceed, it's always a good idea to home the robot first by running the following script so that we have the correct joint positions being published on the /joint_states topic. This is necessary for planning trajectories on Stretch with MoveIt. stretch_robot_home.py Planning with MoveIt 2 Using RViz The easiest way to run MoveIt 2 on your robot is through RViz. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To launch RViz with MoveIt 2, run the following command. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py Follow instructions in this tutorial to plan and execute trajectories using the interactive markers in RViz. Use the interactive markers to drag joints to desired positions or go to the manipulation tab in the Motion Planning pane to fine-tune joint values using the sliders. Next, click the 'Plan' button to plan the trajectory. If the plan is valid, you should be able to execute the trajectory by clicking the 'Execute' button. Below we see Stretch raising its arm without any obstacle in the way. To plan with obstacles, you can insert objects like a box, cyclinder or sphere, in the planning scene to plan trajectories around the object. This can be done by adding an object using the Scene Objects tab in the Motion Planning pane. Below we see Stretch raising its arm with a flat cuboid obstacle in the way. The mobile base allows Stretch to move forward and then back again while raising the arm to avoid the obstacle. Planning with MoveIt 2 Using the MoveGroup C++ API If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. Execute the launch file again and go through the comments in the code to understand what's going on. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py Follow instructions in this tutorial to plan and execute trajectories using the MoveGroup C++ API.","title":"MoveIt Basics"},{"location":"stretch-tutorials/ros2/moveit_basics/#moveit-basics","text":"NOTE : ROS 2 tutorials are still under active development.","title":"MoveIt! Basics"},{"location":"stretch-tutorials/ros2/moveit_basics/#overview","text":"MoveIt 2 is a whole-body motion planning framework for mobile manipulators that allows planning pose and joint goals in environments with and without obstacles. Stretch being a mobile manipulator is uniquely well-suited to utilize the planning capabilities of MoveIt 2 in different scenarios.","title":"Overview"},{"location":"stretch-tutorials/ros2/moveit_basics/#motivation","text":"Stretch has a kinematically simple 3 DoF arm (+2 with DexWrist) that is suitable for pick and place tasks of varied objects. Its mobile base provides it with 2 additional degrees of freedom that afford it more manipulability and also the ability to move around freely in its environment. To fully utilize these capabilities, we need a planner that can plan for both the arm and the mobile base at the same time. With MoveIt 2 and ROS 2, it is now possible to achieve this, empowering users to plan more complicated robot trajectories in difficult and uncertain environments.","title":"Motivation"},{"location":"stretch-tutorials/ros2/moveit_basics/#demo-with-stretch-robot","text":"Before we proceed, it's always a good idea to home the robot first by running the following script so that we have the correct joint positions being published on the /joint_states topic. This is necessary for planning trajectories on Stretch with MoveIt. stretch_robot_home.py","title":"Demo with Stretch Robot"},{"location":"stretch-tutorials/ros2/moveit_basics/#planning-with-moveit-2-using-rviz","text":"The easiest way to run MoveIt 2 on your robot is through RViz. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To launch RViz with MoveIt 2, run the following command. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py Follow instructions in this tutorial to plan and execute trajectories using the interactive markers in RViz. Use the interactive markers to drag joints to desired positions or go to the manipulation tab in the Motion Planning pane to fine-tune joint values using the sliders. Next, click the 'Plan' button to plan the trajectory. If the plan is valid, you should be able to execute the trajectory by clicking the 'Execute' button. Below we see Stretch raising its arm without any obstacle in the way. To plan with obstacles, you can insert objects like a box, cyclinder or sphere, in the planning scene to plan trajectories around the object. This can be done by adding an object using the Scene Objects tab in the Motion Planning pane. Below we see Stretch raising its arm with a flat cuboid obstacle in the way. The mobile base allows Stretch to move forward and then back again while raising the arm to avoid the obstacle.","title":"Planning with MoveIt 2 Using RViz"},{"location":"stretch-tutorials/ros2/moveit_basics/#planning-with-moveit-2-using-the-movegroup-c-api","text":"If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. Execute the launch file again and go through the comments in the code to understand what's going on. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py Follow instructions in this tutorial to plan and execute trajectories using the MoveGroup C++ API.","title":"Planning with MoveIt 2 Using the MoveGroup C++ API"},{"location":"stretch-tutorials/ros2/moveit_movegroup_demo/","text":"Planning with MoveIt 2 Using the MoveGroup C++ API If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. For this tutorial we are going to use the RViz Visual Tools plugin to execute the C++ source code part by part to explore more sophisticated functionalities. Execute the launch file again to begin the tutorial. You can follow along in the C++ code to inspect finer details. (Press Ctrl+C in the terminal to terminate) (Ensure you have enough room around the robot before running the script) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py To execute the script and interact with the robot, all you need to do is press the Next button in the RViz Visual Tools windown at the bottom left. Follow the prompts on the terminal to run through the tutorial. While executing the script, it's also a good idea to study and understand the script that is being executed. Find it here . Let's begin by joggin the camera pan and tilt joints. For having a complete 3D representation of its environment, Stretch needs to point its head in all directions, up, down, left, right, you name it! Luckily, we have a planning group that allows you to do just that - the stretch_head planning group. Go ahead and press the Next button to jog the camera. What good is a robot that can't hold your hand on your worst days. We gave Stretch a gripper to do just that and more! Let's exercise it using the stretch_gripper planning group. All you have to do is press Next. What about the good days you ask? Stretch always wants to reach out to you, no matter what. Speaking of reaching out, let's make Stretch exercise its arm for the next time you need it. Press Next. Stretch doesn't like sitting in a corner fretting about the future. It is the future. Stretch wants to explore and in style. What better way to do it than by rolling around? Press Next and you'll see. That's the mobile_base planning group. All that exploring does get tiring and sometimes Stretch just wants to relax and dream about its next adventure. Stretch prefers to relax with its arm down, lest someone trips over it and disturb Stretch's peaceful slumber. Press Next to see the mobile_base_arm planning group. Did someone say adventure? How about dodging some pesky obstacles? They're everywhere, but they don't bother Stretch a lot. It can go around them. Nothing stops Stretch! You know what to do. Stretch is smart, you don't need to tell it which joint goes where. Just say what you want done and it does it. How about planning a pose goal to see it in action? Press Next. Press Ctrl+C to end this demo. To wrap it up, the final act! This one is a surprise that's only a click away. Go on and execute the following command: ros2 launch stretch_moveit_config moveit_draw.launch.py","title":"MoveGroup C++ API"},{"location":"stretch-tutorials/ros2/moveit_movegroup_demo/#planning-with-moveit-2-using-the-movegroup-c-api","text":"If you want to integrate MoveIt 2 into your planning pipeline and want greater control over its various functionalities, using the MoveGroup API is the way to go. For this tutorial we are going to use the RViz Visual Tools plugin to execute the C++ source code part by part to explore more sophisticated functionalities. Execute the launch file again to begin the tutorial. You can follow along in the C++ code to inspect finer details. (Press Ctrl+C in the terminal to terminate) (Ensure you have enough room around the robot before running the script) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py To execute the script and interact with the robot, all you need to do is press the Next button in the RViz Visual Tools windown at the bottom left. Follow the prompts on the terminal to run through the tutorial. While executing the script, it's also a good idea to study and understand the script that is being executed. Find it here . Let's begin by joggin the camera pan and tilt joints. For having a complete 3D representation of its environment, Stretch needs to point its head in all directions, up, down, left, right, you name it! Luckily, we have a planning group that allows you to do just that - the stretch_head planning group. Go ahead and press the Next button to jog the camera. What good is a robot that can't hold your hand on your worst days. We gave Stretch a gripper to do just that and more! Let's exercise it using the stretch_gripper planning group. All you have to do is press Next. What about the good days you ask? Stretch always wants to reach out to you, no matter what. Speaking of reaching out, let's make Stretch exercise its arm for the next time you need it. Press Next. Stretch doesn't like sitting in a corner fretting about the future. It is the future. Stretch wants to explore and in style. What better way to do it than by rolling around? Press Next and you'll see. That's the mobile_base planning group. All that exploring does get tiring and sometimes Stretch just wants to relax and dream about its next adventure. Stretch prefers to relax with its arm down, lest someone trips over it and disturb Stretch's peaceful slumber. Press Next to see the mobile_base_arm planning group. Did someone say adventure? How about dodging some pesky obstacles? They're everywhere, but they don't bother Stretch a lot. It can go around them. Nothing stops Stretch! You know what to do. Stretch is smart, you don't need to tell it which joint goes where. Just say what you want done and it does it. How about planning a pose goal to see it in action? Press Next. Press Ctrl+C to end this demo. To wrap it up, the final act! This one is a surprise that's only a click away. Go on and execute the following command: ros2 launch stretch_moveit_config moveit_draw.launch.py","title":"Planning with MoveIt 2 Using the MoveGroup C++ API"},{"location":"stretch-tutorials/ros2/moveit_rviz_demo/","text":"Planning with MoveIt 2 Using RViz The easiest way to run MoveIt 2 on your robot is through the RViz plugin. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To get started using MoveIt 2 with RViz, execute the following command in a terminal. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py You should see Stretch visualized in RViz with joint positions exactly as they appear on the actual robot (If not, home the robot and start from step 1!). You can select a Planning Group from the drop down menu that allows you to choose a group of joints to plan for and control using MoveIt. When you select a Planning Group the joints that can be controlled are highlighted with interactive markers. Let\u2019s go ahead and select the stretch_arm planning group. Now, click and drag the arrow down to slide the arm lift downwards and then use the wheel to turn the gripper inwards so that it fits squarely over the robot base. At this point if the robot base glows red in RViz, it means the robot arm is in collision with the base. You should move the lift upwards slightly until the red highlight disappears. Now click on the Plan button to see the simulated motion of the robot in RViz Before proceeding, ensure that the robot is in an open space without obstalces. Click the Execute button to execute the plan on the actual robot. Congratulations, you just stowed the robot arm using MoveIt! (Alternatively, if you do not want to review the simulated plan, you can click \u2018Plan and Execute\u2019 to execute the planned trajectory directly) Now, let\u2019s move Stretch\u2019s mobile base! Select the mobile_base_arm planning_group from the drop down menu. You should see the base interactive marker appear in RViz. Use the arrow to drag the base forward or backward for about 1m. Click Plan and Execute when you are done. Voila! The mobile_base_arm planning group also allows you to execute a coordinated base and arm motion plan. Go ahead and move the markers around to plan some fun trajectories, maybe make Stretch do a Pirouette! Similarly, the stretch_gripper and stretch_head planning groups allow opening/closing the gripper and panning/tilting the camera. The interactive markers are just one way to control the joints. If you want a finer control, you can switch to the Joints tab of the plugin and use the sliders to adjust the desired end state of the joints. MoveIt allows you to plan not just simple trajectories but also avoid obstacles. Let\u2019s add an obstacle to the planning scene. Click on the Scene Objects tab and select the Box object. Define a cube of dimensions 0.1x0.1x0.1m and add it to the scene using the green + button next to it. Now, place it just in front of the mobile base using the fine controls in the Change object pose/scale buttons to the right. Click on the Publish button for MoveIt to account for the object while planning. Now return back to the Planning tab and define an end state such that the Box is in between the robot start and end states. Again, ensure that the robot has enough space around it. Plan and Execute! With a fully functional perception pipeline, the planning scene can represent Stretch\u2019s surroundings accurately and allow Stretch to manipulate and navigate in a cluttered environment Feel free to explore more sophisticated planners shipped along with MoveIt 2 in the Context tab. End!","title":"MoveIt with RViz"},{"location":"stretch-tutorials/ros2/moveit_rviz_demo/#planning-with-moveit-2-using-rviz","text":"The easiest way to run MoveIt 2 on your robot is through the RViz plugin. With RViz you can plan, visualize, and also execute trajectories for various planning groups on your robot. To get started using MoveIt 2 with RViz, execute the following command in a terminal. (Press Ctrl+C in the terminal to terminate) ros2 launch stretch_moveit_config movegroup_moveit2.launch.py You should see Stretch visualized in RViz with joint positions exactly as they appear on the actual robot (If not, home the robot and start from step 1!). You can select a Planning Group from the drop down menu that allows you to choose a group of joints to plan for and control using MoveIt. When you select a Planning Group the joints that can be controlled are highlighted with interactive markers. Let\u2019s go ahead and select the stretch_arm planning group. Now, click and drag the arrow down to slide the arm lift downwards and then use the wheel to turn the gripper inwards so that it fits squarely over the robot base. At this point if the robot base glows red in RViz, it means the robot arm is in collision with the base. You should move the lift upwards slightly until the red highlight disappears. Now click on the Plan button to see the simulated motion of the robot in RViz Before proceeding, ensure that the robot is in an open space without obstalces. Click the Execute button to execute the plan on the actual robot. Congratulations, you just stowed the robot arm using MoveIt! (Alternatively, if you do not want to review the simulated plan, you can click \u2018Plan and Execute\u2019 to execute the planned trajectory directly) Now, let\u2019s move Stretch\u2019s mobile base! Select the mobile_base_arm planning_group from the drop down menu. You should see the base interactive marker appear in RViz. Use the arrow to drag the base forward or backward for about 1m. Click Plan and Execute when you are done. Voila! The mobile_base_arm planning group also allows you to execute a coordinated base and arm motion plan. Go ahead and move the markers around to plan some fun trajectories, maybe make Stretch do a Pirouette! Similarly, the stretch_gripper and stretch_head planning groups allow opening/closing the gripper and panning/tilting the camera. The interactive markers are just one way to control the joints. If you want a finer control, you can switch to the Joints tab of the plugin and use the sliders to adjust the desired end state of the joints. MoveIt allows you to plan not just simple trajectories but also avoid obstacles. Let\u2019s add an obstacle to the planning scene. Click on the Scene Objects tab and select the Box object. Define a cube of dimensions 0.1x0.1x0.1m and add it to the scene using the green + button next to it. Now, place it just in front of the mobile base using the fine controls in the Change object pose/scale buttons to the right. Click on the Publish button for MoveIt to account for the object while planning. Now return back to the Planning tab and define an end state such that the Box is in between the robot start and end states. Again, ensure that the robot has enough space around it. Plan and Execute! With a fully functional perception pipeline, the planning scene can represent Stretch\u2019s surroundings accurately and allow Stretch to manipulate and navigate in a cluttered environment Feel free to explore more sophisticated planners shipped along with MoveIt 2 in the Context tab. End!","title":"Planning with MoveIt 2 Using RViz"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/","text":"Nav2 Stack Using Simple Commander Python API In this tutorial, we will work with Stretch to explore the Simple Commander Python API to enable autonomous navigation programatically. We will also demonstrate a security patrol routine for Stretch developed using this API. If you just landed here, it might be a good idea to first review the previous tutorial which covered mapping and navigation using RViz as an interface. The Simple Commander Python API To develop complex behaviors with Stretch where navigation is just one aspect of the autonomy stack, we need to be able to plan and execute navigation routines as part of a bigger program. Luckily, the Nav2 stack exposes a Python API that abstracts the ROS layer and the Behavior Tree framework (more on that later!) from the user through a pre-configured library called the robot navigator . This library defines a class called BasicNavigator which wraps the planner, controller and recovery action servers and exposes methods such as goToPose() , goToPoses() and followWaypoints() to execute navigation behaviors. Let's first see the demo in action and then explore the code to understand how this works! Warning We will not be using the arm for this demo. We recommend stowing the arm to avoid inadvertently bumping it into walls while the robot is navigating. Execute: stretch_robot_stow.py Setup Let's set the patrol route up before you can execute this demo in your map. This requires reading the position of the robot at various locations in the map and entering the co-ordinates in the array called security_route in the simple_commander_demo.py file. First, execute the following command while passing the correct map YAML. Then, press the 'Startup' button: ros2 launch stretch_navigation navigation.launch.py map: = ${ HELLO_ROBOT_FLEET } /maps/<map_name>.yaml Since we expect the first point in the patrol route to be at the origin of the map, the first co-ordinates should be (0.0, 0.0). Next, to define the route, the easiest way to define the waypoints in the security_route array is by setting the robot at random locations in the map using the '2D Pose Estimate' button in RViz as shown below. For each location, note the x, y co-ordinates in the position field of the base_footprint frame and add it to the security_route array in simple_commander_demo.py . Finally, Press Ctrl+C to exit out of navigation and save the simple_commander_demo.py file. Now, build the workspace to make the updated file available for the next launch command. cd ~/ament_ws/ colcon build See It In Action Go ahead and execute the following command to run the demo and visualize the result in RViz. Be sure to pass the correct path to the map YAML: Terminal 1: ros2 launch stretch_navigation demo_security.launch.py map: = ${ HELLO_ROBOT_FLEET } /maps/<map_name>.yaml Code Breakdown Now, let's jump into the code to see how things work under the hood. Follow along in the code to have a look at the entire script. First, we import the BasicNavigator class from the robot_navigator library which comes standard with the Nav2 stack. This class wraps around the planner, controller and recovery action servers. from stretch_navigation.robot_navigator import BasicNavigator , TaskResult In the main method, we initialize the node and create an instance of the BasicNavigator class called navigator. def main (): rclpy . init () navigator = BasicNavigator () Then, we set up a path for Stretch to patrol consisting of the co-ordinates in the map reference frame. These co-ordinates are specific to the map generated for this tutorial and would not be suitable for your robot. To define co-ordinates that work with your robot, first command the robot to at least three random locations in the map you have generated of your environment, then read the base_link x and y co-ordinates for each of them from the RViz TF plugin. Plug them in the security_route list. Keep in mind that for this demo, the robot is starting from the [0.0, 0.0] which is the origin of the map. This might not be the case for you. security_route = [ [ 0.0 , 0.0 ], [ 1.057 , 1.3551 ], [ 1.5828 , 5.0823 ], [ - 0.5390 , 5.6623 ], [ 0.8975 , 9.7033 ]] Next, we set an initial pose for the robot which would help AMCL localize the robot by providing an initial estimate of the robot's location. For this, we pass a PoseStamped message in the map reference frame with the robot's pose to the setInitialPose() method. The Nav2 stack recommends this before starting the lifecycle nodes using the \"Startup\" button in RViz. The waitUntilNav2Active() method waits until precisely this event. initial_pose = PoseStamped () initial_pose . header . frame_id = 'map' initial_pose . header . stamp = navigator . get_clock () . now () . to_msg () initial_pose . pose . position . x = 0.0 initial_pose . pose . position . y = 0.0 initial_pose . pose . orientation . z = 0.0 initial_pose . pose . orientation . w = 1.0 navigator . setInitialPose ( initial_pose ) navigator . waitUntilNav2Active () Once the nodes are active, the navigator is ready to receive pose goals either through the goToPose() , goToPoses() or followWaypoints() methods. For this demo, we will be using the followWaypoints() method which takes a list of poses as an argument. Since we intend for the robot to patrol the route indefinitely or until the node is killed (or the robot runs out of battery!), we wrap the method in an infinite while loop with rclpy.ok() . Then, we generate pose goals with the security_route list and append them to a new list called route_poses which is passed to the followWaypoints() method. while rclpy . ok (): route_poses = [] pose = PoseStamped () pose . header . frame_id = 'map' pose . header . stamp = navigator . get_clock () . now () . to_msg () pose . pose . orientation . w = 1.0 for pt in security_route [ 1 :]: pose . pose . position . x = pt [ 0 ] pose . pose . position . y = pt [ 1 ] route_poses . append ( deepcopy ( pose )) nav_start = navigator . get_clock () . now () navigator . followWaypoints ( route_poses ) Since we are utilizing an action server built into Nav2, it's possible to seek feedback on this long running task through the action interface. The isTaskComplete() method returns a boolean depending on whether the patrolling task is complete. For the follow waypoints action server, the feedback message tells us which waypoint is currently being executed through the feedback.current_waypoint attribute. It is possible to cancel a goal using the cancelTask() method if the robot gets stuck. For this demo, we have set the timeout at 600 seconds to allow sufficient time for the robot to succeed. However, if you wish to see it in action, you can reduce the timeout to 30 seconds. i = 0 while not navigator . isTaskComplete (): i += 1 feedback = navigator . getFeedback () if feedback and i % 5 == 0 : navigator . get_logger () . info ( 'Executing current waypoint: ' + str ( feedback . current_waypoint + 1 ) + '/' + str ( len ( route_poses ))) now = navigator . get_clock () . now () if now - nav_start > Duration ( seconds = 600.0 ): navigator . cancelTask () Once the robot reaches the end of the route, we reverse the security_route list to generate the goal pose list that would be used by the followWaypoints() method in the next iteration of this loop. security_route . reverse () Finally, after a leg of the patrol route is executed, we call the getResult() method to know whether the task succeeded, canceled or failed to log a message. result = navigator . getResult () if result == TaskResult . SUCCEEDED : navigator . get_logger () . info ( 'Route complete! Restarting...' ) elif result == TaskResult . CANCELED : navigator . get_logger () . info ( 'Security route was canceled, exiting.' ) rclpy . shutdown () elif result == TaskResult . FAILED : navigator . get_logger () . info ( 'Security route failed! Restarting from other side...' ) That's it! Using the Simple Commander API is as simple as that. Be sure to follow more examples in the nav2_simple_commander package if you wish to work with other useful methods exposed by the library.","title":"Nav2 Simple Commander"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/#nav2-stack-using-simple-commander-python-api","text":"In this tutorial, we will work with Stretch to explore the Simple Commander Python API to enable autonomous navigation programatically. We will also demonstrate a security patrol routine for Stretch developed using this API. If you just landed here, it might be a good idea to first review the previous tutorial which covered mapping and navigation using RViz as an interface.","title":"Nav2 Stack Using Simple Commander Python API"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/#the-simple-commander-python-api","text":"To develop complex behaviors with Stretch where navigation is just one aspect of the autonomy stack, we need to be able to plan and execute navigation routines as part of a bigger program. Luckily, the Nav2 stack exposes a Python API that abstracts the ROS layer and the Behavior Tree framework (more on that later!) from the user through a pre-configured library called the robot navigator . This library defines a class called BasicNavigator which wraps the planner, controller and recovery action servers and exposes methods such as goToPose() , goToPoses() and followWaypoints() to execute navigation behaviors. Let's first see the demo in action and then explore the code to understand how this works! Warning We will not be using the arm for this demo. We recommend stowing the arm to avoid inadvertently bumping it into walls while the robot is navigating. Execute: stretch_robot_stow.py","title":"The Simple Commander Python API"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/#setup","text":"Let's set the patrol route up before you can execute this demo in your map. This requires reading the position of the robot at various locations in the map and entering the co-ordinates in the array called security_route in the simple_commander_demo.py file. First, execute the following command while passing the correct map YAML. Then, press the 'Startup' button: ros2 launch stretch_navigation navigation.launch.py map: = ${ HELLO_ROBOT_FLEET } /maps/<map_name>.yaml Since we expect the first point in the patrol route to be at the origin of the map, the first co-ordinates should be (0.0, 0.0). Next, to define the route, the easiest way to define the waypoints in the security_route array is by setting the robot at random locations in the map using the '2D Pose Estimate' button in RViz as shown below. For each location, note the x, y co-ordinates in the position field of the base_footprint frame and add it to the security_route array in simple_commander_demo.py . Finally, Press Ctrl+C to exit out of navigation and save the simple_commander_demo.py file. Now, build the workspace to make the updated file available for the next launch command. cd ~/ament_ws/ colcon build","title":"Setup"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/#see-it-in-action","text":"Go ahead and execute the following command to run the demo and visualize the result in RViz. Be sure to pass the correct path to the map YAML: Terminal 1: ros2 launch stretch_navigation demo_security.launch.py map: = ${ HELLO_ROBOT_FLEET } /maps/<map_name>.yaml","title":"See It In Action"},{"location":"stretch-tutorials/ros2/navigation_simple_commander/#code-breakdown","text":"Now, let's jump into the code to see how things work under the hood. Follow along in the code to have a look at the entire script. First, we import the BasicNavigator class from the robot_navigator library which comes standard with the Nav2 stack. This class wraps around the planner, controller and recovery action servers. from stretch_navigation.robot_navigator import BasicNavigator , TaskResult In the main method, we initialize the node and create an instance of the BasicNavigator class called navigator. def main (): rclpy . init () navigator = BasicNavigator () Then, we set up a path for Stretch to patrol consisting of the co-ordinates in the map reference frame. These co-ordinates are specific to the map generated for this tutorial and would not be suitable for your robot. To define co-ordinates that work with your robot, first command the robot to at least three random locations in the map you have generated of your environment, then read the base_link x and y co-ordinates for each of them from the RViz TF plugin. Plug them in the security_route list. Keep in mind that for this demo, the robot is starting from the [0.0, 0.0] which is the origin of the map. This might not be the case for you. security_route = [ [ 0.0 , 0.0 ], [ 1.057 , 1.3551 ], [ 1.5828 , 5.0823 ], [ - 0.5390 , 5.6623 ], [ 0.8975 , 9.7033 ]] Next, we set an initial pose for the robot which would help AMCL localize the robot by providing an initial estimate of the robot's location. For this, we pass a PoseStamped message in the map reference frame with the robot's pose to the setInitialPose() method. The Nav2 stack recommends this before starting the lifecycle nodes using the \"Startup\" button in RViz. The waitUntilNav2Active() method waits until precisely this event. initial_pose = PoseStamped () initial_pose . header . frame_id = 'map' initial_pose . header . stamp = navigator . get_clock () . now () . to_msg () initial_pose . pose . position . x = 0.0 initial_pose . pose . position . y = 0.0 initial_pose . pose . orientation . z = 0.0 initial_pose . pose . orientation . w = 1.0 navigator . setInitialPose ( initial_pose ) navigator . waitUntilNav2Active () Once the nodes are active, the navigator is ready to receive pose goals either through the goToPose() , goToPoses() or followWaypoints() methods. For this demo, we will be using the followWaypoints() method which takes a list of poses as an argument. Since we intend for the robot to patrol the route indefinitely or until the node is killed (or the robot runs out of battery!), we wrap the method in an infinite while loop with rclpy.ok() . Then, we generate pose goals with the security_route list and append them to a new list called route_poses which is passed to the followWaypoints() method. while rclpy . ok (): route_poses = [] pose = PoseStamped () pose . header . frame_id = 'map' pose . header . stamp = navigator . get_clock () . now () . to_msg () pose . pose . orientation . w = 1.0 for pt in security_route [ 1 :]: pose . pose . position . x = pt [ 0 ] pose . pose . position . y = pt [ 1 ] route_poses . append ( deepcopy ( pose )) nav_start = navigator . get_clock () . now () navigator . followWaypoints ( route_poses ) Since we are utilizing an action server built into Nav2, it's possible to seek feedback on this long running task through the action interface. The isTaskComplete() method returns a boolean depending on whether the patrolling task is complete. For the follow waypoints action server, the feedback message tells us which waypoint is currently being executed through the feedback.current_waypoint attribute. It is possible to cancel a goal using the cancelTask() method if the robot gets stuck. For this demo, we have set the timeout at 600 seconds to allow sufficient time for the robot to succeed. However, if you wish to see it in action, you can reduce the timeout to 30 seconds. i = 0 while not navigator . isTaskComplete (): i += 1 feedback = navigator . getFeedback () if feedback and i % 5 == 0 : navigator . get_logger () . info ( 'Executing current waypoint: ' + str ( feedback . current_waypoint + 1 ) + '/' + str ( len ( route_poses ))) now = navigator . get_clock () . now () if now - nav_start > Duration ( seconds = 600.0 ): navigator . cancelTask () Once the robot reaches the end of the route, we reverse the security_route list to generate the goal pose list that would be used by the followWaypoints() method in the next iteration of this loop. security_route . reverse () Finally, after a leg of the patrol route is executed, we call the getResult() method to know whether the task succeeded, canceled or failed to log a message. result = navigator . getResult () if result == TaskResult . SUCCEEDED : navigator . get_logger () . info ( 'Route complete! Restarting...' ) elif result == TaskResult . CANCELED : navigator . get_logger () . info ( 'Security route was canceled, exiting.' ) rclpy . shutdown () elif result == TaskResult . FAILED : navigator . get_logger () . info ( 'Security route failed! Restarting from other side...' ) That's it! Using the Simple Commander API is as simple as that. Be sure to follow more examples in the nav2_simple_commander package if you wish to work with other useful methods exposed by the library.","title":"Code Breakdown"},{"location":"stretch-tutorials/ros2/navigation_stack/","text":"Nav2 Stack Using RViz In this tutorial, we will explore the ROS 2 navigation stack using slam_toolbox for mapping an environment and the core Nav2 packages to navigate in the mapped environment. If you want to know more about teleoperating the mobile base or working with the RPlidar 2D scanner on Stretch, we recommend visiting the previous tutorials on Teleoperating stretch and Filtering Laser Scans . These topics are a vital part of how Stretch's mobile base can be velocity controlled using Twist messages, and how the RPlidar's LaserScan messages enable Obstacle Avoidance for autonomous navigation. Navigation is a key aspect of an autonomous agent because, often, to do anything meaningful, the agent needs to traverse an environment to reach a specific spot to perform a specific task. With a robot like Stretch, the task could be anything from delivering water or medicines for the elderly to performing a routine patrol of an establishment for security. Stretch's mobile base enables this capability and this tutorial will explore how we can autonomously plan and execute mobile base trajectories. Running this tutorial will require the robot to be untethered, so please ensure that the robot is adequetly charged. Mapping The first step is to map the space that the robot will navigate in. The offline_mapping.launch.py file will enable you to do this. First run: ros2 launch stretch_navigation offline_mapping.launch.py Rviz will show the robot and the map that is being constructed. Now, use the xbox controller (see instructions below for using a keyboard) to teleoperate the robot around. To teleoperate the robot using the xbox controller, keep the front left (LB) button pressed while using the right joystick for translation and rotation. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, open a new terminal and run the following commands to save the map to the stretch_user/ directory. mkdir ${ HELLO_FLEET_PATH } /maps ros2 run nav2_map_server map_saver_cli -f ${ HELLO_FLEET_PATH } /maps/<map_name> Note The <map_name> does not include an extension. The map_saver node will save two files as <map_name>.pgm and <map_name>.yaml . Tip For a quick sanity check, you can inspect the saved map using a pre-installed tool called Eye of Gnome (eog) by running the following command: eog ${ HELLO_FLEET_PATH } /maps/<map_name>.pgm Navigation Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: ros2 launch stretch_navigation navigation.launch.py map: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml A new RViz window should pop up with a Startup button in a menu at the bottom left of the window. Press the Startup button to kick-start all navigation related lifecycle nodes. Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location in the map does not match the robot's location in the real space. To correct this, from the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. This gives an initial estimate of the robot's location to AMCL, the localization package. AMCL will better localize the robot once we pass the robot a 2D Nav Goal . In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to navigate. In the terminal, you'll see Nav2 go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior - spinning around 180 degrees in place or backing up. Tip If navigation fails or the robot becomes unresponsive to subsequent goals through RViz, you can still teleoperate the robot using Xbox controller. Note The launch files expose the launch argument \"teleop_type\". By default, this argument is set to \"joystick\", which launches joystick teleop in the terminal with the Xbox controller that ships with Stretch. The Xbox controller utilizes a dead man's switch safety feature to avoid unintended movement of the robot. This is the switch located on the front left side of the controller marked \"LB\". Keep this switch pressed while translating or rotating the base using the joystick located on the right side of the Xbox controller. If the Xbox controller is not available, the following commands will launch mapping or navigation, respectively, with keyboard teleop: ros2 launch stretch_navigation offline_mapping.launch.py teleop_type: = keyboard or ros2 launch stretch_navigation navigation.launch.py teleop_type: = keyboard map: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml Simple Commander API It is also possible to send 2D Pose Estimates and Nav Goals programatically. In your own launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send pose goals using the Nav2 simple commander API in order to navigate the robot programatically. We will explore this in the next tutorial.","title":"Nav2 Basics"},{"location":"stretch-tutorials/ros2/navigation_stack/#nav2-stack-using-rviz","text":"In this tutorial, we will explore the ROS 2 navigation stack using slam_toolbox for mapping an environment and the core Nav2 packages to navigate in the mapped environment. If you want to know more about teleoperating the mobile base or working with the RPlidar 2D scanner on Stretch, we recommend visiting the previous tutorials on Teleoperating stretch and Filtering Laser Scans . These topics are a vital part of how Stretch's mobile base can be velocity controlled using Twist messages, and how the RPlidar's LaserScan messages enable Obstacle Avoidance for autonomous navigation. Navigation is a key aspect of an autonomous agent because, often, to do anything meaningful, the agent needs to traverse an environment to reach a specific spot to perform a specific task. With a robot like Stretch, the task could be anything from delivering water or medicines for the elderly to performing a routine patrol of an establishment for security. Stretch's mobile base enables this capability and this tutorial will explore how we can autonomously plan and execute mobile base trajectories. Running this tutorial will require the robot to be untethered, so please ensure that the robot is adequetly charged.","title":"Nav2 Stack Using RViz"},{"location":"stretch-tutorials/ros2/navigation_stack/#mapping","text":"The first step is to map the space that the robot will navigate in. The offline_mapping.launch.py file will enable you to do this. First run: ros2 launch stretch_navigation offline_mapping.launch.py Rviz will show the robot and the map that is being constructed. Now, use the xbox controller (see instructions below for using a keyboard) to teleoperate the robot around. To teleoperate the robot using the xbox controller, keep the front left (LB) button pressed while using the right joystick for translation and rotation. Avoid sharp turns and revisit previously visited spots to form loop closures. In Rviz, once you see a map that has reconstructed the space well enough, open a new terminal and run the following commands to save the map to the stretch_user/ directory. mkdir ${ HELLO_FLEET_PATH } /maps ros2 run nav2_map_server map_saver_cli -f ${ HELLO_FLEET_PATH } /maps/<map_name> Note The <map_name> does not include an extension. The map_saver node will save two files as <map_name>.pgm and <map_name>.yaml . Tip For a quick sanity check, you can inspect the saved map using a pre-installed tool called Eye of Gnome (eog) by running the following command: eog ${ HELLO_FLEET_PATH } /maps/<map_name>.pgm","title":"Mapping"},{"location":"stretch-tutorials/ros2/navigation_stack/#navigation","text":"Next, with <map_name>.yaml , we can navigate the robot around the mapped space. Run: ros2 launch stretch_navigation navigation.launch.py map: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml A new RViz window should pop up with a Startup button in a menu at the bottom left of the window. Press the Startup button to kick-start all navigation related lifecycle nodes. Rviz will show the robot in the previously mapped space, however, it's likely that the robot's location in the map does not match the robot's location in the real space. To correct this, from the top bar of Rviz, use 2D Pose Estimate to lay an arrow down roughly where the robot is located in the real space. This gives an initial estimate of the robot's location to AMCL, the localization package. AMCL will better localize the robot once we pass the robot a 2D Nav Goal . In the top bar of Rviz, use 2D Nav Goal to lay down an arrow where you'd like the robot to navigate. In the terminal, you'll see Nav2 go through the planning phases and then navigate the robot to the goal. If planning fails, the robot will begin a recovery behavior - spinning around 180 degrees in place or backing up. Tip If navigation fails or the robot becomes unresponsive to subsequent goals through RViz, you can still teleoperate the robot using Xbox controller.","title":"Navigation"},{"location":"stretch-tutorials/ros2/navigation_stack/#note","text":"The launch files expose the launch argument \"teleop_type\". By default, this argument is set to \"joystick\", which launches joystick teleop in the terminal with the Xbox controller that ships with Stretch. The Xbox controller utilizes a dead man's switch safety feature to avoid unintended movement of the robot. This is the switch located on the front left side of the controller marked \"LB\". Keep this switch pressed while translating or rotating the base using the joystick located on the right side of the Xbox controller. If the Xbox controller is not available, the following commands will launch mapping or navigation, respectively, with keyboard teleop: ros2 launch stretch_navigation offline_mapping.launch.py teleop_type: = keyboard or ros2 launch stretch_navigation navigation.launch.py teleop_type: = keyboard map: = ${ HELLO_FLEET_PATH } /maps/<map_name>.yaml","title":"Note"},{"location":"stretch-tutorials/ros2/navigation_stack/#simple-commander-api","text":"It is also possible to send 2D Pose Estimates and Nav Goals programatically. In your own launch file, you may include navigation.launch to bring up the navigation stack. Then, you can send pose goals using the Nav2 simple commander API in order to navigate the robot programatically. We will explore this in the next tutorial.","title":"Simple Commander API"},{"location":"stretch-tutorials/ros2/obstacle_avoider/","text":"Obstacle Avoider In this tutorial we will work with Stretch to detect and avoid obstacles using the onboard RPlidar A1 laser scanner and learn how to filter laser scan data. If you want to know more about the laser scanner setup on Stretch and how to get it up and running, we recommend visiting the previous tutorials on Filtering Laser Scans and Mobile Base Collision Avoidance . A major drawback of using any ToF (Time of Flight) sensor is the inherent inaccuracies as a result of occlusions and weird reflection and diffraction phenomena the light pulses are subject to in an unstructured environment. This results in unexpected and undesired noise that can get in the way of an otherwise extremely useful sensor. Fortunately, it is easy to account for and eliminate these inaccuracies to a great extent by filering out the noise. We will do this with a ROS package called laser_filters that comes prebuilt with some pretty handy laser scan message filters. By the end of this tutorial, you will be able to tweak them for your particular use case and publish and visualize them on the /scan_filtered topic using RViz. So let\u2019s jump in! We will look at three filters from this package that have been tuned to work well with Stretch in an array of scenarios. LaserScan Filtering LaserScanAngularBoundsFilterInPlace - This filter removes laser scans belonging to an angular range. For Stretch, we use this filter to discount points that are occluded by the mast because it is a part of Stretch\u2019s body and not really an object we need to account for as an obstacle while navigating the mobile base. LaserScanSpeckleFilter - We use this filter to remove phantom detections in the middle of empty space that are a result of reflections around corners. These disjoint speckles can be detected as false positives and result in jerky motion of the base through empty space. Removing them returns a relatively noise-free scan. LaserScanBoxFilter - Stretch is prone to returning false detections right over the mobile base. While navigating, since it\u2019s safe to assume that Stretch is not standing right above an obstacle, we filter out any detections that are in a box shape over the mobile base. Beware that filtering laser scans comes at the cost of a sparser scan that might not be ideal for all applications. If you want to tweak the values for your end application, you could do so by changing the values in the laser_filter_params.yaml file and by following the laser_filters package wiki. Also, if you are feeling zany and want to use the raw unfiltered scans from the laser scanner, simply subscribe to the /scan topic instead of the /scan_filtered topic. Avoidance logic Now, let\u2019s use what we have learned so far to upgrade the collision avoidance demo in a way that Stretch is able to scan an entire room autonomously without bumping into things or people. To account for dynamic obstacles getting too close to the robot, we will define a keepout distance of 0.4 m - detections below this value stop the robot. To keep Stretch from getting too close to static obstacles, we will define another variable called turning distance of 0.75 m - frontal detections below this value make Stretch turn to the left until it sees a clear path ahead. Building up on the teleoperation using velocity commands tutorial, let's implement a simple logic for obstacle avoidance. The logic can be broken down into three steps: 1. If the minimum value from the frontal scans is greater than 0.75 m, then continue to move forward 2. If the minimum value from the frontal scans is less than 0.75 m, then turn to the right until this is no longer true 3. If the minimum value from the overall scans is less than 0.4 m, then stop the robot Warnings If you see Stretch try to run over your lazy cat or headbutt a wall, just press the bright runstop button on Stretch's head to calm it down. For pure navigation tasks, it's also safer to stow Stretch's arm in. Execute the command: stretch_robot_stow.py See It In Action Alright, let's see it in action! Execute the following command to run the scripts: ros2 launch stretch_core rplidar_keepout.launch.py Code Breakdown: Let's jump into the code to see how things work under the hood. Follow along here to have a look at the entire script. The turning distance is defined by the distance attribute and the keepout distance is defined by the keepout attribute. self . distance = 0.75 # robot turns at this distance self . keepout = 0.4 # robot stops at this distance To pass velocity commands to the mobile base, we publish the translational and rotational velocities to the /stretch/cmd_vel topic. To subscribe to the filtered laser scans from the laser scanner, we subscribe to the /scan_filtered topic. While you are at it, go ahead and check the behavior by switching to the /scan topic instead. See why filtering is necessary? self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo self . subscriber_ = self . create_subscription ( LaserScan , '/scan_filtered' , self . lidar_callback , 10 ) lidar_callback() is the callback function for the laser scanner that gets called every time a new message is received. def lidar_callback ( self , msg ): When the scan message is filtered, all the ranges that are filtered out are assigned the nan (not a number) value. This can get in the way of computing the minimum. Therefore, we reassign these values to inf (infinity). all_points = [ r if ( not isnan ( r )) else inf for r in msg . ranges ] Next, we compute the two minimums that are neccessary for the avoidance logic to work - the overall minimum and the frontal minimum named min_all and min_front respectively. front_points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] front_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , front_points )] min_front = min ( front_ranges ) min_all = min ( all_points ) Finally, we check the minimum values against the distance and keepout attributes to set the rotational and linear velocities of the mobile base with the set_speed() method. if ( min_all < self . keepout ): lin_vel = 0.0 rot_vel = 0.0 elif ( min_front < self . distance ): lin_vel = 0.0 rot_vel = 0.25 else : lin_vel = 0.5 rot_vel = 0.0 self . set_speed ( lin_vel , rot_vel ) That wasn't too hard, was it? Now, feel free to play with this code and change the attributes to see how it affects Stretch's behavior.","title":"Obstacle Avoider"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#obstacle-avoider","text":"In this tutorial we will work with Stretch to detect and avoid obstacles using the onboard RPlidar A1 laser scanner and learn how to filter laser scan data. If you want to know more about the laser scanner setup on Stretch and how to get it up and running, we recommend visiting the previous tutorials on Filtering Laser Scans and Mobile Base Collision Avoidance . A major drawback of using any ToF (Time of Flight) sensor is the inherent inaccuracies as a result of occlusions and weird reflection and diffraction phenomena the light pulses are subject to in an unstructured environment. This results in unexpected and undesired noise that can get in the way of an otherwise extremely useful sensor. Fortunately, it is easy to account for and eliminate these inaccuracies to a great extent by filering out the noise. We will do this with a ROS package called laser_filters that comes prebuilt with some pretty handy laser scan message filters. By the end of this tutorial, you will be able to tweak them for your particular use case and publish and visualize them on the /scan_filtered topic using RViz. So let\u2019s jump in! We will look at three filters from this package that have been tuned to work well with Stretch in an array of scenarios.","title":"Obstacle Avoider"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#laserscan-filtering","text":"LaserScanAngularBoundsFilterInPlace - This filter removes laser scans belonging to an angular range. For Stretch, we use this filter to discount points that are occluded by the mast because it is a part of Stretch\u2019s body and not really an object we need to account for as an obstacle while navigating the mobile base. LaserScanSpeckleFilter - We use this filter to remove phantom detections in the middle of empty space that are a result of reflections around corners. These disjoint speckles can be detected as false positives and result in jerky motion of the base through empty space. Removing them returns a relatively noise-free scan. LaserScanBoxFilter - Stretch is prone to returning false detections right over the mobile base. While navigating, since it\u2019s safe to assume that Stretch is not standing right above an obstacle, we filter out any detections that are in a box shape over the mobile base. Beware that filtering laser scans comes at the cost of a sparser scan that might not be ideal for all applications. If you want to tweak the values for your end application, you could do so by changing the values in the laser_filter_params.yaml file and by following the laser_filters package wiki. Also, if you are feeling zany and want to use the raw unfiltered scans from the laser scanner, simply subscribe to the /scan topic instead of the /scan_filtered topic.","title":"LaserScan Filtering"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#avoidance-logic","text":"Now, let\u2019s use what we have learned so far to upgrade the collision avoidance demo in a way that Stretch is able to scan an entire room autonomously without bumping into things or people. To account for dynamic obstacles getting too close to the robot, we will define a keepout distance of 0.4 m - detections below this value stop the robot. To keep Stretch from getting too close to static obstacles, we will define another variable called turning distance of 0.75 m - frontal detections below this value make Stretch turn to the left until it sees a clear path ahead. Building up on the teleoperation using velocity commands tutorial, let's implement a simple logic for obstacle avoidance. The logic can be broken down into three steps: 1. If the minimum value from the frontal scans is greater than 0.75 m, then continue to move forward 2. If the minimum value from the frontal scans is less than 0.75 m, then turn to the right until this is no longer true 3. If the minimum value from the overall scans is less than 0.4 m, then stop the robot","title":"Avoidance logic"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#warnings","text":"If you see Stretch try to run over your lazy cat or headbutt a wall, just press the bright runstop button on Stretch's head to calm it down. For pure navigation tasks, it's also safer to stow Stretch's arm in. Execute the command: stretch_robot_stow.py","title":"Warnings"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#see-it-in-action","text":"Alright, let's see it in action! Execute the following command to run the scripts: ros2 launch stretch_core rplidar_keepout.launch.py","title":"See It In Action"},{"location":"stretch-tutorials/ros2/obstacle_avoider/#code-breakdown","text":"Let's jump into the code to see how things work under the hood. Follow along here to have a look at the entire script. The turning distance is defined by the distance attribute and the keepout distance is defined by the keepout attribute. self . distance = 0.75 # robot turns at this distance self . keepout = 0.4 # robot stops at this distance To pass velocity commands to the mobile base, we publish the translational and rotational velocities to the /stretch/cmd_vel topic. To subscribe to the filtered laser scans from the laser scanner, we subscribe to the /scan_filtered topic. While you are at it, go ahead and check the behavior by switching to the /scan topic instead. See why filtering is necessary? self . publisher_ = self . create_publisher ( Twist , '/stretch/cmd_vel' , 1 ) #/stretch_diff_drive_controller/cmd_vel for gazebo self . subscriber_ = self . create_subscription ( LaserScan , '/scan_filtered' , self . lidar_callback , 10 ) lidar_callback() is the callback function for the laser scanner that gets called every time a new message is received. def lidar_callback ( self , msg ): When the scan message is filtered, all the ranges that are filtered out are assigned the nan (not a number) value. This can get in the way of computing the minimum. Therefore, we reassign these values to inf (infinity). all_points = [ r if ( not isnan ( r )) else inf for r in msg . ranges ] Next, we compute the two minimums that are neccessary for the avoidance logic to work - the overall minimum and the frontal minimum named min_all and min_front respectively. front_points = [ r * sin ( theta ) if ( theta < - 2.5 or theta > 2.5 ) else inf for r , theta in zip ( msg . ranges , angles )] front_ranges = [ r if abs ( y ) < self . extent else inf for r , y in zip ( msg . ranges , front_points )] min_front = min ( front_ranges ) min_all = min ( all_points ) Finally, we check the minimum values against the distance and keepout attributes to set the rotational and linear velocities of the mobile base with the set_speed() method. if ( min_all < self . keepout ): lin_vel = 0.0 rot_vel = 0.0 elif ( min_front < self . distance ): lin_vel = 0.0 rot_vel = 0.25 else : lin_vel = 0.5 rot_vel = 0.0 self . set_speed ( lin_vel , rot_vel ) That wasn't too hard, was it? Now, feel free to play with this code and change the attributes to see how it affects Stretch's behavior.","title":"Code Breakdown:"},{"location":"stretch-tutorials/ros2/rviz_basics/","text":"Visualizing with RViz NOTE : ROS 2 tutorials are still under active development. You can utilize RViz to visualize Stretch's sensor information. To begin, run the stretch driver launch file. ros2 launch stretch_core stretch_driver.launch.py Then run the following command to bring up a simple RViz configuration of the Stretch robot. ros2 run rviz2 rviz2 -d ` ros2 pkg prefix --share stretch_calibration ` /rviz/stretch_simple_test.rviz An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window. If you want to visualize Stretch's tf transform tree , you need to add the display type to the RViz window. First, click on the Add button and include the TF type to the display. You will then see all of the transform frames of the Stretch robot and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference. There are further tutorials for RViz that can be found here .","title":"RViz Basics"},{"location":"stretch-tutorials/ros2/rviz_basics/#visualizing-with-rviz","text":"NOTE : ROS 2 tutorials are still under active development. You can utilize RViz to visualize Stretch's sensor information. To begin, run the stretch driver launch file. ros2 launch stretch_core stretch_driver.launch.py Then run the following command to bring up a simple RViz configuration of the Stretch robot. ros2 run rviz2 rviz2 -d ` ros2 pkg prefix --share stretch_calibration ` /rviz/stretch_simple_test.rviz An RViz window should open, allowing you to see the various DisplayTypes in the display tree on the left side of the window. If you want to visualize Stretch's tf transform tree , you need to add the display type to the RViz window. First, click on the Add button and include the TF type to the display. You will then see all of the transform frames of the Stretch robot and the visualization can be toggled off and on by clicking the checkbox next to the tree. Below is a gif for reference. There are further tutorials for RViz that can be found here .","title":"Visualizing with RViz"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/","text":"Teleoperating Stretch NOTE Teleoperation support for Stretch in ROS 2 is under active development. Please reach out to us if you want to teleoperate Stretch in ROS 2. Refer to the instructions below if you want to test this functionality in ROS 1. Xbox Controller Teleoperating If you have not already had a look at the Xbox Controller Teleoperation section in the Quick Start guide, now might be a good time to try it. Keyboard Teleoperating: Full Body For full body teleoperation with the keyboard, you first need to run the stretch_driver.launch in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then in a new terminal, type the following command # Terminal 2 rosrun stretch_core keyboard_teleop Below are the keyboard commands that allow a user to control all of Stretch's joints. ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- To stop the node from sending twist messages, type Ctrl + c . Keyboard Teleoperating: Mobile Base Begin by running the following command in your terminal: # Terminal 1 roslaunch stretch_core stretch_driver.launch To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to nagivation for the robot to receive Twist messages. This is done using a rosservice call in a new terminal. In the same terminal run the teleop_twist_keyboard node with the argument remapping the cmd_vel topic name to stretch/cmd_vel . # Terminal 2 rosservice call /switch_to_navigation_mode rosrun teleop_twist_keyboard teleop_twist_keyboard.py cmd_vel: = stretch/cmd_vel Below are the keyboard commands that allow a user to move Stretch's base. Reading from the keyboard and Publishing to Twist! --------------------------- Moving around: u i o j k l m , . For Holonomic mode (strafing), hold down the shift key: --------------------------- U I O J K L M < > t : up (+z) b : down (-z) anything else : stop q/z : increase/decrease max speeds by 10% w/x : increase/decrease only linear speed by 10% e/c : increase/decrease only angular speed by 10% CTRL-C to quit currently: speed 0.5 turn 1.0 To stop the node from sending twist messages, type Ctrl + c . Create a node for Mobile Base Teleoperating To move Stretch's mobile base using a python script, please look at example 1 for reference.","title":"Teleoperating stretch"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#teleoperating-stretch","text":"","title":"Teleoperating Stretch"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#note","text":"Teleoperation support for Stretch in ROS 2 is under active development. Please reach out to us if you want to teleoperate Stretch in ROS 2. Refer to the instructions below if you want to test this functionality in ROS 1.","title":"NOTE"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#xbox-controller-teleoperating","text":"If you have not already had a look at the Xbox Controller Teleoperation section in the Quick Start guide, now might be a good time to try it.","title":"Xbox Controller Teleoperating"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#keyboard-teleoperating-full-body","text":"For full body teleoperation with the keyboard, you first need to run the stretch_driver.launch in a terminal. # Terminal 1 roslaunch stretch_core stretch_driver.launch Then in a new terminal, type the following command # Terminal 2 rosrun stretch_core keyboard_teleop Below are the keyboard commands that allow a user to control all of Stretch's joints. ---------- KEYBOARD TELEOP MENU ----------- i HEAD UP j HEAD LEFT l HEAD RIGHT , HEAD DOWN 7 BASE ROTATE LEFT 9 BASE ROTATE RIGHT home page-up 8 LIFT UP up-arrow 4 BASE FORWARD 6 BASE BACK left-arrow right-arrow 2 LIFT DOWN down-arrow w ARM OUT a WRIST FORWARD d WRIST BACK x ARM IN 5 GRIPPER CLOSE 0 GRIPPER OPEN step size: b BIG, m MEDIUM, s SMALL q QUIT ------------------------------------------- To stop the node from sending twist messages, type Ctrl + c .","title":"Keyboard Teleoperating: Full Body"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#keyboard-teleoperating-mobile-base","text":"Begin by running the following command in your terminal: # Terminal 1 roslaunch stretch_core stretch_driver.launch To teleoperate a Stretch's mobile base with the keyboard, you first need to switch the mode to nagivation for the robot to receive Twist messages. This is done using a rosservice call in a new terminal. In the same terminal run the teleop_twist_keyboard node with the argument remapping the cmd_vel topic name to stretch/cmd_vel . # Terminal 2 rosservice call /switch_to_navigation_mode rosrun teleop_twist_keyboard teleop_twist_keyboard.py cmd_vel: = stretch/cmd_vel Below are the keyboard commands that allow a user to move Stretch's base. Reading from the keyboard and Publishing to Twist! --------------------------- Moving around: u i o j k l m , . For Holonomic mode (strafing), hold down the shift key: --------------------------- U I O J K L M < > t : up (+z) b : down (-z) anything else : stop q/z : increase/decrease max speeds by 10% w/x : increase/decrease only linear speed by 10% e/c : increase/decrease only angular speed by 10% CTRL-C to quit currently: speed 0.5 turn 1.0 To stop the node from sending twist messages, type Ctrl + c .","title":"Keyboard Teleoperating: Mobile Base"},{"location":"stretch-tutorials/ros2/teleoperating_stretch/#create-a-node-for-mobile-base-teleoperating","text":"To move Stretch's mobile base using a python script, please look at example 1 for reference.","title":"Create a node for Mobile Base Teleoperating"},{"location":"stretch-tutorials/stretch_body/","text":"Tutorial Track: Stretch Body Stretch Body is a set of Python packages that allow a developer to directly program the hardware of the Stretch robots. The Stretch Body interface is intended for users who are looking for an alternative to ROS. Stretch Body currently supports both Python2 and Python3. These tutorials assume a general familiarity with Python as well as basic robot motion control. Basics Tutorial Description 1 Introduction Introduction to the Stretch Body package 2 Command line Tools Using the Stretch Body command line tools 3 Stretch Body API Walk through of the Stretch Body API 4 Robot Motion How to command robot motion 4 Robot Sensors How to read robot sensors Advanced Tutorial Description 1 Dynamixel Servos How to configure and work with the Dynamixel servos 2 Parameter Management How to work with parameter system 3 Splined Trajectories How to generated coordinated, smooth, and full-body motion 4 Collision Avoidance How to work with the collision avoidance system 5 Contact Models How to work with the contact detection system 6 Changing Tools How to configure Stretch to work with a different tool 7 Custom Wrist DOF How to integrate custom DOF onto the wrist 8 Safety Features Learn about Stretch Body features that keep the robot safe","title":"Overview"},{"location":"stretch-tutorials/stretch_body/#tutorial-track-stretch-body","text":"Stretch Body is a set of Python packages that allow a developer to directly program the hardware of the Stretch robots. The Stretch Body interface is intended for users who are looking for an alternative to ROS. Stretch Body currently supports both Python2 and Python3. These tutorials assume a general familiarity with Python as well as basic robot motion control.","title":"Tutorial Track: Stretch Body"},{"location":"stretch-tutorials/stretch_body/#basics","text":"Tutorial Description 1 Introduction Introduction to the Stretch Body package 2 Command line Tools Using the Stretch Body command line tools 3 Stretch Body API Walk through of the Stretch Body API 4 Robot Motion How to command robot motion 4 Robot Sensors How to read robot sensors","title":"Basics"},{"location":"stretch-tutorials/stretch_body/#advanced","text":"Tutorial Description 1 Dynamixel Servos How to configure and work with the Dynamixel servos 2 Parameter Management How to work with parameter system 3 Splined Trajectories How to generated coordinated, smooth, and full-body motion 4 Collision Avoidance How to work with the collision avoidance system 5 Contact Models How to work with the contact detection system 6 Changing Tools How to configure Stretch to work with a different tool 7 Custom Wrist DOF How to integrate custom DOF onto the wrist 8 Safety Features Learn about Stretch Body features that keep the robot safe","title":"Advanced"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/","text":"Tutorial: Collision Avoidance In this tutorial, we will discuss the simple collision avoidance system that runs as a part of Stretch Body. Overview Stretch Body includes a system to prevent inadvertent self-collisions. It will dynamically limit the range of motion of each joint to prevent self-collisions. Warning Self collisions are still possible while using the collision-avoidance system. The factory default collision models are coarse and not necessarily complete. This system is turned off by default starting with Stretch 2. It may be turned off by default on many RE1 systems. First check if the collision detection system is turned on: stretch_params.py | grep use_collision_manager Output: stretch_body.robot_params.nominal_params param.robot.use_collision_manager 1 If it is turned off you can enable it by adding the following to your stretch_user_yaml.py: robot: use_collision_manager: 1 Common Self Collisions Fortunately, the simple kinematics of Stretch make self-collisions fairly uncommon and simple to predict. The primary places where self-collisions may occur are The lift lowering the wrist or tool into the base The arm retracting the wrist or tool into the base The head_pan at pos==0 and head_tilt at pos=-90 deg and the lift raising the arm into the camera (minor collision) The Dex Wrist (if installed) colliding with itself The Dex Wrist (if installed) colliding with the base Joint Limits The collision avoidance system works by dynamically modifying the acceptable range of motion for each joint. By default, a joint's range is set to the physical hard stop limits. For example, the lift has a mechanical throw of 1.1m: stretch_params.py | grep range | grep lift Output: stretch_body.robot_params.factory_params param.lift.range_m [ 0 .0, 1 .1 ] A reduced range of motion can be set at run-time by setting the Soft Motion Limit. For example, to limit the lift range of motion to 0.3 meters off the base: import stretch_body.robot as robot r = robot . Robot () r . startup () r . lift . set_soft_motion_limit_min ( 0.3 ) We see in the API , the value of None is used to designate no soft limit. It is possible that when setting the Soft Motion Limit the joint's current position is outside of the specified range. In this case, the joint will move to the nearest soft limit to comply with the limits. This can be demonstrated by: import stretch_body.robot as robot import time r = robot . Robot () r . startup () #Move to 0.2 r . lift . move_to ( 0.2 ) r . push_command () time . sleep ( 5.0 ) #Will move to 0.3 r . lift . set_soft_motion_limit_min ( 0.3 ) Collision Models The RobotCollision class manages a set of RobotCollisionModels . Each RobotCollisionModel computes the soft limits for a subset of joints based on a simple geometric model. This geometric model captures the enumerated set of potential collisions listed above. We can see which collision models will execute when use_collision_manager is set to 1: stretch_params.py | grep collision | grep enabled Output: stretch_body.robot_params.nominal_params param.collision_arm_camera.enabled 1 stretch_body.robot_params.nominal_params param.collision_stretch_gripper.enabled 1 We see two models. One that protects the camera from the arm, and one that protects the base from the gripper. Each model is registered with the RobotCollision instance as a loadable plug-in. The Robot class calls the RobotCollision.step method periodically at approximately 10hz. RobotCollision.step computes the 'AND' of the limits specified across each Collision Model such that the most restrictive joint limits are set for each joint using the set_soft_motion_limit_min and set_soft_motion_limt_max methods. Default Collision Models The default collision models for Stretch Body are found in robot_collision_models.py . As of this writing, the provided models are: CollisionArmCamera : Avoid collision of the head camera with the arm CollisionStretchGripper : Avoid collision of the wrist-yaw and gripper with the base and ground Warning The provided collision models are coarse and are provided to avoid common potentially harmful collisions only. Using these models it is still possible to collide the robot with itself in some cases. Info Additional collision models are provided for the DexWrist Working with Models The collision models to be used by Stretch Body are defined with the robot_collision parameter. For example, we see in robot_params.py that the CollisionArmCamera is loaded by default: \"robot_collision\" : { 'models' : [ 'collision_arm_camera' ]}, We also see that model collision_arm_camera is defined as: \"collision_arm_camera\" : { 'enabled' : 1 , 'py_class_name' : 'CollisionArmCamera' , 'py_module_name' : 'stretch_body.robot_collision_models' } This instructs RobotCollision to construct a model of type CollisionArmCamera and enable it by default. One can disable this model by default by specifying the following stretch_re1_user_params.yaml : collision_arm_camera : enabled : 0 The entire collision avoidance system can be disabled in stretch_re1_user_params.yaml by: robot : use_collision_manager : 0 A specific collision model can be enabled or disabled during runtime by: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () ... #Do some work r . collision . disable_model ( 'collsion_arm_camera' ) ... #Do some work r . collision . enable_model ( 'collsion_arm_camera' ) Finally, if we want to also use the CollisionStretchGripper model, we can add to stretch_re1_user_params.py : robot_collision : models : - collision_arm_camera - collision_stretch_gripper Creating Custom Collision Models The step method of a RobotCollisionModel returns the desired joint limits given that model. For example, the base class is simply: class RobotCollisionModel ( Device ): def step ( self , status ): return { 'head_pan' : [ None , None ], 'head_tilt' : [ None , None ], 'lift' : [ None , None ], 'arm' : [ None , None ], 'wrist_yaw' : [ None , None ]} where the value of None specifies that no limit is specified and the full range of motion for the joint is acceptable. We could define a new collision model that simply limits the lift range of motion to 1 meter by: class MyCollisionModel ( Device ): def step ( self , status ): return { 'head_pan' : [ None , None ], 'head_tilt' : [ None , None ], 'lift' : [ None , 1.0 ], 'arm' : [ None , None ], 'wrist_yaw' : [ None , None ]} It is straightforward to create a custom collision model. As an example, we will create a model that avoids collision of the arm with a tabletop by Preventing the lift from descending below the table top when the arm is extended Allowing the lift to descend below the tabletop so long as the arm retracted This assumes the arm is initially above the tabletop. To start, in a file collision_arm_table.py we add: from stretch_body.robot_collision import * from stretch_body.hello_utils import * class CollisionArmTable ( RobotCollisionModel ): def __init__ ( self , collision_manager ): RobotCollisionModel . __init__ ( self , collision_manager , 'collision_arm_table' ) def step ( self , status ): limits = { 'lift' : [ None , None ], 'arm' : [ None , None ]} table_height = 0.5 #m arm_safe_retract = 0.1 #m safety_margin = .05 #m x_arm = status [ 'arm' ][ 'pos' ] x_lift = status [ 'lift' ][ 'pos' ] #Force arm to stay retracted if below table if x_lift < table_height : limits [ 'arm' ] = [ None , arm_safe_retract - safety_margin ] else : limits [ 'arm' ] = [ None , None ] #Force lift to stay above table unless arm is retracted if x_arm < arm_safe_retract : limits [ 'lift' ] = [ None , None ] else : limits [ 'lift' ] = [ table_height + safety_margin , None ] return limits In this example, we include the safety_margin as a way to introduce some hysteresis around state changes to avoid toggling between the soft limits. The following command should be run to add the working directory to the PYTHONPATH env. This can also be added to our .bashrc to permanently edit the path: export PYTHONPATH = $PYTHONPATH :/<path_to_modules> Next, we configure RobotCollision to use our CollisionArmTable model in stretch_re1_user_yaml : robot_collision : models : - collision_arm_table collision_arm_table : enabled : 1 py_class_name : 'CollisionArmTable' py_module_name : 'collision_arm_table' Finally, test out the model by driving the arm and lift around using the Xbox teleoperation tool: stretch_xbox_controller_teleop.py All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Collision Avoidance"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#tutorial-collision-avoidance","text":"In this tutorial, we will discuss the simple collision avoidance system that runs as a part of Stretch Body.","title":"Tutorial: Collision Avoidance"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#overview","text":"Stretch Body includes a system to prevent inadvertent self-collisions. It will dynamically limit the range of motion of each joint to prevent self-collisions. Warning Self collisions are still possible while using the collision-avoidance system. The factory default collision models are coarse and not necessarily complete. This system is turned off by default starting with Stretch 2. It may be turned off by default on many RE1 systems. First check if the collision detection system is turned on: stretch_params.py | grep use_collision_manager Output: stretch_body.robot_params.nominal_params param.robot.use_collision_manager 1 If it is turned off you can enable it by adding the following to your stretch_user_yaml.py: robot: use_collision_manager: 1","title":"Overview"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#common-self-collisions","text":"Fortunately, the simple kinematics of Stretch make self-collisions fairly uncommon and simple to predict. The primary places where self-collisions may occur are The lift lowering the wrist or tool into the base The arm retracting the wrist or tool into the base The head_pan at pos==0 and head_tilt at pos=-90 deg and the lift raising the arm into the camera (minor collision) The Dex Wrist (if installed) colliding with itself The Dex Wrist (if installed) colliding with the base","title":"Common Self Collisions"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#joint-limits","text":"The collision avoidance system works by dynamically modifying the acceptable range of motion for each joint. By default, a joint's range is set to the physical hard stop limits. For example, the lift has a mechanical throw of 1.1m: stretch_params.py | grep range | grep lift Output: stretch_body.robot_params.factory_params param.lift.range_m [ 0 .0, 1 .1 ] A reduced range of motion can be set at run-time by setting the Soft Motion Limit. For example, to limit the lift range of motion to 0.3 meters off the base: import stretch_body.robot as robot r = robot . Robot () r . startup () r . lift . set_soft_motion_limit_min ( 0.3 ) We see in the API , the value of None is used to designate no soft limit. It is possible that when setting the Soft Motion Limit the joint's current position is outside of the specified range. In this case, the joint will move to the nearest soft limit to comply with the limits. This can be demonstrated by: import stretch_body.robot as robot import time r = robot . Robot () r . startup () #Move to 0.2 r . lift . move_to ( 0.2 ) r . push_command () time . sleep ( 5.0 ) #Will move to 0.3 r . lift . set_soft_motion_limit_min ( 0.3 )","title":"Joint Limits"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#collision-models","text":"The RobotCollision class manages a set of RobotCollisionModels . Each RobotCollisionModel computes the soft limits for a subset of joints based on a simple geometric model. This geometric model captures the enumerated set of potential collisions listed above. We can see which collision models will execute when use_collision_manager is set to 1: stretch_params.py | grep collision | grep enabled Output: stretch_body.robot_params.nominal_params param.collision_arm_camera.enabled 1 stretch_body.robot_params.nominal_params param.collision_stretch_gripper.enabled 1 We see two models. One that protects the camera from the arm, and one that protects the base from the gripper. Each model is registered with the RobotCollision instance as a loadable plug-in. The Robot class calls the RobotCollision.step method periodically at approximately 10hz. RobotCollision.step computes the 'AND' of the limits specified across each Collision Model such that the most restrictive joint limits are set for each joint using the set_soft_motion_limit_min and set_soft_motion_limt_max methods.","title":"Collision Models"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#default-collision-models","text":"The default collision models for Stretch Body are found in robot_collision_models.py . As of this writing, the provided models are: CollisionArmCamera : Avoid collision of the head camera with the arm CollisionStretchGripper : Avoid collision of the wrist-yaw and gripper with the base and ground Warning The provided collision models are coarse and are provided to avoid common potentially harmful collisions only. Using these models it is still possible to collide the robot with itself in some cases. Info Additional collision models are provided for the DexWrist","title":"Default Collision Models"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#working-with-models","text":"The collision models to be used by Stretch Body are defined with the robot_collision parameter. For example, we see in robot_params.py that the CollisionArmCamera is loaded by default: \"robot_collision\" : { 'models' : [ 'collision_arm_camera' ]}, We also see that model collision_arm_camera is defined as: \"collision_arm_camera\" : { 'enabled' : 1 , 'py_class_name' : 'CollisionArmCamera' , 'py_module_name' : 'stretch_body.robot_collision_models' } This instructs RobotCollision to construct a model of type CollisionArmCamera and enable it by default. One can disable this model by default by specifying the following stretch_re1_user_params.yaml : collision_arm_camera : enabled : 0 The entire collision avoidance system can be disabled in stretch_re1_user_params.yaml by: robot : use_collision_manager : 0 A specific collision model can be enabled or disabled during runtime by: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () ... #Do some work r . collision . disable_model ( 'collsion_arm_camera' ) ... #Do some work r . collision . enable_model ( 'collsion_arm_camera' ) Finally, if we want to also use the CollisionStretchGripper model, we can add to stretch_re1_user_params.py : robot_collision : models : - collision_arm_camera - collision_stretch_gripper","title":"Working with Models"},{"location":"stretch-tutorials/stretch_body/tutorial_collision_avoidance/#creating-custom-collision-models","text":"The step method of a RobotCollisionModel returns the desired joint limits given that model. For example, the base class is simply: class RobotCollisionModel ( Device ): def step ( self , status ): return { 'head_pan' : [ None , None ], 'head_tilt' : [ None , None ], 'lift' : [ None , None ], 'arm' : [ None , None ], 'wrist_yaw' : [ None , None ]} where the value of None specifies that no limit is specified and the full range of motion for the joint is acceptable. We could define a new collision model that simply limits the lift range of motion to 1 meter by: class MyCollisionModel ( Device ): def step ( self , status ): return { 'head_pan' : [ None , None ], 'head_tilt' : [ None , None ], 'lift' : [ None , 1.0 ], 'arm' : [ None , None ], 'wrist_yaw' : [ None , None ]} It is straightforward to create a custom collision model. As an example, we will create a model that avoids collision of the arm with a tabletop by Preventing the lift from descending below the table top when the arm is extended Allowing the lift to descend below the tabletop so long as the arm retracted This assumes the arm is initially above the tabletop. To start, in a file collision_arm_table.py we add: from stretch_body.robot_collision import * from stretch_body.hello_utils import * class CollisionArmTable ( RobotCollisionModel ): def __init__ ( self , collision_manager ): RobotCollisionModel . __init__ ( self , collision_manager , 'collision_arm_table' ) def step ( self , status ): limits = { 'lift' : [ None , None ], 'arm' : [ None , None ]} table_height = 0.5 #m arm_safe_retract = 0.1 #m safety_margin = .05 #m x_arm = status [ 'arm' ][ 'pos' ] x_lift = status [ 'lift' ][ 'pos' ] #Force arm to stay retracted if below table if x_lift < table_height : limits [ 'arm' ] = [ None , arm_safe_retract - safety_margin ] else : limits [ 'arm' ] = [ None , None ] #Force lift to stay above table unless arm is retracted if x_arm < arm_safe_retract : limits [ 'lift' ] = [ None , None ] else : limits [ 'lift' ] = [ table_height + safety_margin , None ] return limits In this example, we include the safety_margin as a way to introduce some hysteresis around state changes to avoid toggling between the soft limits. The following command should be run to add the working directory to the PYTHONPATH env. This can also be added to our .bashrc to permanently edit the path: export PYTHONPATH = $PYTHONPATH :/<path_to_modules> Next, we configure RobotCollision to use our CollisionArmTable model in stretch_re1_user_yaml : robot_collision : models : - collision_arm_table collision_arm_table : enabled : 1 py_class_name : 'CollisionArmTable' py_module_name : 'collision_arm_table' Finally, test out the model by driving the arm and lift around using the Xbox teleoperation tool: stretch_xbox_controller_teleop.py All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Creating Custom Collision Models"},{"location":"stretch-tutorials/stretch_body/tutorial_command_line_tools/","text":"Tutorial: Stretch Body Command Line Tools Stretch Body includes the package hello-robot-stretch-body-tools - a suite of command line tools that allow direct interaction with hardware subsystems. These tools are useful when developing and debugging applications. They also serve as code examples when developing applications for Stretch_Body. These tools can be found by tab completion of 'stretch_' from a terminal. stretch_ stretch_about.py stretch_about_text.py stretch_arm_home.py stretch_arm_jog.py stretch_audio_test.py stretch_base_jog.py stretch_gripper_home.py stretch_gripper_jog.py stretch_hardware_echo.py stretch_head_jog.py stretch_lift_home.py stretch_lift_jog.py stretch_params.py stretch_pimu_jog.py stretch_pimu_scope.py stretch_realsense_visualizer.py stretch_respeaker_test.py stretch_robot_battery_check.py stretch_robot_dynamixel_reboot.py stretch_robot_home.py stretch_robot_jog.py stretch_robot_keyboard_teleop.py stretch_robot_monitor.py stretch_robot_stow.py stretch_robot_system_check.py stretch_robot_urdf_visualizer.py stretch_rp_lidar_jog.py stretch_trajectory_jog.py stretch_version.sh stretch_wacc_jog.py stretch_wacc_scope.py stretch_wrist_yaw_home.py stretch_wrist_yaw_jog.py stretch_xbox_controller_teleop.py All tools accept the '--help' flag as a command line argument to describe its function. For example: stretch_pimu_scope.py --help Output: For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- usage: stretch_pimu_scope.py [ -h ] [ --cliff ] [ --at_cliff ] [ --voltage ] [ --current ] [ --temp ] [ --ax ] [ --ay ] [ --az ] [ --mx ] [ --my ] [ --mz ] [ --gx ] [ --gy ] [ --gz ] [ --roll ] [ --pitch ] [ --heading ] [ --bump ] Visualize Pimu ( Power+IMU ) board data with an oscilloscope optional arguments: -h, --help show this help message and exit --cliff Scope base cliff sensors --at_cliff Scope base at_cliff signal --voltage Scope bus voltage ( V ) --current Scope bus current ( A ) --temp Scope base internal temperature ( C ) --ax Scope base accelerometer AX --ay Scope base accelerometer AY --az Scope base accelerometer AZ --mx Scope base magnetometer MX --my Scope base magnetometer MY --mz Scope base magnetometer MZ --gx Scope base gyro GX --gy Scope base gyro GY --gz Scope base gyro GZ --roll Scope base imu Roll --pitch Scope base imu Pitch --heading Scope base imu Heading --bump Scope base imu bump level Commonly Used Tools These are the tools a typical user is expected to interact with regularly and would benefit from becoming familiar with. Tool Utility stretch_robot_home.py Commonly run after booting up the robot in-order to calibrate the joints stretch_robot_system_check.py Scans for all hardware devices and ensures they are present on the bus and reporting valid values. Useful to verify that the robot is in good working order prior to commanding motion. It will report all success in green, failures in red. stretch_robot_stow.py Useful to return the robot arm and tool to a safe position within the base footprint. It can also be useful if a program fails to exit cleanly and the robot joints are not backdriveable. It will restore them to their 'Safety' state. stretch_robot_battery_check.py Quick way to check the battery voltage / current consumption stretch_xbox_controller_teleop.py Useful to quickly test if a robot can achieve a task by manually teleoperating the robot stretch_robot_dynamixel_reboot.py Resets all Dynamixels in the robot, which might be necessary if a servo overheats during use and enters an error state. Take a minute to explore each of these tools from the console. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Command line Tools"},{"location":"stretch-tutorials/stretch_body/tutorial_command_line_tools/#tutorial-stretch-body-command-line-tools","text":"Stretch Body includes the package hello-robot-stretch-body-tools - a suite of command line tools that allow direct interaction with hardware subsystems. These tools are useful when developing and debugging applications. They also serve as code examples when developing applications for Stretch_Body. These tools can be found by tab completion of 'stretch_' from a terminal. stretch_ stretch_about.py stretch_about_text.py stretch_arm_home.py stretch_arm_jog.py stretch_audio_test.py stretch_base_jog.py stretch_gripper_home.py stretch_gripper_jog.py stretch_hardware_echo.py stretch_head_jog.py stretch_lift_home.py stretch_lift_jog.py stretch_params.py stretch_pimu_jog.py stretch_pimu_scope.py stretch_realsense_visualizer.py stretch_respeaker_test.py stretch_robot_battery_check.py stretch_robot_dynamixel_reboot.py stretch_robot_home.py stretch_robot_jog.py stretch_robot_keyboard_teleop.py stretch_robot_monitor.py stretch_robot_stow.py stretch_robot_system_check.py stretch_robot_urdf_visualizer.py stretch_rp_lidar_jog.py stretch_trajectory_jog.py stretch_version.sh stretch_wacc_jog.py stretch_wacc_scope.py stretch_wrist_yaw_home.py stretch_wrist_yaw_jog.py stretch_xbox_controller_teleop.py All tools accept the '--help' flag as a command line argument to describe its function. For example: stretch_pimu_scope.py --help Output: For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- usage: stretch_pimu_scope.py [ -h ] [ --cliff ] [ --at_cliff ] [ --voltage ] [ --current ] [ --temp ] [ --ax ] [ --ay ] [ --az ] [ --mx ] [ --my ] [ --mz ] [ --gx ] [ --gy ] [ --gz ] [ --roll ] [ --pitch ] [ --heading ] [ --bump ] Visualize Pimu ( Power+IMU ) board data with an oscilloscope optional arguments: -h, --help show this help message and exit --cliff Scope base cliff sensors --at_cliff Scope base at_cliff signal --voltage Scope bus voltage ( V ) --current Scope bus current ( A ) --temp Scope base internal temperature ( C ) --ax Scope base accelerometer AX --ay Scope base accelerometer AY --az Scope base accelerometer AZ --mx Scope base magnetometer MX --my Scope base magnetometer MY --mz Scope base magnetometer MZ --gx Scope base gyro GX --gy Scope base gyro GY --gz Scope base gyro GZ --roll Scope base imu Roll --pitch Scope base imu Pitch --heading Scope base imu Heading --bump Scope base imu bump level","title":"Tutorial: Stretch Body Command Line Tools"},{"location":"stretch-tutorials/stretch_body/tutorial_command_line_tools/#commonly-used-tools","text":"These are the tools a typical user is expected to interact with regularly and would benefit from becoming familiar with. Tool Utility stretch_robot_home.py Commonly run after booting up the robot in-order to calibrate the joints stretch_robot_system_check.py Scans for all hardware devices and ensures they are present on the bus and reporting valid values. Useful to verify that the robot is in good working order prior to commanding motion. It will report all success in green, failures in red. stretch_robot_stow.py Useful to return the robot arm and tool to a safe position within the base footprint. It can also be useful if a program fails to exit cleanly and the robot joints are not backdriveable. It will restore them to their 'Safety' state. stretch_robot_battery_check.py Quick way to check the battery voltage / current consumption stretch_xbox_controller_teleop.py Useful to quickly test if a robot can achieve a task by manually teleoperating the robot stretch_robot_dynamixel_reboot.py Resets all Dynamixels in the robot, which might be necessary if a servo overheats during use and enters an error state. Take a minute to explore each of these tools from the console. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Commonly Used Tools"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/","text":"Tutorial: Contact Models This tutorial introduces the Stretch Body contact detection system and explains how to configure it for your application. What is Guarded Contact? Guarded contact is our term for the Stretch contact sensitive behaviors. The guarded contact behavior is simply: Detect when the actuator effort exceeds a user-specified threshold during joint motion. If the threshold is exceeded: Enable the default safety controller for the joint Remain in safety mode until a subsequent joint command is received Practically this enables the arm, for example, to move out yet stop upon collision. Let's test this out with the following script: #!/usr/bin/env python import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move to full retraction robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint () input ( 'Arm will extend and respond to contact. Manually attempt to stop it. Hit enter when ready' ) robot . arm . move_to ( 0.3 ) robot . push_command () robot . arm . wait_until_at_setpoint ( timeout = 5.0 ) #Now turn off guarded contacts input ( 'Arm will retract but will not respond to contact. Manually attempt to stop it. Hit enter when ready' ) robot . arm . motor . disable_guarded_mode () robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint ( timeout = 5.0 ) robot . stop () You should see that the arm stops on contact when it extends, however, it doesn't stop on contact when it then retracts. This is the guarded contact behavior in action. Specifying Guarded Contacts The four stepper joints (base, arm, and lift) all support guarded contact settings when executing motion. This is evident in their move_to and move_by methods. For example, we see in the Arm's base class of PrismaticJoint : def move_by ( self , x_m , v_m = None , a_m = None , stiffness = None , contact_thresh_pos_N = None , contact_thresh_neg_N = None , req_calibration = True , contact_thresh_pos = None , contact_thresh_neg = None ) In this method, you can optionally specify a contact threshold in the positive and negative direction with contact_thresh_pos and contact_thresh_neg respectively. Note These optional parameters will default to None , in which case the motion will adopt the default settings as defined by the robot's parameters. Warning The parameters contact_thresh_pos_N and contact_thresh_neg_N are deprecated and no longer supported. stretch_params.py | grep arm | grep contact ... stretch_configuration_params.yaml param.arm.contact_models.effort_pct.contact_thresh_default [ -45.0, 45 .0 ] ... Contact Models A contact model is simply a function that, given a user-specified contact threshold, computes the motor current at which the motor controller will trigger a guarded contact. The following contact models are currently implemented: The Effort-Pct Contact Model Effort-Pct is the default contact model for Stretch 2. It simply scales the maximum range of motor currents into the range of [-100,100]. Thus, if you desire to have the robot arm extend but stop at 50% of its maximum current, you would write: robot . arm . move_by ( 0.1 , contact_thresh_pos = 50.0 ) Adjusting Contact Behaviors The default factory settings for contact thresholds are tuned to allow Stretch to move throughout its workspace without triggering false-positive guarded contact events. These settings are the worst-case tuning as they account for the internal disturbances of the Stretch drive-train across its entire workspace. It is possible to obtain greater contact sensitivity in carefully selected portions of the arm and lift workspace. Users who wish to programmatically adjust contact behaviors can create a simple test script and experiment with different values. For example: #!/usr/bin/env python import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () cpos = 30.0 cneg = - 30.0 robot . arm . move_to ( 0.0 , contact_thresh_pos = cpos , contact_thresh_neg = cneg ) robot . push_command () robot . arm . wait_until_at_setpoint () robot . arm . move_to ( 0.5 , contact_thresh_pos = cpos , contact_thresh_neg = cneg ) robot . push_command () robot . arm . wait_until_at_setpoint ( timeout = 5.0 ) robot . stop () Guarded Contact with the Base Guarded contacts are enabled by default for the arm and lift as they typically require safe and contact-sensitive motion. They are turned off on the base by default as varying surface terrain can produce undesired false-positive events. That being said, guarded contacts can be enabled on the base. They may be useful as a simple bump detector such that the base will stop when it runs into a wall. #!/usr/bin/env python import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . base . left_wheel . enable_guarded_mode () robot . base . right_wheel . enable_guarded_mode () robot . base . move_by ( 0.025 ) robot . push_command () robot . base . wait_until_at_setpoint () robot . base . move_by ( - 0.025 ) robot . push_command () robot . base . wait_until_at_setpoint () robot . stop () Advanced: Calibrating Contact Thresholds The Stretch Factory package provides a tool to allow advanced users to recalibrate the default guarded contact thresholds. This tool can be useful if you've added additional payload to the arm and are experiencing false-positive guarded contact detections. The tool sweeps the joint through its range of motion for n-cycle iterations. It computes the maximum contact forces in both directions, adds padding, contact_thresh_calibration_margin , to this value, and stores it to the robot's configuration YAML. REx_calibrate_guarded_contact.py -h For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- usage: REx_calibrate_guarded_contact.py [ -h ] ( --lift | --arm ) [ --ncycle NCYCLE ] Calibrate the default guarded contacts for a joint. optional arguments: -h, --help show this help message and exit --lift Calibrate the lift joint --arm Calibrate the arm joint --ncycle NCYCLE Number of sweeps to run [ 4 ] All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Contact Models"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#tutorial-contact-models","text":"This tutorial introduces the Stretch Body contact detection system and explains how to configure it for your application.","title":"Tutorial: Contact Models"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#what-is-guarded-contact","text":"Guarded contact is our term for the Stretch contact sensitive behaviors. The guarded contact behavior is simply: Detect when the actuator effort exceeds a user-specified threshold during joint motion. If the threshold is exceeded: Enable the default safety controller for the joint Remain in safety mode until a subsequent joint command is received Practically this enables the arm, for example, to move out yet stop upon collision. Let's test this out with the following script: #!/usr/bin/env python import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move to full retraction robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint () input ( 'Arm will extend and respond to contact. Manually attempt to stop it. Hit enter when ready' ) robot . arm . move_to ( 0.3 ) robot . push_command () robot . arm . wait_until_at_setpoint ( timeout = 5.0 ) #Now turn off guarded contacts input ( 'Arm will retract but will not respond to contact. Manually attempt to stop it. Hit enter when ready' ) robot . arm . motor . disable_guarded_mode () robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint ( timeout = 5.0 ) robot . stop () You should see that the arm stops on contact when it extends, however, it doesn't stop on contact when it then retracts. This is the guarded contact behavior in action.","title":"What is Guarded Contact?"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#specifying-guarded-contacts","text":"The four stepper joints (base, arm, and lift) all support guarded contact settings when executing motion. This is evident in their move_to and move_by methods. For example, we see in the Arm's base class of PrismaticJoint : def move_by ( self , x_m , v_m = None , a_m = None , stiffness = None , contact_thresh_pos_N = None , contact_thresh_neg_N = None , req_calibration = True , contact_thresh_pos = None , contact_thresh_neg = None ) In this method, you can optionally specify a contact threshold in the positive and negative direction with contact_thresh_pos and contact_thresh_neg respectively. Note These optional parameters will default to None , in which case the motion will adopt the default settings as defined by the robot's parameters. Warning The parameters contact_thresh_pos_N and contact_thresh_neg_N are deprecated and no longer supported. stretch_params.py | grep arm | grep contact ... stretch_configuration_params.yaml param.arm.contact_models.effort_pct.contact_thresh_default [ -45.0, 45 .0 ] ...","title":"Specifying Guarded Contacts"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#contact-models","text":"A contact model is simply a function that, given a user-specified contact threshold, computes the motor current at which the motor controller will trigger a guarded contact. The following contact models are currently implemented:","title":"Contact Models"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#the-effort-pct-contact-model","text":"Effort-Pct is the default contact model for Stretch 2. It simply scales the maximum range of motor currents into the range of [-100,100]. Thus, if you desire to have the robot arm extend but stop at 50% of its maximum current, you would write: robot . arm . move_by ( 0.1 , contact_thresh_pos = 50.0 )","title":"The Effort-Pct Contact Model"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#adjusting-contact-behaviors","text":"The default factory settings for contact thresholds are tuned to allow Stretch to move throughout its workspace without triggering false-positive guarded contact events. These settings are the worst-case tuning as they account for the internal disturbances of the Stretch drive-train across its entire workspace. It is possible to obtain greater contact sensitivity in carefully selected portions of the arm and lift workspace. Users who wish to programmatically adjust contact behaviors can create a simple test script and experiment with different values. For example: #!/usr/bin/env python import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () cpos = 30.0 cneg = - 30.0 robot . arm . move_to ( 0.0 , contact_thresh_pos = cpos , contact_thresh_neg = cneg ) robot . push_command () robot . arm . wait_until_at_setpoint () robot . arm . move_to ( 0.5 , contact_thresh_pos = cpos , contact_thresh_neg = cneg ) robot . push_command () robot . arm . wait_until_at_setpoint ( timeout = 5.0 ) robot . stop ()","title":"Adjusting Contact Behaviors"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#guarded-contact-with-the-base","text":"Guarded contacts are enabled by default for the arm and lift as they typically require safe and contact-sensitive motion. They are turned off on the base by default as varying surface terrain can produce undesired false-positive events. That being said, guarded contacts can be enabled on the base. They may be useful as a simple bump detector such that the base will stop when it runs into a wall. #!/usr/bin/env python import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . base . left_wheel . enable_guarded_mode () robot . base . right_wheel . enable_guarded_mode () robot . base . move_by ( 0.025 ) robot . push_command () robot . base . wait_until_at_setpoint () robot . base . move_by ( - 0.025 ) robot . push_command () robot . base . wait_until_at_setpoint () robot . stop ()","title":"Guarded Contact with the Base"},{"location":"stretch-tutorials/stretch_body/tutorial_contact_models/#advanced-calibrating-contact-thresholds","text":"The Stretch Factory package provides a tool to allow advanced users to recalibrate the default guarded contact thresholds. This tool can be useful if you've added additional payload to the arm and are experiencing false-positive guarded contact detections. The tool sweeps the joint through its range of motion for n-cycle iterations. It computes the maximum contact forces in both directions, adds padding, contact_thresh_calibration_margin , to this value, and stores it to the robot's configuration YAML. REx_calibrate_guarded_contact.py -h For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- usage: REx_calibrate_guarded_contact.py [ -h ] ( --lift | --arm ) [ --ncycle NCYCLE ] Calibrate the default guarded contacts for a joint. optional arguments: -h, --help show this help message and exit --lift Calibrate the lift joint --arm Calibrate the arm joint --ncycle NCYCLE Number of sweeps to run [ 4 ] All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Advanced: Calibrating Contact Thresholds"},{"location":"stretch-tutorials/stretch_body/tutorial_custom_wrist_dof/","text":"Tutorial: Custom Wrist DOF In this tutorial, we explore how to add additional degrees of freedom to the Stretch wrist. Stretch exposes a Dynamixel X-Series TTL control bus at the end of its arm. It uses the Dynamixel XL430-W250 for the Wrist Yaw and the Stretch Gripper that comes standard with the robot. See the Hardware User Guide to learn how to mechanically attach additional DOFs to the robot. Note Stretch is compatible with any Dynamixel X Series servo that utilizes the TTL level Multidrop Bus. Adding a Custom DOF Adding one or more custom Dynamixel X Series servos to Stretch wrist involves: Creating a new class that derives from DynamixelHelloXL430 Adding YAML parameters to stretch_user_params.yaml that configure the servo as desired Adding YAML parameters to stretch_user_params.yaml that tell Stretch to include this class in its EndOfArm list of servos Let's create a new DOF called MyWristPitch in a file named my_wrist_pitch.py . Place the file somewhere on the $PYTHONPATH. from stretch_body.dynamixel_hello_XL430 import DynamixelHelloXL430 from stretch_body.hello_utils import * class MyWristPitch ( DynamixelHelloXL430 ): def __init__ ( self , chain = None ): DynamixelHelloXL430 . __init__ ( self , 'my_wrist_pitch' , chain ) self . poses = { 'tool_up' : deg_to_rad ( 45 ), 'tool_down' : deg_to_rad ( - 45 )} def pose ( self , p , v_r = None , a_r = None ): self . move_to ( self . poses [ p ], v_r , a_r ) Now let's add the tools' parameters to your stretch_user_params.yaml to configure this servo. You may want to adapt these parameters to your application but the nominal values found here usually work well. Below we highlight some of the more useful parameters. my_wrist_pitch : id : 1 #ID on the Dynamixel bus range_t : #Range of servo, in ticks - 0 - 4096 req_calibration : 0 #Does the joint require homing after startup use_multiturn : 0 #Single turn or multi-turn mode of rotation zero_t : 2048 #Position in ticks that corresponds to zero radians For this example, we are assuming a single-turn joint that doesn't require hard stop-based homing. We also assume the servo has the Robotis default ID of 1. At this point, your MyWristPitch class is ready to use. Plug the servo into the cable leaving the Stretch WristYaw joint. Experiment with the API from iPython In [ 1 ]: import my_wrist_pitch In [ 2 ]: w = wrist_pitch . WristPitch () In [ 3 ]: w . startup () In [ 4 ]: w . move_by ( 0.1 ) In [ 5 ]: w . pose ( 'tool_up' ) In [ 6 ]: w . pose ( 'tool_down' ) Finally, you'll want to make your WristPitch available from stretch_body.robot . Add the following YAML to your stretch_user_params.yaml end_of_arm : devices : wrist_pitch : py_class_name : WristPitch py_module_name : wrist_pitch This tells stretch_body.robot to manage a wrist_pitch.WristPitch instance and add it to the EndOfArm list of tools. Try it from iPython: In [ 1 ]: import stretch_body.robot as robot In [ 2 ]: r = robot . Robot () In [ 3 ]: r . startup () In [ 4 ]: r . end_of_arm . move_by ( 'wrist_pitch' , 0.1 ) All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Custom Wrist DOF"},{"location":"stretch-tutorials/stretch_body/tutorial_custom_wrist_dof/#tutorial-custom-wrist-dof","text":"In this tutorial, we explore how to add additional degrees of freedom to the Stretch wrist. Stretch exposes a Dynamixel X-Series TTL control bus at the end of its arm. It uses the Dynamixel XL430-W250 for the Wrist Yaw and the Stretch Gripper that comes standard with the robot. See the Hardware User Guide to learn how to mechanically attach additional DOFs to the robot. Note Stretch is compatible with any Dynamixel X Series servo that utilizes the TTL level Multidrop Bus.","title":"Tutorial: Custom Wrist DOF"},{"location":"stretch-tutorials/stretch_body/tutorial_custom_wrist_dof/#adding-a-custom-dof","text":"Adding one or more custom Dynamixel X Series servos to Stretch wrist involves: Creating a new class that derives from DynamixelHelloXL430 Adding YAML parameters to stretch_user_params.yaml that configure the servo as desired Adding YAML parameters to stretch_user_params.yaml that tell Stretch to include this class in its EndOfArm list of servos Let's create a new DOF called MyWristPitch in a file named my_wrist_pitch.py . Place the file somewhere on the $PYTHONPATH. from stretch_body.dynamixel_hello_XL430 import DynamixelHelloXL430 from stretch_body.hello_utils import * class MyWristPitch ( DynamixelHelloXL430 ): def __init__ ( self , chain = None ): DynamixelHelloXL430 . __init__ ( self , 'my_wrist_pitch' , chain ) self . poses = { 'tool_up' : deg_to_rad ( 45 ), 'tool_down' : deg_to_rad ( - 45 )} def pose ( self , p , v_r = None , a_r = None ): self . move_to ( self . poses [ p ], v_r , a_r ) Now let's add the tools' parameters to your stretch_user_params.yaml to configure this servo. You may want to adapt these parameters to your application but the nominal values found here usually work well. Below we highlight some of the more useful parameters. my_wrist_pitch : id : 1 #ID on the Dynamixel bus range_t : #Range of servo, in ticks - 0 - 4096 req_calibration : 0 #Does the joint require homing after startup use_multiturn : 0 #Single turn or multi-turn mode of rotation zero_t : 2048 #Position in ticks that corresponds to zero radians For this example, we are assuming a single-turn joint that doesn't require hard stop-based homing. We also assume the servo has the Robotis default ID of 1. At this point, your MyWristPitch class is ready to use. Plug the servo into the cable leaving the Stretch WristYaw joint. Experiment with the API from iPython In [ 1 ]: import my_wrist_pitch In [ 2 ]: w = wrist_pitch . WristPitch () In [ 3 ]: w . startup () In [ 4 ]: w . move_by ( 0.1 ) In [ 5 ]: w . pose ( 'tool_up' ) In [ 6 ]: w . pose ( 'tool_down' ) Finally, you'll want to make your WristPitch available from stretch_body.robot . Add the following YAML to your stretch_user_params.yaml end_of_arm : devices : wrist_pitch : py_class_name : WristPitch py_module_name : wrist_pitch This tells stretch_body.robot to manage a wrist_pitch.WristPitch instance and add it to the EndOfArm list of tools. Try it from iPython: In [ 1 ]: import stretch_body.robot as robot In [ 2 ]: r = robot . Robot () In [ 3 ]: r . startup () In [ 4 ]: r . end_of_arm . move_by ( 'wrist_pitch' , 0.1 ) All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Adding a Custom DOF"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/","text":"Tutorial: Working with Dynamixel Servos In this tutorial, we will go into the details with Dynamixel servos and Stretch. Overview Stretch comes with two Dynamixel buses - one for the head and one for the end-of-arm: ls /dev/hello-dynamixel-* Output: /dev/hello-dynamixel-head /dev/hello-dynamixel-wrist Typically, users will interact with these devices through either the Head or the EndOfArm interfaces. This tutorial is for users looking to work directly with the servos from the provided servo tools or through Stretch Body's low-level Dynamixel API. Servo Tools Note The servo tools here are part of the Stretch Factory package which is installed as a part of Stretch Body. Jogging the Servos You can directly command each servo using the command line tool REx_dynamixel_servo_jog.py . This can be useful for debugging new servos added to the end-of-arm tool during system bring-up. For example, to command the head pan servo: REx_dynamixel_jog.py /dev/hello-dynamixel-head 11 Output: [ Dynamixel ID:011 ] ping Succeeded. Dynamixel model number : 1080 ------ MENU ------- m: menu a: increment position 50 tick b: decrement position 50 tick A: increment position 500 ticks B: decrement position 500 ticks v: set profile velocity u: set profile acceleration z: zero position h: show homing offset o: zero homing offset q: got to position p: ping r: reboot w: set max pwm t: set max temp i: set id d: disable torque e: enable torque ------------------- Rebooting the Servos Under high-load conditions, the servos may enter an error state to protect themselves from thermal overload. In this case, the red LED on the servo will flash (if visible). In addition, the servo will be unresponsive to motion commands. In this case, allow the overheating servo to cool down and reboot the servos using the stretch_robot_dynamixel_reboot.py tool: stretch_robot_dynamixel_reboot.py Output: For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ---- Rebooting Head ---- [ Dynamixel ID:011 ] Reboot Succeeded. [ Dynamixel ID:012 ] Reboot Succeeded. ---- Rebooting Wrist ---- [ Dynamixel ID:013 ] Reboot Succeeded. [ Dynamixel ID:014 ] Reboot Succeeded. Identify Servos on the Bus If it is unclear which servos are on the bus, and at what baud rate, you can use the REx_dynamixel_id_scan.py tool. Here we see that the two head servos are at ID 11 and 12 at baud 57600 . REx_dynamixel_id_scan.py /dev/hello-dynamixel-head --baud 57600 Output: Scanning bus /dev/hello-dynamixel-head at baud rate 57600 ---------------------------------------------------------- [ Dynamixel ID:000 ] ping Failed. [ Dynamixel ID:001 ] ping Failed. [ Dynamixel ID:002 ] ping Failed. [ Dynamixel ID:003 ] ping Failed. [ Dynamixel ID:004 ] ping Failed. [ Dynamixel ID:005 ] ping Failed. [ Dynamixel ID:006 ] ping Failed. [ Dynamixel ID:007 ] ping Failed. [ Dynamixel ID:008 ] ping Failed. [ Dynamixel ID:009 ] ping Failed. [ Dynamixel ID:010 ] ping Failed. [ Dynamixel ID:011 ] ping Succeeded. Dynamixel model number : 1080 [ Dynamixel ID:012 ] ping Succeeded. Dynamixel model number : 1060 [ Dynamixel ID:013 ] ping Failed. [ Dynamixel ID:014 ] ping Failed. [ Dynamixel ID:015 ] ping Failed. [ Dynamixel ID:016 ] ping Failed. [ Dynamixel ID:017 ] ping Failed. [ Dynamixel ID:018 ] ping Failed. [ Dynamixel ID:019 ] ping Failed. [ Dynamixel ID:020 ] ping Failed. [ Dynamixel ID:021 ] ping Failed. [ Dynamixel ID:022 ] ping Failed. [ Dynamixel ID:023 ] ping Failed. [ Dynamixel ID:024 ] ping Failed. Setting the Servo Baud Rate Stretch ships with its Dynamixel servos configured to baudrate=115200 . When adding your servos to the end-of-arm tool, you may want to set the servo baud using the REx_dynamixel_set_baud.py tool. For example: REx_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 Output: --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud Note Earlier units of Stretch RE1 may be running Dynamixel servos at baud 57600. Setting the Servo ID Dynamixel servos come with ID=1 from the factory. When adding your servos to the end-of-arm tool, you may want to set the servo ID using the REx_dynamixel_id_change.py tool. For example: REx_dynamixel_id_change.py /dev/hello-dynamixel-wrist 1 13 --baud 115200 Output: [ Dynamixel ID:001 ] ping Succeeded. Dynamixel model number : 1080 Ready to change ID 1 to 13 . Hit enter to continue : [ Dynamixel ID:013 ] ping Succeeded. Dynamixel model number : 1080 Success at setting ID to 13 Stretch Body Dynamixel API Stretch Body's low-level Dynamixel API includes a hierarchy of three classes Class DynamixelXChain DynamixelHelloXL430 DynamixelXL430 Note The naming of XL430 is for legacy reasons. These classes will work with all X Series servos. DynamixelXChain DynamixelXChain manages a set of daisy-chained servos on a single bus (for example the head_pan and head_tilt servos). It allows for greater communication bandwidth by doing a group read/write over USB. The EndOfArm class derives from DynamixelXChain to provide an extensible interface that supports a user in integrating additional degrees of freedom to the robot. The tutorial Adding Custom Wrist DoF explains how to do this. DynamixelHelloXL430 DynamixelHelloXL430 provides an interface to servo motion that is consistent with the Stretch Body lift, arm, and base joints. It also manages the servo parameters and calibration. Let's explore this interface further. From iPython, let's look at the status message for DynamixelHelloXL430 import stretch_body.dynamixel_hello_XL430 m = stretch_body . dynamixel_hello_XL430 . DynamixelHelloXL430 ( 'head_pan' ) m . startup () m . pretty_print () Output: ----- HelloXL430 ------ Name head_pan Position ( rad ) - 0.0 Position ( deg ) - 0.0 Position ( ticks ) 1250 Velocity ( rad / s ) - 0.0 Velocity ( ticks / s ) 0 Effort ( % ) 0.0 Effort ( ticks ) 0 Temp 34.0 Comm Errors 0 Hardware Error 0 Hardware Error : Input Voltage Error : False Hardware Error : Overheating Error : False Hardware Error : Motor Encoder Error : False Hardware Error : Electrical Shock Error : False Hardware Error : Overload Error : False Watchdog Errors : 0 Timestamp PC 1661552966.7202659 Range ( ticks ) [ 0 , 3827 ] Range ( rad ) [ 1.9174759848570513 , - 3.953068490381297 ] Stalled True Stall Overload False Is Calibrated 0 We see that it reports the position in both radians (with respect to the joint frame) and ticks (with respect to the servo encoder). DynamixelHelloXL430 handles the calibration between the two using its method ticks_to_world_rad through the following params: stretch_params.py | grep head_pan | grep '_t ' Output: stretch_configuration_params.yaml param.head_pan.range_t [ 0 , 3827 ] stretch_configuration_params.yaml param.head_pan.zero_t 1250 In addition to move_to and move_by , the class also implements a splined trajectory interface as discussed in the Splined Trajectory Tutorial . DynamixelXL430 DynamixelXL430 provides a thin wrapper to the Robotis Dynamixel SDK . You may choose to interact with the servo at this level as well. For example, to jog the head_pan 200 ticks: import stretch_body.dynamixel_XL430 import time m = stretch_body . dynamixel_XL430 . DynamixelXL430 ( 11 , '/dev/hello-dynamixel-head' , baud = 115200 ) m . startup () x = m . get_pos () #In encoder ticks m . go_to_pos ( x + 200 ) #Move 200 ticks incremental time . sleep ( 2.0 ) m . stop () All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Dynamixel Servos"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#tutorial-working-with-dynamixel-servos","text":"In this tutorial, we will go into the details with Dynamixel servos and Stretch.","title":"Tutorial: Working with Dynamixel Servos"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#overview","text":"Stretch comes with two Dynamixel buses - one for the head and one for the end-of-arm: ls /dev/hello-dynamixel-* Output: /dev/hello-dynamixel-head /dev/hello-dynamixel-wrist Typically, users will interact with these devices through either the Head or the EndOfArm interfaces. This tutorial is for users looking to work directly with the servos from the provided servo tools or through Stretch Body's low-level Dynamixel API.","title":"Overview"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#servo-tools","text":"Note The servo tools here are part of the Stretch Factory package which is installed as a part of Stretch Body.","title":"Servo Tools"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#jogging-the-servos","text":"You can directly command each servo using the command line tool REx_dynamixel_servo_jog.py . This can be useful for debugging new servos added to the end-of-arm tool during system bring-up. For example, to command the head pan servo: REx_dynamixel_jog.py /dev/hello-dynamixel-head 11 Output: [ Dynamixel ID:011 ] ping Succeeded. Dynamixel model number : 1080 ------ MENU ------- m: menu a: increment position 50 tick b: decrement position 50 tick A: increment position 500 ticks B: decrement position 500 ticks v: set profile velocity u: set profile acceleration z: zero position h: show homing offset o: zero homing offset q: got to position p: ping r: reboot w: set max pwm t: set max temp i: set id d: disable torque e: enable torque -------------------","title":"Jogging the Servos"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#rebooting-the-servos","text":"Under high-load conditions, the servos may enter an error state to protect themselves from thermal overload. In this case, the red LED on the servo will flash (if visible). In addition, the servo will be unresponsive to motion commands. In this case, allow the overheating servo to cool down and reboot the servos using the stretch_robot_dynamixel_reboot.py tool: stretch_robot_dynamixel_reboot.py Output: For use with S T R E T C H ( TM ) RESEARCH EDITION from Hello Robot Inc. ---- Rebooting Head ---- [ Dynamixel ID:011 ] Reboot Succeeded. [ Dynamixel ID:012 ] Reboot Succeeded. ---- Rebooting Wrist ---- [ Dynamixel ID:013 ] Reboot Succeeded. [ Dynamixel ID:014 ] Reboot Succeeded.","title":"Rebooting the Servos"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#identify-servos-on-the-bus","text":"If it is unclear which servos are on the bus, and at what baud rate, you can use the REx_dynamixel_id_scan.py tool. Here we see that the two head servos are at ID 11 and 12 at baud 57600 . REx_dynamixel_id_scan.py /dev/hello-dynamixel-head --baud 57600 Output: Scanning bus /dev/hello-dynamixel-head at baud rate 57600 ---------------------------------------------------------- [ Dynamixel ID:000 ] ping Failed. [ Dynamixel ID:001 ] ping Failed. [ Dynamixel ID:002 ] ping Failed. [ Dynamixel ID:003 ] ping Failed. [ Dynamixel ID:004 ] ping Failed. [ Dynamixel ID:005 ] ping Failed. [ Dynamixel ID:006 ] ping Failed. [ Dynamixel ID:007 ] ping Failed. [ Dynamixel ID:008 ] ping Failed. [ Dynamixel ID:009 ] ping Failed. [ Dynamixel ID:010 ] ping Failed. [ Dynamixel ID:011 ] ping Succeeded. Dynamixel model number : 1080 [ Dynamixel ID:012 ] ping Succeeded. Dynamixel model number : 1060 [ Dynamixel ID:013 ] ping Failed. [ Dynamixel ID:014 ] ping Failed. [ Dynamixel ID:015 ] ping Failed. [ Dynamixel ID:016 ] ping Failed. [ Dynamixel ID:017 ] ping Failed. [ Dynamixel ID:018 ] ping Failed. [ Dynamixel ID:019 ] ping Failed. [ Dynamixel ID:020 ] ping Failed. [ Dynamixel ID:021 ] ping Failed. [ Dynamixel ID:022 ] ping Failed. [ Dynamixel ID:023 ] ping Failed. [ Dynamixel ID:024 ] ping Failed.","title":"Identify Servos on the Bus"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#setting-the-servo-baud-rate","text":"Stretch ships with its Dynamixel servos configured to baudrate=115200 . When adding your servos to the end-of-arm tool, you may want to set the servo baud using the REx_dynamixel_set_baud.py tool. For example: REx_dynamixel_set_baud.py /dev/hello-dynamixel-wrist 13 115200 Output: --------------------- Checking servo current baud for 57600 ---- Identified current baud of 57600 . Changing baud to 115200 Success at changing baud Note Earlier units of Stretch RE1 may be running Dynamixel servos at baud 57600.","title":"Setting the Servo Baud Rate"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#setting-the-servo-id","text":"Dynamixel servos come with ID=1 from the factory. When adding your servos to the end-of-arm tool, you may want to set the servo ID using the REx_dynamixel_id_change.py tool. For example: REx_dynamixel_id_change.py /dev/hello-dynamixel-wrist 1 13 --baud 115200 Output: [ Dynamixel ID:001 ] ping Succeeded. Dynamixel model number : 1080 Ready to change ID 1 to 13 . Hit enter to continue : [ Dynamixel ID:013 ] ping Succeeded. Dynamixel model number : 1080 Success at setting ID to 13","title":"Setting the Servo ID"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#stretch-body-dynamixel-api","text":"Stretch Body's low-level Dynamixel API includes a hierarchy of three classes Class DynamixelXChain DynamixelHelloXL430 DynamixelXL430 Note The naming of XL430 is for legacy reasons. These classes will work with all X Series servos.","title":"Stretch Body Dynamixel API"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#dynamixelxchain","text":"DynamixelXChain manages a set of daisy-chained servos on a single bus (for example the head_pan and head_tilt servos). It allows for greater communication bandwidth by doing a group read/write over USB. The EndOfArm class derives from DynamixelXChain to provide an extensible interface that supports a user in integrating additional degrees of freedom to the robot. The tutorial Adding Custom Wrist DoF explains how to do this.","title":"DynamixelXChain"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#dynamixelhelloxl430","text":"DynamixelHelloXL430 provides an interface to servo motion that is consistent with the Stretch Body lift, arm, and base joints. It also manages the servo parameters and calibration. Let's explore this interface further. From iPython, let's look at the status message for DynamixelHelloXL430 import stretch_body.dynamixel_hello_XL430 m = stretch_body . dynamixel_hello_XL430 . DynamixelHelloXL430 ( 'head_pan' ) m . startup () m . pretty_print () Output: ----- HelloXL430 ------ Name head_pan Position ( rad ) - 0.0 Position ( deg ) - 0.0 Position ( ticks ) 1250 Velocity ( rad / s ) - 0.0 Velocity ( ticks / s ) 0 Effort ( % ) 0.0 Effort ( ticks ) 0 Temp 34.0 Comm Errors 0 Hardware Error 0 Hardware Error : Input Voltage Error : False Hardware Error : Overheating Error : False Hardware Error : Motor Encoder Error : False Hardware Error : Electrical Shock Error : False Hardware Error : Overload Error : False Watchdog Errors : 0 Timestamp PC 1661552966.7202659 Range ( ticks ) [ 0 , 3827 ] Range ( rad ) [ 1.9174759848570513 , - 3.953068490381297 ] Stalled True Stall Overload False Is Calibrated 0 We see that it reports the position in both radians (with respect to the joint frame) and ticks (with respect to the servo encoder). DynamixelHelloXL430 handles the calibration between the two using its method ticks_to_world_rad through the following params: stretch_params.py | grep head_pan | grep '_t ' Output: stretch_configuration_params.yaml param.head_pan.range_t [ 0 , 3827 ] stretch_configuration_params.yaml param.head_pan.zero_t 1250 In addition to move_to and move_by , the class also implements a splined trajectory interface as discussed in the Splined Trajectory Tutorial .","title":"DynamixelHelloXL430"},{"location":"stretch-tutorials/stretch_body/tutorial_dynamixel_servos/#dynamixelxl430","text":"DynamixelXL430 provides a thin wrapper to the Robotis Dynamixel SDK . You may choose to interact with the servo at this level as well. For example, to jog the head_pan 200 ticks: import stretch_body.dynamixel_XL430 import time m = stretch_body . dynamixel_XL430 . DynamixelXL430 ( 11 , '/dev/hello-dynamixel-head' , baud = 115200 ) m . startup () x = m . get_pos () #In encoder ticks m . go_to_pos ( x + 200 ) #Move 200 ticks incremental time . sleep ( 2.0 ) m . stop () All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"DynamixelXL430"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/","text":"Tutorial: Introduction to Stretch Body The Stretch_Body package provides a low-level Python API to the Stretch hardware. The Stretch_Body package is intended for advanced users who prefer to not use ROS to control the robot. It assumes a moderate level of experience programming robot sensors and actuators. The package is available on Git and installable via Pip . It encapsulates the: Mobile base Arm Lift Head actuators End-of-arm-actuators Wrist board with accelerometer (Wacc) Base power and IMU board (Pimu) As shown below, the primary programming interface to Stretch Body is the Robot class . This class encapsulates the various hardware module classes (e.g. Lift, Arm, etc). Each of these modules then communicates with the robot firmware over USB using various utility classes. Stretch also includes 3rd party hardware devices that are not accessible through Stretch_Body. However, it is possible to directly access this hardware through open-source Python packages: Laser range finder: rplidar Respeaker: respeaker_python_library D435i: pyrealsense2 Robot Interface The primary developer interface to Stretch_Body is the Robot class . Let's write some code to explore the interface. Launch an interactive Python terminal: ipython Then type in the following: 1 2 3 4 5 6 7 8 9 10 11 import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () for i in range ( 10 ): robot . pretty_print () time . sleep ( 0.25 ) robot . stop () As you can see, this prints all robot sensors and state data to the console every 250ms. Looking at this in detail: 4 5 6 import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () Here we instantiated an instance of our robot through the Robot class. The call to startup() opens the serial ports to the various devices, loads the robot YAML parameters, and launches a few helper threads. 7 8 9 for i in range ( 10 ): robot . pretty_print () time . sleep ( 0.25 ) The call to pretty_print() prints to console all of the robot's sensor and state data. The sensor data is automatically updated in the background by a helper thread. 11 robot . stop () Finally, the stop() method shuts down the threads and cleanly closes the open serial ports. Units The Robot API uses SI units: meters radians seconds Newtons Amps Volts Parameters may be named with a suffix to help describe the unit type. For example: pos_m : meters pos_r: radians The Robot Status The Robot derives from the Device class . It also encapsulates several other Devices: robot.head robot.arm robot.lift robot.base robot.wacc robot.pimu robot.end_of_arm All devices contain a Status dictionary. The Status contains the most recent sensor and state data of that device. For example, looking at the Arm class we see: class Arm ( Device ): def __init__ ( self ): ... self . status = { 'pos' : 0.0 , 'vel' : 0.0 , 'force' : 0.0 , \\ 'motor' : self . motor . status , 'timestamp_pc' : 0 } The Status dictionaries are automatically updated by a background thread of the Robot class at around 25Hz. The Status data can be accessed via the Robot class as below: if robot . arm . status [ 'pos' ] > 0.25 : print ( 'Arm extension greater than 0.25m' ) If an instantaneous snapshot of the entire Robot Status is needed, the get_status() method can be used instead: status = robot . get_status () The Robot Command In contrast to the Robot Status which pulls data from the Devices, the Robot Command pushes data to the Devices. Consider the following example which extends and then retracts the arm by 0.1 meters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . move_by ( 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . arm . move_by ( - 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . stop () A few important things are going on: 7 robot . arm . move_by ( 0.1 ) The move_by() method queues up the command to the stepper motor controller. However, the command does not yet execute. 8 robot . push_command () The push_command() causes all queued-up commands to be executed at once. This allows for the synchronization of motion across joints. For example, the following code will cause the base, arm, and lift to initiate motion simultaneously: 1 2 3 4 5 6 7 8 9 10 11 12 13 import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . move_by ( 0.1 ) robot . lift . move_by ( 0.1 ) robot . base . translate_by ( 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . stop () Note In this example we call sleep() to allow time for the motion to complete before initiating a new motion. Note The Dynamixel servos do not use the Hello Robot communication protocol. As such, the head, wrist, and gripper will move immediately upon issuing a motion command. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Introduction"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/#tutorial-introduction-to-stretch-body","text":"The Stretch_Body package provides a low-level Python API to the Stretch hardware. The Stretch_Body package is intended for advanced users who prefer to not use ROS to control the robot. It assumes a moderate level of experience programming robot sensors and actuators. The package is available on Git and installable via Pip . It encapsulates the: Mobile base Arm Lift Head actuators End-of-arm-actuators Wrist board with accelerometer (Wacc) Base power and IMU board (Pimu) As shown below, the primary programming interface to Stretch Body is the Robot class . This class encapsulates the various hardware module classes (e.g. Lift, Arm, etc). Each of these modules then communicates with the robot firmware over USB using various utility classes. Stretch also includes 3rd party hardware devices that are not accessible through Stretch_Body. However, it is possible to directly access this hardware through open-source Python packages: Laser range finder: rplidar Respeaker: respeaker_python_library D435i: pyrealsense2","title":"Tutorial: Introduction to Stretch Body"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/#robot-interface","text":"The primary developer interface to Stretch_Body is the Robot class . Let's write some code to explore the interface. Launch an interactive Python terminal: ipython Then type in the following: 1 2 3 4 5 6 7 8 9 10 11 import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () for i in range ( 10 ): robot . pretty_print () time . sleep ( 0.25 ) robot . stop () As you can see, this prints all robot sensors and state data to the console every 250ms. Looking at this in detail: 4 5 6 import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () Here we instantiated an instance of our robot through the Robot class. The call to startup() opens the serial ports to the various devices, loads the robot YAML parameters, and launches a few helper threads. 7 8 9 for i in range ( 10 ): robot . pretty_print () time . sleep ( 0.25 ) The call to pretty_print() prints to console all of the robot's sensor and state data. The sensor data is automatically updated in the background by a helper thread. 11 robot . stop () Finally, the stop() method shuts down the threads and cleanly closes the open serial ports.","title":"Robot Interface"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/#units","text":"The Robot API uses SI units: meters radians seconds Newtons Amps Volts Parameters may be named with a suffix to help describe the unit type. For example: pos_m : meters pos_r: radians","title":"Units"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/#the-robot-status","text":"The Robot derives from the Device class . It also encapsulates several other Devices: robot.head robot.arm robot.lift robot.base robot.wacc robot.pimu robot.end_of_arm All devices contain a Status dictionary. The Status contains the most recent sensor and state data of that device. For example, looking at the Arm class we see: class Arm ( Device ): def __init__ ( self ): ... self . status = { 'pos' : 0.0 , 'vel' : 0.0 , 'force' : 0.0 , \\ 'motor' : self . motor . status , 'timestamp_pc' : 0 } The Status dictionaries are automatically updated by a background thread of the Robot class at around 25Hz. The Status data can be accessed via the Robot class as below: if robot . arm . status [ 'pos' ] > 0.25 : print ( 'Arm extension greater than 0.25m' ) If an instantaneous snapshot of the entire Robot Status is needed, the get_status() method can be used instead: status = robot . get_status ()","title":"The Robot Status"},{"location":"stretch-tutorials/stretch_body/tutorial_introduction/#the-robot-command","text":"In contrast to the Robot Status which pulls data from the Devices, the Robot Command pushes data to the Devices. Consider the following example which extends and then retracts the arm by 0.1 meters: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . move_by ( 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . arm . move_by ( - 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . stop () A few important things are going on: 7 robot . arm . move_by ( 0.1 ) The move_by() method queues up the command to the stepper motor controller. However, the command does not yet execute. 8 robot . push_command () The push_command() causes all queued-up commands to be executed at once. This allows for the synchronization of motion across joints. For example, the following code will cause the base, arm, and lift to initiate motion simultaneously: 1 2 3 4 5 6 7 8 9 10 11 12 13 import time import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . move_by ( 0.1 ) robot . lift . move_by ( 0.1 ) robot . base . translate_by ( 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . stop () Note In this example we call sleep() to allow time for the motion to complete before initiating a new motion. Note The Dynamixel servos do not use the Hello Robot communication protocol. As such, the head, wrist, and gripper will move immediately upon issuing a motion command. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"The Robot Command"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/","text":"Tutorial: Parameter Management In this tutorial, we will discuss how parameters are managed in Stretch Body and show examples of how to customize your robot by overriding parameters. Overview Stretch Body shares a global set of parameters across all of the hardware it manages. All members of the Device class have an instance of RobotParams . This class constructs a dictionary of the device parameters as well as the global parameters for each device. For example, from iPython try: import stretch_body.arm a = stretch_body . arm . Arm () a . params Out [ 7 ]: { 'chain_pitch' : 0.0167 , 'chain_sprocket_teeth' : 10 , 'gr_spur' : 3.875 , 'i_feedforward' : 0 , 'calibration_range_bounds' : [ 0.515 , 0.525 ], 'contact_model' : 'effort_pct' , 'contact_model_homing' : 'effort_pct' , 'contact_models' : { 'effort_pct' : { 'contact_thresh_calibration_margin' : 10.0 , 'contact_thresh_max' : [ - 90.0 , 90.0 ], 'contact_thresh_default' : [ - 45.0 , 45.0 ], 'contact_thresh_homing' : [ - 45.0 , 45.0 ]}}, 'motion' : { 'default' : { 'accel_m' : 0.14 , 'vel_m' : 0.14 }, 'fast' : { 'accel_m' : 0.3 , 'vel_m' : 0.3 }, 'max' : { 'accel_m' : 0.4 , 'vel_m' : 0.4 }, 'slow' : { 'accel_m' : 0.05 , 'vel_m' : 0.05 }, 'trajectory_max' : { 'vel_m' : 0.4 , 'accel_m' : 0.4 }}, 'range_m' : [ 0.0 , 0.52 ]} or to access another device params: a . robot_params [ 'lift' ] Out [ 9 ]: { 'calibration_range_bounds' : [ 1.094 , 1.106 ], 'contact_model' : 'effort_pct' , 'contact_model_homing' : 'effort_pct' , 'contact_models' : { 'effort_pct' : { 'contact_thresh_calibration_margin' : 10.0 , 'contact_thresh_max' : [ - 100 , 100 ], 'contact_thresh_default' : [ - 69.0 , 69.0 ], 'contact_thresh_homing' : [ - 69.0 , 69.0 ]}}, 'belt_pitch_m' : 0.005 , 'motion' : { 'default' : { 'accel_m' : 0.2 , 'vel_m' : 0.11 }, 'fast' : { 'accel_m' : 0.25 , 'vel_m' : 0.13 }, 'max' : { 'accel_m' : 0.3 , 'vel_m' : 0.15 }, 'slow' : { 'accel_m' : 0.05 , 'vel_m' : 0.05 }, 'trajectory_max' : { 'accel_m' : 0.3 , 'vel_m' : 0.15 }}, 'pinion_t' : 12 , 'i_feedforward' : 1.2 , 'range_m' : [ 0.0 , 1.1 ]} Parameter Organization Stretch Body utilizes a prioritized parameter organization such that default settings can be easily overridden Priority Name Location Description 1 user_params $HELLO_FLEET_PATH/$HELLO_FLEET_ID/ stretch_user_params.yaml Yaml file for users to override default settings and to define custom configurations. 2 configuration_params $HELLO_FLEET_PATH/$HELLO_FLEET_ID/ stretch_configuration_params.yaml Robot specific data (eg, serial numbers and calibrations). Calibration tools may update these. 3 external_params Imported via a list defined as params in stretch_user_params.yaml External Python parameter dictionaries for 3rd party devices and peripherals. 4 nominal_params stretch_body.robot_params_RE2V0.py Generic systems settings (common across all robots of a given model. 5 nominal_system_params stretch_body.robot_params.py Generic systems settings (common across all robot models). This allows the user to override any of the parameters by defining it in their stretch_user_params.yaml . It also allows Hello Robot to periodically update parameters defined in the Python files via Pip updates. The tool stretch_params.py will print out all of the robot parameters as well as their origin. For example: stretch_params.py ############################################################ Parameters for stretch-re2-xxxx Origin Parameter Value --------------------------------------------------------------------------------------------------------------------------------- ... stretch_body.robot_params.nominal_params param.arm.chain_pitch 0 .0167 stretch_body.robot_params.nominal_params param.arm.chain_sprocket_teeth 10 ... stretch_configuration_params.yaml param.arm.contact_models.effort_pct.contact_thresh_default [ -45.0, 45 .0 ] Manually Querying and Modifying Parameters A quick way to query parameters is with the stretch_params.py tool. For example, to look at parameters relating to the arm motion: stretch_params.py | grep arm | grep motion stretch_body.robot_params.nominal_params param.arm.motion.default.accel_m 0 .14 stretch_body.robot_params.nominal_params param.arm.motion.default.vel_m 0 .14 stretch_body.robot_params.nominal_params param.arm.motion.fast.accel_m 0 .3 stretch_body.robot_params.nominal_params param.arm.motion.fast.vel_m 0 .3 stretch_body.robot_params.nominal_params param.arm.motion.max.accel_m 0 .4 stretch_body.robot_params.nominal_params param.arm.motion.max.vel_m 0 .4 stretch_body.robot_params.nominal_params param.arm.motion.slow.accel_m 0 .05 stretch_body.robot_params.nominal_params param.arm.motion.slow.vel_m 0 .05 ... The tool displays each parameter's value as well as which parameter file it was loaded from. For example, if you want to override the default motion settings for the arm, you could add the following to your stretch_user_params.yaml : arm : motion : default : vel_m : 0.1 accel_m : 0.1 Run the tool again and we see: stretch_params.py | grep arm | grep motion | grep default stretch_body.robot_params.nominal_params param.arm.motion.default.accel_m 0 .1 stretch_body.robot_params.nominal_params param.arm.motion.default.vel_m 0 .1 Note The factory parameter settings should suffice for most use cases. Programmatically Modifying and Storing Parameters A user can compute the value of a parameter programmatically and modify the robot settings accordingly. For example, in the Stretch Factory tool REx_base_calibrate_wheel_seperation.py we see that the parameter wheel_seperation_m is recomputed as the variable d_avg . This new value could be used during the robot execution by simply: robot . base . params [ 'wheel_seperation_m' ] = d_vag or it could be saved as a user override: robot . write_user_param_to_YAML ( 'base.wheel_separation_m' , d_avg ) This will update the file stretch_user_params.yaml . All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Parameter Management"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/#tutorial-parameter-management","text":"In this tutorial, we will discuss how parameters are managed in Stretch Body and show examples of how to customize your robot by overriding parameters.","title":"Tutorial: Parameter Management"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/#overview","text":"Stretch Body shares a global set of parameters across all of the hardware it manages. All members of the Device class have an instance of RobotParams . This class constructs a dictionary of the device parameters as well as the global parameters for each device. For example, from iPython try: import stretch_body.arm a = stretch_body . arm . Arm () a . params Out [ 7 ]: { 'chain_pitch' : 0.0167 , 'chain_sprocket_teeth' : 10 , 'gr_spur' : 3.875 , 'i_feedforward' : 0 , 'calibration_range_bounds' : [ 0.515 , 0.525 ], 'contact_model' : 'effort_pct' , 'contact_model_homing' : 'effort_pct' , 'contact_models' : { 'effort_pct' : { 'contact_thresh_calibration_margin' : 10.0 , 'contact_thresh_max' : [ - 90.0 , 90.0 ], 'contact_thresh_default' : [ - 45.0 , 45.0 ], 'contact_thresh_homing' : [ - 45.0 , 45.0 ]}}, 'motion' : { 'default' : { 'accel_m' : 0.14 , 'vel_m' : 0.14 }, 'fast' : { 'accel_m' : 0.3 , 'vel_m' : 0.3 }, 'max' : { 'accel_m' : 0.4 , 'vel_m' : 0.4 }, 'slow' : { 'accel_m' : 0.05 , 'vel_m' : 0.05 }, 'trajectory_max' : { 'vel_m' : 0.4 , 'accel_m' : 0.4 }}, 'range_m' : [ 0.0 , 0.52 ]} or to access another device params: a . robot_params [ 'lift' ] Out [ 9 ]: { 'calibration_range_bounds' : [ 1.094 , 1.106 ], 'contact_model' : 'effort_pct' , 'contact_model_homing' : 'effort_pct' , 'contact_models' : { 'effort_pct' : { 'contact_thresh_calibration_margin' : 10.0 , 'contact_thresh_max' : [ - 100 , 100 ], 'contact_thresh_default' : [ - 69.0 , 69.0 ], 'contact_thresh_homing' : [ - 69.0 , 69.0 ]}}, 'belt_pitch_m' : 0.005 , 'motion' : { 'default' : { 'accel_m' : 0.2 , 'vel_m' : 0.11 }, 'fast' : { 'accel_m' : 0.25 , 'vel_m' : 0.13 }, 'max' : { 'accel_m' : 0.3 , 'vel_m' : 0.15 }, 'slow' : { 'accel_m' : 0.05 , 'vel_m' : 0.05 }, 'trajectory_max' : { 'accel_m' : 0.3 , 'vel_m' : 0.15 }}, 'pinion_t' : 12 , 'i_feedforward' : 1.2 , 'range_m' : [ 0.0 , 1.1 ]}","title":"Overview"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/#parameter-organization","text":"Stretch Body utilizes a prioritized parameter organization such that default settings can be easily overridden Priority Name Location Description 1 user_params $HELLO_FLEET_PATH/$HELLO_FLEET_ID/ stretch_user_params.yaml Yaml file for users to override default settings and to define custom configurations. 2 configuration_params $HELLO_FLEET_PATH/$HELLO_FLEET_ID/ stretch_configuration_params.yaml Robot specific data (eg, serial numbers and calibrations). Calibration tools may update these. 3 external_params Imported via a list defined as params in stretch_user_params.yaml External Python parameter dictionaries for 3rd party devices and peripherals. 4 nominal_params stretch_body.robot_params_RE2V0.py Generic systems settings (common across all robots of a given model. 5 nominal_system_params stretch_body.robot_params.py Generic systems settings (common across all robot models). This allows the user to override any of the parameters by defining it in their stretch_user_params.yaml . It also allows Hello Robot to periodically update parameters defined in the Python files via Pip updates. The tool stretch_params.py will print out all of the robot parameters as well as their origin. For example: stretch_params.py ############################################################ Parameters for stretch-re2-xxxx Origin Parameter Value --------------------------------------------------------------------------------------------------------------------------------- ... stretch_body.robot_params.nominal_params param.arm.chain_pitch 0 .0167 stretch_body.robot_params.nominal_params param.arm.chain_sprocket_teeth 10 ... stretch_configuration_params.yaml param.arm.contact_models.effort_pct.contact_thresh_default [ -45.0, 45 .0 ]","title":"Parameter Organization"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/#manually-querying-and-modifying-parameters","text":"A quick way to query parameters is with the stretch_params.py tool. For example, to look at parameters relating to the arm motion: stretch_params.py | grep arm | grep motion stretch_body.robot_params.nominal_params param.arm.motion.default.accel_m 0 .14 stretch_body.robot_params.nominal_params param.arm.motion.default.vel_m 0 .14 stretch_body.robot_params.nominal_params param.arm.motion.fast.accel_m 0 .3 stretch_body.robot_params.nominal_params param.arm.motion.fast.vel_m 0 .3 stretch_body.robot_params.nominal_params param.arm.motion.max.accel_m 0 .4 stretch_body.robot_params.nominal_params param.arm.motion.max.vel_m 0 .4 stretch_body.robot_params.nominal_params param.arm.motion.slow.accel_m 0 .05 stretch_body.robot_params.nominal_params param.arm.motion.slow.vel_m 0 .05 ... The tool displays each parameter's value as well as which parameter file it was loaded from. For example, if you want to override the default motion settings for the arm, you could add the following to your stretch_user_params.yaml : arm : motion : default : vel_m : 0.1 accel_m : 0.1 Run the tool again and we see: stretch_params.py | grep arm | grep motion | grep default stretch_body.robot_params.nominal_params param.arm.motion.default.accel_m 0 .1 stretch_body.robot_params.nominal_params param.arm.motion.default.vel_m 0 .1 Note The factory parameter settings should suffice for most use cases.","title":"Manually Querying and Modifying Parameters"},{"location":"stretch-tutorials/stretch_body/tutorial_parameter_management/#programmatically-modifying-and-storing-parameters","text":"A user can compute the value of a parameter programmatically and modify the robot settings accordingly. For example, in the Stretch Factory tool REx_base_calibrate_wheel_seperation.py we see that the parameter wheel_seperation_m is recomputed as the variable d_avg . This new value could be used during the robot execution by simply: robot . base . params [ 'wheel_seperation_m' ] = d_vag or it could be saved as a user override: robot . write_user_param_to_YAML ( 'base.wheel_separation_m' , d_avg ) This will update the file stretch_user_params.yaml . All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Programmatically Modifying and Storing Parameters"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/","text":"Tutorial: Robot Motion As we've seen in previous tutorials, commanding robot motion is simple and straightforward. For example, the incremental motion of the arm can be commanded by: 1 2 3 4 5 6 7 8 9 import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . move_by ( 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . stop () The absolute motion can be commanded by: 1 2 3 4 5 6 7 8 9 import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . move_to ( 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . stop () Waiting on Motion In the above examples, we executed a time.sleep() after robot.push_command() . This allows the joint time to complete its motion. As an alternative, we can use the wait_until_at_setpoint() method that polls the joint position versus the target position. We can also interrupt a motion by sending a new motion command at any time. For example, try the following script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move to full retraction robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint () #Move to full extension robot . arm . move_to ( 0.5 ) robot . push_command () #Interrupt motion midway and retract again time . sleep ( 2.0 ) robot . arm . move_to ( 0.0 ) robot . arm . wait_until_at_setpoint () robot . stop () You will see the arm fully retract, begin to extend, and then fully retract again. Motion Profiles All joints support trapezoidal motion profile generation. Other types of controllers are available (splined trajectory, PID, velocity, etc) but they are not covered here. The trapezoidal motion controllers require three values: x: target position of joint v: maximum velocity of motion a: acceleration of motion We provide 'defaults' for the velocity and acceleration settings, as well as 'fast', and 'slow' settings. These values have been tuned to be appropriate for the safe movement of the robot. These values can be queried using the stretch_params.py tool: stretch_params.py | grep arm | grep motion | grep default stretch_body.robot_params.nominal_params param.arm.motion.fast.accel_m 0 .14 stretch_body.robot_params.nominal_params param.arm.motion.fast.vel_m 0 .14 We see that the arm motion in 'default' mode will move with a velocity of 0.14 m/s and an acceleration of 0.14 m/s^2. The move_by and move_to commands support optional motion profile parameters. For example, for a fast motion: v = robot . arm . params [ 'motion' ][ 'fast' ][ 'vel_m' ] a = robot . arm . params [ 'motion' ][ 'fast' ][ 'accel_m' ] robot . arm . move_by ( x_m = 0.1 , v_m = v , a_m = a ) robot . push_command () The motion will use the 'default' motion profile settings if no values are specified. Motion Limits All joints obey motion limits which are specified in the robot parameters. stretch_params.py | grep arm | grep range_m stretch_user_params.yaml param.arm.range_m [ 0 .0, 0 .515 ] These are the mechanical limits of the joints and have been set at the factory to prevent damage to the hardware. It is not recommended to set them to be greater than the factory-specified values. However, they can be further limited if desired by setting soft motion limits: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . set_soft_motion_limit_min ( 0.2 ) robot . arm . set_soft_motion_limit_max ( 0.4 ) #Will move to position 0.2 robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint () #Will move to position 0.4 robot . arm . move_to ( 0.5 ) robot . push_command () robot . arm . wait_until_at_setpoint () robot . stop () Controlling Dynamixel Motion The above examples have focused on the motion of the arm. Like the lift and the base, the arm utilizes Hello Robot's custom stepper motor controller. Control of the Dynamixels of the head and the end-of-arm is very similar to that of the arm, though not identical. As we see here, the robot.push_command() call is not required as the motion begins instantaneously and is not queued. In addition, the Dynamixel servos are managed as a chain of devices, so we must pass in the joint name along with the command. import stretch_body.robot from stretch_body.hello_utils import deg_to_rad robot = stretch_body . robot . Robot () robot . startup () robot . head . move_to ( 'head_pan' , 0 ) robot . head . move_to ( 'head_tilt' , 0 ) time . sleep ( 3.0 ) robot . head . move_to ( 'head_pan' , deg_to_rad ( 90.0 )) robot . head . move_to ( 'head_tilt' , deg_to_rad ( 45.0 )) time . sleep ( 3.0 ) robot . stop () Similar to the stepper joints, the Dynamixel joints accept motion profile and motion limit commands. For example, here we restrict the head pan range of motion while executing both a fast and slow move: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Limit range of motion of head_pan robot . head . get_motor ( 'head_pan' ) . set_soft_motion_limit_min ( deg_to_rad ( - 45.0 )) robot . head . get_motor ( 'head_pan' ) . set_soft_motion_limit_max ( deg_to_rad ( 45.0 )) #Do a fast motion v = robot . params [ 'head_pan' ][ 'motion' ][ 'fast' ][ 'vel' ] a = robot . params [ 'head_pan' ][ 'motion' ][ 'fast' ][ 'accel' ] robot . head . move_to ( 'head_pan' , deg_to_rad ( - 90.0 ), v_r = v , a_r = a ) time . sleep ( 3.0 ) #Do a slow motion v = robot . params [ 'head_pan' ][ 'motion' ][ 'slow' ][ 'vel' ] a = robot . params [ 'head_pan' ][ 'motion' ][ 'slow' ][ 'accel' ] robot . head . move_to ( 'head_pan' , deg_to_rad ( 90.0 ), v_r = v , a_r = a ) time . sleep ( 3.0 ) robot . stop () Base Velocity Control The Base also supports a velocity control mode which can be useful with navigation planners. The Base controllers will automatically switch between velocity and position control. For example: robot . base . translate_by ( x_m = 0.5 ) robot . push_command () time . sleep ( 4.0 ) #wait robot . base . set_rotational_velocity ( v_r = 0.1 ) #switch to velocity controller robot . push_command () time . sleep ( 4.0 ) #wait robot . base . set_rotational_velocity ( v_r = 0.0 ) #stop motion robot . push_command () Warning As shown, care should be taken to set commanded velocities to zero on exit to avoid runaway. Advanced Topics Stepper Control Modes Most users will control robot motion using the move_to and move_by commands as described above. However, there are numerous other low-level controller modes available. While this is a topic for advanced users, it is worth noting that each joint has a default safety mode and a default position control mode. These are: Joint Default Safety Mode Default Position Control Mode left_wheel Freewheel Trapezoidal position control right_wheel Freewheel Trapezoidal position control lift Gravity compensated 'float' Trapezoidal position control arm Freewheel Trapezoidal position control head_pan Torque disabled Trapezoidal position control head_tilt Torque disabled Trapezoidal position control wrist_yaw Torque disabled Trapezoidal position control stretch_gripper Torque disabled Trapezoidal position control Each joint remains in its Safety Mode when no program is running. When the <device>.startup() function is called, the joint controller transitions from Safety Mode to its Default Position Control Mode . It is then placed back in Safety Mode when <device>.stop() is called. Motion Runstop Runstop activation will cause the Base, Arm, and Lift to switch to Safety Mode and subsequent motion commands will be ignored. The motion commands will resume smoothly when the Runstop is deactivated. This is usually done via the Runstop button. However, it can also be done via the Pimu interface. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move to full retraction robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint () #Move to full extension robot . arm . move_to ( 0.5 ) robot . push_command () #Runstop motion midway through the motion input ( 'Hit enter to runstop motion' ) robot . pimu . runstop_event_trigger () robot . push_command () input ( 'Hit enter to restart motion' ) robot . pimu . runstop_event_reset () robot . push_command () robot . arm . move_to ( 0.5 ) robot . push_command () robot . arm . wait_until_at_setpoint () robot . stop () Guarded Motion The Arm, Lift, and Base support a guarded motion function. It will automatically transition the actuator from Control mode to Safety mode when the exerted motor torque exceeds a threshold. This functionality is most useful for the Lift and the Arm. It allows these joints to safely stop upon contact. It can be used to: Safely stop when contacting an actuator hard stop Safely stop when making unexpected contact with the environment or a person Make a guarded motion where the robot reaches a surface and then stops For more information on guarded motion, see the Contact Models Tutorial Synchronized Motion The Arm, Lift, and Base actuators have a hardware synchronization mechanism. This allows for stepper controller commands to be time synchronized across joints. This behavior can be disabled via the user YAML. By default the settings are: >>$ stretch_params.py | grep enable_sync_mode stretch_body.robot_params.nominal_params param.hello-motor-arm.gains.enable_sync_mode 1 stretch_body.robot_params.nominal_params param.hello-motor-left-wheel.gains.enable_sync_mode 1 stretch_body.robot_params.nominal_params param.hello-motor-lift.gains.enable_sync_mode 1 stretch_body.robot_params.nominal_params param.hello-motor-right-wheel.gains.enable_sync_mode 1 Motion Status It can be useful to poll the status of a joint during a motion to modify the robot's behavior, etc. The useful status values include: robot . arm . status [ 'pos' ] #Joint position robot . arm . status [ 'vel' ] #Joint velocity robot . arm . motor . status [ 'effort_pct' ] #Joint effort (-100 to 100) (derived from motor current) robot . arm . motor . status [ 'near_pos_setpoint' ] #Is sensed position near commanded position robot . arm . motor . status [ 'near_vel_setpoint' ] #Is sensed velocity near commanded velocity robot . arm . motor . status [ 'is_moving' ] #Is the joint in motion robot . arm . motor . status [ 'in_guarded_event' ] #Has a guarded event occured robot . arm . motor . status [ 'in_safety_event' ] #Has a safety event occured Update Rates The following update rates apply to Stretch: Item Rate Notes Status data for Arm, Lift, Base, Wacc, and Pimu 25Hz Polled automatically by Robot thread Status data for End of Arm and Head servos 15Hz Polled automatically by Robot thread Command data for Arm, Lift, Base, Wacc, Pimu N/A Commands are queued and executed upon calling robot.push_command( ) Command data for End of Arm and Head servos N/A Commands execute immediately Note Motion commands are non-blocking and it is the responsibility of the user code to poll the Robot Status to determine when and if a motion target has been achieved. Info The Stretch Body interface is not designed to support high bandwidth control applications. The natural dynamics of the robot actuators do not support high bandwidth control, and the USB-based interface limits high-rate communication. Tip In practice, a Python-based control loop that calls push_command() at 1Hz to 10Hz is sufficiently matched to the robot's natural dynamics. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Robot Motion"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#tutorial-robot-motion","text":"As we've seen in previous tutorials, commanding robot motion is simple and straightforward. For example, the incremental motion of the arm can be commanded by: 1 2 3 4 5 6 7 8 9 import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . move_by ( 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . stop () The absolute motion can be commanded by: 1 2 3 4 5 6 7 8 9 import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . move_to ( 0.1 ) robot . push_command () time . sleep ( 2.0 ) robot . stop ()","title":"Tutorial: Robot Motion"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#waiting-on-motion","text":"In the above examples, we executed a time.sleep() after robot.push_command() . This allows the joint time to complete its motion. As an alternative, we can use the wait_until_at_setpoint() method that polls the joint position versus the target position. We can also interrupt a motion by sending a new motion command at any time. For example, try the following script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move to full retraction robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint () #Move to full extension robot . arm . move_to ( 0.5 ) robot . push_command () #Interrupt motion midway and retract again time . sleep ( 2.0 ) robot . arm . move_to ( 0.0 ) robot . arm . wait_until_at_setpoint () robot . stop () You will see the arm fully retract, begin to extend, and then fully retract again.","title":"Waiting on Motion"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#motion-profiles","text":"All joints support trapezoidal motion profile generation. Other types of controllers are available (splined trajectory, PID, velocity, etc) but they are not covered here. The trapezoidal motion controllers require three values: x: target position of joint v: maximum velocity of motion a: acceleration of motion We provide 'defaults' for the velocity and acceleration settings, as well as 'fast', and 'slow' settings. These values have been tuned to be appropriate for the safe movement of the robot. These values can be queried using the stretch_params.py tool: stretch_params.py | grep arm | grep motion | grep default stretch_body.robot_params.nominal_params param.arm.motion.fast.accel_m 0 .14 stretch_body.robot_params.nominal_params param.arm.motion.fast.vel_m 0 .14 We see that the arm motion in 'default' mode will move with a velocity of 0.14 m/s and an acceleration of 0.14 m/s^2. The move_by and move_to commands support optional motion profile parameters. For example, for a fast motion: v = robot . arm . params [ 'motion' ][ 'fast' ][ 'vel_m' ] a = robot . arm . params [ 'motion' ][ 'fast' ][ 'accel_m' ] robot . arm . move_by ( x_m = 0.1 , v_m = v , a_m = a ) robot . push_command () The motion will use the 'default' motion profile settings if no values are specified.","title":"Motion Profiles"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#motion-limits","text":"All joints obey motion limits which are specified in the robot parameters. stretch_params.py | grep arm | grep range_m stretch_user_params.yaml param.arm.range_m [ 0 .0, 0 .515 ] These are the mechanical limits of the joints and have been set at the factory to prevent damage to the hardware. It is not recommended to set them to be greater than the factory-specified values. However, they can be further limited if desired by setting soft motion limits: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () robot . arm . set_soft_motion_limit_min ( 0.2 ) robot . arm . set_soft_motion_limit_max ( 0.4 ) #Will move to position 0.2 robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint () #Will move to position 0.4 robot . arm . move_to ( 0.5 ) robot . push_command () robot . arm . wait_until_at_setpoint () robot . stop ()","title":"Motion Limits"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#controlling-dynamixel-motion","text":"The above examples have focused on the motion of the arm. Like the lift and the base, the arm utilizes Hello Robot's custom stepper motor controller. Control of the Dynamixels of the head and the end-of-arm is very similar to that of the arm, though not identical. As we see here, the robot.push_command() call is not required as the motion begins instantaneously and is not queued. In addition, the Dynamixel servos are managed as a chain of devices, so we must pass in the joint name along with the command. import stretch_body.robot from stretch_body.hello_utils import deg_to_rad robot = stretch_body . robot . Robot () robot . startup () robot . head . move_to ( 'head_pan' , 0 ) robot . head . move_to ( 'head_tilt' , 0 ) time . sleep ( 3.0 ) robot . head . move_to ( 'head_pan' , deg_to_rad ( 90.0 )) robot . head . move_to ( 'head_tilt' , deg_to_rad ( 45.0 )) time . sleep ( 3.0 ) robot . stop () Similar to the stepper joints, the Dynamixel joints accept motion profile and motion limit commands. For example, here we restrict the head pan range of motion while executing both a fast and slow move: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Limit range of motion of head_pan robot . head . get_motor ( 'head_pan' ) . set_soft_motion_limit_min ( deg_to_rad ( - 45.0 )) robot . head . get_motor ( 'head_pan' ) . set_soft_motion_limit_max ( deg_to_rad ( 45.0 )) #Do a fast motion v = robot . params [ 'head_pan' ][ 'motion' ][ 'fast' ][ 'vel' ] a = robot . params [ 'head_pan' ][ 'motion' ][ 'fast' ][ 'accel' ] robot . head . move_to ( 'head_pan' , deg_to_rad ( - 90.0 ), v_r = v , a_r = a ) time . sleep ( 3.0 ) #Do a slow motion v = robot . params [ 'head_pan' ][ 'motion' ][ 'slow' ][ 'vel' ] a = robot . params [ 'head_pan' ][ 'motion' ][ 'slow' ][ 'accel' ] robot . head . move_to ( 'head_pan' , deg_to_rad ( 90.0 ), v_r = v , a_r = a ) time . sleep ( 3.0 ) robot . stop ()","title":"Controlling Dynamixel Motion"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#base-velocity-control","text":"The Base also supports a velocity control mode which can be useful with navigation planners. The Base controllers will automatically switch between velocity and position control. For example: robot . base . translate_by ( x_m = 0.5 ) robot . push_command () time . sleep ( 4.0 ) #wait robot . base . set_rotational_velocity ( v_r = 0.1 ) #switch to velocity controller robot . push_command () time . sleep ( 4.0 ) #wait robot . base . set_rotational_velocity ( v_r = 0.0 ) #stop motion robot . push_command () Warning As shown, care should be taken to set commanded velocities to zero on exit to avoid runaway.","title":"Base Velocity Control"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#stepper-control-modes","text":"Most users will control robot motion using the move_to and move_by commands as described above. However, there are numerous other low-level controller modes available. While this is a topic for advanced users, it is worth noting that each joint has a default safety mode and a default position control mode. These are: Joint Default Safety Mode Default Position Control Mode left_wheel Freewheel Trapezoidal position control right_wheel Freewheel Trapezoidal position control lift Gravity compensated 'float' Trapezoidal position control arm Freewheel Trapezoidal position control head_pan Torque disabled Trapezoidal position control head_tilt Torque disabled Trapezoidal position control wrist_yaw Torque disabled Trapezoidal position control stretch_gripper Torque disabled Trapezoidal position control Each joint remains in its Safety Mode when no program is running. When the <device>.startup() function is called, the joint controller transitions from Safety Mode to its Default Position Control Mode . It is then placed back in Safety Mode when <device>.stop() is called.","title":"Stepper Control Modes"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#motion-runstop","text":"Runstop activation will cause the Base, Arm, and Lift to switch to Safety Mode and subsequent motion commands will be ignored. The motion commands will resume smoothly when the Runstop is deactivated. This is usually done via the Runstop button. However, it can also be done via the Pimu interface. For example: import stretch_body.robot robot = stretch_body . robot . Robot () robot . startup () #Move to full retraction robot . arm . move_to ( 0.0 ) robot . push_command () robot . arm . wait_until_at_setpoint () #Move to full extension robot . arm . move_to ( 0.5 ) robot . push_command () #Runstop motion midway through the motion input ( 'Hit enter to runstop motion' ) robot . pimu . runstop_event_trigger () robot . push_command () input ( 'Hit enter to restart motion' ) robot . pimu . runstop_event_reset () robot . push_command () robot . arm . move_to ( 0.5 ) robot . push_command () robot . arm . wait_until_at_setpoint () robot . stop ()","title":"Motion Runstop"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#guarded-motion","text":"The Arm, Lift, and Base support a guarded motion function. It will automatically transition the actuator from Control mode to Safety mode when the exerted motor torque exceeds a threshold. This functionality is most useful for the Lift and the Arm. It allows these joints to safely stop upon contact. It can be used to: Safely stop when contacting an actuator hard stop Safely stop when making unexpected contact with the environment or a person Make a guarded motion where the robot reaches a surface and then stops For more information on guarded motion, see the Contact Models Tutorial","title":"Guarded Motion"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#synchronized-motion","text":"The Arm, Lift, and Base actuators have a hardware synchronization mechanism. This allows for stepper controller commands to be time synchronized across joints. This behavior can be disabled via the user YAML. By default the settings are: >>$ stretch_params.py | grep enable_sync_mode stretch_body.robot_params.nominal_params param.hello-motor-arm.gains.enable_sync_mode 1 stretch_body.robot_params.nominal_params param.hello-motor-left-wheel.gains.enable_sync_mode 1 stretch_body.robot_params.nominal_params param.hello-motor-lift.gains.enable_sync_mode 1 stretch_body.robot_params.nominal_params param.hello-motor-right-wheel.gains.enable_sync_mode 1","title":"Synchronized Motion"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#motion-status","text":"It can be useful to poll the status of a joint during a motion to modify the robot's behavior, etc. The useful status values include: robot . arm . status [ 'pos' ] #Joint position robot . arm . status [ 'vel' ] #Joint velocity robot . arm . motor . status [ 'effort_pct' ] #Joint effort (-100 to 100) (derived from motor current) robot . arm . motor . status [ 'near_pos_setpoint' ] #Is sensed position near commanded position robot . arm . motor . status [ 'near_vel_setpoint' ] #Is sensed velocity near commanded velocity robot . arm . motor . status [ 'is_moving' ] #Is the joint in motion robot . arm . motor . status [ 'in_guarded_event' ] #Has a guarded event occured robot . arm . motor . status [ 'in_safety_event' ] #Has a safety event occured","title":"Motion Status"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_motion/#update-rates","text":"The following update rates apply to Stretch: Item Rate Notes Status data for Arm, Lift, Base, Wacc, and Pimu 25Hz Polled automatically by Robot thread Status data for End of Arm and Head servos 15Hz Polled automatically by Robot thread Command data for Arm, Lift, Base, Wacc, Pimu N/A Commands are queued and executed upon calling robot.push_command( ) Command data for End of Arm and Head servos N/A Commands execute immediately Note Motion commands are non-blocking and it is the responsibility of the user code to poll the Robot Status to determine when and if a motion target has been achieved. Info The Stretch Body interface is not designed to support high bandwidth control applications. The natural dynamics of the robot actuators do not support high bandwidth control, and the USB-based interface limits high-rate communication. Tip In practice, a Python-based control loop that calls push_command() at 1Hz to 10Hz is sufficiently matched to the robot's natural dynamics. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Update Rates"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/","text":"Tutorial: Robot Sensors Introduction Stretch Body exposes a host of sensor data through the status dictionaries of its devices. In this tutorial, we'll cover how to access, view, and configure this sensor data. Tools to View Sensor Data There are two useful tools for scoping Pimu and Wacc sensor data in real-time: stretch_pimu_scope.py --help For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- usage: stretch_pimu_scope.py [ -h ] ( --cliff | --at_cliff | --voltage | --current | --temp | --ax | --ay | --az | --mx | --my | --mz | --gx | --gy | --gz | --roll | --pitch | --heading | --bump ) Visualize Pimu ( Power+IMU ) board data with an oscilloscope optional arguments: -h, --help show this help message and exit --cliff Scope base cliff sensors --at_cliff Scope base at_cliff signal --voltage Scope bus voltage ( V ) --current Scope bus current ( A ) --temp Scope base internal temperature ( C ) --ax Scope base accelerometer AX --ay Scope base accelerometer AY --az Scope base accelerometer AZ --mx Scope base magnetometer MX --my Scope base magnetometer MY --mz Scope base magnetometer MZ --gx Scope base gyro GX --gy Scope base gyro GY --gz Scope base gyro GZ --roll Scope base imu Roll --pitch Scope base imu Pitch --heading Scope base imu Heading --bump Scope base imu bump level and, stretch_wacc_scope.py --help For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- usage: stretch_wacc_scope.py [ -h ] [ --ax ] [ --ay ] [ --az ] [ --a0 ] [ --d0 ] [ --d1 ] [ --tap ] Visualize Wacc ( Wrist+Accel ) board data with an oscilloscope optional arguments: -h, --help show this help message and exit --ax Scope accelerometer AX --ay Scope accelerometer AY --az Scope accelerometer AZ --a0 Scope analog-in-0 --d0 Scope digital-in-0 --d1 Scope digital-in-1 --tap Scope single tap Each motor also has associated sensor data available in its status dictionaries. The corresponding 'jog' tool for each joint will pretty-print the sensor data for that motor to the console. For example: stretch_arm_jog.py For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- ... ----- Arm ------ Pos ( m ) : 0 .0032848120914969895 Vel ( m/s ) : 0 .0002031017627742426 Soft motion limits ( m ) [ 0 .0, 0 .52 ] Timestamp PC ( s ) : 1661797443 .1212385 ----------- Mode MODE_SAFETY x_des ( rad ) 0 ( deg ) 0 .0 v_des ( rad ) 25 ( deg ) 1432 .3944878270581 a_des ( rad ) 15 ( deg ) 859 .4366926962349 Stiffness 1 Feedforward 0 Pos ( rad ) 0 .47890087962150574 ( deg ) 27 .438999207414973 Vel ( rad/s ) 0 .029610708355903625 ( deg ) 1 .6965686171860386 Effort ( Ticks ) 0 .0 Effort ( Pct ) 0 .0 Current ( A ) 0 .0 Error ( deg ) 0 .0 Debug 0 .0 Guarded Events: 0 Diag 00000000000000000000000100000000 Position Calibrated: False Runstop on: False Near Pos Setpoint: False Near Vel Setpoint: False Is Moving: False Is Moving Filtered: 0 At Current Limit: False Is MG Accelerating: False Is MG Moving: False Encoder Calibration in Flash: True In Guarded Event: False In Safety Event: False Waiting on Sync: False Waypoint Trajectory State: idle Setpoint: ( rad ) 0 .0 | ( deg ) 0 .0 Segment ID: 0 Timestamp ( s ) 1661797443 .110996 Read error 0 Board variant: Stepper.1 Firmware version: Stepper.v0.2.0p1 ... Accessing the Status Dictionaries Each Robot device has a status dictionary that is automatically updated with the latest sensor data. The primary dictionaries are: Stepper Status Wacc Status Pimu Status Dynamixel Status Each of these dictionaries can be accessed through the Robot instance. For example, try in iPython: import stretch_body.robot import time r = stretch_body . robot . Robot () r . startup () for i in range ( 10 ): print ( 'Arm position (m) %f ' % r . arm . status [ 'pos' ]) time . sleep ( 0.1 ) Base IMU The base has a 9-DoF IMU using the 9-DoF FXOS8700 + FXAS21002 chipset. This is the same chipset used on the Adafruit NXP IMU board . The Pimu reports back the IMU sensor readings in its IMU status dictionary . For example, from iPython try: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () r . pimu . status [ 'imu' ] Out [ 6 ]: { 'ax' : 0.30007487535476685 , 'ay' : - 0.355493426322937 , 'az' : - 9.736297607421875 , 'gx' : 0.0009544769418425858 , 'gy' : 0.00013635384675581008 , 'gz' : 0.00027270769351162016 , 'mx' : - 10.699999809265137 , 'my' : - 42.900001525878906 , 'mz' : - 51.0 , 'roll' : 0.03657745230780231 , 'pitch' : - 0.02960890640560868 , 'heading' : 1.2786458241584955 , 'timestamp' : 1661788669.3042662 , 'qw' : 0.0009681061492301524 , 'qx' : 0.59670090675354 , 'qy' : - 0.8021157383918762 , 'qz' : 0.023505505174398422 , 'bump' : - 0.9188174605369568 } r . pimu . pretty_print () ---------- IMU ------------- AX ( m / s ^ 2 ) 0.30007487535476685 AY ( m / s ^ 2 ) - 0.355493426322937 AZ ( m / s ^ 2 ) - 9.736297607421875 GX ( rad / s ) 0.0009544769418425858 GY ( rad / s ) 0.00013635384675581008 GZ ( rad / s ) 0.00027270769351162016 MX ( uTesla ) - 10.699999809265137 MY ( uTesla ) - 42.900001525878906 MZ ( uTesla ) - 51.0 QW 0.0009681061492301524 QX 0.59670090675354 QY - 0.8021157383918762 QZ 0.023505505174398422 Roll ( deg ) 2.095733642578125 Pitch ( deg ) - 1.6964653730392456 Heading ( deg ) 73.26100921630858 It reports: Acceleration (AX, AY, AZ) Gravity (GX, GY GZ) Magnetic field (MX, MY, MZ) Quaternion orientation (QW, QX, QY, QZ) Euler angle orientation (Roll, Pitch, Heading) These values are computed on the Pimu. As we can see in its firmware code , a 100Hz Madgwick filter is used to compute the orientation. Stretch Body also implements a bump detector using the IMU accelerometers. This detector simply computes the sum of squares of AX, AY, and AZ . This value is then compared to the following threshold to determine if a bump is detected: stretch_params.py | grep pimu | grep bump stretch_body.robot_params.nominal_params param.pimu.config.bump_thresh 20 .0 You can experiment with the bump detector with the following code: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () r . pimu . config [ 'bump_thresh' ] = 20.0 #Experiment with values r . pimu . set_config ( p . config ) r . push_command () for i in range ( 100 ): time . sleep ( 0.1 ) print ( 'Bump %f ' % r . pimu . status [ 'bump' ]) print ( 'Bump event count %d ' % r . pimu . status [ 'bump_event_cnt' ]) Note The IMU is calibrated by Hello Robot at the factory. Please contact Hello Robot support for details on recalibrating your IMU. Wrist Accelerometer The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The Wacc reports back AX, AY, and AZ in its status dictionary . For example, from iPython try: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () r . wacc . status Out [ 5 ]: { 'ax' : 10.093315124511719 , 'ay' : 0.10557432472705841 , 'az' : - 0.45386940240859985 , 'a0' : 155 , 'd0' : 1 , 'd1' : 1 , 'd2' : 0 , 'd3' : 0 , 'single_tap_count' : 15 , 'state' : 0 , 'debug' : 0 , 'timestamp' : 1661795676.203578 , 'transport' : { 'rate' : 0.4572487091345871 , 'read_error' : 0 , 'write_error' : 0 , 'itr' : 3 , 'transaction_time_avg' : 0 , 'transaction_time_max' : 0 , 'timestamp_pc' : 0 }} r . wacc . pretty_print () ------------------------------ Ax ( m / s ^ 2 ) 10.093315124511719 Ay ( m / s ^ 2 ) 0.10557432472705841 Az ( m / s ^ 2 ) - 0.45386940240859985 A0 155 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 15 State 0 Debug 0 Timestamp ( s ) 1661795676.203578 Board variant : Wacc .1 Firmware version : Wacc . v0 .2.0 p1 In addition to AX, AY, and AZ we also see the single_tap_count value which reports back a count of the number of single-tap contacts the accelerometer has experienced since power-up. The following Wacc parameters configure the accelerometer low-pass filter and single-tap settings. See the ADXL343 datasheet for more details. stretch_params.py | grep wacc stretch_body.robot_params.nominal_params param.wacc.config.accel_LPF 10 .0 stretch_body.robot_params.nominal_params param.wacc.config.accel_range_g 4 stretch_body.robot_params.nominal_params param.wacc.config.accel_single_tap_dur 70 stretch_body.robot_params.nominal_params param.wacc.config.accel_single_tap_thresh 50 stretch_configuration_params.yaml param.wacc.config.accel_gravity_scale 1 .0 Cliff Sensors Stretch has four Sharp GP2Y0A51SK0F IR cliff sensors pointed toward the floor. These report the distance to the floor, allowing for the detection of thresholds, stair edges, etc. Relevant parameters for the cliff sensors are: stretch_params.py | grep cliff stretch_body.robot_params.nominal_params param.pimu.config.cliff_LPF 10 .0 stretch_body.robot_params.nominal_params param.pimu.config.cliff_thresh -50 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_cliff 0 stretch_configuration_params.yaml param.pimu.config.cliff_zero [ 518 .6307312011719, 530 .9835095214844, 500 .7268048095703, 509 .92264434814456 ] stretch_body.robot_params.nominal_params param.robot_monitor.monitor_base_cliff_event 1 The sensors are calibrated such that a zero value (as defined by cliff_zero ) indicates the sensor is at the correct height from the floor surface. A negative value indicates a drop off such as a stair ledge while a positive value indicates an obstacle like a threshold or high pile carpet. You may want to recalibrate this zero based on the surface the robot is on (eg, carpet, tile, etc). To do this: REx_cliff_sensor_calibrate.py For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- Ensure cliff sensors are not obstructed and base is on a flat surface Hit enter when ready Itr 0 Val [ 518 .630126953125, 530 .4168701171875, 500 .863037109375, 510 .0032958984375 ] ... Itr 99 Val [ 518 .8374633789062, 530 .858154296875, 500 .5805969238281, 509 .9013671875 ] Got cliff zeros of: [ 518 .6307312011719, 530 .9835095214844, 500 .7268048095703, 509 .92264434814456 ] Calibration passed. Storing to YAML... The stop_at_cliff field causes the robot to execute a Runstop when the cliff sensor readings exceed the value cliff_thresh . The parameter cliff_LPF defines the low-pass-filter rate (Hz) on the analog sensor readings. Note As configured at the factory, stop_at_cliff is set to zero and Stretch does not stop its motion based on the cliff sensor readings. Hello Robot makes no guarantees as to the reliability of Stretch's ability to avoid driving over ledges and stairs when this flag is enabled. The range values from the sensors can be read from the robot.pimu.status message. The relevant fields are: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () r . pimu . status [ 'cliff_range' ] Out [ 4 ]: [ 0.39227294921875 , - 0.2047119140625 , - 0.26422119140625 , 0.006134033203125 ] r . pimu . status [ 'at_cliff' ] Out [ 5 ]: [ False , False , False , False ] r . pimu . status [ 'cliff_event' ] Out [ 5 ]: False The cliff_event flag is set when any of the four sensor readings exceed cliff_thresh and stop_at_cliff is enabled. In the event of a Cliff Event, it must be reset by robot.pimu.cliff_event_reset() to reset the generated Runstop. The cliff detection logic can be found in the Pimu firmware . All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Robot Sensors"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#tutorial-robot-sensors","text":"","title":"Tutorial: Robot Sensors"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#introduction","text":"Stretch Body exposes a host of sensor data through the status dictionaries of its devices. In this tutorial, we'll cover how to access, view, and configure this sensor data.","title":"Introduction"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#tools-to-view-sensor-data","text":"There are two useful tools for scoping Pimu and Wacc sensor data in real-time: stretch_pimu_scope.py --help For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- usage: stretch_pimu_scope.py [ -h ] ( --cliff | --at_cliff | --voltage | --current | --temp | --ax | --ay | --az | --mx | --my | --mz | --gx | --gy | --gz | --roll | --pitch | --heading | --bump ) Visualize Pimu ( Power+IMU ) board data with an oscilloscope optional arguments: -h, --help show this help message and exit --cliff Scope base cliff sensors --at_cliff Scope base at_cliff signal --voltage Scope bus voltage ( V ) --current Scope bus current ( A ) --temp Scope base internal temperature ( C ) --ax Scope base accelerometer AX --ay Scope base accelerometer AY --az Scope base accelerometer AZ --mx Scope base magnetometer MX --my Scope base magnetometer MY --mz Scope base magnetometer MZ --gx Scope base gyro GX --gy Scope base gyro GY --gz Scope base gyro GZ --roll Scope base imu Roll --pitch Scope base imu Pitch --heading Scope base imu Heading --bump Scope base imu bump level and, stretch_wacc_scope.py --help For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- usage: stretch_wacc_scope.py [ -h ] [ --ax ] [ --ay ] [ --az ] [ --a0 ] [ --d0 ] [ --d1 ] [ --tap ] Visualize Wacc ( Wrist+Accel ) board data with an oscilloscope optional arguments: -h, --help show this help message and exit --ax Scope accelerometer AX --ay Scope accelerometer AY --az Scope accelerometer AZ --a0 Scope analog-in-0 --d0 Scope digital-in-0 --d1 Scope digital-in-1 --tap Scope single tap Each motor also has associated sensor data available in its status dictionaries. The corresponding 'jog' tool for each joint will pretty-print the sensor data for that motor to the console. For example: stretch_arm_jog.py For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- ... ----- Arm ------ Pos ( m ) : 0 .0032848120914969895 Vel ( m/s ) : 0 .0002031017627742426 Soft motion limits ( m ) [ 0 .0, 0 .52 ] Timestamp PC ( s ) : 1661797443 .1212385 ----------- Mode MODE_SAFETY x_des ( rad ) 0 ( deg ) 0 .0 v_des ( rad ) 25 ( deg ) 1432 .3944878270581 a_des ( rad ) 15 ( deg ) 859 .4366926962349 Stiffness 1 Feedforward 0 Pos ( rad ) 0 .47890087962150574 ( deg ) 27 .438999207414973 Vel ( rad/s ) 0 .029610708355903625 ( deg ) 1 .6965686171860386 Effort ( Ticks ) 0 .0 Effort ( Pct ) 0 .0 Current ( A ) 0 .0 Error ( deg ) 0 .0 Debug 0 .0 Guarded Events: 0 Diag 00000000000000000000000100000000 Position Calibrated: False Runstop on: False Near Pos Setpoint: False Near Vel Setpoint: False Is Moving: False Is Moving Filtered: 0 At Current Limit: False Is MG Accelerating: False Is MG Moving: False Encoder Calibration in Flash: True In Guarded Event: False In Safety Event: False Waiting on Sync: False Waypoint Trajectory State: idle Setpoint: ( rad ) 0 .0 | ( deg ) 0 .0 Segment ID: 0 Timestamp ( s ) 1661797443 .110996 Read error 0 Board variant: Stepper.1 Firmware version: Stepper.v0.2.0p1 ...","title":"Tools to View Sensor Data"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#accessing-the-status-dictionaries","text":"Each Robot device has a status dictionary that is automatically updated with the latest sensor data. The primary dictionaries are: Stepper Status Wacc Status Pimu Status Dynamixel Status Each of these dictionaries can be accessed through the Robot instance. For example, try in iPython: import stretch_body.robot import time r = stretch_body . robot . Robot () r . startup () for i in range ( 10 ): print ( 'Arm position (m) %f ' % r . arm . status [ 'pos' ]) time . sleep ( 0.1 )","title":"Accessing the Status Dictionaries"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#base-imu","text":"The base has a 9-DoF IMU using the 9-DoF FXOS8700 + FXAS21002 chipset. This is the same chipset used on the Adafruit NXP IMU board . The Pimu reports back the IMU sensor readings in its IMU status dictionary . For example, from iPython try: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () r . pimu . status [ 'imu' ] Out [ 6 ]: { 'ax' : 0.30007487535476685 , 'ay' : - 0.355493426322937 , 'az' : - 9.736297607421875 , 'gx' : 0.0009544769418425858 , 'gy' : 0.00013635384675581008 , 'gz' : 0.00027270769351162016 , 'mx' : - 10.699999809265137 , 'my' : - 42.900001525878906 , 'mz' : - 51.0 , 'roll' : 0.03657745230780231 , 'pitch' : - 0.02960890640560868 , 'heading' : 1.2786458241584955 , 'timestamp' : 1661788669.3042662 , 'qw' : 0.0009681061492301524 , 'qx' : 0.59670090675354 , 'qy' : - 0.8021157383918762 , 'qz' : 0.023505505174398422 , 'bump' : - 0.9188174605369568 } r . pimu . pretty_print () ---------- IMU ------------- AX ( m / s ^ 2 ) 0.30007487535476685 AY ( m / s ^ 2 ) - 0.355493426322937 AZ ( m / s ^ 2 ) - 9.736297607421875 GX ( rad / s ) 0.0009544769418425858 GY ( rad / s ) 0.00013635384675581008 GZ ( rad / s ) 0.00027270769351162016 MX ( uTesla ) - 10.699999809265137 MY ( uTesla ) - 42.900001525878906 MZ ( uTesla ) - 51.0 QW 0.0009681061492301524 QX 0.59670090675354 QY - 0.8021157383918762 QZ 0.023505505174398422 Roll ( deg ) 2.095733642578125 Pitch ( deg ) - 1.6964653730392456 Heading ( deg ) 73.26100921630858 It reports: Acceleration (AX, AY, AZ) Gravity (GX, GY GZ) Magnetic field (MX, MY, MZ) Quaternion orientation (QW, QX, QY, QZ) Euler angle orientation (Roll, Pitch, Heading) These values are computed on the Pimu. As we can see in its firmware code , a 100Hz Madgwick filter is used to compute the orientation. Stretch Body also implements a bump detector using the IMU accelerometers. This detector simply computes the sum of squares of AX, AY, and AZ . This value is then compared to the following threshold to determine if a bump is detected: stretch_params.py | grep pimu | grep bump stretch_body.robot_params.nominal_params param.pimu.config.bump_thresh 20 .0 You can experiment with the bump detector with the following code: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () r . pimu . config [ 'bump_thresh' ] = 20.0 #Experiment with values r . pimu . set_config ( p . config ) r . push_command () for i in range ( 100 ): time . sleep ( 0.1 ) print ( 'Bump %f ' % r . pimu . status [ 'bump' ]) print ( 'Bump event count %d ' % r . pimu . status [ 'bump_event_cnt' ]) Note The IMU is calibrated by Hello Robot at the factory. Please contact Hello Robot support for details on recalibrating your IMU.","title":"Base IMU"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#wrist-accelerometer","text":"The wrist includes a 3 axis ADXL343 accelerometer which provides bump and tap detection capabilities. The Wacc reports back AX, AY, and AZ in its status dictionary . For example, from iPython try: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () r . wacc . status Out [ 5 ]: { 'ax' : 10.093315124511719 , 'ay' : 0.10557432472705841 , 'az' : - 0.45386940240859985 , 'a0' : 155 , 'd0' : 1 , 'd1' : 1 , 'd2' : 0 , 'd3' : 0 , 'single_tap_count' : 15 , 'state' : 0 , 'debug' : 0 , 'timestamp' : 1661795676.203578 , 'transport' : { 'rate' : 0.4572487091345871 , 'read_error' : 0 , 'write_error' : 0 , 'itr' : 3 , 'transaction_time_avg' : 0 , 'transaction_time_max' : 0 , 'timestamp_pc' : 0 }} r . wacc . pretty_print () ------------------------------ Ax ( m / s ^ 2 ) 10.093315124511719 Ay ( m / s ^ 2 ) 0.10557432472705841 Az ( m / s ^ 2 ) - 0.45386940240859985 A0 155 D0 ( In ) 1 D1 ( In ) 1 D2 ( Out ) 0 D3 ( Out ) 0 Single Tap Count 15 State 0 Debug 0 Timestamp ( s ) 1661795676.203578 Board variant : Wacc .1 Firmware version : Wacc . v0 .2.0 p1 In addition to AX, AY, and AZ we also see the single_tap_count value which reports back a count of the number of single-tap contacts the accelerometer has experienced since power-up. The following Wacc parameters configure the accelerometer low-pass filter and single-tap settings. See the ADXL343 datasheet for more details. stretch_params.py | grep wacc stretch_body.robot_params.nominal_params param.wacc.config.accel_LPF 10 .0 stretch_body.robot_params.nominal_params param.wacc.config.accel_range_g 4 stretch_body.robot_params.nominal_params param.wacc.config.accel_single_tap_dur 70 stretch_body.robot_params.nominal_params param.wacc.config.accel_single_tap_thresh 50 stretch_configuration_params.yaml param.wacc.config.accel_gravity_scale 1 .0","title":"Wrist Accelerometer"},{"location":"stretch-tutorials/stretch_body/tutorial_robot_sensors/#cliff-sensors","text":"Stretch has four Sharp GP2Y0A51SK0F IR cliff sensors pointed toward the floor. These report the distance to the floor, allowing for the detection of thresholds, stair edges, etc. Relevant parameters for the cliff sensors are: stretch_params.py | grep cliff stretch_body.robot_params.nominal_params param.pimu.config.cliff_LPF 10 .0 stretch_body.robot_params.nominal_params param.pimu.config.cliff_thresh -50 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_cliff 0 stretch_configuration_params.yaml param.pimu.config.cliff_zero [ 518 .6307312011719, 530 .9835095214844, 500 .7268048095703, 509 .92264434814456 ] stretch_body.robot_params.nominal_params param.robot_monitor.monitor_base_cliff_event 1 The sensors are calibrated such that a zero value (as defined by cliff_zero ) indicates the sensor is at the correct height from the floor surface. A negative value indicates a drop off such as a stair ledge while a positive value indicates an obstacle like a threshold or high pile carpet. You may want to recalibrate this zero based on the surface the robot is on (eg, carpet, tile, etc). To do this: REx_cliff_sensor_calibrate.py For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- Ensure cliff sensors are not obstructed and base is on a flat surface Hit enter when ready Itr 0 Val [ 518 .630126953125, 530 .4168701171875, 500 .863037109375, 510 .0032958984375 ] ... Itr 99 Val [ 518 .8374633789062, 530 .858154296875, 500 .5805969238281, 509 .9013671875 ] Got cliff zeros of: [ 518 .6307312011719, 530 .9835095214844, 500 .7268048095703, 509 .92264434814456 ] Calibration passed. Storing to YAML... The stop_at_cliff field causes the robot to execute a Runstop when the cliff sensor readings exceed the value cliff_thresh . The parameter cliff_LPF defines the low-pass-filter rate (Hz) on the analog sensor readings. Note As configured at the factory, stop_at_cliff is set to zero and Stretch does not stop its motion based on the cliff sensor readings. Hello Robot makes no guarantees as to the reliability of Stretch's ability to avoid driving over ledges and stairs when this flag is enabled. The range values from the sensors can be read from the robot.pimu.status message. The relevant fields are: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () r . pimu . status [ 'cliff_range' ] Out [ 4 ]: [ 0.39227294921875 , - 0.2047119140625 , - 0.26422119140625 , 0.006134033203125 ] r . pimu . status [ 'at_cliff' ] Out [ 5 ]: [ False , False , False , False ] r . pimu . status [ 'cliff_event' ] Out [ 5 ]: False The cliff_event flag is set when any of the four sensor readings exceed cliff_thresh and stop_at_cliff is enabled. In the event of a Cliff Event, it must be reset by robot.pimu.cliff_event_reset() to reset the generated Runstop. The cliff detection logic can be found in the Pimu firmware . All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Cliff Sensors"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/","text":"Tutorial: Safety Features Stretch includes several built-in functions that help it maintain safe operating conditions. These functions can be disabled and enabled via the robot user parameters. Logging Upon instantiation, the Robot class opens a new log file for warning and informational messages to be written to. These timestamped logs are found under $HELLO_FLEET_DIRECTORY/log. The logging messages can additionally be echoed to the console by setting: robot : log_to_console : 1 Runstop Functions The Runstop deactivates all robot motion. It can be triggered by the physical button on the robot's head. It can also be triggered by internal monitors of the system state. The default configuration of these parameters is: stretch_params.py | grep stop_at stretch_body.robot_params.nominal_params param.pimu.config.stop_at_cliff 0 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_high_current 0 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_low_voltage 1 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_runstop 1 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_tilt 0 Parameter Function stop_at_low_voltage Trigger runstop / beep when voltage too low stop_at_high_current Trigger runstop when bus current too high stop_at_cliff Trigger runstop when a cliff sensor is outside of range stop_at_runstop Allow runstop to disable motors stop_at_tilt Trigger runstop when robot tilts too far The Pimu firmware details the implementation of these functions. Warning The stop_at_cliff and stop_at_tilt functions are disabled by default as they are not robust to the normal operating conditions of the robot. Therefore do not rely on these functions for robot safety. Robot Monitor The Robot Monitor is a thread that monitors the Robot Status data for significant events. For example, it can monitor the error flags from the Dynamixel servos and notify when a thermal overload occurs. The Robot Monitor logs warnings to a log file by default. The default parameters associated with RobotMonitor are: stretch_params.py | grep monitor ... stretch_body.robot_params.nominal_params param.robot.use_monitor 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_base_bump_event 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_base_cliff_event 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_current 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_dynamixel_flags 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_guarded_contact 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_over_tilt_alert 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_runstop 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_voltage 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_wrist_single_tap 1 YAML Function monitor_base_bump_event Report when the accelerometer detects a bump event monitor_base_cliff_event Report when a cliff sensor event occurs monitor_current Report when the battery current exceeds desired range monitor_dynamixel_flags Report when a Dynamixel servo enters an error state monitor_guarded_contact Report when a guarded contact event occurs monitor_over_tilt_alert Report when an over-tilt event occurs monitor_runstop Report when the runstop is activated / deactivated monitor_voltage Report when the battery voltage is out of range monitor_wrist_single_tap Report when the wrist accelerometer reports a single tap event Test out the RobotMonitor system by first enabling the console logging in stretch_user_params.yaml: robot : log_to_console : 1 Then run the tool and hit the Runstop button, and then hold it down for 2 seconds: stretch_robot_monitor.py For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- Starting Robot Monitor. Ctrl-C to exit [ INFO ] [ robot_monitor ] : Runstop activated [ INFO ] [ robot_monitor ] : Runstop deactivated Robot Sentry The Robot Sentry is a thread that can override and also generate commands to the robot hardware. Its purpose is to keep the robot operating within a safe regime. For example, the Robot Sentry monitors the position of the Lift and Arm and limits the maximum base velocity and acceleration to reduce the chance of toppling. The Robot Sentry reports events to the log file as well. YAML Function base_fan_control Turn the fan on when CPU temp exceeds range base_max_velocity Limit the base velocity when robot CG is high stretch_gripper_overload Reset commanded position to prevent thermal overload during grasp wrist_yaw_overload Reset commanded position to prevent thermal overload during pushing Collision Avoidance See the Collision Avoidance Tutorial for more information on the Stretch collision avoidance system. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Safety Features"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#tutorial-safety-features","text":"Stretch includes several built-in functions that help it maintain safe operating conditions. These functions can be disabled and enabled via the robot user parameters.","title":"Tutorial: Safety Features"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#logging","text":"Upon instantiation, the Robot class opens a new log file for warning and informational messages to be written to. These timestamped logs are found under $HELLO_FLEET_DIRECTORY/log. The logging messages can additionally be echoed to the console by setting: robot : log_to_console : 1","title":"Logging"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#runstop-functions","text":"The Runstop deactivates all robot motion. It can be triggered by the physical button on the robot's head. It can also be triggered by internal monitors of the system state. The default configuration of these parameters is: stretch_params.py | grep stop_at stretch_body.robot_params.nominal_params param.pimu.config.stop_at_cliff 0 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_high_current 0 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_low_voltage 1 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_runstop 1 stretch_body.robot_params.nominal_params param.pimu.config.stop_at_tilt 0 Parameter Function stop_at_low_voltage Trigger runstop / beep when voltage too low stop_at_high_current Trigger runstop when bus current too high stop_at_cliff Trigger runstop when a cliff sensor is outside of range stop_at_runstop Allow runstop to disable motors stop_at_tilt Trigger runstop when robot tilts too far The Pimu firmware details the implementation of these functions. Warning The stop_at_cliff and stop_at_tilt functions are disabled by default as they are not robust to the normal operating conditions of the robot. Therefore do not rely on these functions for robot safety.","title":"Runstop Functions"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#robot-monitor","text":"The Robot Monitor is a thread that monitors the Robot Status data for significant events. For example, it can monitor the error flags from the Dynamixel servos and notify when a thermal overload occurs. The Robot Monitor logs warnings to a log file by default. The default parameters associated with RobotMonitor are: stretch_params.py | grep monitor ... stretch_body.robot_params.nominal_params param.robot.use_monitor 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_base_bump_event 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_base_cliff_event 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_current 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_dynamixel_flags 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_guarded_contact 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_over_tilt_alert 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_runstop 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_voltage 1 stretch_body.robot_params.nominal_params param.robot_monitor.monitor_wrist_single_tap 1 YAML Function monitor_base_bump_event Report when the accelerometer detects a bump event monitor_base_cliff_event Report when a cliff sensor event occurs monitor_current Report when the battery current exceeds desired range monitor_dynamixel_flags Report when a Dynamixel servo enters an error state monitor_guarded_contact Report when a guarded contact event occurs monitor_over_tilt_alert Report when an over-tilt event occurs monitor_runstop Report when the runstop is activated / deactivated monitor_voltage Report when the battery voltage is out of range monitor_wrist_single_tap Report when the wrist accelerometer reports a single tap event Test out the RobotMonitor system by first enabling the console logging in stretch_user_params.yaml: robot : log_to_console : 1 Then run the tool and hit the Runstop button, and then hold it down for 2 seconds: stretch_robot_monitor.py For use with S T R E T C H ( R ) RESEARCH EDITION from Hello Robot Inc. --------------------------------------------------------------------- Starting Robot Monitor. Ctrl-C to exit [ INFO ] [ robot_monitor ] : Runstop activated [ INFO ] [ robot_monitor ] : Runstop deactivated","title":"Robot Monitor"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#robot-sentry","text":"The Robot Sentry is a thread that can override and also generate commands to the robot hardware. Its purpose is to keep the robot operating within a safe regime. For example, the Robot Sentry monitors the position of the Lift and Arm and limits the maximum base velocity and acceleration to reduce the chance of toppling. The Robot Sentry reports events to the log file as well. YAML Function base_fan_control Turn the fan on when CPU temp exceeds range base_max_velocity Limit the base velocity when robot CG is high stretch_gripper_overload Reset commanded position to prevent thermal overload during grasp wrist_yaw_overload Reset commanded position to prevent thermal overload during pushing","title":"Robot Sentry"},{"location":"stretch-tutorials/stretch_body/tutorial_safe_coding/#collision-avoidance","text":"See the Collision Avoidance Tutorial for more information on the Stretch collision avoidance system. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Collision Avoidance"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/","text":"Tutorial: Splined Trajectories Stretch Body supports splined trajectory controllers across all of its joints. This enables Stretch to achieve smooth and coordinated full-body control of the robot. What are Splined Trajectories? A splined trajectory is a smooth path that a robot joint follows over a specific period of time. Cubic or quintic splines are used to represent the trajectory. As shown below, the splines (blue) are defined by a series of user-provided waypoints (black dot). A waypoint is simply a target position, velocity, and optional acceleration at a given time. The spline ensures continuity and smoothness when interpolating between the waypoint targets. During execution, the trajectory controller uses this splined representation to compute the instantaneous desired position, velocity, and acceleration of the joint (red). On Stretch, this instantaneous target is then passed to a lower-level position or velocity controller. Splined trajectories are particularly useful when you want to coordinate motion across several joints. Because the trajectory representation is time-based, it is straightforward to encode multi-joint coordination. Stretch Body supports both cubic and quintic splines. A quintic spline waypoint includes acceleration in the waypoint target, while a cubic spline does not. The Splined Trajectory Tool Stretch Body includes a graphical tool for exploring splined trajectory control on the robot: stretch_trajectory_jog.py -h usage: stretch_trajectory_jog.py [ -h ] [ --text ] [ --preloaded_traj { 1 ,2,3 }] ( --head_pan | --head_tilt | --wrist_yaw | --gripper | --arm | --lift | --base_translate | --base_rotate | --full_body ) Test out splined trajectories on the various joint from a GUI or text menu. optional arguments: -h, --help show this help message and exit --text, -t Use text options instead of GUI --preloaded_traj { 1 ,2,3 } , -p { 1 ,2,3 } Load one of three predefined trajectories --head_pan Test trajectories on the head_pan joint --head_tilt Test trajectories on the head_tilt joint --wrist_yaw Test trajectories on the wrist_yaw joint --gripper Test trajectories on the stretch_gripper joint --arm Test trajectories on the arm joint --lift Test trajectories on the lift joint --base_translate Test translational trajectories on diff-drive base --base_rotate Test rotational trajectories on diff-drive base --full_body Test trajectories on all joints at once The tool GUI allows you to interactively construct a splined trajectory and then execute it on the robot. For example, on the arm: Note Use caution when commanding the base. Ensure that the attached cables are long enough to support the base motion. Alternatively, you may want to put the base on top of a book so the wheels don't touch the ground. Finally, you can explore a full-body trajectory using the non-GUI version of the tool: stretch_trajectory_jog.py --full_body Programming Trajectories Programming a splined trajectory is straightforward. For example, try the following from iPython: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () #Define the waypoints times = [ 0.0 , 10.0 , 20.0 ] positions = [ r . arm . status [ 'pos' ], 0.45 , 0.0 ] velocities = [ r . arm . status [ 'vel' ], 0.0 , 0.0 ] #Create the spline trajectory for waypoint in zip ( times , positions , velocities ): r . arm . trajectory . add ( waypoint [ 0 ], waypoint [ 1 ], waypoint [ 2 ]) #Begin execution r . arm . follow_trajectory () #Wait unti completion while r . arm . is_trajectory_active (): print ( 'Execution time: %f ' % r . arm . get_trajectory_time_remaining ()) time . sleep ( 0.1 ) r . stop () This will cause the arm to move from its current position to 0.45m, then back to fully retracted. A few things to note: This will execute a Cubic spline as we did not pass in accelerations to in r.arm.trajectory.add The call to r.arm.follow_trajectory is non-blocking and the trajectory generation is handled by a background thread of the Robot class If you're interested in exploring the trajectory API further the code for the stretch_trajectory_jog.py is a great reference to get started. Advanced: Controller Parameters Sometimes the robot's motion isn't quite what is expected when executing a splined trajectory. It is important that the trajectory be well-formed, meaning that it: Respects the maximum velocity and accelerations limits of the joint Doesn't create a large 'excursion' outside of the acceptable range of motion to hit a target waypoint Doesn't have waypoints so closely spaced together that it exceeds the nominal control rates of Stretch (~10-20 Hz) For example, the arm trajectory below has a large excursion outside of the joint's range of motion (white). This is because the second waypoint expects a non-zero velocity when the arm reaches full extension. Often the trajectory waypoints will be generated from a motion planner. The planner needs to incorporate the position, velocity, and acceleration constraints of the joint. These can be found by, for example: stretch_params.py | grep arm | grep motion | grep trajectory stretch_body.robot_params.nominal_params param.arm.motion.trajectory_max.vel_m 0 .4 stretch_body.robot_params.nominal_params param.arm.motion.trajectory_max.accel_m 0 .4 stretch_params.py | grep arm | grep range_m stretch_user_params.yaml param.arm.range_m [ 0 .0, 0 .515 ] Fortunately, the Stretch Body Trajectory classes do some preliminary feasibility checking of trajectories using the is_segment_feasible function . This checks if the generated motions lie within the constraints of the trajectory_max parameters. It is generally important for the waypoints to be spaced far apart. Stretch isn't a dynamic and fast-moving robot, so there isn't a practical advantage to closely spaced waypoints at any rate. The stepper controllers (arm, lift, and base) can be updated at approximately 20 Hz maximum. Therefore, if your waypoints are spaced 50 ms apart, you run the risk of overflowing the stepper controller. Likewise, the Dynamixel joints can be updated at approximately 12 Hz. Tip As a rule of thumb, spacing the waypoints over 100 ms apart is a good idea. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Splined Trajectories"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/#tutorial-splined-trajectories","text":"Stretch Body supports splined trajectory controllers across all of its joints. This enables Stretch to achieve smooth and coordinated full-body control of the robot.","title":"Tutorial: Splined Trajectories"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/#what-are-splined-trajectories","text":"A splined trajectory is a smooth path that a robot joint follows over a specific period of time. Cubic or quintic splines are used to represent the trajectory. As shown below, the splines (blue) are defined by a series of user-provided waypoints (black dot). A waypoint is simply a target position, velocity, and optional acceleration at a given time. The spline ensures continuity and smoothness when interpolating between the waypoint targets. During execution, the trajectory controller uses this splined representation to compute the instantaneous desired position, velocity, and acceleration of the joint (red). On Stretch, this instantaneous target is then passed to a lower-level position or velocity controller. Splined trajectories are particularly useful when you want to coordinate motion across several joints. Because the trajectory representation is time-based, it is straightforward to encode multi-joint coordination. Stretch Body supports both cubic and quintic splines. A quintic spline waypoint includes acceleration in the waypoint target, while a cubic spline does not.","title":"What are Splined Trajectories?"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/#the-splined-trajectory-tool","text":"Stretch Body includes a graphical tool for exploring splined trajectory control on the robot: stretch_trajectory_jog.py -h usage: stretch_trajectory_jog.py [ -h ] [ --text ] [ --preloaded_traj { 1 ,2,3 }] ( --head_pan | --head_tilt | --wrist_yaw | --gripper | --arm | --lift | --base_translate | --base_rotate | --full_body ) Test out splined trajectories on the various joint from a GUI or text menu. optional arguments: -h, --help show this help message and exit --text, -t Use text options instead of GUI --preloaded_traj { 1 ,2,3 } , -p { 1 ,2,3 } Load one of three predefined trajectories --head_pan Test trajectories on the head_pan joint --head_tilt Test trajectories on the head_tilt joint --wrist_yaw Test trajectories on the wrist_yaw joint --gripper Test trajectories on the stretch_gripper joint --arm Test trajectories on the arm joint --lift Test trajectories on the lift joint --base_translate Test translational trajectories on diff-drive base --base_rotate Test rotational trajectories on diff-drive base --full_body Test trajectories on all joints at once The tool GUI allows you to interactively construct a splined trajectory and then execute it on the robot. For example, on the arm: Note Use caution when commanding the base. Ensure that the attached cables are long enough to support the base motion. Alternatively, you may want to put the base on top of a book so the wheels don't touch the ground. Finally, you can explore a full-body trajectory using the non-GUI version of the tool: stretch_trajectory_jog.py --full_body","title":"The Splined Trajectory Tool"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/#programming-trajectories","text":"Programming a splined trajectory is straightforward. For example, try the following from iPython: import stretch_body.robot r = stretch_body . robot . Robot () r . startup () #Define the waypoints times = [ 0.0 , 10.0 , 20.0 ] positions = [ r . arm . status [ 'pos' ], 0.45 , 0.0 ] velocities = [ r . arm . status [ 'vel' ], 0.0 , 0.0 ] #Create the spline trajectory for waypoint in zip ( times , positions , velocities ): r . arm . trajectory . add ( waypoint [ 0 ], waypoint [ 1 ], waypoint [ 2 ]) #Begin execution r . arm . follow_trajectory () #Wait unti completion while r . arm . is_trajectory_active (): print ( 'Execution time: %f ' % r . arm . get_trajectory_time_remaining ()) time . sleep ( 0.1 ) r . stop () This will cause the arm to move from its current position to 0.45m, then back to fully retracted. A few things to note: This will execute a Cubic spline as we did not pass in accelerations to in r.arm.trajectory.add The call to r.arm.follow_trajectory is non-blocking and the trajectory generation is handled by a background thread of the Robot class If you're interested in exploring the trajectory API further the code for the stretch_trajectory_jog.py is a great reference to get started.","title":"Programming Trajectories"},{"location":"stretch-tutorials/stretch_body/tutorial_splined_trajectories/#advanced-controller-parameters","text":"Sometimes the robot's motion isn't quite what is expected when executing a splined trajectory. It is important that the trajectory be well-formed, meaning that it: Respects the maximum velocity and accelerations limits of the joint Doesn't create a large 'excursion' outside of the acceptable range of motion to hit a target waypoint Doesn't have waypoints so closely spaced together that it exceeds the nominal control rates of Stretch (~10-20 Hz) For example, the arm trajectory below has a large excursion outside of the joint's range of motion (white). This is because the second waypoint expects a non-zero velocity when the arm reaches full extension. Often the trajectory waypoints will be generated from a motion planner. The planner needs to incorporate the position, velocity, and acceleration constraints of the joint. These can be found by, for example: stretch_params.py | grep arm | grep motion | grep trajectory stretch_body.robot_params.nominal_params param.arm.motion.trajectory_max.vel_m 0 .4 stretch_body.robot_params.nominal_params param.arm.motion.trajectory_max.accel_m 0 .4 stretch_params.py | grep arm | grep range_m stretch_user_params.yaml param.arm.range_m [ 0 .0, 0 .515 ] Fortunately, the Stretch Body Trajectory classes do some preliminary feasibility checking of trajectories using the is_segment_feasible function . This checks if the generated motions lie within the constraints of the trajectory_max parameters. It is generally important for the waypoints to be spaced far apart. Stretch isn't a dynamic and fast-moving robot, so there isn't a practical advantage to closely spaced waypoints at any rate. The stepper controllers (arm, lift, and base) can be updated at approximately 20 Hz maximum. Therefore, if your waypoints are spaced 50 ms apart, you run the risk of overflowing the stepper controller. Likewise, the Dynamixel joints can be updated at approximately 12 Hz. Tip As a rule of thumb, spacing the waypoints over 100 ms apart is a good idea. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Advanced: Controller Parameters"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/","text":"Stretch Body API Reference Stretch Body is the Python interface for working with Stretch. This page serves as a reference for the interfaces defined in the stretch_body library. See the Stretch Body Tutorials for additional information on working with this library. The Robot Class The most common interface to Stretch is the Robot class. This class encapsulates all devices on the robot. It is typically initialized as: 1 2 3 4 5 6 7 8 9 10 11 import stretch_body.robot r = stretch_body . robot . Robot () if not r . startup (): exit () # failed to start robot! # home the joints to find zero, if necessary if not r . is_calibrated (): r . home () # interact with the robot here The startup() and home() methods start communication with and home each of the robot's devices, respectively. Through the Robot class, users can interact with all devices on the robot. For example, continuing the example above: 12 13 14 15 16 17 18 19 20 21 22 23 # moving joints on the robot r . arm . pretty_print () r . lift . pretty_print () r . base . pretty_print () r . head . pretty_print () r . end_of_arm . pretty_print () # other devices on the robot r . wacc . pretty_print () r . pimu . pretty_print () r . stop () Each of these devices is defined in separate modules within stretch_body . In the following section, we'll look at the API of these classes. The stop() method shuts down communication with the robot's devices. All methods in the Robot class are documented below. stretch_body.robot.Robot ( Device ) API to the Stretch Robot __init__ ( self ) special startup ( self ) To be called once after class instantiation. Prepares devices for communications and motion Returns bool true if startup of robot succeeded stop ( self ) To be called once before exiting a program Cleanly stops down motion and communication get_status ( self ) Thread safe and atomic read of current Robot status data Returns as a dict. pretty_print ( self ) push_command ( self ) Cause all queued up RPC commands to be sent down to Devices is_trajectory_active ( self ) follow_trajectory ( self ) stop_trajectory ( self ) is_calibrated ( self ) Returns true if homing-calibration has been run all joints that require it get_stow_pos ( self , joint ) Return the stow position of a joint. Allow the end_of_arm to override the defaults in order to accomodate stowing different tools stow ( self ) Cause the robot to move to its stow position Blocking. home ( self ) Cause the robot to home its joints by moving to hardstops Blocking. The Device Class The stretch_body library is modular in design. Each subcomponent of Stretch is defined in its class and the Robot class provides an interface that ties all of these classes together. This modularity allows users to plug in new/modified subcomponents into the Robot interface by extending the Device class. It is possible to interface with a single subcomponent of Stretch by initializing its device class directly. In this section, we'll look at the API of seven subclasses of the Device class: the Arm , Lift , Base , Head , EndOfArm , Wacc , and Pimu subcomponents of Stretch. Using the Arm class The interface to Stretch's telescoping arm is the Arm class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 10 import stretch_body.arm a = stretch_body . arm . Arm () a . motor . disable_sync_mode () if not a . startup (): exit () # failed to start arm! a . home () # interact with the arm here Since both Arm and Robot are subclasses of the Device class, the same startup() and stop() methods are available here, as well as other Device methods such as home() . Using the Arm class, we can read the arm's current state and send commands to the joint. For example, continuing the example above: 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 starting_position = a . status [ 'pos' ] # move out by 10cm a . move_to ( starting_position + 0.1 ) a . push_command () a . motor . wait_until_at_setpoint () # move back to starting position quickly a . move_to ( starting_position , v_m = 0.2 , a_m = 0.25 ) a . push_command () a . motor . wait_until_at_setpoint () a . move_by ( 0.1 ) # move out by 10cm a . push_command () a . motor . wait_until_at_setpoint () The move_to() and move_by() methods queue absolute and relative position commands to the arm, respectively, while the nonblocking push_command() method pushes the queued position commands to the hardware for execution. The attribute motor , an instance of the Stepper class, has the method wait_until_at_setpoint() which blocks program execution until the joint reaches the commanded goal. With firmware P1 or greater installed, it is also possible to queue a waypoint trajectory for the arm to follow: 26 27 28 29 30 31 32 33 34 35 36 starting_position = a . status [ 'pos' ] # queue a trajectory consisting of four waypoints a . trajectory . add ( t_s = 0 , x_m = starting_position ) a . trajectory . add ( t_s = 3 , x_m = 0.15 ) a . trajectory . add ( t_s = 6 , x_m = 0.1 ) a . trajectory . add ( t_s = 9 , x_m = 0.2 ) # trigger trajectory execution a . follow_trajectory () import time ; time . sleep ( 9 ) The attribute trajectory , an instance of the PrismaticTrajectory class, has the method add() which adds a single waypoint in a linear sliding trajectory. For a well-formed trajectory (see is_valid() ), the follow_trajectory() method starts tracking the trajectory for the telescoping arm. It is also possible to dynamically restrict the arm joint range: 37 38 39 40 41 42 43 44 45 46 47 48 49 50 range_upper_limit = 0.3 # meters # set soft limits on arm's range a . set_soft_motion_limit_min ( 0 ) a . set_soft_motion_limit_max ( range_upper_limit ) a . push_command () # command the arm outside the valid range a . move_to ( 0.4 ) a . push_command () a . motor . wait_until_at_setpoint () print ( a . status [ 'pos' ]) # we should expect to see ~0.3 a . stop () The set_soft_motion_limit_min/max() methods form the basis of an experimental self-collision avoidance system built into Stretch Body. All methods in the Arm class are documented below. stretch_body.arm.Arm ( PrismaticJoint ) API to the Stretch Arm __init__ ( self ) special motor_rad_to_translate_m ( self , ang ) translate_m_to_motor_rad ( self , x ) home ( self , end_pos = 0.1 , to_positive_stop = False , measuring = False ) end_pos: position to end on to_positive_stop: -- True: Move to the positive direction stop and mark to range_m[1] -- False: Move to the negative direction stop and mark to range_m[0] measuring: After homing to stop, move to opposite stop and report back measured distance return measured range-of-motion if measuring. Return None if not a valide measurement Using the Lift class The interface to Stretch's lift is the Lift class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 10 import stretch_body.lift l = stretch_body . lift . Lift () l . motor . disable_sync_mode () if not l . startup (): exit () # failed to start lift! l . home () # interact with the lift here The startup() and home() methods are extended from the Device class. Reading the lift's current state and sending commands to the joint occurs similarly to the Arm class: 11 12 13 14 15 16 starting_position = l . status [ 'pos' ] # move up by 10cm l . move_to ( starting_position + 0.1 ) l . push_command () l . motor . wait_until_at_setpoint () The attribute status is a dictionary of the joint's current status. This state information is updated in the background in real-time by default (disable by initializing as startup(threading=False) ). Use the pretty_print() method to print out this state info in a human-interpretable format. Setting up waypoint trajectories for the lift is also similar to the Arm : 17 18 19 20 21 22 23 24 25 26 starting_position = l . status [ 'pos' ] # queue a trajectory consisting of three waypoints l . trajectory . add ( t_s = 0 , x_m = starting_position , v_m = 0.0 ) l . trajectory . add ( t_s = 3 , x_m = 0.5 , v_m = 0.0 ) l . trajectory . add ( t_s = 6 , x_m = 0.6 , v_m = 0.0 ) # trigger trajectory execution l . follow_trajectory () import time ; time . sleep ( 6 ) The attribute trajectory is also an instance of the PrismaticTrajectory class, and by providing the instantaneous velocity argument v_m to the add() method, a cubic spline can be loaded into the joint's trajectory . The call to follow_trajectory() begins hardware tracking of the spline. Finally, setting soft motion limits for the lift's range can be done using: 27 28 29 30 31 32 # cut out 0.2m from the top and bottom of the lift's range l . set_soft_motion_limit_min ( 0.2 ) l . set_soft_motion_limit_max ( 0.8 ) l . push_command () l . stop () The set_soft_motion_limit_min/max() methods perform clipping of the joint's range at the firmware level (can persist across reboots). All methods in the Lift class are documented below. stretch_body.lift.Lift ( PrismaticJoint ) API to the Stretch Lift __init__ ( self ) special motor_rad_to_translate_m ( self , ang ) translate_m_to_motor_rad ( self , x ) home ( self , end_pos = 0.6 , to_positive_stop = True , measuring = False ) end_pos: position to end on to_positive_stop: -- True: Move to the positive direction stop and mark to range_m[1] -- False: Move to the negative direction stop and mark to range_m[0] measuring: After homing to stop, move to opposite stop and report back measured distance return measured range-of-motion if measuring. Return None if not a valide measurement Using the Base class Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm The interface to Stretch's mobile base is the Base class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.base b = stretch_body . base . Base () b . left_wheel . disable_sync_mode () b . right_wheel . disable_sync_mode () if not b . startup (): exit () # failed to start base! # interact with the base here Stretch's mobile base is a differential drive configuration. The left and right wheels are accessible through Base left_wheel and right_wheel attributes, both of which are instances of the Stepper class. The startup() method is extended from the Device class. Since the mobile base is unconstrained, there is no homing method. The pretty_print() method prints out mobile base state information in a human-interpretable format. We can read the base's current state and send commands using: 10 11 12 13 14 15 16 17 18 19 20 b . pretty_print () # translate forward by 10cm b . translate_by ( 0.1 ) b . push_command () b . left_wheel . wait_until_at_setpoint () # rotate counter-clockwise by 90 degrees b . rotate_by ( 1.57 ) b . push_command () b . left_wheel . wait_until_at_setpoint () The translate_by() and rotate_by() methods send relative commands similar to the way move_by() behaves for the single degree of freedom joints. The mobile base also supports velocity control: 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # command the base to translate forward at 5cm / second b . set_translate_velocity ( 0.05 ) b . push_command () import time ; time . sleep ( 1 ) # command the base to rotate counter-clockwise at 0.1rad / second b . set_rotational_velocity ( 0.1 ) b . push_command () time . sleep ( 1 ) # command the base with translational and rotational velocities b . set_velocity ( 0.05 , 0.1 ) b . push_command () time . sleep ( 1 ) # stop base motion b . enable_freewheel_mode () b . push_command () The set_translate_velocity() and set_rotational_velocity() methods give velocity control over the translational and rotational components of the mobile base independently. The set_velocity() method gives control over both of these components simultaneously. To halt motion, you can command zero velocities or command the base into freewheel mode using enable_freewheel_mode() . The mobile base also supports waypoint trajectory following, but the waypoints are part of the SE2 group, where a desired waypoint is defined as an (x, y) point and a theta orientation: 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # reset odometry calculation b . first_step = True b . pull_status () # queue a trajectory consisting of three waypoints b . trajectory . add ( time = 0 , x = 0.0 , y = 0.0 , theta = 0.0 ) b . trajectory . add ( time = 3 , x = 0.1 , y = 0.0 , theta = 0.0 ) b . trajectory . add ( time = 6 , x = 0.0 , y = 0.0 , theta = 0.0 ) # trigger trajectory execution b . follow_trajectory () import time ; time . sleep ( 6 ) print ( b . status [ 'x' ], b . status [ 'y' ], b . status [ 'theta' ]) # we should expect to see around (0.0, 0.0, 0.0 or 6.28) b . stop () Warning The Base waypoint trajectory following has no notion of obstacles in the environment. It will blindly follow the commanded waypoints. For obstacle avoidance, we recommend employing perception and a path planner. The attribute trajectory is an instance of the DiffDriveTrajectory class. The call to follow_trajectory() begins hardware tracking of the spline. All methods of the Base class are documented below. stretch_body.base.Base ( Device ) API to the Stretch Mobile Base __init__ ( self ) special startup ( self , threaded = True ) Starts machinery required to interface with this device Parameters threaded : bool whether a thread manages hardware polling/pushing in the background Returns bool whether the startup procedure succeeded stop ( self ) Shuts down machinery started in startup() pretty_print ( self ) enable_freewheel_mode ( self ) Force motors into freewheel enable_pos_incr_mode ( self ) Force motors into incremental position mode wait_for_contact ( self , timeout = 5.0 ) wait_until_at_setpoint ( self , timeout = 15.0 ) contact_thresh_to_motor_current ( self , is_translate , contact_thresh ) This model converts from a specified percentage effort (-100 to 100) of translate/rotational effort to motor currents translate_by ( self , x_m , v_m = None , a_m = None , stiffness = None , contact_thresh_N = None , contact_thresh = None ) Incremental translation of the base x_m: desired motion (m) v_m: velocity for trapezoidal motion profile (m/s) a_m: acceleration for trapezoidal motion profile (m/s^2) stiffness: stiffness of motion. Range 0.0 (min) to 1.0 (max) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100)) rotate_by ( self , x_r , v_r = None , a_r = None , stiffness = None , contact_thresh_N = None , contact_thresh = None ) Incremental rotation of the base x_r: desired motion (radians) v_r: velocity for trapezoidal motion profile (rad/s) a_r: acceleration for trapezoidal motion profile (rad/s^2) stiffness: stiffness of motion. Range 0.0 (min) to 1.0 (max) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100)) set_translate_velocity ( self , v_m , a_m = None , stiffness = None , contact_thresh_N = None , contact_thresh = None ) Command the bases translational velocity. Use care to prevent collisions / avoid runaways v_m: desired velocity (m/s) a_m: acceleration of motion profile (m/s^2) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100)) set_rotational_velocity ( self , v_r , a_r = None , stiffness = None , contact_thresh_N = None , contact_thresh = None ) Command the bases rotational velocity. Use care to prevent collisions / avoid runaways v_r: desired rotational velocity (rad/s) a_r: acceleration of motion profile (rad/s^2) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100)) set_velocity ( self , v_m , w_r , a = None , stiffness = None , contact_thresh_N = None , contact_thresh = None ) Command the bases translational and rotational velocities simultaneously. Use care to prevent collisions / avoid runaways v_m: desired velocity (m/s) w_r: desired rotational velocity (rad/s) a: acceleration of motion profile (m/s^2 and rad/s^2) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100)) follow_trajectory ( self , v_r = None , a_r = None , stiffness = None , contact_thresh_N = None , contact_thresh = None ) Starts executing a waypoint trajectory self.trajectory must be populated with a valid trajectory before calling this method. Parameters v_r : float velocity limit for trajectory in motor space in meters per second a_r : float acceleration limit for trajectory in motor space in meters per second squared stiffness : float stiffness of motion. Range 0.0 (min) to 1.0 (max) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100)) is_trajectory_active ( self ) get_trajectory_ts ( self ) get_trajectory_time_remaining ( self ) update_trajectory ( self ) Updates hardware with the next segment of self.trajectory This method must be called frequently to enable complete trajectory execution and preemption of future segments. If used with stretch_body.robot.Robot or with self.startup(threaded=True) , a background thread is launched for this. Otherwise, the user must handle calling this method. stop_trajectory ( self ) Stop waypoint trajectory immediately and resets hardware step_sentry ( self , robot ) Only allow fast mobile base motion if the lift is low, the arm is retracted, and the wrist is stowed. This is intended to keep the center of mass low for increased stability and avoid catching the arm or tool on something. push_command ( self ) pull_status ( self ) Computes base odometery based on stepper positions / velocities motor_current_to_translation_force ( self , il , ir ) motor_current_to_rotation_torque ( self , il , ir ) translation_force_to_motor_current ( self , f_N ) rotation_torque_to_motor_current ( self , tq_Nm ) rotation_effort_pct_to_motor_current ( self , e_pct ) translation_effort_pct_to_motor_current ( self , e_pct ) motor_current_to_translate_effort_pct ( self , il , ir ) motor_current_to_rotation_effort_pct ( self , il , ir ) translate_to_motor_rad ( self , x_m ) motor_rad_to_translate ( self , x_r ) rotate_to_motor_rad ( self , x_r ) motor_rad_to_rotate ( self , x_r ) translation_to_rotation ( self , x_m ) rotation_to_translation ( self , x_r ) Using the Head class The interface to Stretch's head is the Head class. The head contains an Intel Realsense D435i depth camera. The pan and tilt joints in the head allow Stretch to swivel and capture depth imagery of its surrounding. The head is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.head h = stretch_body . head . Head () if not h . startup (): exit () # failed to start head! h . home () # interact with the head here Head is a subclass of the DynamixelXChain class, which in turn is a subclass of the Device class. Therefore, some of Head's methods, such as startup() and home() are extended from the Device class, while others come from the DynamixelXChain class. Reading the head's current state and sending commands to its revolute joints (head pan and tilt) can be achieved using: 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 starting_position = h . status [ 'head_pan' ][ 'pos' ] # look right by 90 degrees h . move_to ( 'head_pan' , starting_position + 1.57 ) h . get_joint ( 'head_pan' ) . wait_until_at_setpoint () # tilt up by 30 degrees h . move_by ( 'head_tilt' , - 1.57 / 3 ) h . get_joint ( 'head_tilt' ) . wait_until_at_setpoint () # look down towards the wheels h . pose ( 'wheels' ) import time ; time . sleep ( 3 ) # look ahead h . pose ( 'ahead' ) time . sleep ( 3 ) The attribute status is a dictionary of dictionaries, where each subdictionary is the status of one of the head's joints. This state information is updated in the background in real-time by default (disable by initializing as startup(threading=False) ). Use the pretty_print() method to print out this state information in a human-interpretable format. Commanding the head's revolute joints is done through the move_to() and move_by() methods. Notice that, unlike the previous joints, no push command call is required here. These joints are Dynamixel servos, which behave differently than the Hello Robot steppers. Their commands are not queued and are executed as soon as they're received. Head's two joints, the 'head_pan' and 'head_tilt' are instances of the DynamixelHelloXL430 class and are retrievable using the get_joint() method. They have the wait_until_at_setpoint() method, which blocks program execution until the joint reaches the commanded goal. The pose() method makes it easy to command the head to common head poses (e.g. looking 'ahead', at the end-of-arm 'tool', obstacles in front of the 'wheels', or 'up'). The head supports waypoint trajectories as well: 27 28 29 30 31 32 33 34 35 36 37 # queue a trajectory consisting of three waypoints h . get_joint ( 'head_tilt' ) . trajectory . add ( t_s = 0 , x_r = 0.0 ) h . get_joint ( 'head_tilt' ) . trajectory . add ( t_s = 3 , x_r =- 1.0 ) h . get_joint ( 'head_tilt' ) . trajectory . add ( t_s = 6 , x_r = 0.0 ) h . get_joint ( 'head_pan' ) . trajectory . add ( t_s = 0 , x_r = 0.1 ) h . get_joint ( 'head_pan' ) . trajectory . add ( t_s = 3 , x_r =- 0.9 ) h . get_joint ( 'head_pan' ) . trajectory . add ( t_s = 6 , x_r = 0.1 ) # trigger trajectory execution h . follow_trajectory () import time ; time . sleep ( 6 ) The head pan and tilt DynamixelHelloXL430 instances have an attribute trajectory , which is an instance of the RevoluteTrajectory class. The call to follow_trajectory() begins software tracking of the spline. Finally, setting soft motion limits for the head's pan and tilt range can be achieved using: 38 39 40 41 42 43 44 45 46 # clip the head_pan's range h . get_joint ( 'head_pan' ) . set_soft_motion_limit_min ( - 1.0 ) h . get_joint ( 'head_pan' ) . set_soft_motion_limit_max ( 1.0 ) # clip the head_tilt's range h . get_joint ( 'head_tilt' ) . set_soft_motion_limit_min ( - 1.0 ) h . get_joint ( 'head_tilt' ) . set_soft_motion_limit_max ( 0.1 ) h . stop () The set_soft_motion_limit_min/max() methods perform clipping of the joint's range at the software level (cannot persist across reboots). All methods of the Head class are documented below. stretch_body.head.Head ( DynamixelXChain ) API to the Stretch RE1 Head __init__ ( self ) special startup ( self , threaded = True ) Starts machinery required to interface with this device Parameters threaded : bool whether a thread manages hardware polling/pushing in the background Returns bool whether the startup procedure succeeded get_joint ( self , joint_name ) Retrieves joint by name. Parameters joint_name : str valid joints defined in joints Returns DynamixelHelloXL430 or None Motor object on valid joint name, else None move_to ( self , joint , x_r , v_r = None , a_r = None ) joint: Name of the joint to move ('head_pan' or 'head_tilt') x_r: commanded absolute position (radians). v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2) move_by ( self , joint , x_r , v_r = None , a_r = None ) joint: Name of the joint to move ('head_pan' or 'head_tilt') x_r: commanded incremental motion (radians). v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2) home ( self ) pose ( self , p , v_r = [ None , None ], a_r = [ None , None ]) p: Dictionary key to named pose (eg 'ahead') v_r: list, velocities for trapezoidal motion profile (rad/s). a_r: list, accelerations for trapezoidal motion profile (rad/s^2) Using the EndOfArm class The interface to Stretch's end-of-arm is the EndOfArm class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.end_of_arm e = stretch_body . end_of_arm . EndOfArm () if not e . startup ( threaded = True ): exit () # failed to start end of arm! # interact with the end of arm here e . stop () All methods of the EndOfArm class are documented below. stretch_body.end_of_arm.EndOfArm ( DynamixelXChain ) The EndOfArm class allows for an extensible serial chain of Dynamixel X series devices It allows the specific type of device to be declared at runtime via the Yaml parameters In this way, a user can add their own custom Dynamixel based tools to the robot end-of-arm by simply deriving it from DynamixelHelloXL430 and declaring the class name / Python module name in the User YAML file __init__ ( self , name = 'end_of_arm' ) special startup ( self , threaded = True ) Starts machinery required to interface with this device Parameters threaded : bool whether a thread manages hardware polling/pushing in the background Returns bool whether the startup procedure succeeded get_joint ( self , joint_name ) Retrieves joint by name. Parameters joint_name : str valid joints defined as defined in params['devices'] Returns DynamixelHelloXL430 or None Motor object on valid joint name, else None move_to ( self , joint , x_r , v_r = None , a_r = None ) joint: name of joint (string) x_r: commanded absolute position (radians). v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2) move_by ( self , joint , x_r , v_r = None , a_r = None ) joint: name of joint (string) x_r: commanded incremental motion (radians). v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2) pose ( self , joint , p , v_r = None , a_r = None ) joint: name of joint (string) p: named pose of joint v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2) stow ( self ) home ( self , joint = None ) Home to hardstops is_tool_present ( self , class_name ) Return true if the given tool type is present (eg. StretchGripper) Allows for conditional logic when switching end-of-arm tools Using the Wacc class The interface to Stretch's wrist board is the Wacc (wrist + accelerometer) class. This board provides an Arduino and accelerometer sensor that is accessible from the Wacc class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.wacc w = stretch_body . wacc . Wacc () if not w . startup ( threaded = True ): exit () # failed to start wacc! # interact with the wacc here w . stop () All methods of the Wacc class are documented below. stretch_body.wacc.Wacc ( WaccBase ) API to the Stretch Wrist Accelerometer (Wacc) Board __init__ ( self ) special startup ( self , threaded = False ) First determine which protocol version the uC firmware is running. Based on that version, replaces PimuBase class inheritance with a inheritance to a child class of PimuBase that supports that protocol Using the Pimu class The interface to Stretch's power board is the Pimu (power + IMU) class. This board provides a 9-DOF IMU that is accessible from the Pimu class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.pimu p = stretch_body . pimu . Pimu () if not p . startup ( threaded = True ): exit () # failed to start pimu! # interact with the pimu here p . stop () All methods of the Pimu class are documented below. stretch_body.pimu.Pimu ( PimuBase ) API to the Stretch Power and IMU board (Pimu) __init__ ( self , event_reset = False ) special startup ( self , threaded = False ) First determine which protocol version the uC firmware is running. Based on that version, replaces PimuBase class inheritance with a inheritance to a child class of PimuBase that supports that protocol All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Stretch Body API"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch-body-api-reference","text":"Stretch Body is the Python interface for working with Stretch. This page serves as a reference for the interfaces defined in the stretch_body library. See the Stretch Body Tutorials for additional information on working with this library.","title":"Stretch Body API Reference"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#the-robot-class","text":"The most common interface to Stretch is the Robot class. This class encapsulates all devices on the robot. It is typically initialized as: 1 2 3 4 5 6 7 8 9 10 11 import stretch_body.robot r = stretch_body . robot . Robot () if not r . startup (): exit () # failed to start robot! # home the joints to find zero, if necessary if not r . is_calibrated (): r . home () # interact with the robot here The startup() and home() methods start communication with and home each of the robot's devices, respectively. Through the Robot class, users can interact with all devices on the robot. For example, continuing the example above: 12 13 14 15 16 17 18 19 20 21 22 23 # moving joints on the robot r . arm . pretty_print () r . lift . pretty_print () r . base . pretty_print () r . head . pretty_print () r . end_of_arm . pretty_print () # other devices on the robot r . wacc . pretty_print () r . pimu . pretty_print () r . stop () Each of these devices is defined in separate modules within stretch_body . In the following section, we'll look at the API of these classes. The stop() method shuts down communication with the robot's devices. All methods in the Robot class are documented below.","title":"The Robot Class"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot","text":"API to the Stretch Robot","title":"Robot"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.__init__","text":"","title":"__init__()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.startup","text":"To be called once after class instantiation. Prepares devices for communications and motion","title":"startup()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.startup--returns","text":"bool true if startup of robot succeeded","title":"Returns"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.stop","text":"To be called once before exiting a program Cleanly stops down motion and communication","title":"stop()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.get_status","text":"Thread safe and atomic read of current Robot status data Returns as a dict.","title":"get_status()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.pretty_print","text":"","title":"pretty_print()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.push_command","text":"Cause all queued up RPC commands to be sent down to Devices","title":"push_command()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.is_trajectory_active","text":"","title":"is_trajectory_active()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.follow_trajectory","text":"","title":"follow_trajectory()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.stop_trajectory","text":"","title":"stop_trajectory()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.is_calibrated","text":"Returns true if homing-calibration has been run all joints that require it","title":"is_calibrated()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.get_stow_pos","text":"Return the stow position of a joint. Allow the end_of_arm to override the defaults in order to accomodate stowing different tools","title":"get_stow_pos()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.stow","text":"Cause the robot to move to its stow position Blocking.","title":"stow()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.robot.Robot.home","text":"Cause the robot to home its joints by moving to hardstops Blocking.","title":"home()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#the-device-class","text":"The stretch_body library is modular in design. Each subcomponent of Stretch is defined in its class and the Robot class provides an interface that ties all of these classes together. This modularity allows users to plug in new/modified subcomponents into the Robot interface by extending the Device class. It is possible to interface with a single subcomponent of Stretch by initializing its device class directly. In this section, we'll look at the API of seven subclasses of the Device class: the Arm , Lift , Base , Head , EndOfArm , Wacc , and Pimu subcomponents of Stretch.","title":"The Device Class"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#using-the-arm-class","text":"The interface to Stretch's telescoping arm is the Arm class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 10 import stretch_body.arm a = stretch_body . arm . Arm () a . motor . disable_sync_mode () if not a . startup (): exit () # failed to start arm! a . home () # interact with the arm here Since both Arm and Robot are subclasses of the Device class, the same startup() and stop() methods are available here, as well as other Device methods such as home() . Using the Arm class, we can read the arm's current state and send commands to the joint. For example, continuing the example above: 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 starting_position = a . status [ 'pos' ] # move out by 10cm a . move_to ( starting_position + 0.1 ) a . push_command () a . motor . wait_until_at_setpoint () # move back to starting position quickly a . move_to ( starting_position , v_m = 0.2 , a_m = 0.25 ) a . push_command () a . motor . wait_until_at_setpoint () a . move_by ( 0.1 ) # move out by 10cm a . push_command () a . motor . wait_until_at_setpoint () The move_to() and move_by() methods queue absolute and relative position commands to the arm, respectively, while the nonblocking push_command() method pushes the queued position commands to the hardware for execution. The attribute motor , an instance of the Stepper class, has the method wait_until_at_setpoint() which blocks program execution until the joint reaches the commanded goal. With firmware P1 or greater installed, it is also possible to queue a waypoint trajectory for the arm to follow: 26 27 28 29 30 31 32 33 34 35 36 starting_position = a . status [ 'pos' ] # queue a trajectory consisting of four waypoints a . trajectory . add ( t_s = 0 , x_m = starting_position ) a . trajectory . add ( t_s = 3 , x_m = 0.15 ) a . trajectory . add ( t_s = 6 , x_m = 0.1 ) a . trajectory . add ( t_s = 9 , x_m = 0.2 ) # trigger trajectory execution a . follow_trajectory () import time ; time . sleep ( 9 ) The attribute trajectory , an instance of the PrismaticTrajectory class, has the method add() which adds a single waypoint in a linear sliding trajectory. For a well-formed trajectory (see is_valid() ), the follow_trajectory() method starts tracking the trajectory for the telescoping arm. It is also possible to dynamically restrict the arm joint range: 37 38 39 40 41 42 43 44 45 46 47 48 49 50 range_upper_limit = 0.3 # meters # set soft limits on arm's range a . set_soft_motion_limit_min ( 0 ) a . set_soft_motion_limit_max ( range_upper_limit ) a . push_command () # command the arm outside the valid range a . move_to ( 0.4 ) a . push_command () a . motor . wait_until_at_setpoint () print ( a . status [ 'pos' ]) # we should expect to see ~0.3 a . stop () The set_soft_motion_limit_min/max() methods form the basis of an experimental self-collision avoidance system built into Stretch Body. All methods in the Arm class are documented below.","title":"Using the Arm class"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.arm.Arm","text":"API to the Stretch Arm","title":"Arm"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.arm.Arm.__init__","text":"","title":"__init__()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.arm.Arm.motor_rad_to_translate_m","text":"","title":"motor_rad_to_translate_m()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.arm.Arm.translate_m_to_motor_rad","text":"","title":"translate_m_to_motor_rad()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.arm.Arm.home","text":"end_pos: position to end on to_positive_stop: -- True: Move to the positive direction stop and mark to range_m[1] -- False: Move to the negative direction stop and mark to range_m[0] measuring: After homing to stop, move to opposite stop and report back measured distance return measured range-of-motion if measuring. Return None if not a valide measurement","title":"home()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#using-the-lift-class","text":"The interface to Stretch's lift is the Lift class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 10 import stretch_body.lift l = stretch_body . lift . Lift () l . motor . disable_sync_mode () if not l . startup (): exit () # failed to start lift! l . home () # interact with the lift here The startup() and home() methods are extended from the Device class. Reading the lift's current state and sending commands to the joint occurs similarly to the Arm class: 11 12 13 14 15 16 starting_position = l . status [ 'pos' ] # move up by 10cm l . move_to ( starting_position + 0.1 ) l . push_command () l . motor . wait_until_at_setpoint () The attribute status is a dictionary of the joint's current status. This state information is updated in the background in real-time by default (disable by initializing as startup(threading=False) ). Use the pretty_print() method to print out this state info in a human-interpretable format. Setting up waypoint trajectories for the lift is also similar to the Arm : 17 18 19 20 21 22 23 24 25 26 starting_position = l . status [ 'pos' ] # queue a trajectory consisting of three waypoints l . trajectory . add ( t_s = 0 , x_m = starting_position , v_m = 0.0 ) l . trajectory . add ( t_s = 3 , x_m = 0.5 , v_m = 0.0 ) l . trajectory . add ( t_s = 6 , x_m = 0.6 , v_m = 0.0 ) # trigger trajectory execution l . follow_trajectory () import time ; time . sleep ( 6 ) The attribute trajectory is also an instance of the PrismaticTrajectory class, and by providing the instantaneous velocity argument v_m to the add() method, a cubic spline can be loaded into the joint's trajectory . The call to follow_trajectory() begins hardware tracking of the spline. Finally, setting soft motion limits for the lift's range can be done using: 27 28 29 30 31 32 # cut out 0.2m from the top and bottom of the lift's range l . set_soft_motion_limit_min ( 0.2 ) l . set_soft_motion_limit_max ( 0.8 ) l . push_command () l . stop () The set_soft_motion_limit_min/max() methods perform clipping of the joint's range at the firmware level (can persist across reboots). All methods in the Lift class are documented below.","title":"Using the Lift class"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.lift.Lift","text":"API to the Stretch Lift","title":"Lift"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.lift.Lift.__init__","text":"","title":"__init__()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.lift.Lift.motor_rad_to_translate_m","text":"","title":"motor_rad_to_translate_m()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.lift.Lift.translate_m_to_motor_rad","text":"","title":"translate_m_to_motor_rad()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.lift.Lift.home","text":"end_pos: position to end on to_positive_stop: -- True: Move to the positive direction stop and mark to range_m[1] -- False: Move to the negative direction stop and mark to range_m[0] measuring: After homing to stop, move to opposite stop and report back measured distance return measured range-of-motion if measuring. Return None if not a valide measurement","title":"home()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#using-the-base-class","text":"Item Notes A Drive wheels 4 inch diameter, urethane rubber shore 60A B Cliff sensors Sharp GP2Y0A51SK0F, Analog, range 2-15 cm C Mecanum wheel Diameter 50mm The interface to Stretch's mobile base is the Base class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.base b = stretch_body . base . Base () b . left_wheel . disable_sync_mode () b . right_wheel . disable_sync_mode () if not b . startup (): exit () # failed to start base! # interact with the base here Stretch's mobile base is a differential drive configuration. The left and right wheels are accessible through Base left_wheel and right_wheel attributes, both of which are instances of the Stepper class. The startup() method is extended from the Device class. Since the mobile base is unconstrained, there is no homing method. The pretty_print() method prints out mobile base state information in a human-interpretable format. We can read the base's current state and send commands using: 10 11 12 13 14 15 16 17 18 19 20 b . pretty_print () # translate forward by 10cm b . translate_by ( 0.1 ) b . push_command () b . left_wheel . wait_until_at_setpoint () # rotate counter-clockwise by 90 degrees b . rotate_by ( 1.57 ) b . push_command () b . left_wheel . wait_until_at_setpoint () The translate_by() and rotate_by() methods send relative commands similar to the way move_by() behaves for the single degree of freedom joints. The mobile base also supports velocity control: 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # command the base to translate forward at 5cm / second b . set_translate_velocity ( 0.05 ) b . push_command () import time ; time . sleep ( 1 ) # command the base to rotate counter-clockwise at 0.1rad / second b . set_rotational_velocity ( 0.1 ) b . push_command () time . sleep ( 1 ) # command the base with translational and rotational velocities b . set_velocity ( 0.05 , 0.1 ) b . push_command () time . sleep ( 1 ) # stop base motion b . enable_freewheel_mode () b . push_command () The set_translate_velocity() and set_rotational_velocity() methods give velocity control over the translational and rotational components of the mobile base independently. The set_velocity() method gives control over both of these components simultaneously. To halt motion, you can command zero velocities or command the base into freewheel mode using enable_freewheel_mode() . The mobile base also supports waypoint trajectory following, but the waypoints are part of the SE2 group, where a desired waypoint is defined as an (x, y) point and a theta orientation: 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # reset odometry calculation b . first_step = True b . pull_status () # queue a trajectory consisting of three waypoints b . trajectory . add ( time = 0 , x = 0.0 , y = 0.0 , theta = 0.0 ) b . trajectory . add ( time = 3 , x = 0.1 , y = 0.0 , theta = 0.0 ) b . trajectory . add ( time = 6 , x = 0.0 , y = 0.0 , theta = 0.0 ) # trigger trajectory execution b . follow_trajectory () import time ; time . sleep ( 6 ) print ( b . status [ 'x' ], b . status [ 'y' ], b . status [ 'theta' ]) # we should expect to see around (0.0, 0.0, 0.0 or 6.28) b . stop () Warning The Base waypoint trajectory following has no notion of obstacles in the environment. It will blindly follow the commanded waypoints. For obstacle avoidance, we recommend employing perception and a path planner. The attribute trajectory is an instance of the DiffDriveTrajectory class. The call to follow_trajectory() begins hardware tracking of the spline. All methods of the Base class are documented below.","title":"Using the Base class"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base","text":"API to the Stretch Mobile Base","title":"Base"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.__init__","text":"","title":"__init__()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.startup","text":"Starts machinery required to interface with this device","title":"startup()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.startup--parameters","text":"threaded : bool whether a thread manages hardware polling/pushing in the background","title":"Parameters"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.startup--returns","text":"bool whether the startup procedure succeeded","title":"Returns"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.stop","text":"Shuts down machinery started in startup()","title":"stop()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.pretty_print","text":"","title":"pretty_print()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.enable_freewheel_mode","text":"Force motors into freewheel","title":"enable_freewheel_mode()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.enable_pos_incr_mode","text":"Force motors into incremental position mode","title":"enable_pos_incr_mode()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.wait_for_contact","text":"","title":"wait_for_contact()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.wait_until_at_setpoint","text":"","title":"wait_until_at_setpoint()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.contact_thresh_to_motor_current","text":"This model converts from a specified percentage effort (-100 to 100) of translate/rotational effort to motor currents","title":"contact_thresh_to_motor_current()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.translate_by","text":"Incremental translation of the base x_m: desired motion (m) v_m: velocity for trapezoidal motion profile (m/s) a_m: acceleration for trapezoidal motion profile (m/s^2) stiffness: stiffness of motion. Range 0.0 (min) to 1.0 (max) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100))","title":"translate_by()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.rotate_by","text":"Incremental rotation of the base x_r: desired motion (radians) v_r: velocity for trapezoidal motion profile (rad/s) a_r: acceleration for trapezoidal motion profile (rad/s^2) stiffness: stiffness of motion. Range 0.0 (min) to 1.0 (max) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100))","title":"rotate_by()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.set_translate_velocity","text":"Command the bases translational velocity. Use care to prevent collisions / avoid runaways v_m: desired velocity (m/s) a_m: acceleration of motion profile (m/s^2) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100))","title":"set_translate_velocity()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.set_rotational_velocity","text":"Command the bases rotational velocity. Use care to prevent collisions / avoid runaways v_r: desired rotational velocity (rad/s) a_r: acceleration of motion profile (rad/s^2) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100))","title":"set_rotational_velocity()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.set_velocity","text":"Command the bases translational and rotational velocities simultaneously. Use care to prevent collisions / avoid runaways v_m: desired velocity (m/s) w_r: desired rotational velocity (rad/s) a: acceleration of motion profile (m/s^2 and rad/s^2) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100))","title":"set_velocity()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.follow_trajectory","text":"Starts executing a waypoint trajectory self.trajectory must be populated with a valid trajectory before calling this method.","title":"follow_trajectory()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.follow_trajectory--parameters","text":"v_r : float velocity limit for trajectory in motor space in meters per second a_r : float acceleration limit for trajectory in motor space in meters per second squared stiffness : float stiffness of motion. Range 0.0 (min) to 1.0 (max) contact_thresh_N: (deprecated) effort to stop at (units of pseudo_N) contact_thresh: effort to stop at (units of effort_pct (-100, 100))","title":"Parameters"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.is_trajectory_active","text":"","title":"is_trajectory_active()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.get_trajectory_ts","text":"","title":"get_trajectory_ts()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.get_trajectory_time_remaining","text":"","title":"get_trajectory_time_remaining()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.update_trajectory","text":"Updates hardware with the next segment of self.trajectory This method must be called frequently to enable complete trajectory execution and preemption of future segments. If used with stretch_body.robot.Robot or with self.startup(threaded=True) , a background thread is launched for this. Otherwise, the user must handle calling this method.","title":"update_trajectory()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.stop_trajectory","text":"Stop waypoint trajectory immediately and resets hardware","title":"stop_trajectory()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.step_sentry","text":"Only allow fast mobile base motion if the lift is low, the arm is retracted, and the wrist is stowed. This is intended to keep the center of mass low for increased stability and avoid catching the arm or tool on something.","title":"step_sentry()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.push_command","text":"","title":"push_command()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.pull_status","text":"Computes base odometery based on stepper positions / velocities","title":"pull_status()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.motor_current_to_translation_force","text":"","title":"motor_current_to_translation_force()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.motor_current_to_rotation_torque","text":"","title":"motor_current_to_rotation_torque()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.translation_force_to_motor_current","text":"","title":"translation_force_to_motor_current()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.rotation_torque_to_motor_current","text":"","title":"rotation_torque_to_motor_current()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.rotation_effort_pct_to_motor_current","text":"","title":"rotation_effort_pct_to_motor_current()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.translation_effort_pct_to_motor_current","text":"","title":"translation_effort_pct_to_motor_current()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.motor_current_to_translate_effort_pct","text":"","title":"motor_current_to_translate_effort_pct()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.motor_current_to_rotation_effort_pct","text":"","title":"motor_current_to_rotation_effort_pct()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.translate_to_motor_rad","text":"","title":"translate_to_motor_rad()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.motor_rad_to_translate","text":"","title":"motor_rad_to_translate()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.rotate_to_motor_rad","text":"","title":"rotate_to_motor_rad()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.motor_rad_to_rotate","text":"","title":"motor_rad_to_rotate()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.translation_to_rotation","text":"","title":"translation_to_rotation()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.base.Base.rotation_to_translation","text":"","title":"rotation_to_translation()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#using-the-head-class","text":"The interface to Stretch's head is the Head class. The head contains an Intel Realsense D435i depth camera. The pan and tilt joints in the head allow Stretch to swivel and capture depth imagery of its surrounding. The head is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.head h = stretch_body . head . Head () if not h . startup (): exit () # failed to start head! h . home () # interact with the head here Head is a subclass of the DynamixelXChain class, which in turn is a subclass of the Device class. Therefore, some of Head's methods, such as startup() and home() are extended from the Device class, while others come from the DynamixelXChain class. Reading the head's current state and sending commands to its revolute joints (head pan and tilt) can be achieved using: 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 starting_position = h . status [ 'head_pan' ][ 'pos' ] # look right by 90 degrees h . move_to ( 'head_pan' , starting_position + 1.57 ) h . get_joint ( 'head_pan' ) . wait_until_at_setpoint () # tilt up by 30 degrees h . move_by ( 'head_tilt' , - 1.57 / 3 ) h . get_joint ( 'head_tilt' ) . wait_until_at_setpoint () # look down towards the wheels h . pose ( 'wheels' ) import time ; time . sleep ( 3 ) # look ahead h . pose ( 'ahead' ) time . sleep ( 3 ) The attribute status is a dictionary of dictionaries, where each subdictionary is the status of one of the head's joints. This state information is updated in the background in real-time by default (disable by initializing as startup(threading=False) ). Use the pretty_print() method to print out this state information in a human-interpretable format. Commanding the head's revolute joints is done through the move_to() and move_by() methods. Notice that, unlike the previous joints, no push command call is required here. These joints are Dynamixel servos, which behave differently than the Hello Robot steppers. Their commands are not queued and are executed as soon as they're received. Head's two joints, the 'head_pan' and 'head_tilt' are instances of the DynamixelHelloXL430 class and are retrievable using the get_joint() method. They have the wait_until_at_setpoint() method, which blocks program execution until the joint reaches the commanded goal. The pose() method makes it easy to command the head to common head poses (e.g. looking 'ahead', at the end-of-arm 'tool', obstacles in front of the 'wheels', or 'up'). The head supports waypoint trajectories as well: 27 28 29 30 31 32 33 34 35 36 37 # queue a trajectory consisting of three waypoints h . get_joint ( 'head_tilt' ) . trajectory . add ( t_s = 0 , x_r = 0.0 ) h . get_joint ( 'head_tilt' ) . trajectory . add ( t_s = 3 , x_r =- 1.0 ) h . get_joint ( 'head_tilt' ) . trajectory . add ( t_s = 6 , x_r = 0.0 ) h . get_joint ( 'head_pan' ) . trajectory . add ( t_s = 0 , x_r = 0.1 ) h . get_joint ( 'head_pan' ) . trajectory . add ( t_s = 3 , x_r =- 0.9 ) h . get_joint ( 'head_pan' ) . trajectory . add ( t_s = 6 , x_r = 0.1 ) # trigger trajectory execution h . follow_trajectory () import time ; time . sleep ( 6 ) The head pan and tilt DynamixelHelloXL430 instances have an attribute trajectory , which is an instance of the RevoluteTrajectory class. The call to follow_trajectory() begins software tracking of the spline. Finally, setting soft motion limits for the head's pan and tilt range can be achieved using: 38 39 40 41 42 43 44 45 46 # clip the head_pan's range h . get_joint ( 'head_pan' ) . set_soft_motion_limit_min ( - 1.0 ) h . get_joint ( 'head_pan' ) . set_soft_motion_limit_max ( 1.0 ) # clip the head_tilt's range h . get_joint ( 'head_tilt' ) . set_soft_motion_limit_min ( - 1.0 ) h . get_joint ( 'head_tilt' ) . set_soft_motion_limit_max ( 0.1 ) h . stop () The set_soft_motion_limit_min/max() methods perform clipping of the joint's range at the software level (cannot persist across reboots). All methods of the Head class are documented below.","title":"Using the Head class"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head","text":"API to the Stretch RE1 Head","title":"Head"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.__init__","text":"","title":"__init__()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.startup","text":"Starts machinery required to interface with this device","title":"startup()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.startup--parameters","text":"threaded : bool whether a thread manages hardware polling/pushing in the background","title":"Parameters"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.startup--returns","text":"bool whether the startup procedure succeeded","title":"Returns"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.get_joint","text":"Retrieves joint by name.","title":"get_joint()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.get_joint--parameters","text":"joint_name : str valid joints defined in joints","title":"Parameters"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.get_joint--returns","text":"DynamixelHelloXL430 or None Motor object on valid joint name, else None","title":"Returns"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.move_to","text":"joint: Name of the joint to move ('head_pan' or 'head_tilt') x_r: commanded absolute position (radians). v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2)","title":"move_to()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.move_by","text":"joint: Name of the joint to move ('head_pan' or 'head_tilt') x_r: commanded incremental motion (radians). v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2)","title":"move_by()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.home","text":"","title":"home()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.head.Head.pose","text":"p: Dictionary key to named pose (eg 'ahead') v_r: list, velocities for trapezoidal motion profile (rad/s). a_r: list, accelerations for trapezoidal motion profile (rad/s^2)","title":"pose()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#using-the-endofarm-class","text":"The interface to Stretch's end-of-arm is the EndOfArm class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.end_of_arm e = stretch_body . end_of_arm . EndOfArm () if not e . startup ( threaded = True ): exit () # failed to start end of arm! # interact with the end of arm here e . stop () All methods of the EndOfArm class are documented below.","title":"Using the EndOfArm class"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm","text":"The EndOfArm class allows for an extensible serial chain of Dynamixel X series devices It allows the specific type of device to be declared at runtime via the Yaml parameters In this way, a user can add their own custom Dynamixel based tools to the robot end-of-arm by simply deriving it from DynamixelHelloXL430 and declaring the class name / Python module name in the User YAML file","title":"EndOfArm"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.__init__","text":"","title":"__init__()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.startup","text":"Starts machinery required to interface with this device","title":"startup()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.startup--parameters","text":"threaded : bool whether a thread manages hardware polling/pushing in the background","title":"Parameters"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.startup--returns","text":"bool whether the startup procedure succeeded","title":"Returns"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.get_joint","text":"Retrieves joint by name.","title":"get_joint()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.get_joint--parameters","text":"joint_name : str valid joints defined as defined in params['devices']","title":"Parameters"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.get_joint--returns","text":"DynamixelHelloXL430 or None Motor object on valid joint name, else None","title":"Returns"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.move_to","text":"joint: name of joint (string) x_r: commanded absolute position (radians). v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2)","title":"move_to()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.move_by","text":"joint: name of joint (string) x_r: commanded incremental motion (radians). v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2)","title":"move_by()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.pose","text":"joint: name of joint (string) p: named pose of joint v_r: velocity for trapezoidal motion profile (rad/s). a_r: acceleration for trapezoidal motion profile (rad/s^2)","title":"pose()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.stow","text":"","title":"stow()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.home","text":"Home to hardstops","title":"home()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.end_of_arm.EndOfArm.is_tool_present","text":"Return true if the given tool type is present (eg. StretchGripper) Allows for conditional logic when switching end-of-arm tools","title":"is_tool_present()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#using-the-wacc-class","text":"The interface to Stretch's wrist board is the Wacc (wrist + accelerometer) class. This board provides an Arduino and accelerometer sensor that is accessible from the Wacc class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.wacc w = stretch_body . wacc . Wacc () if not w . startup ( threaded = True ): exit () # failed to start wacc! # interact with the wacc here w . stop () All methods of the Wacc class are documented below.","title":"Using the Wacc class"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.wacc.Wacc","text":"API to the Stretch Wrist Accelerometer (Wacc) Board","title":"Wacc"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.wacc.Wacc.__init__","text":"","title":"__init__()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.wacc.Wacc.startup","text":"First determine which protocol version the uC firmware is running. Based on that version, replaces PimuBase class inheritance with a inheritance to a child class of PimuBase that supports that protocol","title":"startup()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#using-the-pimu-class","text":"The interface to Stretch's power board is the Pimu (power + IMU) class. This board provides a 9-DOF IMU that is accessible from the Pimu class. It is typically initialized as: 1 2 3 4 5 6 7 8 9 import stretch_body.pimu p = stretch_body . pimu . Pimu () if not p . startup ( threaded = True ): exit () # failed to start pimu! # interact with the pimu here p . stop () All methods of the Pimu class are documented below.","title":"Using the Pimu class"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.pimu.Pimu","text":"API to the Stretch Power and IMU board (Pimu)","title":"Pimu"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.pimu.Pimu.__init__","text":"","title":"__init__()"},{"location":"stretch-tutorials/stretch_body/tutorial_stretch_body_api/#stretch_body.pimu.Pimu.startup","text":"First determine which protocol version the uC firmware is running. Based on that version, replaces PimuBase class inheritance with a inheritance to a child class of PimuBase that supports that protocol All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"startup()"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/","text":"Tutorial: Tool Change Many users will want to work with tools other than the default Stretch Gripper that ships with the robot. In this tutorial, you will learn how to configure the Stretch software interfaces to support other tools. Changing Tool Interfaces in Stretch Body Stretch Body supports a plug-in-based architecture for tools. A tool is an extension of the EndOfArm class that supports additional degrees of freedom. Standard Tools Stretch Body supports two tool interfaces by default: The ToolNone & ToolStretchGripper . We will explore swapping between these default tools. ToolStretchGripper Stretch is configured to load the ToolStretchGripper interface by default. This tool is loaded according to the robot.tool parameter: stretch_params.py | grep robot.tool stretch_body.robot_params.nominal_params param.robot.tool tool_stretch_gripper We can interact with this tool from iPython ipython In [ 1 ]: import stretch_body.robot as robot In [ 2 ]: r = robot . Robot () In [ 3 ]: r . startup () In [ 4 ]: r . end_of_arm Out [ 4 ]: < stretch_body . end_of_arm_tools . ToolStretchGripper instance at 0x7f99109155a0 > In [ 5 ]: r . end_of_arm . motors Out [ 5 ]: { 'stretch_gripper' : < stretch_body . stretch_gripper . StretchGripper instance at 0x7f99109159b0 > , 'wrist_yaw' : < stretch_body . wrist_yaw . WristYaw instance at 0x7f9910915820 > } In [ 6 ]: r . end_of_arm . stow () --------- Stowing Wrist Yaw ---- --------- Stowing Gripper ---- In [ 7 ]: r . stop () ToolNone The ToolNone interface can be loaded when no tool is attached to the Wrist Yaw joint. To switch to this interface, simply update the field in your stretch_re1_user_params.yaml to: robot : tool : tool_none After updating the YAML we can interact with the ToolNone via iPython In [ 1 ]: import stretch_body.robot as robot In [ 2 ]: r = robot . Robot () In [ 3 ]: r . startup () In [ 4 ]: r . end_of_arm Out [ 4 ]: < stretch_body . end_of_arm_tools . ToolNone instance at 0x7f245f786fa0 > In [ 5 ]: r . end_of_arm . motors Out [ 5 ]: { 'wrist_yaw' : < stretch_body . wrist_yaw . WristYaw instance at 0x7f245e69e410 > } In [ 6 ]: r . end_of_arm . stow () --------- Stowing Wrist Yaw ---- In [ 7 ]: r . stop () Loading Tool Interfaces from Stretch Tool Share The Stretch Tool Share is an open Git repository for non-standard Stretch tools. It hosts the CAD, URDF, and Python files needed to integrate these tools with your robot. To use Stretch Tool Share tools, first update your installation: pip install -U hello-robot-stretch-tool-share As an example, we see on the Tool Share that there is a tool, the ToolDryEraseToolHolderV1 which extends the EndOfArm class. To load this tool interface, modify your stretch_user_params.yaml to load the tool as before. We will also need to tell it where to find the tool's parameter file : robot : tool : tool_dry_erase_holder_v1 params : - stretch_tool_share.dry_erase_holder_v1.params We can now interact with the tool in iPython: In [ 1 ]: import stretch_body.robot as robot In [ 2 ]: r = robot . Robot () In [ 3 ]: r . startup () In [ 4 ]: r . end_of_arm Out [ 4 ]: < stretch_tool_share . dry_erase_holder_v1 . tool . ToolDryEraseHolderV1 instance at 0x7f3b61c17f00 > In [ 5 ]: r . end_of_arm . motors Out [ 5 ]: { 'wrist_yaw' : < stretch_body . wrist_yaw . WristYaw instance at 0x7f3b61c59280 > } In [ 6 ]: r . end_of_arm . stow () --------- Stowing Wrist Yaw ---- Changing Tool Interfaces in Stretch ROS Next, we'll see how to change the ROS interface for a tool. Here we will continue with the ToolDryEraseHolderV1 example. First, configure Stretch Body to use the tool as in the previous exercise. Next, ensure ROS is up to date: cd ~/catkin_ws/src/stretch_ros/ git pull To access the URDF data for the ToolDryEraseHolderV1 we'll need to clone the Tool Share repository: cd ~/repos git clone https://github.com/hello-robot/stretch_tool_share Copy the tool's URDF data into the Stretch ROS repository: cd ~/repos/stretch_tool_share/tool_share/dry_erase_holder_v1 cp stretch_description/urdf/*.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf/ cp stretch_description/meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes/ Now we will update the tool Xacro for Stretch. Open the file ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro in an editor. Comment out the current tool Xacro and include the Xacro for the dry-erase holder. <?xml version=\"1.0\"?> <robot xmlns:xacro= \"http://www.ros.org/wiki/xacro\" name= \"stretch_description\" > <!--<xacro:include filename=\"stretch_gripper.xacro\" />--> <xacro:include filename= \"stretch_dry_erase_marker.xacro\" /> <xacro:include filename= \"stretch_main.xacro\" /> <xacro:include filename= \"stretch_aruco.xacro\" /> <xacro:include filename= \"stretch_d435i.xacro\" /> <xacro:include filename= \"stretch_laser_range_finder.xacro\" /> <xacro:include filename= \"stretch_respeaker.xacro\" /> </robot> Finally, we'll update the calibrated URDF to use this new tool: cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp stretch.urdf stretch.urdf.bak rosrun stretch_calibration update_urdf_after_xacro_change.sh Press Ctrl-C when the rosrun command terminates and you're ready to visualize the tool in RViz: roslaunch stretch_calibration simple_test_head_calibration.launch Advanced Topics Understanding How the Tool Plug-In Works For users looking to create their custom tools, it can be useful to understand how the tool plug-in architecture works. Here we will walk through the basics of the system for both Stretch Body and Stretch ROS Stretch Body The Robot class expects an instance of the EndOfArm tool to be present. The EndOfArm tool is an extension of the DynamixelXChain class, which manages a chain of Dynamixel servos. A tool is defined via its parameters (either in user YAML or Python). For example, the ToolStretchGripper is defined in robot_params.py . These parameters tell the plug-in which DynamixelHelloXL430 instances to load and manage. Here we see: \"tool_stretch_gripper\" : { 'use_group_sync_read' : 1 , 'retry_on_comm_failure' : 1 , 'baud' : 115200 , 'verbose' : 0 , 'py_class_name' : 'ToolStretchGripper' , 'py_module_name' : 'stretch_body.end_of_arm_tools' , 'stow' : { 'stretch_gripper' : 0 , 'wrist_yaw' : 3.4 }, 'devices' : { 'stretch_gripper' : { 'py_class_name' : 'StretchGripper' , 'py_module_name' : 'stretch_body.stretch_gripper' }, 'wrist_yaw' : { 'py_class_name' : 'WristYaw' , 'py_module_name' : 'stretch_body.wrist_yaw' } } }, This dictionary defines a tool of the class ToolStretchGripper with two DynamixelHelloXL430 devices on its bus (StretchGripper and WristYaw). We see that the ToolStretchGripper class extends the EndOfArm class and provides its stowing behavior: class ToolStretchGripper ( EndOfArm ): def __init__ ( self , name = 'tool_stretch_gripper' ): EndOfArm . __init__ ( self , name ) def stow ( self ): # Fold in wrist and gripper print ( '--------- Stowing Wrist Yaw ----' ) self . move_to ( 'wrist_yaw' , self . params [ 'stow' ][ 'wrist_yaw' ]) print ( '--------- Stowing Gripper ----' ) self . move_to ( 'stretch_gripper' , self . params [ 'stow' ][ 'stretch_gripper' ]) For tools that are not a part of Stretch Body, such as from the Tool Share, you must include the tool parameters as well in your stretch_user_params.yaml . A robot that supports many tools may have a user YAML that looks like: params : - stretch_tool_share.usbcam_wrist_v1.params - stretch_tool_share.stretch_dex_wrist_beta.params - stretch_tool_share.dry_erase_holder_v1.params robot : tool : tool_dry_erase_holder_v1 #tool: tool_none #tool: tool_stretch_gripper #tool: tool_usbcam_wrist_v1 #tool: tool_stretch_dex_wrist_beta Tip For a more complex implementation of a tool, we recommend reviewing the Stretch Dex Wrist implementation on the Stretch Tool Share. Stretch ROS Stretch ROS also supports the tool plug-in architecture. Under ROS this is managed by extending the SimpleCommandGroup . More coming soon. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Changing Tools"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#tutorial-tool-change","text":"Many users will want to work with tools other than the default Stretch Gripper that ships with the robot. In this tutorial, you will learn how to configure the Stretch software interfaces to support other tools.","title":"Tutorial: Tool Change"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#changing-tool-interfaces-in-stretch-body","text":"Stretch Body supports a plug-in-based architecture for tools. A tool is an extension of the EndOfArm class that supports additional degrees of freedom.","title":"Changing Tool Interfaces in Stretch Body"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#standard-tools","text":"Stretch Body supports two tool interfaces by default: The ToolNone & ToolStretchGripper . We will explore swapping between these default tools.","title":"Standard Tools"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#toolstretchgripper","text":"Stretch is configured to load the ToolStretchGripper interface by default. This tool is loaded according to the robot.tool parameter: stretch_params.py | grep robot.tool stretch_body.robot_params.nominal_params param.robot.tool tool_stretch_gripper We can interact with this tool from iPython ipython In [ 1 ]: import stretch_body.robot as robot In [ 2 ]: r = robot . Robot () In [ 3 ]: r . startup () In [ 4 ]: r . end_of_arm Out [ 4 ]: < stretch_body . end_of_arm_tools . ToolStretchGripper instance at 0x7f99109155a0 > In [ 5 ]: r . end_of_arm . motors Out [ 5 ]: { 'stretch_gripper' : < stretch_body . stretch_gripper . StretchGripper instance at 0x7f99109159b0 > , 'wrist_yaw' : < stretch_body . wrist_yaw . WristYaw instance at 0x7f9910915820 > } In [ 6 ]: r . end_of_arm . stow () --------- Stowing Wrist Yaw ---- --------- Stowing Gripper ---- In [ 7 ]: r . stop ()","title":"ToolStretchGripper"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#toolnone","text":"The ToolNone interface can be loaded when no tool is attached to the Wrist Yaw joint. To switch to this interface, simply update the field in your stretch_re1_user_params.yaml to: robot : tool : tool_none After updating the YAML we can interact with the ToolNone via iPython In [ 1 ]: import stretch_body.robot as robot In [ 2 ]: r = robot . Robot () In [ 3 ]: r . startup () In [ 4 ]: r . end_of_arm Out [ 4 ]: < stretch_body . end_of_arm_tools . ToolNone instance at 0x7f245f786fa0 > In [ 5 ]: r . end_of_arm . motors Out [ 5 ]: { 'wrist_yaw' : < stretch_body . wrist_yaw . WristYaw instance at 0x7f245e69e410 > } In [ 6 ]: r . end_of_arm . stow () --------- Stowing Wrist Yaw ---- In [ 7 ]: r . stop ()","title":"ToolNone"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#loading-tool-interfaces-from-stretch-tool-share","text":"The Stretch Tool Share is an open Git repository for non-standard Stretch tools. It hosts the CAD, URDF, and Python files needed to integrate these tools with your robot. To use Stretch Tool Share tools, first update your installation: pip install -U hello-robot-stretch-tool-share As an example, we see on the Tool Share that there is a tool, the ToolDryEraseToolHolderV1 which extends the EndOfArm class. To load this tool interface, modify your stretch_user_params.yaml to load the tool as before. We will also need to tell it where to find the tool's parameter file : robot : tool : tool_dry_erase_holder_v1 params : - stretch_tool_share.dry_erase_holder_v1.params We can now interact with the tool in iPython: In [ 1 ]: import stretch_body.robot as robot In [ 2 ]: r = robot . Robot () In [ 3 ]: r . startup () In [ 4 ]: r . end_of_arm Out [ 4 ]: < stretch_tool_share . dry_erase_holder_v1 . tool . ToolDryEraseHolderV1 instance at 0x7f3b61c17f00 > In [ 5 ]: r . end_of_arm . motors Out [ 5 ]: { 'wrist_yaw' : < stretch_body . wrist_yaw . WristYaw instance at 0x7f3b61c59280 > } In [ 6 ]: r . end_of_arm . stow () --------- Stowing Wrist Yaw ----","title":"Loading Tool Interfaces from Stretch Tool Share"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#changing-tool-interfaces-in-stretch-ros","text":"Next, we'll see how to change the ROS interface for a tool. Here we will continue with the ToolDryEraseHolderV1 example. First, configure Stretch Body to use the tool as in the previous exercise. Next, ensure ROS is up to date: cd ~/catkin_ws/src/stretch_ros/ git pull To access the URDF data for the ToolDryEraseHolderV1 we'll need to clone the Tool Share repository: cd ~/repos git clone https://github.com/hello-robot/stretch_tool_share Copy the tool's URDF data into the Stretch ROS repository: cd ~/repos/stretch_tool_share/tool_share/dry_erase_holder_v1 cp stretch_description/urdf/*.xacro ~/catkin_ws/src/stretch_ros/stretch_description/urdf/ cp stretch_description/meshes/*.STL ~/catkin_ws/src/stretch_ros/stretch_description/meshes/ Now we will update the tool Xacro for Stretch. Open the file ~/catkin_ws/src/stretch_ros/stretch_description/urdf/stretch_description.xacro in an editor. Comment out the current tool Xacro and include the Xacro for the dry-erase holder. <?xml version=\"1.0\"?> <robot xmlns:xacro= \"http://www.ros.org/wiki/xacro\" name= \"stretch_description\" > <!--<xacro:include filename=\"stretch_gripper.xacro\" />--> <xacro:include filename= \"stretch_dry_erase_marker.xacro\" /> <xacro:include filename= \"stretch_main.xacro\" /> <xacro:include filename= \"stretch_aruco.xacro\" /> <xacro:include filename= \"stretch_d435i.xacro\" /> <xacro:include filename= \"stretch_laser_range_finder.xacro\" /> <xacro:include filename= \"stretch_respeaker.xacro\" /> </robot> Finally, we'll update the calibrated URDF to use this new tool: cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf cp stretch.urdf stretch.urdf.bak rosrun stretch_calibration update_urdf_after_xacro_change.sh Press Ctrl-C when the rosrun command terminates and you're ready to visualize the tool in RViz: roslaunch stretch_calibration simple_test_head_calibration.launch","title":"Changing Tool Interfaces in Stretch ROS"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#understanding-how-the-tool-plug-in-works","text":"For users looking to create their custom tools, it can be useful to understand how the tool plug-in architecture works. Here we will walk through the basics of the system for both Stretch Body and Stretch ROS","title":"Understanding How the Tool Plug-In Works"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#stretch-body","text":"The Robot class expects an instance of the EndOfArm tool to be present. The EndOfArm tool is an extension of the DynamixelXChain class, which manages a chain of Dynamixel servos. A tool is defined via its parameters (either in user YAML or Python). For example, the ToolStretchGripper is defined in robot_params.py . These parameters tell the plug-in which DynamixelHelloXL430 instances to load and manage. Here we see: \"tool_stretch_gripper\" : { 'use_group_sync_read' : 1 , 'retry_on_comm_failure' : 1 , 'baud' : 115200 , 'verbose' : 0 , 'py_class_name' : 'ToolStretchGripper' , 'py_module_name' : 'stretch_body.end_of_arm_tools' , 'stow' : { 'stretch_gripper' : 0 , 'wrist_yaw' : 3.4 }, 'devices' : { 'stretch_gripper' : { 'py_class_name' : 'StretchGripper' , 'py_module_name' : 'stretch_body.stretch_gripper' }, 'wrist_yaw' : { 'py_class_name' : 'WristYaw' , 'py_module_name' : 'stretch_body.wrist_yaw' } } }, This dictionary defines a tool of the class ToolStretchGripper with two DynamixelHelloXL430 devices on its bus (StretchGripper and WristYaw). We see that the ToolStretchGripper class extends the EndOfArm class and provides its stowing behavior: class ToolStretchGripper ( EndOfArm ): def __init__ ( self , name = 'tool_stretch_gripper' ): EndOfArm . __init__ ( self , name ) def stow ( self ): # Fold in wrist and gripper print ( '--------- Stowing Wrist Yaw ----' ) self . move_to ( 'wrist_yaw' , self . params [ 'stow' ][ 'wrist_yaw' ]) print ( '--------- Stowing Gripper ----' ) self . move_to ( 'stretch_gripper' , self . params [ 'stow' ][ 'stretch_gripper' ]) For tools that are not a part of Stretch Body, such as from the Tool Share, you must include the tool parameters as well in your stretch_user_params.yaml . A robot that supports many tools may have a user YAML that looks like: params : - stretch_tool_share.usbcam_wrist_v1.params - stretch_tool_share.stretch_dex_wrist_beta.params - stretch_tool_share.dry_erase_holder_v1.params robot : tool : tool_dry_erase_holder_v1 #tool: tool_none #tool: tool_stretch_gripper #tool: tool_usbcam_wrist_v1 #tool: tool_stretch_dex_wrist_beta Tip For a more complex implementation of a tool, we recommend reviewing the Stretch Dex Wrist implementation on the Stretch Tool Share.","title":"Stretch Body"},{"location":"stretch-tutorials/stretch_body/tutorial_tool_change/#stretch-ros","text":"Stretch ROS also supports the tool plug-in architecture. Under ROS this is managed by extending the SimpleCommandGroup . More coming soon. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks.","title":"Stretch ROS"},{"location":"stretch-tutorials/stretch_tool_share/","text":"Overview We designed Stretch's hardware to be easily extended. You can make your own tool and attach it to the wrist to creatively expand what Stretch can do. Your tool can also use Dynamixel X-series servos from Robotis via the provided TTL bus. In this tutorial, we provide examples of some of the tools that we've created. We've released them with a permissive Apache 2.0 license, so you're free to use them as you wish. We hope they'll inspire you to create your own. We also include URDF and mesh files for many of the tools in their stretch_description folder. See the Stretch ROS documentation for guidance on integrating these tools into your robot model. We'd love it if you shared your creations with the community. We recommend you create a GitHub repository for your own tools and then post an announcement to the forum to let people know about it. Tool Description Puller Attachment to pull drawers, handles or push buttons Dry Erase Holder Compliant attachment for drawing on white-boards DexWrist A 3-DoF upgrade for the standard gripper Updating URDF Updating the URDF after changing a tool Licenses The contents in this tutorial that represent parts of the Stretch robot, such as its head, arm, wrist, and default gripper, are covered by the CC BY-NC-SA 4.0 license. Please note that the Stretch robot and its default gripper are also covered by pending patents. Please see the robot license for details. Other contents in this tutorial created by Hello Robot Inc. that specifically pertain to the tools that attach to Stretch as accessories are covered by the Apache 2.0 license. Please see the tool license for details. The contents of this tutorial are intended for use with the Stretch mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc.","title":"Overview"},{"location":"stretch-tutorials/stretch_tool_share/#overview","text":"We designed Stretch's hardware to be easily extended. You can make your own tool and attach it to the wrist to creatively expand what Stretch can do. Your tool can also use Dynamixel X-series servos from Robotis via the provided TTL bus. In this tutorial, we provide examples of some of the tools that we've created. We've released them with a permissive Apache 2.0 license, so you're free to use them as you wish. We hope they'll inspire you to create your own. We also include URDF and mesh files for many of the tools in their stretch_description folder. See the Stretch ROS documentation for guidance on integrating these tools into your robot model. We'd love it if you shared your creations with the community. We recommend you create a GitHub repository for your own tools and then post an announcement to the forum to let people know about it. Tool Description Puller Attachment to pull drawers, handles or push buttons Dry Erase Holder Compliant attachment for drawing on white-boards DexWrist A 3-DoF upgrade for the standard gripper Updating URDF Updating the URDF after changing a tool","title":"Overview"},{"location":"stretch-tutorials/stretch_tool_share/#licenses","text":"The contents in this tutorial that represent parts of the Stretch robot, such as its head, arm, wrist, and default gripper, are covered by the CC BY-NC-SA 4.0 license. Please note that the Stretch robot and its default gripper are also covered by pending patents. Please see the robot license for details. Other contents in this tutorial created by Hello Robot Inc. that specifically pertain to the tools that attach to Stretch as accessories are covered by the Apache 2.0 license. Please see the tool license for details. The contents of this tutorial are intended for use with the Stretch mobile manipulator, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc.","title":"Licenses"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/","text":"Dex to Standard Wrist This tutorial will guide you through the steps of replacing a gripper with a Dex Wrist (3-DoF) for one with a Standard Wrist (1-DoF). Parts and Tools Required Please note that this procedure does not require any additional parts or tools apart from the ones that were shipped with the robot: 8 M2x6mm Torx FHCS bolts 4 M2.5x4mm Torx FHCS bolts 2 M2.5x8mm SHCS bolts T6 Torx wrench T8 Torx wrench 2mm Hex key Removing Dex Wrist Gripper Here we describe removing the Dex Wrist gripper. Please ensure that the robot is turned off before proceeding. First, inspect the parts and ensure that you have everything you need for the procedure. Now, remove the cable clip by unscrewing the M2.5x8mm bolts and then unplug the Dynamixel cable out of the wrist pitch servo (pink). Next, rotate the wrist yaw joint so that the wrist pitch servo body is accessible. Detach the pitch servo from the mounting bracket by unscrewing the four M2.5x4mm screws (C) with the T8 Torx wrench. Slide the wrist module out horizontally so that the bearing unmates from its post. Then, lower the wrist module vertically away from the mounting bracket. Lastly, detach the wrist mount bracket (A) from the bottom of the tool plate by removing the M2x6mm bolts (B) using a T6 Torx wrench. Attaching Standard Wrist Gripper Here we describe attaching the Standard Wrist gripper. First, note where the forward direction is on the wrist yaw tool plate. This is indicated by the additional alignment hole that is just outside the bolt pattern shown pointing down in the image. Then, route the Dynamixel cable through the center of the standard gripper mounting bracket and install the bracket with the eight screws and T6 Torx wrench. Make sure the forward marking on the bracket matches the forward marking on the wrist yaw. Now, affix the four screws, with the shorter two going to the servo side, to hold the gripper to the bracket. Lastly, route the Dynamixel cable through the back of the gripper and plug it securely into the servo. Software Instructions Once the hardware has been replaced, it's time to make the software changes for Stretch to recognize the Standart Wrist gripper. Turn on the robot and follow the instructions below. To revert the changes in stretch_configuration_params.yaml, download the dex_to_standard_configure_params.py script and execute it in a terminal as below: python3 dex_to_standard_configure_params.py Next, to ensure the correct gripper is recognized by ROS, we need to update the URDF. For this, first open the stretch_description.xacro file like below. cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf gedit stretch_description.xacro Then, replace the contents of the file with the default stretch_description.xacro . Lastly, to generate the updated URDF, execute the following commands in a terminal. rosrun stretch_calibration update_urdf_after_xacro_change.sh cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf ./export_urdf.sh You can ensure that the gripper is functional by homing the Dynamixel servos with the following commands: stretch_gripper_home.py stretch_wrist_yaw_home.py If you encounter any issues, please contact support.","title":"Dex to Standard Wrist"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/#dex-to-standard-wrist","text":"This tutorial will guide you through the steps of replacing a gripper with a Dex Wrist (3-DoF) for one with a Standard Wrist (1-DoF).","title":"Dex to Standard Wrist"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/#parts-and-tools-required","text":"Please note that this procedure does not require any additional parts or tools apart from the ones that were shipped with the robot: 8 M2x6mm Torx FHCS bolts 4 M2.5x4mm Torx FHCS bolts 2 M2.5x8mm SHCS bolts T6 Torx wrench T8 Torx wrench 2mm Hex key","title":"Parts and Tools Required"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/#removing-dex-wrist-gripper","text":"Here we describe removing the Dex Wrist gripper. Please ensure that the robot is turned off before proceeding. First, inspect the parts and ensure that you have everything you need for the procedure. Now, remove the cable clip by unscrewing the M2.5x8mm bolts and then unplug the Dynamixel cable out of the wrist pitch servo (pink). Next, rotate the wrist yaw joint so that the wrist pitch servo body is accessible. Detach the pitch servo from the mounting bracket by unscrewing the four M2.5x4mm screws (C) with the T8 Torx wrench. Slide the wrist module out horizontally so that the bearing unmates from its post. Then, lower the wrist module vertically away from the mounting bracket. Lastly, detach the wrist mount bracket (A) from the bottom of the tool plate by removing the M2x6mm bolts (B) using a T6 Torx wrench.","title":"Removing Dex Wrist Gripper"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/#attaching-standard-wrist-gripper","text":"Here we describe attaching the Standard Wrist gripper. First, note where the forward direction is on the wrist yaw tool plate. This is indicated by the additional alignment hole that is just outside the bolt pattern shown pointing down in the image. Then, route the Dynamixel cable through the center of the standard gripper mounting bracket and install the bracket with the eight screws and T6 Torx wrench. Make sure the forward marking on the bracket matches the forward marking on the wrist yaw. Now, affix the four screws, with the shorter two going to the servo side, to hold the gripper to the bracket. Lastly, route the Dynamixel cable through the back of the gripper and plug it securely into the servo.","title":"Attaching Standard Wrist Gripper"},{"location":"stretch-tutorials/stretch_tool_share/dex_to_standard/#software-instructions","text":"Once the hardware has been replaced, it's time to make the software changes for Stretch to recognize the Standart Wrist gripper. Turn on the robot and follow the instructions below. To revert the changes in stretch_configuration_params.yaml, download the dex_to_standard_configure_params.py script and execute it in a terminal as below: python3 dex_to_standard_configure_params.py Next, to ensure the correct gripper is recognized by ROS, we need to update the URDF. For this, first open the stretch_description.xacro file like below. cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf gedit stretch_description.xacro Then, replace the contents of the file with the default stretch_description.xacro . Lastly, to generate the updated URDF, execute the following commands in a terminal. rosrun stretch_calibration update_urdf_after_xacro_change.sh cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf ./export_urdf.sh You can ensure that the gripper is functional by homing the Dynamixel servos with the following commands: stretch_gripper_home.py stretch_wrist_yaw_home.py If you encounter any issues, please contact support.","title":"Software Instructions"},{"location":"stretch-tutorials/stretch_tool_share/dexwrist/","text":"Stretch Dex Wrist Created by : Hello Robot Inc The Stretch Dex Wrist is commercially available from Hello Robot. The following hardware guides are available: Stretch RE1 DexWrist Hardware Guide Stretch 2 DexWrist Hardware Guide Additional resources available here include: Gazebo Support URDF","title":"Dex Wrist"},{"location":"stretch-tutorials/stretch_tool_share/dexwrist/#stretch-dex-wrist","text":"Created by : Hello Robot Inc The Stretch Dex Wrist is commercially available from Hello Robot. The following hardware guides are available: Stretch RE1 DexWrist Hardware Guide Stretch 2 DexWrist Hardware Guide Additional resources available here include: Gazebo Support URDF","title":"Stretch Dex Wrist"},{"location":"stretch-tutorials/stretch_tool_share/dry_erase_holder/","text":"Dry Erase Holder Created by : Hello Robot Inc This tool allows Stretch to hold a dry erase marker. It is spring loaded, allowing for compliant interaction between the marker and a white board. The tool can be integrated into your robot URDF by integrating its stretch_description as described in the Stretch ROS documentation . Parts List Item Qty Vendor Expo Dry Erase 1 Amazon M5x50mm Hex Head Bolt 1 McMaster-Carr M5 Nut 2 McMaster-Carr wrist_end_cap_5mm 1 PLA 3D Printer dry_erase_bushing_block 1 PLA 3D Printer Size 30 Rubber Band 2 McMaster-Carr 3/4\" Shaft Collar 1 McMaster-Carr Assembly instructions View 3D assembly Install the bolt into the dry_erase_bushing block and secure from below with an M5 nut. Attach the dry erase bushing block to the tool plate, securing from below with the wrist_end_cap_5mm and an M5 nut. Orient the block so the marker points forward. Attach the shaft collar to your dry erase marker, approximately 8mm from the back of the marker. Slide the marker into the bushing block. Loop a rubber band around the back of the marker and over to one of the pegs on the side of the bushing block. Repeat with the other peg. The marker should now easily spring back when pushed against. You're ready to write!","title":"Dry Erase Holder"},{"location":"stretch-tutorials/stretch_tool_share/dry_erase_holder/#dry-erase-holder","text":"Created by : Hello Robot Inc This tool allows Stretch to hold a dry erase marker. It is spring loaded, allowing for compliant interaction between the marker and a white board. The tool can be integrated into your robot URDF by integrating its stretch_description as described in the Stretch ROS documentation .","title":"Dry Erase Holder"},{"location":"stretch-tutorials/stretch_tool_share/dry_erase_holder/#parts-list","text":"Item Qty Vendor Expo Dry Erase 1 Amazon M5x50mm Hex Head Bolt 1 McMaster-Carr M5 Nut 2 McMaster-Carr wrist_end_cap_5mm 1 PLA 3D Printer dry_erase_bushing_block 1 PLA 3D Printer Size 30 Rubber Band 2 McMaster-Carr 3/4\" Shaft Collar 1 McMaster-Carr","title":"Parts List"},{"location":"stretch-tutorials/stretch_tool_share/dry_erase_holder/#assembly-instructions","text":"View 3D assembly Install the bolt into the dry_erase_bushing block and secure from below with an M5 nut. Attach the dry erase bushing block to the tool plate, securing from below with the wrist_end_cap_5mm and an M5 nut. Orient the block so the marker points forward. Attach the shaft collar to your dry erase marker, approximately 8mm from the back of the marker. Slide the marker into the bushing block. Loop a rubber band around the back of the marker and over to one of the pegs on the side of the bushing block. Repeat with the other peg. The marker should now easily spring back when pushed against. You're ready to write!","title":"Assembly instructions"},{"location":"stretch-tutorials/stretch_tool_share/gripper_puller/","text":"Puller Created by : Hello Robot Inc This is a a simple puller attachment for the Stretch Compliant Gripper. We've used it to pull open many common drawers, cabinet doors, and even a mini-fridge door. You can also use it to push things closed. You can think of it as a circular hook used to pull things or a finger used to push things. It attaches to the 6-32 stud on the side of the gripper. By turning the gripper sideways during manipulation, the hook can drop over the drawer handle, allowing the arm to retract and pull the door open. Parts List Item Qty Vendor 6-32 x 0.5\" BHCS 1 McMaster-Carr 6-32 x 1\" aluminum threaded standoff 1 McMaster-Carr Puller_V1.STL 1 PLA 3D printer Assembly instructions View 3D assembly Screw the standoff on to the gripper's threaded post. Secure tightly and add a drop of light duty Loctite if desired. Attach the plastic pull to the standoff using the BHCS.","title":"Gripper Puller"},{"location":"stretch-tutorials/stretch_tool_share/gripper_puller/#puller","text":"Created by : Hello Robot Inc This is a a simple puller attachment for the Stretch Compliant Gripper. We've used it to pull open many common drawers, cabinet doors, and even a mini-fridge door. You can also use it to push things closed. You can think of it as a circular hook used to pull things or a finger used to push things. It attaches to the 6-32 stud on the side of the gripper. By turning the gripper sideways during manipulation, the hook can drop over the drawer handle, allowing the arm to retract and pull the door open.","title":"Puller"},{"location":"stretch-tutorials/stretch_tool_share/gripper_puller/#parts-list","text":"Item Qty Vendor 6-32 x 0.5\" BHCS 1 McMaster-Carr 6-32 x 1\" aluminum threaded standoff 1 McMaster-Carr Puller_V1.STL 1 PLA 3D printer","title":"Parts List"},{"location":"stretch-tutorials/stretch_tool_share/gripper_puller/#assembly-instructions","text":"View 3D assembly Screw the standoff on to the gripper's threaded post. Secure tightly and add a drop of light duty Loctite if desired. Attach the plastic pull to the standoff using the BHCS.","title":"Assembly instructions"},{"location":"stretch-tutorials/stretch_tool_share/gripper_removal/","text":"Gripper The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position. As shown, it includes mounting features on one side to allow for attachment of simple rigid tools such as hooks and pullers . Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 The attachment features are spaced at 9mm. The weight of the Stretch Compliant Gripper is 240g. Gripper Removal Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse. Unplug the Dynamixel cable from the back of the gripper. Remove the 4 screws holding the gripper to the bracket. Remove the gripper from the mounting bracket Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate.","title":"Gripper Removal"},{"location":"stretch-tutorials/stretch_tool_share/gripper_removal/#gripper","text":"The Stretch Compliant Gripper utilizes a Dynamixel XL430-W250-T servo to drive the spring grasper mechanism. The kinematics of the grasper mechanism are complex and non-linear relative to the motor position. As shown, it includes mounting features on one side to allow for attachment of simple rigid tools such as hooks and pullers . Item Notes A Stud attachment Threaded 6-32 B Thread attahcment Threaded M4 The attachment features are spaced at 9mm. The weight of the Stretch Compliant Gripper is 240g.","title":"Gripper"},{"location":"stretch-tutorials/stretch_tool_share/gripper_removal/#gripper-removal","text":"Here we describe removing the Stretch Compliant gripper. Installation is simply these steps in reverse. Unplug the Dynamixel cable from the back of the gripper. Remove the 4 screws holding the gripper to the bracket. Remove the gripper from the mounting bracket Unscrew the 8 screws holding the mounting bracket to the bottom of the tool plate.","title":"Gripper Removal"},{"location":"stretch-tutorials/stretch_tool_share/updating_urdf/","text":"Changing the Tool If you wish to remove the default gripper and add a different tool, you will typically edit /stretch_description/urdf/stretch_description.xacro. Specifically, you will replace the following line in order to include the xacro for the new tool and then follow directions within stretch_ros/stretch_calibration to generate a new calibrated urdf file (stretch.urdf) that includes the new tool. <xacro:include filename=\"stretch_gripper.xacro\" /> As an example we provide the xacro stretch_dry_erase_marker.xacro and its dependent mesh files with stretch_ros. Some of the tools found in the Stretch Body Tool Share include URDF data. To integrate these tools into the URDF for your Stretch >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_tool_share >>$ cd stretch_tool_share/<tool name> >>$ cp stretch_description/urdf/* ~/catkin_ws/src/stretch_ros/stretch_description/urdf/ >>$ cp stretch_description/meshes/* ~/catkin_ws/src/stretch_ros/stretch_description/meshes/ Next add the xacro for the particular tool to /stretch_description/urdf/stretch_description.xacro . Then you can generate and preview the uncalibrated URDF: >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp stretch.urdf stretch.urdf.bak >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh Now visualize the new tool >>$ roslaunch stretch_calibration simple_test_head_calibration.launch","title":"Updating URDF"},{"location":"stretch-tutorials/stretch_tool_share/updating_urdf/#changing-the-tool","text":"If you wish to remove the default gripper and add a different tool, you will typically edit /stretch_description/urdf/stretch_description.xacro. Specifically, you will replace the following line in order to include the xacro for the new tool and then follow directions within stretch_ros/stretch_calibration to generate a new calibrated urdf file (stretch.urdf) that includes the new tool. <xacro:include filename=\"stretch_gripper.xacro\" /> As an example we provide the xacro stretch_dry_erase_marker.xacro and its dependent mesh files with stretch_ros. Some of the tools found in the Stretch Body Tool Share include URDF data. To integrate these tools into the URDF for your Stretch >>$ cd ~/repos >>$ git clone https://github.com/hello-robot/stretch_tool_share >>$ cd stretch_tool_share/<tool name> >>$ cp stretch_description/urdf/* ~/catkin_ws/src/stretch_ros/stretch_description/urdf/ >>$ cp stretch_description/meshes/* ~/catkin_ws/src/stretch_ros/stretch_description/meshes/ Next add the xacro for the particular tool to /stretch_description/urdf/stretch_description.xacro . Then you can generate and preview the uncalibrated URDF: >>$ cd ~/catkin_ws/src/stretch_ros/stretch_description/urdf >>$ cp stretch.urdf stretch.urdf.bak >>$ rosrun stretch_calibration update_urdf_after_xacro_change.sh Now visualize the new tool >>$ roslaunch stretch_calibration simple_test_head_calibration.launch","title":"Changing the Tool"},{"location":"stretch-web-interface/","text":"Table of Contents Overview History The Robot and the Operator both use Web Browsers The Web Server Getting Started Installation Quick Start Slower Start Setting Up a Server Credentials for Robots and Operators Running the Server on the Robot's Onboard Computer Running the Server on Amazon Lightsail Licenses Overview This repository holds code that enables a person (the operator) to remotely teleoperate a Stretch RE1 (the robot) through a recent Chrome/Chromium web browser on an Android mobile phone, laptop, or desktop. The Stretch RE1 is a mobile manipulator from Hello Robot Inc. WARNING: This prototype code has been useful to the community, but is not well tested. There are also security issues, especially if you use the default credentials. Use this code at your own risk. History When we started Hello Robot Inc. back in 2017, part of our goal was to create a robot that could be intuitively teleoperated from afar. We took an iterative approach, building a series of 7 prototype robots before the Stretch RE1 that we sell today. In conjunction with these prototypes, we developed a series of web interfaces, so that we could control our robots via a web browser and test remote teleoperation. While we eventually deemphasized this aspect of the robot, we thought it could be useful to the community. With this goal in mind, we ported parts of our old web interface code to the Stretch RE1 and made them available in this repository back in June of 2020. Since then, we've been gratified to learn of others working with this code. For example, The Human Centered Robotics Lab at the University Washington has made impressive improvements to the code, which can be found in their repository . We've also learned that the The Human Factors and Aging Laboratory at the University of Illinois at Urbana-Champaign has explored this interface as part of their impressive research to improve the lives of older adults. The Robot and the Operator both use Web Browsers This web interface works via Web Real-Time Communication (WebRTC) . Code runs in a browser on the robot, in a browser on the operator's device (e.g., a mobile phone), and on a server. This is analogous to the robot and the operator video conferencing with one another, although they communicate via realtime data in addition to audio and video. By using web browsers, the robot and the operator make use of well-tested high-performance implementations of WebRTC. This symmetry also simplifies development, since a developer can use the same browser-based developer tools on both sides of the communication. The robot's browser and the operator's browser first login to the server, which helps connect them and provides them with the interface code. The robot\u2019s browser uses rosbridge to connect with ROS on the robot. Rosbridge translates JSON from the robot\u2019s browser into ROS communications and vice versa. The JavaScript code used by the robot\u2019s browser to connect with ROS can be found in ros_connect.js under the robot directory , which holds files made available to the robot's browser. With puppeteer the robot can automatically launch and login to its browser. For example, start_robot_browers.js uses puppeteer to launch the robot's browser and login. While the robot\u2019s browser has access to most of the robot via ROS, the operator\u2019s browser can only access the robot indirectly through the robot\u2019s browser. The robotic commands available to the operator\u2019s browser can be found in commands.js under the shared directory , which holds files available to both the operator's browser and the robot's browser. The operator's browser also has access to files in the operator directory . The Web Server In the example below, the server runs on the robot. In a production environment, you would use an external server, instead of the robot, to handle things like connecting robots and operators behind firewalls. In a later section , we provide an example of an external server that uses Amazon Lightsail . When used on a production server with proper certificates, this code uses HTTPS and avoids scary messages. The web server uses the Express web framework with Pug templates. The server provides a WebRTC signaling service using socket.io . It uses Redis to store sessions. passport provides authentication for the robot and the operator. mongoose and a MongoDB database store credentials for robots and operators. The stretch_web_interface repository comes with default MongoDB content found at ./mongodb/ for testing behind a firewall. These default contents come with multiple robot and operator accounts. Make sure not to use these default database contents on a deployed system! By default, send_recv_av.js uses a free STUN server provided by Google. The Amazon Lightsail example below uses coturn as a STUN and TURN server . Getting Started Installation These installation instructions describe how to install both the server and relevant ROS code on the onboard computer of a Stretch RE1 robot. This is suitable for use on a trusted and secure local area network (LAN) behind a strong firewall. The web interface depends on stretch_ros , which is used to control the robot. You should first make sure it is up-to-date and working properly on the robot. Clone the stretch_web_interface repository to ~/catkin_ws/src/ on the robot. cd ~/catkin_ws/src/ git clone https://github.com/hello-robot/stretch_web_interface.git Run catkin_make. cd ~/catkin_ws/ catkin_make rospack profile Run the installation script. cd ~/catkin_ws/src/stretch_web_interface/bash_scripts/ ./web_interface_installation.sh WARNING: The script uninstalls tornado using pip to avoid a rosbridge websocket immediate disconnection issue. This could break other software on your robot. Quick Start When running on a trusted and secure local area network (LAN) behind a strong firewall, you can use the following insecure method to more conveniently start the system. Calibrate the Robot First, make sure the robot is calibrated. For example you can run the following command. stretch_robot_home.py Start ROS Next, in a terminal, run the following command to start ROS. This will start ROS nodes on the robot for the D435i camera, the driver for Stretch RE1, and rosbridge. Rosbridge connects JavaScript running in the robot's browser to ROS using JSON. roslaunch stretch_web_interface web_interface.launch Start the Web Server and the Robot's Browser In another terminal, run the following command to start the web server on the robot, launch the robot's browser, and log the robot into the browser. The convenience script calls start_robot_browser.js, which uses puppeteer to log the robot into its browser. roscd stretch_web_interface/bash_scripts/ ./start_web_server_and_robot_browser.sh Typically, this script can be exited with Ctrl+C and then restarted without issue. WARNING: start_robot_browser.js contains the default robot credentials in plain text! This is only appropriate for simple testing on a local network behind a firewall. The username and password are public on the Internet, so this is not secure! Deployment would require new credentials and security measures. Start the Operator's Browser You will now login to a browser as the operator and connect to the robot. You can use a Chrome browser on a recent Android mobile phone or a recent Chrome/Chromium browser on a laptop or desktop. Open the browser goto the robot\u2019s IP address. You can use ifconfig on the robot to determine its IP address. Select \"Advanced\" and then click on \"Proceed to localhost (unsafe)\". Click on \"Login\" and use the following username and password. username: o1 password xXTgfdH8 WARNING: This is a default operator account provided for simple testing. Since this username and password are public on the Internet, this is not secure. You should only use this behind a firewall during development and testing. Deployment would require new credentials and security measures. You should now see a screen like the following. Click on \"no robot connected\" and select the robot \"r1\" to connect to it. You should now see video from the robot on your mobile phone or other device. Click in the designated regions to command the robot to move. You can also click on \"Drive\", \"Arm\" down, \"Arm\" up, \"Hand\" and \"Look\" to move different joints on the robot. Slower Start The following steps describe how to manually start the web server and the robot's browser on the robot, instead of using the convenience script described above. Calibrate the Robot First, make sure the robot is calibrated. For example you can run the following command. stretch_robot_home.py Start ROS Next, in a terminal, run the following command to start the ROS side of things. This will start ROS nodes on the robot for the D435i camera, the driver for Stretch RE1, and rosbridge. Rosbridge connects JavaScript running in the robot's browser to ROS using JSON. roslaunch stretch_web_interface web_interface.launch Start the Web Server In another terminal, run the following command to start the web server on the robot. roscd stretch_web_interface/bash_scripts/ ./start_desktop_dev_env.sh Start the Robot's Browser Open a Chromium browser on the robot and go to localhost. Select \"Advanced\" and then click on \"Proceed to localhost (unsafe)\". Click on \"Login\" and use the following username and password. username: r1 password NQUeUb98 WARNING: This is a default robot account provided for simple testing. Since this username and password are public on the Internet, this is not secure. You should only use this behind a firewall during development and testing. Deployment would require new credentials and security measures. You should now see video from the robot's camera in the browser window. Start the Operator's Browser Please see the instructions above. Setting Up a Server The server for the web interface typically runs on the robot's onboard computer or on a remote machine connected to the Internet. Credentials for Robots and Operators Credentials for robots and operators are stored on the server using MongoDB . Viewing and Editing Credentials On the server, you can view and edit the credentials using mongodb-compass , which is installed by default. First, use the following command in a terminal to start the application. mongodb-compass Next, use \"Connect to Host\" by typing localhost in the Hostname area at the top of the window and then clicking the green \"CONNECT\" button at the bottom right of the window. This should show you various databases. The node-auth database holds the web interface credentials. Clicking on node-auth will show a collection named users . Clicking on users will show the current credentials. If you've only used the default development credentials in this repository, you should see entries for the following: three robots with the usernames r1, r2, and r3; three operators with the usernames o1, o2, and o3; and an administrator with the username admin. Each entry consists of encrypted password information (i.e., salt and hash), a username, a name, a role, a date, and a Boolean indicating whether or not the user has been approved. Without approval, the user should be denied access. The role indicates whether the entry is for a robot or an operator. You can click on the image below to see what this should look like. Creating New Credentials First, start the server. Next, go to the web page and click register . Now enter a username and a password. This process creates a new user entry in MongoDB. You can now follow the instructions for viewing credentials above to view the new account you just created. In order for this account to function, you will need to edit the role to be operator or robot and edit approved to be true . You can do this by clicking on the elements with mongodb-compass . Prior to testing anything on the Internet, you should delete all of the default credentials. The default credentials are solely for development on a secure local network behind a firewall. Backing Up and Restoring Credentials On the server, you can backup credentials using a command like the following in a terminal. You should change ./ to match the directory into which you want to saved the backup directory. mongodump --db node-auth --out ./ You can restore backed up credentials using the following command in a terminal. You'll need to change ./ to match the directory that holds the backup directory. mongorestore -d node-auth ./node-auth/user.bson Running the Server on the Robot's Onboard Computer Running the server on the robot is useful when the robot and the operator are both on the same local area network (LAN). For example, a person with disabilities might operate the robot in their home, or you might be developing new teleoperation code. In these situations, the robot, the operator's browser, and the server should all be behind a strong firewall, reducing security concerns. Running the Server on Amazon Lightsail Running the server on a remote machine can be useful when the robot and the operator are on separate LANs connected by the Internet. This can enable a person to operate the robot from across the world. In this section, we'll provide an example of setting up the server to run on an Amazon Lightsail instance. This is not a hardened server and is only intended to serve as a helpful example. It likely has significant security shortcomings and is very much a prototype. Use at your own risk. One of the challenges for remote teleoperation is that browsers on different LANs can have difficulty connecting with one another. Peer-to-peer communication may not be achievable due to firewalls and other methods used to help secure networks. For example, home networks, university networks, and corporate networks can all have complex configurations that interfere with peer-to-peer communication. Running the server on a remote machine connected to the Internet helps the robot's browser and the operator's browser connect to one another using standard methods developed for WebRTC video conferencing over the Internet. The server performs a variety of roles, including the following: restricting access to authorized robots and operators; helping operators select from available robots; WebRTC signaling , Session Traversal Utilities for Network Address Translation (STUN) , and Traversal Using Relays around Network Address Translation (TURN) . Notably, when direct peer-to-peer connectivity fails, TURN relays video, audio, and data between the robot's browser and the operator's browser. Relaying data is robust to networking challenges, but can incur charges due to data usage. Amazon Lightsail Setup in Brief This section describes the steps we used to create an Amazon Lightsail instance that runs the server for the web interface. Obtain a domain name to use for your server. Create a new Amazon Lightsail instance. We used an OS only instance with Ubuntu 20.04, 512 MB RAM, 1 vCPU, 20 GB SSD. Create a static IP address and attach it to your instance. Connect your new instance to SSH , so that you can access it. A command like the following can then be used to login to your instance: ssh -i /path/to/private-key.pem username@public-ip-address . While logged into your instance. Run sudo apt-get update to avoid installation issues. Install the net-tools package, which will be used later. You might also want to install your preferred text editor, such as emacs . sudo apt install net-tools Configure Git. git config --global user.name \"FIRST_NAME LAST_NAME\" git config --global user.email \"MY_NAME@example.com\" Clone this GitHub repository. cd mkdir repos git clone https://github.com/hello-robot/stretch_web_interface.git Use certbot from Let's Encrypt to obtain certificates so that your server can use Hypertext Transfer Protocol Secure (HTTPS) . HTTPS is required to fully utilize WebRTC. You will need to first connect your domain name to the static IP address used by your instance. Follow certbot installation instructions for Ubuntu 20.04 . Run the teleoperation server installation script cd ~/repos/stretch_web_interface/bash_scripts/ ./web_server_installation.sh Initialize the database with secure credentials for at least one robot and one operator. For example, you can do the following. Create and export credentials from MongoDB by running a server on your robot. Use scp to copy the exported credentials database from your robot's computer to your Lightsail instance by running a command like scp -ri ./LightsailDefaultKey-us-east-2.pem ./mongodb_credentials ubuntu@public-ip-address:./ on your robot's computer. On your Lightsail instance, restore the credentials database with a command like mongorestore -d node-auth ./mongodb_credentials/node-auth/users.bson While logged into your instance. Edit the coturn configuration file. Find your instance's private IP address by running ifconfig -a and looking at the inet IP address. Confirm your instance's public IP address and your domain name by running ping YOUR-DOMAIN-NAME and looking at the IP address. Add the following lines at appropriate locations in /etc/turnserver.conf . listening-ip=PRIVATE-IP-ADDRESS relay-ip=PRIVATE-IP-ADDRESS external-ip=PUBLIC-IP-ADDRESS Verbose lt-cred-mech pkey=/etc/letsencrypt/live/YOUR-DOMAIN-NAME/privkey.pem cert=/etc/letsencrypt/live/YOUR-DOMAIN-NAME/cert.pem no-multicast-peers secure-stun mobility realm=YOUR-DOMAIN-NAME Create TURN server accounts and credentials. Create an administrator account. sudo turnadmin -A -u ADMIN-NAME -p ADMIN-PASSWORD Create a TURN user. In the next step, you will add these credentials to ./stretch_web_interface/shared/send_recv_av.js . sudo turnadmin -a -u TURN-USER-NAME -r YOUR-DOMAIN-NAME -p TURN-USER-PASSWORD Open ./stretch_web_interface/shared/send_recv_av.js in an editor. Comment out the free STUN server. Uncomment the STUN and TURN servers and fill in the values using your domain name and the credentials you just created (i.e., YOUR-DOMAIN-NAME, TURN-USER-NAME, and TURN-USER-PASSWORD). The relevant code will look similar to var pcConfig = { iceServers: [ {urls: \"stun:YOUR-DOMAIN-NAME\", username \"TURN-USER-NAME\", credentials: \"TURN-USER-PASSWORD}, {urls: \"turn:YOUR-DOMAIN-NAME\", username \"TURN-USER-NAME\", credentials: \"TURN-USER-PASSWORD}]}; Open the following ports for your Amazon Lightsail instance. These are standard ports for HTTPS, STUN, and TURN. HTTPS TCP 443 Custom TCP 3478 Custom TCP 5349 Custom UDP 3478 Custom UDP 5349 Reboot your instance. Login to your instance and run the following commands to start the server. cd ~/repos/stretch_web_interface/bash_scripts/ ./start_server_production_env.sh Your server should now be running and you can test it by taking the following steps. Turn on and calibrate your robot. Use your robot's Chromium browser to visit your server's domain and login with the robot's credentials that you created. Open a Chrome or Chromium browser of your own on another computer or recent Android phone. Visit your server's domain and login with the operator credentials you created. If you want to test communication between distinct networks, you could turn off your phone's Wi-Fi and then either use your phone or tether to your phone to connect from the mobile phone network to your robot. Please note that this has only been tested with the Chrome browser on recent Android phones. Once you've logged in as an operator, you should be able to select your robot from the drop down list and begin controlling it. After trying it out, be sure to shutdown your Amazon Lightsail instance. It is not hardened and likely has security vulnerabilities. It would be risky to leave it on over a significant length of time. Licenses This software is intended for use with S T R E T C H (TM) RESEARCH EDITION, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc. For license details for this repository, see the LICENSE files, including TUTORIAL_LICENSE.md, WEBRTC_PROJECT_LICENSE.md, and LICENSE.md. Some other sources and licenses are described by comments found within the code. The Apache 2.0 license applies to all code written by Hello Robot Inc. contained within this repository. We have attempted to note where code was derived from other sources and the governing licenses. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Overview"},{"location":"stretch-web-interface/#table-of-contents","text":"Overview History The Robot and the Operator both use Web Browsers The Web Server Getting Started Installation Quick Start Slower Start Setting Up a Server Credentials for Robots and Operators Running the Server on the Robot's Onboard Computer Running the Server on Amazon Lightsail Licenses","title":"Table of Contents"},{"location":"stretch-web-interface/#overview","text":"This repository holds code that enables a person (the operator) to remotely teleoperate a Stretch RE1 (the robot) through a recent Chrome/Chromium web browser on an Android mobile phone, laptop, or desktop. The Stretch RE1 is a mobile manipulator from Hello Robot Inc. WARNING: This prototype code has been useful to the community, but is not well tested. There are also security issues, especially if you use the default credentials. Use this code at your own risk.","title":"Overview"},{"location":"stretch-web-interface/#history","text":"When we started Hello Robot Inc. back in 2017, part of our goal was to create a robot that could be intuitively teleoperated from afar. We took an iterative approach, building a series of 7 prototype robots before the Stretch RE1 that we sell today. In conjunction with these prototypes, we developed a series of web interfaces, so that we could control our robots via a web browser and test remote teleoperation. While we eventually deemphasized this aspect of the robot, we thought it could be useful to the community. With this goal in mind, we ported parts of our old web interface code to the Stretch RE1 and made them available in this repository back in June of 2020. Since then, we've been gratified to learn of others working with this code. For example, The Human Centered Robotics Lab at the University Washington has made impressive improvements to the code, which can be found in their repository . We've also learned that the The Human Factors and Aging Laboratory at the University of Illinois at Urbana-Champaign has explored this interface as part of their impressive research to improve the lives of older adults.","title":"History"},{"location":"stretch-web-interface/#the-robot-and-the-operator-both-use-web-browsers","text":"This web interface works via Web Real-Time Communication (WebRTC) . Code runs in a browser on the robot, in a browser on the operator's device (e.g., a mobile phone), and on a server. This is analogous to the robot and the operator video conferencing with one another, although they communicate via realtime data in addition to audio and video. By using web browsers, the robot and the operator make use of well-tested high-performance implementations of WebRTC. This symmetry also simplifies development, since a developer can use the same browser-based developer tools on both sides of the communication. The robot's browser and the operator's browser first login to the server, which helps connect them and provides them with the interface code. The robot\u2019s browser uses rosbridge to connect with ROS on the robot. Rosbridge translates JSON from the robot\u2019s browser into ROS communications and vice versa. The JavaScript code used by the robot\u2019s browser to connect with ROS can be found in ros_connect.js under the robot directory , which holds files made available to the robot's browser. With puppeteer the robot can automatically launch and login to its browser. For example, start_robot_browers.js uses puppeteer to launch the robot's browser and login. While the robot\u2019s browser has access to most of the robot via ROS, the operator\u2019s browser can only access the robot indirectly through the robot\u2019s browser. The robotic commands available to the operator\u2019s browser can be found in commands.js under the shared directory , which holds files available to both the operator's browser and the robot's browser. The operator's browser also has access to files in the operator directory .","title":"The Robot and the Operator both use Web Browsers"},{"location":"stretch-web-interface/#the-web-server","text":"In the example below, the server runs on the robot. In a production environment, you would use an external server, instead of the robot, to handle things like connecting robots and operators behind firewalls. In a later section , we provide an example of an external server that uses Amazon Lightsail . When used on a production server with proper certificates, this code uses HTTPS and avoids scary messages. The web server uses the Express web framework with Pug templates. The server provides a WebRTC signaling service using socket.io . It uses Redis to store sessions. passport provides authentication for the robot and the operator. mongoose and a MongoDB database store credentials for robots and operators. The stretch_web_interface repository comes with default MongoDB content found at ./mongodb/ for testing behind a firewall. These default contents come with multiple robot and operator accounts. Make sure not to use these default database contents on a deployed system! By default, send_recv_av.js uses a free STUN server provided by Google. The Amazon Lightsail example below uses coturn as a STUN and TURN server .","title":"The Web Server"},{"location":"stretch-web-interface/#getting-started","text":"","title":"Getting Started"},{"location":"stretch-web-interface/#installation","text":"These installation instructions describe how to install both the server and relevant ROS code on the onboard computer of a Stretch RE1 robot. This is suitable for use on a trusted and secure local area network (LAN) behind a strong firewall. The web interface depends on stretch_ros , which is used to control the robot. You should first make sure it is up-to-date and working properly on the robot. Clone the stretch_web_interface repository to ~/catkin_ws/src/ on the robot. cd ~/catkin_ws/src/ git clone https://github.com/hello-robot/stretch_web_interface.git Run catkin_make. cd ~/catkin_ws/ catkin_make rospack profile Run the installation script. cd ~/catkin_ws/src/stretch_web_interface/bash_scripts/ ./web_interface_installation.sh WARNING: The script uninstalls tornado using pip to avoid a rosbridge websocket immediate disconnection issue. This could break other software on your robot.","title":"Installation"},{"location":"stretch-web-interface/#quick-start","text":"When running on a trusted and secure local area network (LAN) behind a strong firewall, you can use the following insecure method to more conveniently start the system.","title":"Quick Start"},{"location":"stretch-web-interface/#calibrate-the-robot","text":"First, make sure the robot is calibrated. For example you can run the following command. stretch_robot_home.py","title":"Calibrate the Robot"},{"location":"stretch-web-interface/#start-ros","text":"Next, in a terminal, run the following command to start ROS. This will start ROS nodes on the robot for the D435i camera, the driver for Stretch RE1, and rosbridge. Rosbridge connects JavaScript running in the robot's browser to ROS using JSON. roslaunch stretch_web_interface web_interface.launch","title":"Start ROS"},{"location":"stretch-web-interface/#start-the-web-server-and-the-robots-browser","text":"In another terminal, run the following command to start the web server on the robot, launch the robot's browser, and log the robot into the browser. The convenience script calls start_robot_browser.js, which uses puppeteer to log the robot into its browser. roscd stretch_web_interface/bash_scripts/ ./start_web_server_and_robot_browser.sh Typically, this script can be exited with Ctrl+C and then restarted without issue. WARNING: start_robot_browser.js contains the default robot credentials in plain text! This is only appropriate for simple testing on a local network behind a firewall. The username and password are public on the Internet, so this is not secure! Deployment would require new credentials and security measures.","title":"Start the Web Server and the Robot's Browser"},{"location":"stretch-web-interface/#start-the-operators-browser","text":"You will now login to a browser as the operator and connect to the robot. You can use a Chrome browser on a recent Android mobile phone or a recent Chrome/Chromium browser on a laptop or desktop. Open the browser goto the robot\u2019s IP address. You can use ifconfig on the robot to determine its IP address. Select \"Advanced\" and then click on \"Proceed to localhost (unsafe)\". Click on \"Login\" and use the following username and password. username: o1 password xXTgfdH8 WARNING: This is a default operator account provided for simple testing. Since this username and password are public on the Internet, this is not secure. You should only use this behind a firewall during development and testing. Deployment would require new credentials and security measures. You should now see a screen like the following. Click on \"no robot connected\" and select the robot \"r1\" to connect to it. You should now see video from the robot on your mobile phone or other device. Click in the designated regions to command the robot to move. You can also click on \"Drive\", \"Arm\" down, \"Arm\" up, \"Hand\" and \"Look\" to move different joints on the robot.","title":"Start the Operator's Browser"},{"location":"stretch-web-interface/#slower-start","text":"The following steps describe how to manually start the web server and the robot's browser on the robot, instead of using the convenience script described above.","title":"Slower Start"},{"location":"stretch-web-interface/#calibrate-the-robot_1","text":"First, make sure the robot is calibrated. For example you can run the following command. stretch_robot_home.py","title":"Calibrate the Robot"},{"location":"stretch-web-interface/#start-ros_1","text":"Next, in a terminal, run the following command to start the ROS side of things. This will start ROS nodes on the robot for the D435i camera, the driver for Stretch RE1, and rosbridge. Rosbridge connects JavaScript running in the robot's browser to ROS using JSON. roslaunch stretch_web_interface web_interface.launch","title":"Start ROS"},{"location":"stretch-web-interface/#start-the-web-server","text":"In another terminal, run the following command to start the web server on the robot. roscd stretch_web_interface/bash_scripts/ ./start_desktop_dev_env.sh","title":"Start the Web Server"},{"location":"stretch-web-interface/#start-the-robots-browser","text":"Open a Chromium browser on the robot and go to localhost. Select \"Advanced\" and then click on \"Proceed to localhost (unsafe)\". Click on \"Login\" and use the following username and password. username: r1 password NQUeUb98 WARNING: This is a default robot account provided for simple testing. Since this username and password are public on the Internet, this is not secure. You should only use this behind a firewall during development and testing. Deployment would require new credentials and security measures. You should now see video from the robot's camera in the browser window.","title":"Start the Robot's Browser"},{"location":"stretch-web-interface/#start-the-operators-browser_1","text":"Please see the instructions above.","title":"Start the Operator's Browser"},{"location":"stretch-web-interface/#setting-up-a-server","text":"The server for the web interface typically runs on the robot's onboard computer or on a remote machine connected to the Internet.","title":"Setting Up a Server"},{"location":"stretch-web-interface/#credentials-for-robots-and-operators","text":"Credentials for robots and operators are stored on the server using MongoDB .","title":"Credentials for Robots and Operators"},{"location":"stretch-web-interface/#viewing-and-editing-credentials","text":"On the server, you can view and edit the credentials using mongodb-compass , which is installed by default. First, use the following command in a terminal to start the application. mongodb-compass Next, use \"Connect to Host\" by typing localhost in the Hostname area at the top of the window and then clicking the green \"CONNECT\" button at the bottom right of the window. This should show you various databases. The node-auth database holds the web interface credentials. Clicking on node-auth will show a collection named users . Clicking on users will show the current credentials. If you've only used the default development credentials in this repository, you should see entries for the following: three robots with the usernames r1, r2, and r3; three operators with the usernames o1, o2, and o3; and an administrator with the username admin. Each entry consists of encrypted password information (i.e., salt and hash), a username, a name, a role, a date, and a Boolean indicating whether or not the user has been approved. Without approval, the user should be denied access. The role indicates whether the entry is for a robot or an operator. You can click on the image below to see what this should look like.","title":"Viewing and Editing Credentials"},{"location":"stretch-web-interface/#creating-new-credentials","text":"First, start the server. Next, go to the web page and click register . Now enter a username and a password. This process creates a new user entry in MongoDB. You can now follow the instructions for viewing credentials above to view the new account you just created. In order for this account to function, you will need to edit the role to be operator or robot and edit approved to be true . You can do this by clicking on the elements with mongodb-compass . Prior to testing anything on the Internet, you should delete all of the default credentials. The default credentials are solely for development on a secure local network behind a firewall.","title":"Creating New Credentials"},{"location":"stretch-web-interface/#backing-up-and-restoring-credentials","text":"On the server, you can backup credentials using a command like the following in a terminal. You should change ./ to match the directory into which you want to saved the backup directory. mongodump --db node-auth --out ./ You can restore backed up credentials using the following command in a terminal. You'll need to change ./ to match the directory that holds the backup directory. mongorestore -d node-auth ./node-auth/user.bson","title":"Backing Up and Restoring Credentials"},{"location":"stretch-web-interface/#running-the-server-on-the-robots-onboard-computer","text":"Running the server on the robot is useful when the robot and the operator are both on the same local area network (LAN). For example, a person with disabilities might operate the robot in their home, or you might be developing new teleoperation code. In these situations, the robot, the operator's browser, and the server should all be behind a strong firewall, reducing security concerns.","title":"Running the Server on the Robot's Onboard Computer"},{"location":"stretch-web-interface/#running-the-server-on-amazon-lightsail","text":"Running the server on a remote machine can be useful when the robot and the operator are on separate LANs connected by the Internet. This can enable a person to operate the robot from across the world. In this section, we'll provide an example of setting up the server to run on an Amazon Lightsail instance. This is not a hardened server and is only intended to serve as a helpful example. It likely has significant security shortcomings and is very much a prototype. Use at your own risk. One of the challenges for remote teleoperation is that browsers on different LANs can have difficulty connecting with one another. Peer-to-peer communication may not be achievable due to firewalls and other methods used to help secure networks. For example, home networks, university networks, and corporate networks can all have complex configurations that interfere with peer-to-peer communication. Running the server on a remote machine connected to the Internet helps the robot's browser and the operator's browser connect to one another using standard methods developed for WebRTC video conferencing over the Internet. The server performs a variety of roles, including the following: restricting access to authorized robots and operators; helping operators select from available robots; WebRTC signaling , Session Traversal Utilities for Network Address Translation (STUN) , and Traversal Using Relays around Network Address Translation (TURN) . Notably, when direct peer-to-peer connectivity fails, TURN relays video, audio, and data between the robot's browser and the operator's browser. Relaying data is robust to networking challenges, but can incur charges due to data usage.","title":"Running the Server on Amazon Lightsail"},{"location":"stretch-web-interface/#amazon-lightsail-setup-in-brief","text":"This section describes the steps we used to create an Amazon Lightsail instance that runs the server for the web interface. Obtain a domain name to use for your server. Create a new Amazon Lightsail instance. We used an OS only instance with Ubuntu 20.04, 512 MB RAM, 1 vCPU, 20 GB SSD. Create a static IP address and attach it to your instance. Connect your new instance to SSH , so that you can access it. A command like the following can then be used to login to your instance: ssh -i /path/to/private-key.pem username@public-ip-address . While logged into your instance. Run sudo apt-get update to avoid installation issues. Install the net-tools package, which will be used later. You might also want to install your preferred text editor, such as emacs . sudo apt install net-tools Configure Git. git config --global user.name \"FIRST_NAME LAST_NAME\" git config --global user.email \"MY_NAME@example.com\" Clone this GitHub repository. cd mkdir repos git clone https://github.com/hello-robot/stretch_web_interface.git Use certbot from Let's Encrypt to obtain certificates so that your server can use Hypertext Transfer Protocol Secure (HTTPS) . HTTPS is required to fully utilize WebRTC. You will need to first connect your domain name to the static IP address used by your instance. Follow certbot installation instructions for Ubuntu 20.04 . Run the teleoperation server installation script cd ~/repos/stretch_web_interface/bash_scripts/ ./web_server_installation.sh Initialize the database with secure credentials for at least one robot and one operator. For example, you can do the following. Create and export credentials from MongoDB by running a server on your robot. Use scp to copy the exported credentials database from your robot's computer to your Lightsail instance by running a command like scp -ri ./LightsailDefaultKey-us-east-2.pem ./mongodb_credentials ubuntu@public-ip-address:./ on your robot's computer. On your Lightsail instance, restore the credentials database with a command like mongorestore -d node-auth ./mongodb_credentials/node-auth/users.bson While logged into your instance. Edit the coturn configuration file. Find your instance's private IP address by running ifconfig -a and looking at the inet IP address. Confirm your instance's public IP address and your domain name by running ping YOUR-DOMAIN-NAME and looking at the IP address. Add the following lines at appropriate locations in /etc/turnserver.conf . listening-ip=PRIVATE-IP-ADDRESS relay-ip=PRIVATE-IP-ADDRESS external-ip=PUBLIC-IP-ADDRESS Verbose lt-cred-mech pkey=/etc/letsencrypt/live/YOUR-DOMAIN-NAME/privkey.pem cert=/etc/letsencrypt/live/YOUR-DOMAIN-NAME/cert.pem no-multicast-peers secure-stun mobility realm=YOUR-DOMAIN-NAME Create TURN server accounts and credentials. Create an administrator account. sudo turnadmin -A -u ADMIN-NAME -p ADMIN-PASSWORD Create a TURN user. In the next step, you will add these credentials to ./stretch_web_interface/shared/send_recv_av.js . sudo turnadmin -a -u TURN-USER-NAME -r YOUR-DOMAIN-NAME -p TURN-USER-PASSWORD Open ./stretch_web_interface/shared/send_recv_av.js in an editor. Comment out the free STUN server. Uncomment the STUN and TURN servers and fill in the values using your domain name and the credentials you just created (i.e., YOUR-DOMAIN-NAME, TURN-USER-NAME, and TURN-USER-PASSWORD). The relevant code will look similar to var pcConfig = { iceServers: [ {urls: \"stun:YOUR-DOMAIN-NAME\", username \"TURN-USER-NAME\", credentials: \"TURN-USER-PASSWORD}, {urls: \"turn:YOUR-DOMAIN-NAME\", username \"TURN-USER-NAME\", credentials: \"TURN-USER-PASSWORD}]}; Open the following ports for your Amazon Lightsail instance. These are standard ports for HTTPS, STUN, and TURN. HTTPS TCP 443 Custom TCP 3478 Custom TCP 5349 Custom UDP 3478 Custom UDP 5349 Reboot your instance. Login to your instance and run the following commands to start the server. cd ~/repos/stretch_web_interface/bash_scripts/ ./start_server_production_env.sh Your server should now be running and you can test it by taking the following steps. Turn on and calibrate your robot. Use your robot's Chromium browser to visit your server's domain and login with the robot's credentials that you created. Open a Chrome or Chromium browser of your own on another computer or recent Android phone. Visit your server's domain and login with the operator credentials you created. If you want to test communication between distinct networks, you could turn off your phone's Wi-Fi and then either use your phone or tether to your phone to connect from the mobile phone network to your robot. Please note that this has only been tested with the Chrome browser on recent Android phones. Once you've logged in as an operator, you should be able to select your robot from the drop down list and begin controlling it. After trying it out, be sure to shutdown your Amazon Lightsail instance. It is not hardened and likely has security vulnerabilities. It would be risky to leave it on over a significant length of time.","title":"Amazon Lightsail Setup in Brief"},{"location":"stretch-web-interface/#licenses","text":"This software is intended for use with S T R E T C H (TM) RESEARCH EDITION, which is a robot produced and sold by Hello Robot Inc. For further information, including inquiries about dual licensing, please contact Hello Robot Inc. For license details for this repository, see the LICENSE files, including TUTORIAL_LICENSE.md, WEBRTC_PROJECT_LICENSE.md, and LICENSE.md. Some other sources and licenses are described by comments found within the code. The Apache 2.0 license applies to all code written by Hello Robot Inc. contained within this repository. We have attempted to note where code was derived from other sources and the governing licenses. All materials are Copyright 2022 by Hello Robot Inc. Hello Robot and Stretch are registered trademarks. The Stretch RE1 and RE2 robots are covered by U.S. Patent 11,230,000 and other patents pending.","title":"Licenses"},{"location":"stretch-web-interface/LICENSE/","text":"The following license applies to the contents of this directory created by Hello Robot Inc. (the \"Contents\"), but does not cover materials from other sources. This software is intended for use with the Stretch RE1 mobile manipulator, which is a robot produced and sold by Hello Robot Inc. Copyright 2020 Hello Robot Inc. The Contents are licensed under the Apache License, Version 2.0 (the \"License\"). You may not use the Contents except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, the Contents are distributed under the License are distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further information about the Contents including inquiries about dual licensing, please contact Hello Robot Inc.","title":"LICENSE"},{"location":"stretch-web-interface/TUTORIAL_LICENSE/","text":"Some of the code within this repository is derived from tutorial code found via the following links. This code relates to using mongoose, passport and express. The original code was released under the MIT License described below. https://github.com/didinj/node-express-passport-mongoose-auth https://www.djamware.com/post/58bd823080aca7585c808ebf/nodejs-expressjs-mongoosejs-and-passportjs-authentication ================ MIT License Copyright (c) 2017 Didin Jamaludin Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"TUTORIAL LICENSE"},{"location":"stretch-web-interface/WEBRTC_PROJECT_LICENSE/","text":"The following license covers the original code from which some of the web interface code was derived (e.g., operator_acquire_av.js, robot_acquire_av.js). The original code was released in the following respository, which contains WebRTC example code. https://github.com/webrtc/samples ====================================== Copyright (c) 2014, The WebRTC project authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Google nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"WEBRTC PROJECT LICENSE"}]}